{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions import multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor([0, 1, 2, 3]) 4 torch.Size([4]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# 标量\n",
    "x = torch.tensor(1.0)\n",
    "\n",
    "# 向量\n",
    "y = torch.arange(4)\n",
    "\n",
    "# 长度、维度、形状\n",
    "y_len = len(y)\n",
    "y_dim = y.shape\n",
    "y_size = y.size()\n",
    "\n",
    "print(x, y, y_len, y_dim, y_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[[ 0,  2,  4,  6],\n",
      "         [ 8, 10, 12, 14],\n",
      "         [16, 18, 20, 22]],\n",
      "\n",
      "        [[24, 26, 28, 30],\n",
      "         [32, 34, 36, 38],\n",
      "         [40, 42, 44, 46]]])\n",
      "tensor([[[  0,   1,   4,   9],\n",
      "         [ 16,  25,  36,  49],\n",
      "         [ 64,  81, 100, 121]],\n",
      "\n",
      "        [[144, 169, 196, 225],\n",
      "         [256, 289, 324, 361],\n",
      "         [400, 441, 484, 529]]])\n"
     ]
    }
   ],
   "source": [
    "# 张量\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "print(X)\n",
    "\n",
    "Y = X.clone()\n",
    "print(Y + X)\n",
    "print(Y * X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[12, 14, 16, 18],\n",
      "         [20, 22, 24, 26],\n",
      "         [28, 30, 32, 34]]])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 14, 16, 18],\n",
      "         [20, 22, 24, 26],\n",
      "         [28, 30, 32, 34]]])\n"
     ]
    }
   ],
   "source": [
    "X.sum(axis=0) # 按行求和\n",
    "X.numel() # 元素个数\n",
    "print(X.sum(axis=0, keepdims=True)) # 保持维度\n",
    "print(X.cumsum(axis=0)) # 每一行等于前面行的累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.)\n",
      "tensor(14.)\n",
      "tensor([14.])\n",
      "tensor([[14.]])\n",
      "tensor(2.)\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4, dtype=torch.float32)\n",
    "y = torch.tensor([2.0, 3, 4, 5])\n",
    "# 点积\n",
    "print(torch.dot(x, y))\n",
    "print(torch.sum(x * y))\n",
    "# 矩阵向量积\n",
    "print(torch.mv(x.reshape(1, -1), y))\n",
    "# 矩阵乘法\n",
    "print(torch.mm(x.reshape(1, -1), y.reshape(-1, 1)))\n",
    "# 矩阵按元素相乘是哈达玛积\n",
    "# L2范数或矩阵的Frobenius范数\n",
    "print(torch.norm(x))\n",
    "# L1范数\n",
    "print(torch.abs(x).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "%matplotlib inline\n",
    "from matplotlib_inline import backend_inline\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def use_svg_display():  #@save\n",
    "    # 用矢量图显示\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):    #@save\n",
    "    use_svg_display()\n",
    "    # 设置图的尺寸\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    # 设置坐标轴\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n",
    "\n",
    "#@save\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    # 绘图\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else d2l.plt.gca()\n",
    "    # 如果 `X` 有一个轴，输出 `True`\n",
    "    def has_one_axis(X):\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list) and not hasattr(X[0], \"__len__\"))\n",
    "    if has_one_axis(X):\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"243.529359pt\" height=\"183.35625pt\" viewBox=\"0 0 243.529359 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-10-31T09:11:05.999202</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 243.529359 183.35625 \n",
       "L 243.529359 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 235.903125 145.8 \n",
       "L 235.903125 7.2 \n",
       "L 40.603125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 49.480398 145.8 \n",
       "L 49.480398 7.2 \n",
       "\" clip-path=\"url(#p6a14edeec6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"mfea4c5d157\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfea4c5d157\" x=\"49.480398\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(46.299148 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 110.702968 145.8 \n",
       "L 110.702968 7.2 \n",
       "\" clip-path=\"url(#p6a14edeec6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfea4c5d157\" x=\"110.702968\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(107.521718 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 171.925539 145.8 \n",
       "L 171.925539 7.2 \n",
       "\" clip-path=\"url(#p6a14edeec6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfea4c5d157\" x=\"171.925539\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(168.744289 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 233.148109 145.8 \n",
       "L 233.148109 7.2 \n",
       "\" clip-path=\"url(#p6a14edeec6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfea4c5d157\" x=\"233.148109\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(229.966859 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_5\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(135.29375 174.076563)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 40.603125 116.769994 \n",
       "L 235.903125 116.769994 \n",
       "\" clip-path=\"url(#p6a14edeec6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <defs>\n",
       "       <path id=\"mb2213a7611\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb2213a7611\" x=\"40.603125\" y=\"116.769994\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(27.240625 120.569213)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 40.603125 78.886651 \n",
       "L 235.903125 78.886651 \n",
       "\" clip-path=\"url(#p6a14edeec6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb2213a7611\" x=\"40.603125\" y=\"78.886651\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(27.240625 82.685869)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 40.603125 41.003307 \n",
       "L 235.903125 41.003307 \n",
       "\" clip-path=\"url(#p6a14edeec6)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mb2213a7611\" x=\"40.603125\" y=\"41.003307\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(20.878125 44.802526)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_9\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(14.798437 85.121094)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 49.480398 116.769994 \n",
       "L 55.602655 119.573361 \n",
       "L 61.724912 121.922129 \n",
       "L 67.847169 123.816296 \n",
       "L 73.969426 125.255863 \n",
       "L 80.091683 126.24083 \n",
       "L 86.21394 126.771197 \n",
       "L 92.336197 126.846963 \n",
       "L 98.458454 126.46813 \n",
       "L 104.580711 125.634696 \n",
       "L 110.702968 124.346663 \n",
       "L 116.825225 122.604029 \n",
       "L 122.947482 120.406795 \n",
       "L 129.069739 117.754961 \n",
       "L 135.191996 114.648527 \n",
       "L 141.314254 111.087492 \n",
       "L 147.436511 107.071858 \n",
       "L 153.558768 102.601624 \n",
       "L 159.681025 97.676789 \n",
       "L 165.803282 92.297354 \n",
       "L 171.925539 86.463319 \n",
       "L 178.047796 80.174684 \n",
       "L 184.170053 73.431449 \n",
       "L 190.29231 66.233614 \n",
       "L 196.414567 58.581179 \n",
       "L 202.536824 50.474143 \n",
       "L 208.659081 41.912508 \n",
       "L 214.781338 32.896272 \n",
       "L 220.903595 23.425436 \n",
       "L 227.025852 13.5 \n",
       "\" clip-path=\"url(#p6a14edeec6)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 49.480398 139.5 \n",
       "L 55.602655 137.984666 \n",
       "L 61.724912 136.469333 \n",
       "L 67.847169 134.953999 \n",
       "L 73.969426 133.438665 \n",
       "L 80.091683 131.923331 \n",
       "L 86.21394 130.407998 \n",
       "L 92.336197 128.892664 \n",
       "L 98.458454 127.37733 \n",
       "L 104.580711 125.861996 \n",
       "L 110.702968 124.346663 \n",
       "L 116.825225 122.831329 \n",
       "L 122.947482 121.315995 \n",
       "L 129.069739 119.800661 \n",
       "L 135.191996 118.285328 \n",
       "L 141.314254 116.769994 \n",
       "L 147.436511 115.25466 \n",
       "L 153.558768 113.739327 \n",
       "L 159.681025 112.223993 \n",
       "L 165.803282 110.708659 \n",
       "L 171.925539 109.193325 \n",
       "L 178.047796 107.677992 \n",
       "L 184.170053 106.162658 \n",
       "L 190.29231 104.647324 \n",
       "L 196.414567 103.13199 \n",
       "L 202.536824 101.616657 \n",
       "L 208.659081 100.101323 \n",
       "L 214.781338 98.585989 \n",
       "L 220.903595 97.070655 \n",
       "L 227.025852 95.555322 \n",
       "\" clip-path=\"url(#p6a14edeec6)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 40.603125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 235.903125 145.8 \n",
       "L 235.903125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 40.603125 145.8 \n",
       "L 235.903125 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 40.603125 7.2 \n",
       "L 235.903125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 47.603125 44.55625 \n",
       "L 172.153125 44.55625 \n",
       "Q 174.153125 44.55625 174.153125 42.55625 \n",
       "L 174.153125 14.2 \n",
       "Q 174.153125 12.2 172.153125 12.2 \n",
       "L 47.603125 12.2 \n",
       "Q 45.603125 12.2 45.603125 14.2 \n",
       "L 45.603125 42.55625 \n",
       "Q 45.603125 44.55625 47.603125 44.55625 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_17\">\n",
       "     <path d=\"M 49.603125 20.298438 \n",
       "L 59.603125 20.298438 \n",
       "L 69.603125 20.298438 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- f(x) -->\n",
       "     <g transform=\"translate(77.603125 23.798438)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-66\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"35.205078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"74.21875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"133.398438\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_18\">\n",
       "     <path d=\"M 49.603125 34.976562 \n",
       "L 59.603125 34.976562 \n",
       "L 69.603125 34.976562 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_11\">\n",
       "     <!-- Tangent line (x=1) -->\n",
       "     <g transform=\"translate(77.603125 38.476562)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \n",
       "L 3928 4666 \n",
       "L 3928 4134 \n",
       "L 2272 4134 \n",
       "L 2272 0 \n",
       "L 1638 0 \n",
       "L 1638 4134 \n",
       "L -19 4134 \n",
       "L -19 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \n",
       "L 4684 2906 \n",
       "L 4684 2381 \n",
       "L 678 2381 \n",
       "L 678 2906 \n",
       "z\n",
       "M 678 1631 \n",
       "L 4684 1631 \n",
       "L 4684 1100 \n",
       "L 678 1100 \n",
       "L 678 1631 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-54\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"44.583984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"105.863281\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"169.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"232.71875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"294.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"357.621094\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"396.830078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"428.617188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"456.400391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"484.183594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"547.5625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"609.085938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"640.873047\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"679.886719\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3d\" x=\"739.066406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-31\" x=\"822.855469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"886.478516\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p6a14edeec6\">\n",
       "   <rect x=\"40.603125\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x\n",
    "x = np.arange(0, 3, 0.1)\n",
    "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.], requires_grad=True)\n",
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "tensor([ 0.,  4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(4.0, requires_grad=True)\n",
    "print(x)\n",
    "y = 2 * torch.dot(x, x)\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# y是标量，求梯度直接调用backward\n",
    "y = x.sum()\n",
    "x.grad.zero_()\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
      "tensor([0., 2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# y是向量，求梯度得到的是一个矩阵，相当于是累计的y对x中的每一个梯度\n",
    "y = x * x\n",
    "x.grad.zero_()\n",
    "print(y)\n",
    "y.sum().backward()\n",
    "# y.backward(torch.ones(len(x)))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>)\n",
      "tensor([0., 1., 4., 9.])\n"
     ]
    }
   ],
   "source": [
    "# 分离计算\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "print(y)\n",
    "u = y.detach() # 如果直接u = y，那么最后计算的梯度是 3 * x^2，而不是x\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 函数中有控制流，依然可以计算梯度\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else: \n",
    "        c = 100 * b\n",
    "    return c\n",
    "a = torch.tensor([1,2,3,4], dtype=torch.float32, requires_grad=True)\n",
    "d = f(a)\n",
    "d.sum().backward()\n",
    "\n",
    "print(a.grad == (d / a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"240.982812pt\" height=\"183.35625pt\" viewBox=\"0 0 240.982812 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-10-31T09:13:51.427525</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 183.35625 \n",
       "L 240.982813 183.35625 \n",
       "L 240.982813 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 38.482813 145.8 \n",
       "L 233.782813 145.8 \n",
       "L 233.782813 7.2 \n",
       "L 38.482813 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 65.293969 145.8 \n",
       "L 65.293969 7.2 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m8624097eaa\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8624097eaa\" x=\"65.293969\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −4 -->\n",
       "      <g transform=\"translate(57.922876 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 101.161738 145.8 \n",
       "L 101.161738 7.2 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8624097eaa\" x=\"101.161738\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(93.790644 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 137.029506 145.8 \n",
       "L 137.029506 7.2 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8624097eaa\" x=\"137.029506\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(133.848256 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 172.897274 145.8 \n",
       "L 172.897274 7.2 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8624097eaa\" x=\"172.897274\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(169.716024 160.398438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 208.765042 145.8 \n",
       "L 208.765042 7.2 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m8624097eaa\" x=\"208.765042\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(205.583792 160.398438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- x -->\n",
       "     <g transform=\"translate(133.173438 174.076563)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-78\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 38.482813 139.504837 \n",
       "L 233.782813 139.504837 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"md3a8eafdee\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#md3a8eafdee\" x=\"38.482813\" y=\"139.504837\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −1.0 -->\n",
       "      <g transform=\"translate(7.2 143.304055)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 38.482813 108.003628 \n",
       "L 233.782813 108.003628 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md3a8eafdee\" x=\"38.482813\" y=\"108.003628\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- −0.5 -->\n",
       "      <g transform=\"translate(7.2 111.802846)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"83.789062\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"147.412109\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"179.199219\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 38.482813 76.502418 \n",
       "L 233.782813 76.502418 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md3a8eafdee\" x=\"38.482813\" y=\"76.502418\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(15.579688 80.301637)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 38.482813 45.001209 \n",
       "L 233.782813 45.001209 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md3a8eafdee\" x=\"38.482813\" y=\"45.001209\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(15.579688 48.800428)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 38.482813 13.5 \n",
       "L 233.782813 13.5 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#md3a8eafdee\" x=\"38.482813\" y=\"13.5\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(15.579688 17.299219)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_21\">\n",
       "    <path d=\"M 47.360085 16.087869 \n",
       "L 49.153472 14.60553 \n",
       "L 50.946859 13.741642 \n",
       "L 52.740254 13.504837 \n",
       "L 54.533641 13.897484 \n",
       "L 56.327027 14.915657 \n",
       "L 58.120414 16.549185 \n",
       "L 59.913801 18.781744 \n",
       "L 61.707196 21.591044 \n",
       "L 63.500583 24.948985 \n",
       "L 65.293974 28.822042 \n",
       "L 67.08736 33.171492 \n",
       "L 68.880751 37.953907 \n",
       "L 70.674138 43.121469 \n",
       "L 72.467525 48.622566 \n",
       "L 74.260916 54.40224 \n",
       "L 76.054298 60.402704 \n",
       "L 77.847685 66.564046 \n",
       "L 79.641076 72.824703 \n",
       "L 81.434462 79.122092 \n",
       "L 83.227853 85.39332 \n",
       "L 85.02124 91.575699 \n",
       "L 86.814627 97.60747 \n",
       "L 88.608018 103.428382 \n",
       "L 90.401409 108.980256 \n",
       "L 92.194796 114.207609 \n",
       "L 93.988187 119.058237 \n",
       "L 95.781573 123.483652 \n",
       "L 97.574964 127.439655 \n",
       "L 99.368351 130.8867 \n",
       "L 101.16174 133.790358 \n",
       "L 102.955129 136.121616 \n",
       "L 104.748515 137.857174 \n",
       "L 106.541904 138.979701 \n",
       "L 108.335293 139.477972 \n",
       "L 110.12868 139.347015 \n",
       "L 111.922069 138.588135 \n",
       "L 113.715457 137.208915 \n",
       "L 115.508846 135.223135 \n",
       "L 117.302235 132.650636 \n",
       "L 119.095622 129.517124 \n",
       "L 120.889011 125.853905 \n",
       "L 122.682398 121.697587 \n",
       "L 124.475787 117.08969 \n",
       "L 126.269175 112.076261 \n",
       "L 128.062564 106.707387 \n",
       "L 129.855952 101.036717 \n",
       "L 131.64934 95.120907 \n",
       "L 133.442729 89.019067 \n",
       "L 135.236117 82.792165 \n",
       "L 137.029506 76.502419 \n",
       "L 138.822894 70.212672 \n",
       "L 140.616283 63.98577 \n",
       "L 142.409671 57.88393 \n",
       "L 144.20306 51.96812 \n",
       "L 145.996448 46.297449 \n",
       "L 147.789837 40.928576 \n",
       "L 149.583226 35.915143 \n",
       "L 151.376613 31.30725 \n",
       "L 153.170002 27.150928 \n",
       "L 154.96339 23.487713 \n",
       "L 156.756779 20.354197 \n",
       "L 158.550168 17.781702 \n",
       "L 160.343556 15.795922 \n",
       "L 162.136943 14.416702 \n",
       "L 163.930332 13.657821 \n",
       "L 165.723721 13.526865 \n",
       "L 167.517108 14.025136 \n",
       "L 169.310496 15.147663 \n",
       "L 171.103885 16.883224 \n",
       "L 172.897274 19.214483 \n",
       "L 174.690661 22.118137 \n",
       "L 176.484052 25.565193 \n",
       "L 178.277438 29.521184 \n",
       "L 180.070829 33.946611 \n",
       "L 181.864216 38.797228 \n",
       "L 183.657607 44.024592 \n",
       "L 185.450994 49.576454 \n",
       "L 187.244381 55.397351 \n",
       "L 189.037772 61.429137 \n",
       "L 190.831158 67.611517 \n",
       "L 192.624545 73.88273 \n",
       "L 194.417936 80.180134 \n",
       "L 196.211323 86.440775 \n",
       "L 198.004714 92.602132 \n",
       "L 199.7981 98.60261 \n",
       "L 201.591487 104.382271 \n",
       "L 203.384878 109.883379 \n",
       "L 205.178265 115.050941 \n",
       "L 206.971651 119.833344 \n",
       "L 208.765042 124.182806 \n",
       "L 210.558429 128.055852 \n",
       "L 212.351816 131.413792 \n",
       "L 214.145211 134.223092 \n",
       "L 215.938598 136.455652 \n",
       "L 217.731984 138.08918 \n",
       "L 219.525371 139.107352 \n",
       "L 221.318758 139.5 \n",
       "L 223.112153 139.263195 \n",
       "L 224.90554 138.399306 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_22\">\n",
       "    <path d=\"M 47.360085 58.631014 \n",
       "L 49.153472 64.751682 \n",
       "L 50.946859 70.989759 \n",
       "L 52.740254 77.282946 \n",
       "L 54.533641 83.568305 \n",
       "L 56.327027 89.783064 \n",
       "L 58.120414 95.865127 \n",
       "L 59.913801 101.753725 \n",
       "L 61.707196 107.390046 \n",
       "L 63.500583 112.717723 \n",
       "L 65.293974 117.683558 \n",
       "L 67.08736 122.237916 \n",
       "L 68.880751 126.335308 \n",
       "L 70.674138 129.934777 \n",
       "L 72.467525 133.000369 \n",
       "L 74.260916 135.501461 \n",
       "L 76.054298 137.413042 \n",
       "L 77.847685 138.71603 \n",
       "L 79.641076 139.397403 \n",
       "L 81.434462 139.450348 \n",
       "L 83.227853 138.87434 \n",
       "L 85.02124 137.675131 \n",
       "L 86.814627 135.864709 \n",
       "L 88.608018 133.461151 \n",
       "L 90.401409 130.488479 \n",
       "L 92.194796 126.976402 \n",
       "L 93.988187 122.96 \n",
       "L 95.781573 118.479416 \n",
       "L 97.574964 113.579403 \n",
       "L 99.368351 108.30894 \n",
       "L 101.16174 102.720668 \n",
       "L 102.955129 96.870434 \n",
       "L 104.748515 90.816697 \n",
       "L 106.541904 84.619929 \n",
       "L 108.335293 78.342053 \n",
       "L 110.12868 72.045804 \n",
       "L 111.922069 65.794076 \n",
       "L 113.715457 59.649342 \n",
       "L 115.508846 53.672999 \n",
       "L 117.302235 47.924761 \n",
       "L 119.095622 42.462065 \n",
       "L 120.889011 37.339486 \n",
       "L 122.682398 32.608211 \n",
       "L 124.475787 28.315509 \n",
       "L 126.269175 24.504278 \n",
       "L 128.062564 21.212595 \n",
       "L 129.855952 18.473349 \n",
       "L 131.64934 16.313908 \n",
       "L 133.442729 14.755853 \n",
       "L 135.236117 13.814749 \n",
       "L 137.029506 13.5 \n",
       "L 138.822894 13.814749 \n",
       "L 140.616283 14.755853 \n",
       "L 142.409671 16.313908 \n",
       "L 144.20306 18.473349 \n",
       "L 145.996448 21.212595 \n",
       "L 147.789837 24.504278 \n",
       "L 149.583226 28.315513 \n",
       "L 151.376613 32.608211 \n",
       "L 153.170002 37.33949 \n",
       "L 154.96339 42.462065 \n",
       "L 156.756779 47.924766 \n",
       "L 158.550168 53.673007 \n",
       "L 160.343556 59.649349 \n",
       "L 162.136943 65.794076 \n",
       "L 163.930332 72.045804 \n",
       "L 165.723721 78.34206 \n",
       "L 167.517108 84.619929 \n",
       "L 169.310496 90.816697 \n",
       "L 171.103885 96.870441 \n",
       "L 172.897274 102.720676 \n",
       "L 174.690661 108.30894 \n",
       "L 176.484052 113.579414 \n",
       "L 178.277438 118.479416 \n",
       "L 180.070829 122.960011 \n",
       "L 181.864216 126.976402 \n",
       "L 183.657607 130.488487 \n",
       "L 185.450994 133.461151 \n",
       "L 187.244381 135.864705 \n",
       "L 189.037772 137.675131 \n",
       "L 190.831158 138.87434 \n",
       "L 192.624545 139.450348 \n",
       "L 194.417936 139.397403 \n",
       "L 196.211323 138.716034 \n",
       "L 198.004714 137.413042 \n",
       "L 199.7981 135.501454 \n",
       "L 201.591487 133.000369 \n",
       "L 203.384878 129.93477 \n",
       "L 205.178265 126.3353 \n",
       "L 206.971651 122.237916 \n",
       "L 208.765042 117.683546 \n",
       "L 210.558429 112.717723 \n",
       "L 212.351816 107.390046 \n",
       "L 214.145211 101.753725 \n",
       "L 215.938598 95.865127 \n",
       "L 217.731984 89.783064 \n",
       "L 219.525371 83.568305 \n",
       "L 221.318758 77.282946 \n",
       "L 223.112153 70.989759 \n",
       "L 224.90554 64.751682 \n",
       "\" clip-path=\"url(#pa3ee844176)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 38.482813 145.8 \n",
       "L 38.482813 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 233.782813 145.8 \n",
       "L 233.782813 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 38.482812 145.8 \n",
       "L 233.782813 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 38.482812 7.2 \n",
       "L 233.782813 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 86.64375 92.678125 \n",
       "L 185.621875 92.678125 \n",
       "Q 187.621875 92.678125 187.621875 90.678125 \n",
       "L 187.621875 62.321875 \n",
       "Q 187.621875 60.321875 185.621875 60.321875 \n",
       "L 86.64375 60.321875 \n",
       "Q 84.64375 60.321875 84.64375 62.321875 \n",
       "L 84.64375 90.678125 \n",
       "Q 84.64375 92.678125 86.64375 92.678125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_23\">\n",
       "     <path d=\"M 88.64375 68.420313 \n",
       "L 98.64375 68.420313 \n",
       "L 108.64375 68.420313 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- sin(x) -->\n",
       "     <g transform=\"translate(116.64375 71.920313)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"79.882812\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"143.261719\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"182.275391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"241.455078\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_24\">\n",
       "     <path d=\"M 88.64375 83.098438 \n",
       "L 98.64375 83.098438 \n",
       "L 108.64375 83.098438 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- grad of sin(x) -->\n",
       "     <g transform=\"translate(116.64375 86.598438)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-67\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"104.589844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"165.869141\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"229.345703\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"261.132812\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-66\" x=\"322.314453\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"357.519531\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"389.306641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"441.40625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"469.189453\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"532.568359\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"571.582031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"630.761719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pa3ee844176\">\n",
       "   <rect x=\"38.482813\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sin(x)\n",
    "x = torch.arange(-5, 5, 0.1, requires_grad=True)\n",
    "#x.grad.zero_()\n",
    "y = torch.sin(x)\n",
    "y.sum().backward()\n",
    "plot(x.detach().numpy(), [y.detach().numpy(), x.grad], xlabel='x', legend=['sin(x)', 'grad of sin(x)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1673, 0.1678, 0.1665, 0.1647, 0.1664, 0.1673])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fair_probs = torch.ones([6]) / 6\n",
    "print(fair_probs)\n",
    "counts = multinomial.Multinomial(100000, fair_probs).sample()\n",
    "counts / 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2c1ce499f40>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"615.34375pt\" height=\"377.39625pt\" viewBox=\"0 0 615.34375 377.39625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-10-31T09:28:30.570671</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 377.39625 \n",
       "L 615.34375 377.39625 \n",
       "L 615.34375 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 339.84 \n",
       "L 608.14375 339.84 \n",
       "L 608.14375 7.2 \n",
       "L 50.14375 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m31a8e2eac8\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m31a8e2eac8\" x=\"75.507386\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(72.326136 354.438438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m31a8e2eac8\" x=\"177.063488\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(167.519738 354.438438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m31a8e2eac8\" x=\"278.619589\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(269.075839 354.438438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m31a8e2eac8\" x=\"380.175691\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 600 -->\n",
       "      <g transform=\"translate(370.631941 354.438438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m31a8e2eac8\" x=\"481.731793\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 800 -->\n",
       "      <g transform=\"translate(472.188043 354.438438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m31a8e2eac8\" x=\"583.287894\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(570.562894 354.438438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- Groups of experiments -->\n",
       "     <g transform=\"translate(271.997656 368.116562)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-47\" d=\"M 3809 666 \n",
       "L 3809 1919 \n",
       "L 2778 1919 \n",
       "L 2778 2438 \n",
       "L 4434 2438 \n",
       "L 4434 434 \n",
       "Q 4069 175 3628 42 \n",
       "Q 3188 -91 2688 -91 \n",
       "Q 1594 -91 976 548 \n",
       "Q 359 1188 359 2328 \n",
       "Q 359 3472 976 4111 \n",
       "Q 1594 4750 2688 4750 \n",
       "Q 3144 4750 3555 4637 \n",
       "Q 3966 4525 4313 4306 \n",
       "L 4313 3634 \n",
       "Q 3963 3931 3569 4081 \n",
       "Q 3175 4231 2741 4231 \n",
       "Q 1884 4231 1454 3753 \n",
       "Q 1025 3275 1025 2328 \n",
       "Q 1025 1384 1454 906 \n",
       "Q 1884 428 2741 428 \n",
       "Q 3075 428 3337 486 \n",
       "Q 3600 544 3809 666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \n",
       "L 2375 4384 \n",
       "L 1825 4384 \n",
       "Q 1516 4384 1395 4259 \n",
       "Q 1275 4134 1275 3809 \n",
       "L 1275 3500 \n",
       "L 2222 3500 \n",
       "L 2222 3053 \n",
       "L 1275 3053 \n",
       "L 1275 0 \n",
       "L 697 0 \n",
       "L 697 3053 \n",
       "L 147 3053 \n",
       "L 147 3500 \n",
       "L 697 3500 \n",
       "L 697 3744 \n",
       "Q 697 4328 969 4595 \n",
       "Q 1241 4863 1831 4863 \n",
       "L 2375 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-47\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"77.490234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"116.353516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"177.535156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"240.914062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"304.390625\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"356.490234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"388.277344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-66\" x=\"449.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"484.664062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"516.451172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"576.224609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"635.404297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"698.880859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"760.404297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"801.517578\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"829.300781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"926.712891\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"988.236328\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"1051.615234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"1090.824219\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m977de909c0\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m977de909c0\" x=\"50.14375\" y=\"324.72\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.00 -->\n",
       "      <g transform=\"translate(20.878125 328.519219)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m977de909c0\" x=\"50.14375\" y=\"274.320002\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 278.119221)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m977de909c0\" x=\"50.14375\" y=\"223.920004\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 227.719223)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m977de909c0\" x=\"50.14375\" y=\"173.520006\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 177.319225)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m977de909c0\" x=\"50.14375\" y=\"123.120008\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.878125 126.919227)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m977de909c0\" x=\"50.14375\" y=\"72.72001\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.25 -->\n",
       "      <g transform=\"translate(20.878125 76.519229)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m977de909c0\" x=\"50.14375\" y=\"22.320012\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.30 -->\n",
       "      <g transform=\"translate(20.878125 26.119231)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- Estimated probability -->\n",
       "     <g transform=\"translate(14.798438 227.043438)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-45\" d=\"M 628 4666 \n",
       "L 3578 4666 \n",
       "L 3578 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2753 \n",
       "L 3481 2753 \n",
       "L 3481 2222 \n",
       "L 1259 2222 \n",
       "L 1259 531 \n",
       "L 3634 531 \n",
       "L 3634 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-45\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"63.183594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"115.283203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"154.492188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"182.275391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"279.6875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"340.966797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"380.175781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"441.699219\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"505.175781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"536.962891\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"600.439453\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"639.302734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" x=\"700.484375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"763.960938\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" x=\"825.240234\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"888.716797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"916.5\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"944.283203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"972.066406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"1011.275391\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_14\">\n",
       "    <path d=\"M 75.507386 223.920003 \n",
       "L 76.015167 173.52 \n",
       "L 76.522947 190.319998 \n",
       "L 77.030728 198.720005 \n",
       "L 77.538508 163.44001 \n",
       "L 78.046289 106.320007 \n",
       "L 78.554069 123.120005 \n",
       "L 79.06185 135.720008 \n",
       "L 79.56963 145.520003 \n",
       "L 80.077411 143.28 \n",
       "L 80.585191 123.120005 \n",
       "L 81.092972 114.720013 \n",
       "L 81.600752 130.87385 \n",
       "L 82.108533 137.520004 \n",
       "L 82.616313 129.840013 \n",
       "L 83.124094 142.020001 \n",
       "L 83.631874 146.83765 \n",
       "L 84.139655 139.920003 \n",
       "L 84.647436 144.341054 \n",
       "L 85.662997 142.320003 \n",
       "L 86.170777 146.029103 \n",
       "L 86.678558 153.798269 \n",
       "L 87.186338 156.720002 \n",
       "L 87.694119 163.44001 \n",
       "L 88.201899 158.012309 \n",
       "L 89.725241 164.830356 \n",
       "L 90.233021 160.080013 \n",
       "L 90.740802 158.887741 \n",
       "L 91.248582 164.070009 \n",
       "L 91.756363 162.829102 \n",
       "L 92.264143 164.625883 \n",
       "L 92.771924 163.44001 \n",
       "L 93.279704 165.120008 \n",
       "L 93.787485 169.433513 \n",
       "L 94.295265 162.909476 \n",
       "L 95.310826 155.880003 \n",
       "L 95.818607 155.080979 \n",
       "L 96.326387 151.920002 \n",
       "L 96.834168 153.594428 \n",
       "L 97.341948 152.901824 \n",
       "L 97.849729 156.720002 \n",
       "L 98.357509 158.180883 \n",
       "L 98.86529 155.290213 \n",
       "L 99.37307 150.420008 \n",
       "L 99.880851 153.977147 \n",
       "L 100.388631 153.360005 \n",
       "L 101.404192 156.073855 \n",
       "L 101.911973 151.648314 \n",
       "L 102.419753 152.986674 \n",
       "L 102.927534 156.109093 \n",
       "L 103.435314 153.720013 \n",
       "L 103.943095 153.18317 \n",
       "L 104.450875 154.402764 \n",
       "L 104.958656 152.164068 \n",
       "L 105.974217 154.516724 \n",
       "L 106.481997 152.384522 \n",
       "L 106.989778 153.520002 \n",
       "L 107.497558 153.045013 \n",
       "L 108.005339 147.932319 \n",
       "L 108.513119 150.610917 \n",
       "L 109.0209 151.705076 \n",
       "L 109.52868 151.284716 \n",
       "L 110.544241 147.600009 \n",
       "L 111.052022 148.674941 \n",
       "L 111.559802 148.32001 \n",
       "L 113.590924 152.298951 \n",
       "L 114.098705 150.610917 \n",
       "L 114.606485 151.550771 \n",
       "L 115.114266 149.914948 \n",
       "L 115.622046 150.840008 \n",
       "L 116.129827 152.986674 \n",
       "L 116.637607 150.163911 \n",
       "L 117.145388 149.838074 \n",
       "L 117.653169 150.72001 \n",
       "L 118.160949 152.767061 \n",
       "L 118.66873 153.594428 \n",
       "L 119.17651 155.56139 \n",
       "L 119.684291 155.19273 \n",
       "L 120.192071 153.699781 \n",
       "L 120.699852 153.360005 \n",
       "L 121.207632 155.243079 \n",
       "L 121.715413 154.893915 \n",
       "L 122.223193 155.636132 \n",
       "L 122.730974 157.434896 \n",
       "L 123.238754 154.951578 \n",
       "L 123.746535 155.670003 \n",
       "L 125.269876 154.683645 \n",
       "L 125.777657 152.352003 \n",
       "L 126.285437 152.062575 \n",
       "L 126.793218 150.79059 \n",
       "L 127.300998 150.521951 \n",
       "L 127.808779 149.289244 \n",
       "L 128.316559 150.000008 \n",
       "L 128.82434 149.746419 \n",
       "L 130.347681 151.787898 \n",
       "L 130.855462 153.360005 \n",
       "L 133.394364 156.427825 \n",
       "L 133.902145 155.271738 \n",
       "L 134.409925 153.273848 \n",
       "L 134.917706 153.872545 \n",
       "L 135.425486 155.308237 \n",
       "L 135.933267 155.880003 \n",
       "L 136.441047 157.27538 \n",
       "L 136.948828 156.995415 \n",
       "L 138.472169 158.601603 \n",
       "L 138.97995 158.320002 \n",
       "L 139.48773 157.249139 \n",
       "L 139.995511 157.770001 \n",
       "L 141.011072 157.236928 \n",
       "L 142.026633 153.665459 \n",
       "L 142.534413 154.193696 \n",
       "L 143.042194 153.961796 \n",
       "L 143.549974 152.240011 \n",
       "L 144.057755 153.508241 \n",
       "L 144.565535 153.286435 \n",
       "L 145.073316 154.528695 \n",
       "L 145.581096 153.57756 \n",
       "L 146.088877 154.800008 \n",
       "L 146.596657 153.860439 \n",
       "L 147.104438 154.353813 \n",
       "L 147.612218 154.135387 \n",
       "L 148.119999 154.620004 \n",
       "L 148.627779 154.402764 \n",
       "L 149.13556 155.569321 \n",
       "L 150.658902 154.91598 \n",
       "L 151.166682 156.048005 \n",
       "L 151.674463 155.162389 \n",
       "L 152.182243 155.614743 \n",
       "L 153.197804 155.19273 \n",
       "L 153.705585 154.335488 \n",
       "L 154.213365 154.781548 \n",
       "L 155.228926 154.380759 \n",
       "L 155.736707 155.452087 \n",
       "L 156.244487 155.880003 \n",
       "L 156.752268 156.92871 \n",
       "L 157.260048 156.720002 \n",
       "L 157.767829 157.132281 \n",
       "L 158.275609 156.92488 \n",
       "L 158.78339 156.109093 \n",
       "L 159.29117 156.517602 \n",
       "L 160.814512 159.503441 \n",
       "L 161.322292 158.696472 \n",
       "L 162.337853 158.282796 \n",
       "L 162.845634 157.496885 \n",
       "L 163.353414 157.878628 \n",
       "L 163.861195 157.680014 \n",
       "L 164.876756 156.150519 \n",
       "L 165.384536 156.531241 \n",
       "L 165.892317 155.781454 \n",
       "L 166.400097 156.160012 \n",
       "L 166.907878 155.420559 \n",
       "L 167.923439 156.16919 \n",
       "L 168.431219 155.441753 \n",
       "L 168.939 155.267036 \n",
       "L 169.44678 156.178067 \n",
       "L 170.462341 155.826395 \n",
       "L 172.493463 153.045013 \n",
       "L 173.001244 152.889958 \n",
       "L 173.509024 153.775679 \n",
       "L 174.524585 153.462864 \n",
       "L 175.032366 154.332183 \n",
       "L 176.047927 155.031562 \n",
       "L 176.555707 154.368007 \n",
       "L 177.063488 154.714031 \n",
       "L 177.571268 154.557624 \n",
       "L 178.079049 155.395866 \n",
       "L 178.586829 155.731766 \n",
       "L 179.60239 155.415152 \n",
       "L 180.110171 156.233056 \n",
       "L 180.617951 156.558473 \n",
       "L 182.649074 155.927557 \n",
       "L 183.156854 156.24677 \n",
       "L 184.172415 157.813965 \n",
       "L 185.187976 158.423237 \n",
       "L 185.695757 157.7989 \n",
       "L 186.711318 158.4 \n",
       "L 187.219098 158.240364 \n",
       "L 187.726879 158.99027 \n",
       "L 188.234659 158.829417 \n",
       "L 189.25022 159.408002 \n",
       "L 189.758001 159.247434 \n",
       "L 190.265781 159.532341 \n",
       "L 190.773562 160.256848 \n",
       "L 191.281342 160.094673 \n",
       "L 191.789123 159.495661 \n",
       "L 192.804684 160.051039 \n",
       "L 193.312464 159.459927 \n",
       "L 193.820245 160.166155 \n",
       "L 194.835806 159.852214 \n",
       "L 195.343586 160.122536 \n",
       "L 197.882489 159.358029 \n",
       "L 198.89805 160.713452 \n",
       "L 199.40583 160.971442 \n",
       "L 199.913611 159.588307 \n",
       "L 200.421391 159.440656 \n",
       "L 200.929172 159.700659 \n",
       "L 201.436952 160.363373 \n",
       "L 201.944733 160.214401 \n",
       "L 202.452513 160.468215 \n",
       "L 203.468074 161.766651 \n",
       "L 203.975855 162.011348 \n",
       "L 204.483635 161.858831 \n",
       "L 204.991416 162.101259 \n",
       "L 205.499196 161.949584 \n",
       "L 206.514757 162.428118 \n",
       "L 207.022538 162.276923 \n",
       "L 207.530318 161.740695 \n",
       "L 208.038099 161.593286 \n",
       "L 209.05366 160.538195 \n",
       "L 210.069221 161.014746 \n",
       "L 210.577001 160.495282 \n",
       "L 211.084782 160.355832 \n",
       "L 212.608123 161.059492 \n",
       "L 214.131465 160.644088 \n",
       "L 214.639245 161.240727 \n",
       "L 215.654807 160.965494 \n",
       "L 216.162587 160.466773 \n",
       "L 216.670368 160.332911 \n",
       "L 218.193709 161.009369 \n",
       "L 219.20927 160.742547 \n",
       "L 219.717051 160.964217 \n",
       "L 220.224831 161.536794 \n",
       "L 220.732612 161.051711 \n",
       "L 221.240392 160.920013 \n",
       "L 221.748173 161.138003 \n",
       "L 222.255953 161.702078 \n",
       "L 222.763734 161.223094 \n",
       "L 223.271514 161.092612 \n",
       "L 223.779295 161.651069 \n",
       "L 224.287075 161.862856 \n",
       "L 224.794856 161.389835 \n",
       "L 225.302636 161.941623 \n",
       "L 225.810417 162.150316 \n",
       "L 226.318197 162.019338 \n",
       "L 226.825978 161.214983 \n",
       "L 228.349319 160.836559 \n",
       "L 228.8571 161.044757 \n",
       "L 229.36488 160.920013 \n",
       "L 229.872661 161.126558 \n",
       "L 230.888222 160.878962 \n",
       "L 231.396002 161.410908 \n",
       "L 231.903783 160.960778 \n",
       "L 233.427124 161.566159 \n",
       "L 233.934905 162.087426 \n",
       "L 234.442685 161.963312 \n",
       "L 234.950466 161.520001 \n",
       "L 236.473807 161.15774 \n",
       "L 236.981588 161.354492 \n",
       "L 237.489368 161.865004 \n",
       "L 237.997149 161.116269 \n",
       "L 238.504929 160.998269 \n",
       "L 239.01271 160.568926 \n",
       "L 239.52049 160.764447 \n",
       "L 240.028271 160.648624 \n",
       "L 242.059393 161.417878 \n",
       "L 242.567173 161.30183 \n",
       "L 243.074954 161.795535 \n",
       "L 243.582734 161.982659 \n",
       "L 244.090515 161.86595 \n",
       "L 244.598295 162.353542 \n",
       "L 245.106076 162.236428 \n",
       "L 247.137198 160.585494 \n",
       "L 248.152759 160.956963 \n",
       "L 249.676101 162.385129 \n",
       "L 250.183881 162.271305 \n",
       "L 251.707223 162.802771 \n",
       "L 252.215003 162.689067 \n",
       "L 252.722784 162.288008 \n",
       "L 253.738345 163.210919 \n",
       "L 254.246125 163.382903 \n",
       "L 254.753906 163.838651 \n",
       "L 255.261686 163.723955 \n",
       "L 256.277247 162.931766 \n",
       "L 256.785028 163.102127 \n",
       "L 257.292808 162.990766 \n",
       "L 257.800589 163.44001 \n",
       "L 258.308369 162.769861 \n",
       "L 258.81615 162.381885 \n",
       "L 259.32393 162.273723 \n",
       "L 259.831711 162.720009 \n",
       "L 260.339491 162.887681 \n",
       "L 260.847272 163.329836 \n",
       "L 261.355052 162.945615 \n",
       "L 261.862833 163.385231 \n",
       "L 262.878394 163.712434 \n",
       "L 263.893955 163.494204 \n",
       "L 264.909516 163.817337 \n",
       "L 265.417296 163.44001 \n",
       "L 265.925077 163.33278 \n",
       "L 266.432857 162.958727 \n",
       "L 267.956199 163.44001 \n",
       "L 268.463979 163.863314 \n",
       "L 268.97176 164.020532 \n",
       "L 269.987321 164.857512 \n",
       "L 271.510662 164.533959 \n",
       "L 272.018443 164.687016 \n",
       "L 272.526223 164.580161 \n",
       "L 273.034004 164.990773 \n",
       "L 275.572906 165.736715 \n",
       "L 276.080687 165.120008 \n",
       "L 276.588467 165.522029 \n",
       "L 278.111809 165.960008 \n",
       "L 278.619589 165.853168 \n",
       "L 279.12737 166.248369 \n",
       "L 279.63515 166.391468 \n",
       "L 280.650712 166.177788 \n",
       "L 281.158492 166.320001 \n",
       "L 281.666273 165.966196 \n",
       "L 282.174053 165.861188 \n",
       "L 282.681834 166.003131 \n",
       "L 283.189614 166.390251 \n",
       "L 283.697395 166.284973 \n",
       "L 284.712956 165.587803 \n",
       "L 285.220736 165.972174 \n",
       "L 285.728517 165.868924 \n",
       "L 286.236297 166.008463 \n",
       "L 286.744078 165.663896 \n",
       "L 287.251858 166.044407 \n",
       "L 287.759639 165.701387 \n",
       "L 288.267419 165.599999 \n",
       "L 288.7752 165.977957 \n",
       "L 289.28298 165.876404 \n",
       "L 289.790761 166.251928 \n",
       "L 290.298541 165.912453 \n",
       "L 292.837444 166.588535 \n",
       "L 293.345224 166.487448 \n",
       "L 294.360785 166.753338 \n",
       "L 295.376346 167.481301 \n",
       "L 296.391907 167.277805 \n",
       "L 296.899688 166.946095 \n",
       "L 297.407468 167.306313 \n",
       "L 297.915249 166.976045 \n",
       "L 298.93081 166.777146 \n",
       "L 299.43859 166.906426 \n",
       "L 299.946371 166.352505 \n",
       "L 300.454151 166.709194 \n",
       "L 301.469712 166.06171 \n",
       "L 301.977493 166.191141 \n",
       "L 302.485273 166.095011 \n",
       "L 303.500834 166.352009 \n",
       "L 304.008615 165.809143 \n",
       "L 305.024176 166.065706 \n",
       "L 306.039737 165.87693 \n",
       "L 306.547517 166.004212 \n",
       "L 307.055298 165.689806 \n",
       "L 309.08642 166.195061 \n",
       "L 309.5942 166.538187 \n",
       "L 310.101981 166.226709 \n",
       "L 311.117542 166.474846 \n",
       "L 311.625322 166.38181 \n",
       "L 312.133103 165.641636 \n",
       "L 313.148664 165.46031 \n",
       "L 313.656445 165.584694 \n",
       "L 314.164225 165.494526 \n",
       "L 314.672006 165.618309 \n",
       "L 315.179786 165.315348 \n",
       "L 315.687567 165.226338 \n",
       "L 316.195347 165.349895 \n",
       "L 316.703128 165.684714 \n",
       "L 317.210908 165.8068 \n",
       "L 317.718689 165.717489 \n",
       "L 318.73425 165.960008 \n",
       "L 319.749811 165.782241 \n",
       "L 320.257591 165.902615 \n",
       "L 320.765372 166.23075 \n",
       "L 321.273152 166.141859 \n",
       "L 321.780933 166.260745 \n",
       "L 322.288713 165.965175 \n",
       "L 322.796494 165.87738 \n",
       "L 323.304274 165.996086 \n",
       "L 323.812055 165.908578 \n",
       "L 325.843177 166.37831 \n",
       "L 326.350957 166.087275 \n",
       "L 326.858738 166.203878 \n",
       "L 327.366518 165.914376 \n",
       "L 327.874299 166.030844 \n",
       "L 328.382079 165.742858 \n",
       "L 328.88986 165.657602 \n",
       "L 329.39764 165.773905 \n",
       "L 329.905421 166.09052 \n",
       "L 330.413201 166.005099 \n",
       "L 330.920982 166.120004 \n",
       "L 331.428762 166.434065 \n",
       "L 332.444323 166.660828 \n",
       "L 332.952104 166.178269 \n",
       "L 333.459884 166.489746 \n",
       "L 333.967665 166.009425 \n",
       "L 334.475445 166.122753 \n",
       "L 336.506567 165.788745 \n",
       "L 337.014348 165.510703 \n",
       "L 338.029909 165.347026 \n",
       "L 338.537689 164.877234 \n",
       "L 339.04547 164.990773 \n",
       "L 339.55325 164.716937 \n",
       "L 340.061031 164.830356 \n",
       "L 340.568811 164.750597 \n",
       "L 341.076592 164.478789 \n",
       "L 342.092153 164.321525 \n",
       "L 342.599933 164.434614 \n",
       "L 343.107714 164.165464 \n",
       "L 344.123275 164.010574 \n",
       "L 344.631055 164.313219 \n",
       "L 345.138836 164.046322 \n",
       "L 346.154397 164.648098 \n",
       "L 347.169958 164.869258 \n",
       "L 347.677739 165.166932 \n",
       "L 348.185519 165.276145 \n",
       "L 349.20108 165.120008 \n",
       "L 350.216641 164.593064 \n",
       "L 350.724422 164.516685 \n",
       "L 351.739983 164.734691 \n",
       "L 352.247763 164.473847 \n",
       "L 352.755544 164.39825 \n",
       "L 353.263324 164.690816 \n",
       "L 353.771105 164.798693 \n",
       "L 354.786666 165.37917 \n",
       "L 355.294446 165.120008 \n",
       "L 356.310007 164.968378 \n",
       "L 356.817788 164.711364 \n",
       "L 357.833349 165.285893 \n",
       "L 358.341129 165.390976 \n",
       "L 358.84891 164.59407 \n",
       "L 359.35669 164.520005 \n",
       "L 359.864471 164.625883 \n",
       "L 360.880032 164.478443 \n",
       "L 361.895593 163.975231 \n",
       "L 362.403373 164.259236 \n",
       "L 363.926715 163.510861 \n",
       "L 364.434495 163.44001 \n",
       "L 365.957837 164.28441 \n",
       "L 366.465617 163.685863 \n",
       "L 367.481178 163.545002 \n",
       "L 369.00452 163.857832 \n",
       "L 370.020081 163.717601 \n",
       "L 370.527861 163.994232 \n",
       "L 371.543422 164.199455 \n",
       "L 372.051203 164.473847 \n",
       "L 372.558983 164.059255 \n",
       "L 373.066764 164.332956 \n",
       "L 374.590105 164.635932 \n",
       "L 375.605666 165.17677 \n",
       "L 376.113447 165.105844 \n",
       "L 377.129008 164.625883 \n",
       "L 377.636788 164.725378 \n",
       "L 378.144569 164.48684 \n",
       "L 378.65235 164.586229 \n",
       "L 379.16013 164.853562 \n",
       "L 379.667911 164.784003 \n",
       "L 380.175691 164.546966 \n",
       "L 380.683472 164.64559 \n",
       "L 381.699033 164.508078 \n",
       "L 382.206813 164.772903 \n",
       "L 382.714594 164.205148 \n",
       "L 383.222374 164.469596 \n",
       "L 383.730155 164.567379 \n",
       "L 384.237935 164.333797 \n",
       "L 384.745716 164.266235 \n",
       "L 385.253496 164.363868 \n",
       "L 385.761277 164.131773 \n",
       "L 386.269057 164.2293 \n",
       "L 386.776838 163.669837 \n",
       "L 387.284618 163.931717 \n",
       "L 387.792399 163.865462 \n",
       "L 388.80796 164.386023 \n",
       "L 389.823521 164.252912 \n",
       "L 390.331301 164.511308 \n",
       "L 390.839082 164.606822 \n",
       "L 391.346862 164.054839 \n",
       "L 391.854643 163.98923 \n",
       "L 392.362423 164.08512 \n",
       "L 392.870204 164.341728 \n",
       "L 393.377984 164.275984 \n",
       "L 393.885765 164.531465 \n",
       "L 394.393545 164.465631 \n",
       "L 394.901326 164.560004 \n",
       "L 395.409106 164.813833 \n",
       "L 395.916887 164.747849 \n",
       "L 396.424667 164.841335 \n",
       "L 396.932448 164.775531 \n",
       "L 397.440228 165.027408 \n",
       "L 397.948009 165.120008 \n",
       "L 398.455789 165.054069 \n",
       "L 398.96357 165.146339 \n",
       "L 399.47135 165.396068 \n",
       "L 400.486911 165.264159 \n",
       "L 401.502472 165.446596 \n",
       "L 402.010253 165.693922 \n",
       "L 403.025814 165.562118 \n",
       "L 403.533594 165.340718 \n",
       "L 404.041375 165.43111 \n",
       "L 404.549155 165.365922 \n",
       "L 406.072497 166.099141 \n",
       "L 406.580277 165.878958 \n",
       "L 407.088058 165.967713 \n",
       "L 407.595838 165.748401 \n",
       "L 408.103619 165.990739 \n",
       "L 408.611399 165.925491 \n",
       "L 409.62696 166.407404 \n",
       "L 410.134741 166.189098 \n",
       "L 410.642521 166.276442 \n",
       "L 411.150302 166.515776 \n",
       "L 411.658083 166.450317 \n",
       "L 412.165863 166.53688 \n",
       "L 412.673644 166.16843 \n",
       "L 413.689205 166.3416 \n",
       "L 414.196985 166.276892 \n",
       "L 414.704766 166.363049 \n",
       "L 415.212546 166.59941 \n",
       "L 415.720327 166.68483 \n",
       "L 416.228107 166.620003 \n",
       "L 417.243668 166.790033 \n",
       "L 417.751449 166.72534 \n",
       "L 418.76701 166.894305 \n",
       "L 419.27479 166.829747 \n",
       "L 420.290351 166.404716 \n",
       "L 420.798132 166.341149 \n",
       "L 421.305912 166.425579 \n",
       "L 421.813693 166.214588 \n",
       "L 422.321473 166.446322 \n",
       "L 423.844815 166.697297 \n",
       "L 424.352595 166.633956 \n",
       "L 424.860376 166.717109 \n",
       "L 425.368156 166.946095 \n",
       "L 425.875937 167.028542 \n",
       "L 426.383717 167.256416 \n",
       "L 427.399278 167.419717 \n",
       "L 427.907059 167.355971 \n",
       "L 428.414839 167.147593 \n",
       "L 428.92262 167.229049 \n",
       "L 429.4304 167.165858 \n",
       "L 429.938181 167.247043 \n",
       "L 430.445961 167.040002 \n",
       "L 430.953742 167.121142 \n",
       "L 431.461522 167.345652 \n",
       "L 432.477083 166.933643 \n",
       "L 432.984864 167.157446 \n",
       "L 433.492644 167.23785 \n",
       "L 434.000425 167.460603 \n",
       "L 434.508205 167.540346 \n",
       "L 435.015986 167.477696 \n",
       "L 436.031547 167.920001 \n",
       "L 436.539327 167.715513 \n",
       "L 437.047108 167.79434 \n",
       "L 437.554888 167.590589 \n",
       "L 438.062669 167.528404 \n",
       "L 438.570449 167.747942 \n",
       "L 439.07823 167.685698 \n",
       "L 439.58601 167.764014 \n",
       "L 440.093791 167.98232 \n",
       "L 440.601571 168.060005 \n",
       "L 441.109352 168.27729 \n",
       "L 441.617132 168.214745 \n",
       "L 442.124913 168.431204 \n",
       "L 443.140474 168.584276 \n",
       "L 443.648255 168.105127 \n",
       "L 444.156035 167.904605 \n",
       "L 444.663816 167.843081 \n",
       "L 446.187157 168.073223 \n",
       "L 446.694938 168.011805 \n",
       "L 447.202718 168.088078 \n",
       "L 447.710499 167.752163 \n",
       "L 448.218279 167.142862 \n",
       "L 448.72606 166.946095 \n",
       "L 449.23384 167.023405 \n",
       "L 449.741621 166.827329 \n",
       "L 450.249401 166.904578 \n",
       "L 450.757182 166.845413 \n",
       "L 451.264962 166.650374 \n",
       "L 452.280523 166.804528 \n",
       "L 452.788304 166.474846 \n",
       "L 453.296084 166.416657 \n",
       "L 453.803865 166.223495 \n",
       "L 454.311645 166.30073 \n",
       "L 454.819426 166.243006 \n",
       "L 455.327206 165.916268 \n",
       "L 455.834987 165.724804 \n",
       "L 457.358328 165.956658 \n",
       "L 458.373889 166.377228 \n",
       "L 458.88167 166.18668 \n",
       "L 459.38945 166.262938 \n",
       "L 459.897231 166.471992 \n",
       "L 460.405011 166.547679 \n",
       "L 460.912792 166.357897 \n",
       "L 461.420572 166.43354 \n",
       "L 463.451694 166.207063 \n",
       "L 464.467255 166.357552 \n",
       "L 464.975036 166.301255 \n",
       "L 465.482816 166.50726 \n",
       "L 465.990597 166.581821 \n",
       "L 466.498377 166.394712 \n",
       "L 467.006158 166.5998 \n",
       "L 468.529499 166.821681 \n",
       "L 469.03728 166.76537 \n",
       "L 469.54506 166.838924 \n",
       "L 470.052841 166.653168 \n",
       "L 470.560621 166.726677 \n",
       "L 471.068402 166.929242 \n",
       "L 471.576182 166.744071 \n",
       "L 472.591743 167.147593 \n",
       "L 473.099524 167.091432 \n",
       "L 474.115085 167.492521 \n",
       "L 474.622865 167.564228 \n",
       "L 475.130646 167.507827 \n",
       "L 475.638426 167.323812 \n",
       "L 476.653988 167.466911 \n",
       "L 477.161768 167.410915 \n",
       "L 477.669549 167.609289 \n",
       "L 478.177329 167.553248 \n",
       "L 478.68511 167.624159 \n",
       "L 479.19289 167.568254 \n",
       "L 479.700671 167.385996 \n",
       "L 480.208451 167.330526 \n",
       "L 480.716232 167.14905 \n",
       "L 481.731793 167.290797 \n",
       "L 482.239573 167.487083 \n",
       "L 482.747354 167.431839 \n",
       "L 483.255134 167.502089 \n",
       "L 483.762915 167.697399 \n",
       "L 484.270695 167.517034 \n",
       "L 484.778476 167.586924 \n",
       "L 485.286256 167.40713 \n",
       "L 485.794037 167.47699 \n",
       "L 486.301817 167.422225 \n",
       "L 487.317378 167.561389 \n",
       "L 487.825159 167.382737 \n",
       "L 488.332939 167.576019 \n",
       "L 489.3485 166.972951 \n",
       "L 489.856281 167.042646 \n",
       "L 490.364061 166.988948 \n",
       "L 490.871842 166.812308 \n",
       "L 491.379622 167.004885 \n",
       "L 493.410744 167.281169 \n",
       "L 493.918525 167.227637 \n",
       "L 494.426305 167.29628 \n",
       "L 494.934086 167.120977 \n",
       "L 495.441866 167.067835 \n",
       "L 495.949647 167.258008 \n",
       "L 496.965208 166.909175 \n",
       "L 497.980769 166.804047 \n",
       "L 498.488549 166.993394 \n",
       "L 500.011891 167.197431 \n",
       "L 501.027452 167.092348 \n",
       "L 501.535232 167.16 \n",
       "L 502.043013 167.107639 \n",
       "L 502.550793 167.294823 \n",
       "L 503.058574 167.122855 \n",
       "L 503.566354 167.070719 \n",
       "L 504.074135 167.137995 \n",
       "L 504.581915 167.324263 \n",
       "L 505.089696 167.391073 \n",
       "L 506.105257 166.930609 \n",
       "L 506.613037 167.116246 \n",
       "L 507.120818 166.946095 \n",
       "L 507.628598 167.012965 \n",
       "L 508.136379 167.197851 \n",
       "L 508.64416 167.146241 \n",
       "L 509.15194 166.976841 \n",
       "L 509.659721 166.925607 \n",
       "L 510.167501 166.992117 \n",
       "L 511.183062 167.242011 \n",
       "L 511.690843 167.30792 \n",
       "L 512.198623 166.905375 \n",
       "L 513.721965 167.103343 \n",
       "L 514.229745 167.285555 \n",
       "L 514.737526 167.234561 \n",
       "L 515.245306 167.29993 \n",
       "L 515.753087 167.132903 \n",
       "L 516.260867 167.314259 \n",
       "L 516.768648 167.26346 \n",
       "L 517.276428 167.328498 \n",
       "L 517.784209 167.508998 \n",
       "L 518.291989 167.342693 \n",
       "L 518.79977 167.407415 \n",
       "L 519.815331 167.651511 \n",
       "L 520.323111 167.600743 \n",
       "L 520.830892 167.320462 \n",
       "L 521.338672 167.384855 \n",
       "L 522.862014 167.805726 \n",
       "L 523.369794 167.755107 \n",
       "L 523.877575 167.818643 \n",
       "L 524.385355 167.768145 \n",
       "L 524.893136 167.831516 \n",
       "L 525.400916 167.667463 \n",
       "L 525.908697 167.730819 \n",
       "L 526.416477 167.453858 \n",
       "L 526.924258 167.404051 \n",
       "L 527.432038 167.580615 \n",
       "L 527.939819 167.417764 \n",
       "L 528.95538 167.544161 \n",
       "L 529.46316 167.494534 \n",
       "L 530.478721 167.732712 \n",
       "L 530.986502 167.683039 \n",
       "L 531.494282 167.297091 \n",
       "L 532.509843 167.422781 \n",
       "L 533.017624 167.373665 \n",
       "L 533.525404 167.436285 \n",
       "L 535.556526 166.907432 \n",
       "L 536.572087 166.921987 \n",
       "L 537.079868 166.873847 \n",
       "L 538.095429 167.109486 \n",
       "L 540.126551 166.587258 \n",
       "L 540.634331 166.759707 \n",
       "L 541.649893 166.335682 \n",
       "L 543.173234 166.304395 \n",
       "L 543.681015 166.366804 \n",
       "L 545.204356 166.008989 \n",
       "L 546.219917 165.916554 \n",
       "L 547.743259 166.320001 \n",
       "L 549.774381 166.458609 \n",
       "L 550.282161 166.412316 \n",
       "L 550.789942 166.150961 \n",
       "L 556.375527 165.651654 \n",
       "L 556.883308 165.3944 \n",
       "L 561.453332 165.839003 \n",
       "L 562.468893 165.540008 \n",
       "L 562.976674 165.705653 \n",
       "L 564.500015 165.677685 \n",
       "L 567.038918 165.874181 \n",
       "L 567.546698 165.726185 \n",
       "L 568.054479 165.786117 \n",
       "L 568.562259 165.638527 \n",
       "L 570.085601 165.71447 \n",
       "L 573.640064 165.513482 \n",
       "L 574.147845 165.675447 \n",
       "L 575.163406 165.691383 \n",
       "L 575.671187 165.750428 \n",
       "L 576.686748 165.460085 \n",
       "L 578.210089 165.535352 \n",
       "L 578.71787 165.492589 \n",
       "L 579.733431 165.711556 \n",
       "L 580.241211 165.56745 \n",
       "L 582.780114 165.758404 \n",
       "L 582.780114 165.758404 \n",
       "\" clip-path=\"url(#p6070896839)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 75.507386 123.120005 \n",
       "L 76.015167 173.52 \n",
       "L 77.030728 72.72001 \n",
       "L 77.538508 123.120005 \n",
       "L 78.046289 156.720002 \n",
       "L 78.554069 137.520004 \n",
       "L 79.06185 148.32001 \n",
       "L 79.56963 167.920001 \n",
       "L 80.077411 173.52 \n",
       "L 80.585191 168.938186 \n",
       "L 81.092972 156.720002 \n",
       "L 81.600752 154.135387 \n",
       "L 82.108533 137.520004 \n",
       "L 82.616313 143.28 \n",
       "L 83.124094 142.020001 \n",
       "L 83.631874 134.978827 \n",
       "L 84.139655 139.920003 \n",
       "L 85.155216 138.240005 \n",
       "L 85.662997 132.720004 \n",
       "L 86.170777 132.283647 \n",
       "L 86.678558 136.26783 \n",
       "L 87.186338 135.720008 \n",
       "L 87.694119 131.184006 \n",
       "L 88.201899 134.750773 \n",
       "L 88.70968 134.320004 \n",
       "L 89.21746 137.520004 \n",
       "L 89.725241 143.975173 \n",
       "L 90.233021 136.560007 \n",
       "L 90.740802 136.126459 \n",
       "L 91.248582 142.020001 \n",
       "L 91.756363 144.501832 \n",
       "L 92.264143 140.908239 \n",
       "L 92.771924 140.400009 \n",
       "L 93.279704 137.120011 \n",
       "L 94.295265 136.383172 \n",
       "L 94.803046 133.458466 \n",
       "L 95.310826 133.20001 \n",
       "L 95.818607 135.412691 \n",
       "L 96.326387 139.920003 \n",
       "L 96.834168 141.873493 \n",
       "L 97.341948 146.029103 \n",
       "L 97.849729 145.520003 \n",
       "L 98.357509 142.841751 \n",
       "L 98.86529 144.56681 \n",
       "L 99.37307 144.120014 \n",
       "L 100.896412 148.81412 \n",
       "L 101.404192 144.443087 \n",
       "L 101.911973 147.84454 \n",
       "L 102.927534 143.28 \n",
       "L 103.435314 142.920007 \n",
       "L 104.450875 145.713105 \n",
       "L 104.958656 141.913221 \n",
       "L 105.974217 144.601973 \n",
       "L 106.481997 147.507108 \n",
       "L 106.989778 145.520003 \n",
       "L 108.005339 147.932319 \n",
       "L 108.513119 150.610917 \n",
       "L 109.52868 152.767061 \n",
       "L 110.036461 152.337403 \n",
       "L 110.544241 153.360005 \n",
       "L 111.052022 152.934087 \n",
       "L 111.559802 153.920009 \n",
       "L 112.575363 158.53622 \n",
       "L 113.083144 159.408002 \n",
       "L 113.590924 158.930534 \n",
       "L 114.098705 161.083644 \n",
       "L 114.606485 161.889232 \n",
       "L 116.129827 160.453345 \n",
       "L 116.637607 158.76878 \n",
       "L 117.653169 160.320009 \n",
       "L 118.160949 155.138837 \n",
       "L 120.192071 153.699781 \n",
       "L 120.699852 155.600008 \n",
       "L 121.207632 156.350771 \n",
       "L 122.223193 159.971626 \n",
       "L 123.746535 158.82 \n",
       "L 124.254315 160.530309 \n",
       "L 124.762096 160.148581 \n",
       "L 125.269876 157.738187 \n",
       "L 127.300998 160.308353 \n",
       "L 127.808779 158.981544 \n",
       "L 129.33212 160.802253 \n",
       "L 129.839901 162.320001 \n",
       "L 131.363242 161.260554 \n",
       "L 131.871023 160.020007 \n",
       "L 132.886584 161.141052 \n",
       "L 133.394364 159.933925 \n",
       "L 133.902145 160.485518 \n",
       "L 134.409925 161.889232 \n",
       "L 134.917706 162.414915 \n",
       "L 135.933267 161.760012 \n",
       "L 136.441047 159.774559 \n",
       "L 136.948828 161.126558 \n",
       "L 137.456608 159.178536 \n",
       "L 137.964389 158.887741 \n",
       "L 138.97995 159.920001 \n",
       "L 139.48773 161.217641 \n",
       "L 139.995511 161.7075 \n",
       "L 140.503291 159.84559 \n",
       "L 141.518852 160.823822 \n",
       "L 142.026633 160.538195 \n",
       "L 142.534413 159.49895 \n",
       "L 143.549974 161.94667 \n",
       "L 144.565535 162.851392 \n",
       "L 145.581096 162.279716 \n",
       "L 146.088877 161.280006 \n",
       "L 146.596657 161.009369 \n",
       "L 147.612218 163.299029 \n",
       "L 148.627779 164.135183 \n",
       "L 149.13556 163.854257 \n",
       "L 150.151121 164.665958 \n",
       "L 150.658902 165.740139 \n",
       "L 151.166682 165.455999 \n",
       "L 152.182243 167.551581 \n",
       "L 152.690024 167.261177 \n",
       "L 154.213365 168.35077 \n",
       "L 154.721146 168.062679 \n",
       "L 155.736707 168.765287 \n",
       "L 156.244487 166.590007 \n",
       "L 156.752268 165.693922 \n",
       "L 157.260048 166.675563 \n",
       "L 157.767829 167.026754 \n",
       "L 158.275609 167.988298 \n",
       "L 158.78339 167.716369 \n",
       "L 159.29117 166.840487 \n",
       "L 159.798951 165.371509 \n",
       "L 160.814512 163.678594 \n",
       "L 161.322292 164.032954 \n",
       "L 161.830073 163.793695 \n",
       "L 162.337853 164.729314 \n",
       "L 163.353414 164.251035 \n",
       "L 163.861195 164.592012 \n",
       "L 164.368975 164.356373 \n",
       "L 164.876756 164.692889 \n",
       "L 165.384536 163.326741 \n",
       "L 166.400097 163.999999 \n",
       "L 166.907878 163.217243 \n",
       "L 167.923439 163.880663 \n",
       "L 168.431219 163.659142 \n",
       "L 168.939 163.984874 \n",
       "L 169.954561 163.547811 \n",
       "L 170.970122 164.186673 \n",
       "L 171.477902 163.970529 \n",
       "L 171.985683 164.28441 \n",
       "L 172.493463 164.070009 \n",
       "L 174.524585 165.291436 \n",
       "L 175.032366 166.100718 \n",
       "L 176.047927 165.668748 \n",
       "L 177.063488 166.248369 \n",
       "L 178.079049 164.830356 \n",
       "L 178.586829 164.625883 \n",
       "L 179.09461 165.406837 \n",
       "L 179.60239 165.201554 \n",
       "L 180.110171 164.511308 \n",
       "L 180.617951 164.796935 \n",
       "L 181.125732 165.562118 \n",
       "L 181.633512 165.360004 \n",
       "L 182.649074 165.912453 \n",
       "L 183.156854 165.711556 \n",
       "L 183.664635 166.454583 \n",
       "L 184.172415 166.253025 \n",
       "L 185.187976 164.926456 \n",
       "L 185.695757 165.197078 \n",
       "L 186.203537 165.925491 \n",
       "L 186.711318 165.730917 \n",
       "L 187.219098 165.994209 \n",
       "L 188.234659 164.705656 \n",
       "L 189.25022 165.232 \n",
       "L 189.758001 165.045673 \n",
       "L 190.265781 164.41692 \n",
       "L 190.773562 164.235789 \n",
       "L 191.281342 164.496423 \n",
       "L 191.789123 164.316524 \n",
       "L 192.296903 165.010915 \n",
       "L 193.312464 164.651343 \n",
       "L 193.820245 164.043078 \n",
       "L 194.328025 164.297883 \n",
       "L 195.343586 165.651654 \n",
       "L 196.359147 165.295732 \n",
       "L 196.866928 165.960008 \n",
       "L 197.374708 165.782241 \n",
       "L 197.882489 166.439007 \n",
       "L 198.390269 166.260745 \n",
       "L 199.913611 168.193176 \n",
       "L 202.452513 167.295303 \n",
       "L 202.960294 165.920008 \n",
       "L 203.468074 164.954003 \n",
       "L 203.975855 165.582996 \n",
       "L 204.483635 165.021189 \n",
       "L 204.991416 165.251256 \n",
       "L 205.499196 164.302885 \n",
       "L 206.006977 164.143264 \n",
       "L 206.514757 164.374067 \n",
       "L 207.022538 163.827701 \n",
       "L 207.530318 163.671729 \n",
       "L 208.038099 163.901691 \n",
       "L 208.545879 164.513155 \n",
       "L 209.56144 164.961513 \n",
       "L 210.069221 164.425271 \n",
       "L 210.577001 164.648098 \n",
       "L 211.592562 164.33934 \n",
       "L 212.100343 163.44001 \n",
       "L 214.131465 162.851392 \n",
       "L 214.639245 162.340369 \n",
       "L 215.147026 161.467835 \n",
       "L 215.654807 161.693291 \n",
       "L 216.162587 161.554533 \n",
       "L 216.670368 161.778066 \n",
       "L 217.178148 162.36 \n",
       "L 217.685929 162.220356 \n",
       "L 218.193709 162.439158 \n",
       "L 218.70149 162.300219 \n",
       "L 219.20927 162.517189 \n",
       "L 219.717051 162.025271 \n",
       "L 220.732612 162.456596 \n",
       "L 221.240392 163.02001 \n",
       "L 221.748173 162.881943 \n",
       "L 222.763734 163.301447 \n",
       "L 223.271514 163.163845 \n",
       "L 223.779295 163.371202 \n",
       "L 224.287075 163.234291 \n",
       "L 225.302636 162.282165 \n",
       "L 225.810417 162.489701 \n",
       "L 226.318197 162.357597 \n",
       "L 226.825978 162.900614 \n",
       "L 227.333758 163.104004 \n",
       "L 228.349319 164.174311 \n",
       "L 228.8571 164.371498 \n",
       "L 229.36488 164.898954 \n",
       "L 229.872661 164.431475 \n",
       "L 230.380441 164.955295 \n",
       "L 230.888222 165.14736 \n",
       "L 231.396002 165.665458 \n",
       "L 231.903783 165.527766 \n",
       "L 232.411563 165.065815 \n",
       "L 232.919344 164.930932 \n",
       "L 233.427124 164.473847 \n",
       "L 234.442685 164.21045 \n",
       "L 234.950466 164.720001 \n",
       "L 236.981588 164.198373 \n",
       "L 237.489368 163.44001 \n",
       "L 237.997149 163.628411 \n",
       "L 238.504929 163.502615 \n",
       "L 240.028271 164.060321 \n",
       "L 240.536051 163.934736 \n",
       "L 241.043832 164.118165 \n",
       "L 241.551612 163.07123 \n",
       "L 242.567173 162.829102 \n",
       "L 244.598295 163.560729 \n",
       "L 245.106076 163.44001 \n",
       "L 245.613856 163.619999 \n",
       "L 246.629417 163.380364 \n",
       "L 247.137198 163.856285 \n",
       "L 248.152759 163.026169 \n",
       "L 249.16832 162.793473 \n",
       "L 249.676101 162.971164 \n",
       "L 250.183881 163.44001 \n",
       "L 250.691662 162.449492 \n",
       "L 251.199442 162.336148 \n",
       "L 252.722784 162.864009 \n",
       "L 253.230564 162.75077 \n",
       "L 253.738345 162.351829 \n",
       "L 255.769467 161.911011 \n",
       "L 256.277247 162.36706 \n",
       "L 256.785028 162.538998 \n",
       "L 257.292808 162.429199 \n",
       "L 258.308369 162.769861 \n",
       "L 258.81615 162.381885 \n",
       "L 259.831711 162.166162 \n",
       "L 260.339491 162.335352 \n",
       "L 260.847272 162.228197 \n",
       "L 261.355052 162.396305 \n",
       "L 261.862833 161.741747 \n",
       "L 262.370613 161.910245 \n",
       "L 262.878394 161.805418 \n",
       "L 263.386174 161.429443 \n",
       "L 264.909516 161.930703 \n",
       "L 265.417296 161.827213 \n",
       "L 265.925077 162.26043 \n",
       "L 266.940638 162.586673 \n",
       "L 267.448418 162.216571 \n",
       "L 267.956199 162.644216 \n",
       "L 268.463979 162.540485 \n",
       "L 269.47954 162.861005 \n",
       "L 270.495101 162.13091 \n",
       "L 271.002882 162.029853 \n",
       "L 271.510662 161.668838 \n",
       "L 272.018443 161.569493 \n",
       "L 272.526223 161.729776 \n",
       "L 273.034004 162.147702 \n",
       "L 273.541784 162.305687 \n",
       "L 274.049565 161.691429 \n",
       "L 274.557345 162.106261 \n",
       "L 275.065126 162.263149 \n",
       "L 275.572906 161.908864 \n",
       "L 276.080687 161.810916 \n",
       "L 277.096248 162.629556 \n",
       "L 277.604028 162.277899 \n",
       "L 278.111809 161.676003 \n",
       "L 278.619589 162.082604 \n",
       "L 279.12737 161.985678 \n",
       "L 280.650712 160.95112 \n",
       "L 281.158492 161.10622 \n",
       "L 281.666273 161.012883 \n",
       "L 282.174053 160.425887 \n",
       "L 283.189614 159.752209 \n",
       "L 283.697395 159.908331 \n",
       "L 284.205175 160.308353 \n",
       "L 285.220736 160.128709 \n",
       "L 285.728517 160.282413 \n",
       "L 286.236297 160.193087 \n",
       "L 286.744078 160.587627 \n",
       "L 287.251858 160.739152 \n",
       "L 287.759639 160.64936 \n",
       "L 288.267419 160.320009 \n",
       "L 289.28298 160.621423 \n",
       "L 289.790761 160.294474 \n",
       "L 290.298541 160.444528 \n",
       "L 291.314102 160.2693 \n",
       "L 291.821883 159.474107 \n",
       "L 292.837444 159.304617 \n",
       "L 293.345224 159.454896 \n",
       "L 293.853005 159.370586 \n",
       "L 294.360785 159.053341 \n",
       "L 294.868566 158.970353 \n",
       "L 295.884127 158.342081 \n",
       "L 296.391907 158.723674 \n",
       "L 296.899688 158.642203 \n",
       "L 297.407468 158.330966 \n",
       "L 297.915249 158.70999 \n",
       "L 298.423029 158.4 \n",
       "L 298.93081 158.777146 \n",
       "L 299.43859 158.468418 \n",
       "L 299.946371 158.38863 \n",
       "L 300.454151 158.082169 \n",
       "L 301.469712 157.925386 \n",
       "L 302.485273 158.670006 \n",
       "L 302.993054 158.366324 \n",
       "L 303.500834 158.512007 \n",
       "L 304.008615 158.210022 \n",
       "L 304.516395 158.355405 \n",
       "L 305.024176 157.832591 \n",
       "L 305.531956 157.756137 \n",
       "L 306.039737 156.572321 \n",
       "L 306.547517 156.720002 \n",
       "L 307.055298 157.087625 \n",
       "L 307.563078 156.573283 \n",
       "L 308.070859 156.280791 \n",
       "L 308.578639 156.208708 \n",
       "L 309.08642 156.355577 \n",
       "L 309.5942 156.720002 \n",
       "L 310.101981 156.865144 \n",
       "L 310.609761 156.792415 \n",
       "L 311.117542 156.06968 \n",
       "L 311.625322 156.431596 \n",
       "L 312.133103 156.360264 \n",
       "L 313.656445 156.791499 \n",
       "L 314.164225 156.505991 \n",
       "L 314.672006 156.64882 \n",
       "L 315.687567 156.507343 \n",
       "L 316.195347 156.649271 \n",
       "L 316.703128 156.367068 \n",
       "L 317.210908 155.87473 \n",
       "L 317.718689 156.227949 \n",
       "L 318.226469 156.369276 \n",
       "L 318.73425 156.300002 \n",
       "L 319.24203 156.021464 \n",
       "L 319.749811 155.953197 \n",
       "L 320.765372 156.234063 \n",
       "L 321.780933 156.097783 \n",
       "L 322.288713 155.61611 \n",
       "L 322.796494 155.342964 \n",
       "L 323.304274 155.483194 \n",
       "L 323.812055 155.41715 \n",
       "L 324.827616 155.695612 \n",
       "L 325.335396 155.629538 \n",
       "L 326.350957 155.905462 \n",
       "L 326.858738 155.636132 \n",
       "L 327.874299 156.315188 \n",
       "L 328.382079 156.450671 \n",
       "L 328.88986 156.384011 \n",
       "L 329.39764 156.116408 \n",
       "L 329.905421 155.649094 \n",
       "L 330.413201 155.784818 \n",
       "L 330.920982 155.720006 \n",
       "L 331.428762 155.855054 \n",
       "L 331.936543 155.790361 \n",
       "L 332.444323 155.527114 \n",
       "L 332.952104 155.661742 \n",
       "L 333.459884 155.5978 \n",
       "L 333.967665 155.92942 \n",
       "L 334.475445 155.865207 \n",
       "L 334.983226 155.407507 \n",
       "L 335.491006 155.541053 \n",
       "L 335.998787 154.693543 \n",
       "L 336.506567 154.632245 \n",
       "L 337.014348 154.961867 \n",
       "L 337.522128 154.705304 \n",
       "L 338.537689 154.972036 \n",
       "L 339.04547 154.910783 \n",
       "L 339.55325 155.23671 \n",
       "L 340.061031 155.175172 \n",
       "L 342.092153 155.697956 \n",
       "L 342.599933 156.018686 \n",
       "L 343.107714 155.765457 \n",
       "L 343.615494 155.703753 \n",
       "L 344.123275 156.022651 \n",
       "L 344.631055 155.770849 \n",
       "L 345.646617 155.648343 \n",
       "L 346.154397 155.39866 \n",
       "L 346.662178 155.526738 \n",
       "L 347.169958 155.278211 \n",
       "L 350.216641 156.038092 \n",
       "L 350.724422 155.606196 \n",
       "L 351.232202 155.917073 \n",
       "L 351.739983 155.856886 \n",
       "L 352.247763 155.98154 \n",
       "L 353.263324 155.861618 \n",
       "L 353.771105 155.434754 \n",
       "L 354.278885 155.376009 \n",
       "L 354.786666 155.683341 \n",
       "L 355.294446 155.624356 \n",
       "L 355.802227 155.93014 \n",
       "L 356.310007 155.870915 \n",
       "L 356.817788 155.448663 \n",
       "L 357.325568 155.5718 \n",
       "L 357.833349 155.513551 \n",
       "L 358.341129 155.094197 \n",
       "L 358.84891 155.217319 \n",
       "L 359.35669 155.160001 \n",
       "L 359.864471 155.282567 \n",
       "L 360.372251 155.584056 \n",
       "L 360.880032 155.347365 \n",
       "L 361.387812 155.290213 \n",
       "L 361.895593 155.590094 \n",
       "L 362.403373 155.176539 \n",
       "L 363.418934 155.063675 \n",
       "L 364.434495 154.597909 \n",
       "L 364.942276 154.542769 \n",
       "L 365.450056 154.135387 \n",
       "L 365.957837 154.081268 \n",
       "L 366.465617 154.378536 \n",
       "L 366.973398 154.49948 \n",
       "L 367.481178 154.795006 \n",
       "L 367.988959 154.740107 \n",
       "L 369.00452 154.979081 \n",
       "L 369.5123 155.271738 \n",
       "L 370.020081 154.869402 \n",
       "L 371.035642 154.760489 \n",
       "L 371.543422 155.051509 \n",
       "L 372.558983 155.286563 \n",
       "L 374.590105 155.068482 \n",
       "L 375.097886 155.18498 \n",
       "L 377.636788 154.91598 \n",
       "L 378.65235 155.146768 \n",
       "L 379.16013 155.429857 \n",
       "L 379.667911 155.544012 \n",
       "L 380.175691 155.825494 \n",
       "L 380.683472 155.603733 \n",
       "L 381.191252 155.884193 \n",
       "L 381.699033 155.996831 \n",
       "L 382.206813 155.775881 \n",
       "L 382.714594 155.721988 \n",
       "L 383.222374 155.336145 \n",
       "L 384.237935 155.892424 \n",
       "L 385.253496 155.785149 \n",
       "L 385.761277 155.567068 \n",
       "L 386.269057 155.514136 \n",
       "L 386.776838 155.625542 \n",
       "L 387.284618 155.572686 \n",
       "L 387.792399 155.683641 \n",
       "L 388.80796 155.578258 \n",
       "L 390.331301 155.908406 \n",
       "L 390.839082 155.531575 \n",
       "L 391.346862 155.803158 \n",
       "L 391.854643 155.912311 \n",
       "L 392.362423 155.537283 \n",
       "L 392.870204 155.646526 \n",
       "L 393.377984 155.594646 \n",
       "L 393.885765 155.863961 \n",
       "L 394.901326 155.760005 \n",
       "L 395.409106 156.027773 \n",
       "L 396.932448 155.872057 \n",
       "L 397.948009 156.403027 \n",
       "L 398.455789 156.509011 \n",
       "L 398.96357 156.298695 \n",
       "L 399.47135 156.404514 \n",
       "L 400.486911 156.300663 \n",
       "L 400.994692 156.562994 \n",
       "L 401.502472 156.667746 \n",
       "L 402.010253 156.615655 \n",
       "L 402.518033 156.407443 \n",
       "L 403.025814 156.668001 \n",
       "L 405.056936 157.081857 \n",
       "L 405.564716 157.339367 \n",
       "L 406.072497 157.441475 \n",
       "L 406.580277 157.388919 \n",
       "L 407.088058 157.028265 \n",
       "L 408.611399 156.873435 \n",
       "L 409.11918 156.668947 \n",
       "L 410.134741 156.872729 \n",
       "L 410.642521 157.126663 \n",
       "L 411.150302 156.923032 \n",
       "L 411.658083 157.024074 \n",
       "L 412.165863 156.97302 \n",
       "L 412.673644 157.073686 \n",
       "L 413.181424 157.022707 \n",
       "L 413.689205 157.122998 \n",
       "L 414.196985 157.072109 \n",
       "L 414.704766 157.322694 \n",
       "L 416.228107 156.720002 \n",
       "L 416.735888 156.819857 \n",
       "L 417.243668 156.769854 \n",
       "L 417.751449 157.018667 \n",
       "L 418.259229 156.819421 \n",
       "L 418.76701 156.918526 \n",
       "L 419.782571 157.412786 \n",
       "L 420.290351 157.510599 \n",
       "L 420.798132 157.164064 \n",
       "L 421.305912 157.114136 \n",
       "L 421.813693 157.211949 \n",
       "L 422.829254 156.818115 \n",
       "L 423.844815 156.720002 \n",
       "L 424.352595 156.817679 \n",
       "L 424.860376 156.768773 \n",
       "L 425.875937 156.379625 \n",
       "L 426.891498 156.574559 \n",
       "L 427.399278 156.23585 \n",
       "L 427.907059 156.478279 \n",
       "L 429.4304 156.768142 \n",
       "L 429.938181 156.575806 \n",
       "L 430.445961 156.816012 \n",
       "L 430.953742 156.911737 \n",
       "L 431.461522 156.863596 \n",
       "L 432.477083 157.340463 \n",
       "L 432.984864 157.148938 \n",
       "L 433.492644 157.243522 \n",
       "L 434.000425 157.052673 \n",
       "L 434.508205 156.720002 \n",
       "L 435.015986 156.672612 \n",
       "L 436.539327 156.955957 \n",
       "L 437.047108 156.767136 \n",
       "L 437.554888 156.861178 \n",
       "L 438.062669 156.813999 \n",
       "L 438.570449 156.907711 \n",
       "L 439.07823 156.860592 \n",
       "L 440.601571 156.300002 \n",
       "L 441.109352 156.253995 \n",
       "L 441.617132 156.347707 \n",
       "L 442.124913 156.022906 \n",
       "L 442.632693 156.116693 \n",
       "L 443.140474 156.071182 \n",
       "L 444.156035 156.535131 \n",
       "L 444.663816 156.627701 \n",
       "L 445.171596 156.443462 \n",
       "L 445.679377 156.673979 \n",
       "L 446.187157 156.628077 \n",
       "L 446.694938 156.444603 \n",
       "L 447.202718 156.674175 \n",
       "L 447.710499 156.765784 \n",
       "L 448.218279 156.994289 \n",
       "L 448.72606 157.085222 \n",
       "L 449.23384 156.628828 \n",
       "L 449.741621 156.720002 \n",
       "L 450.249401 156.538135 \n",
       "L 451.264962 156.720002 \n",
       "L 452.280523 156.358236 \n",
       "L 452.788304 156.584518 \n",
       "L 453.296084 156.539607 \n",
       "L 454.311645 156.720002 \n",
       "L 455.327206 156.361135 \n",
       "L 455.834987 156.31681 \n",
       "L 456.342767 156.138383 \n",
       "L 456.850548 155.826395 \n",
       "L 457.358328 155.916817 \n",
       "L 457.866109 155.739637 \n",
       "L 458.373889 155.696424 \n",
       "L 458.88167 155.920002 \n",
       "L 459.897231 155.567504 \n",
       "L 460.912792 155.482113 \n",
       "L 461.420572 155.307126 \n",
       "L 462.943914 155.180744 \n",
       "L 463.451694 155.402355 \n",
       "L 464.467255 155.318181 \n",
       "L 464.975036 155.013763 \n",
       "L 465.482816 155.103359 \n",
       "L 465.990597 154.93091 \n",
       "L 466.498377 154.889649 \n",
       "L 467.006158 155.109638 \n",
       "L 468.021719 155.026981 \n",
       "L 468.529499 155.245948 \n",
       "L 469.03728 154.944744 \n",
       "L 469.54506 154.903784 \n",
       "L 470.052841 154.992494 \n",
       "L 470.560621 154.692792 \n",
       "L 471.068402 154.652313 \n",
       "L 471.576182 154.870078 \n",
       "L 472.083963 154.700573 \n",
       "L 472.591743 154.917707 \n",
       "L 473.099524 154.877152 \n",
       "L 473.607304 154.965097 \n",
       "L 474.622865 154.884167 \n",
       "L 475.130646 154.971781 \n",
       "L 476.146207 154.891151 \n",
       "L 476.653988 154.97842 \n",
       "L 479.19289 154.778303 \n",
       "L 479.700671 154.991518 \n",
       "L 480.208451 154.951578 \n",
       "L 480.716232 155.0379 \n",
       "L 481.224012 154.872001 \n",
       "L 481.731793 155.084058 \n",
       "L 482.239573 155.044194 \n",
       "L 482.747354 155.129975 \n",
       "L 483.255134 155.090156 \n",
       "L 484.270695 154.63564 \n",
       "L 485.286256 154.807143 \n",
       "L 486.301817 154.728901 \n",
       "L 487.317378 154.278621 \n",
       "L 487.825159 154.240304 \n",
       "L 488.332939 154.078234 \n",
       "L 488.84072 154.16394 \n",
       "L 489.3485 154.125894 \n",
       "L 489.856281 154.334692 \n",
       "L 490.364061 154.29654 \n",
       "L 490.871842 154.504617 \n",
       "L 491.379622 154.343419 \n",
       "L 491.887403 154.428164 \n",
       "L 492.395183 154.267446 \n",
       "L 492.902964 154.352085 \n",
       "L 493.410744 154.558841 \n",
       "L 493.918525 154.520734 \n",
       "L 494.426305 154.604758 \n",
       "L 494.934086 154.810447 \n",
       "L 495.441866 154.650435 \n",
       "L 497.472988 154.983467 \n",
       "L 498.488549 154.907058 \n",
       "L 499.50411 155.072162 \n",
       "L 500.011891 155.03398 \n",
       "L 501.535232 155.280014 \n",
       "L 502.043013 155.241772 \n",
       "L 502.550793 155.083908 \n",
       "L 503.058574 155.165559 \n",
       "L 503.566354 155.127587 \n",
       "L 504.074135 154.851137 \n",
       "L 504.581915 154.932773 \n",
       "L 505.089696 154.895207 \n",
       "L 505.597476 154.976617 \n",
       "L 506.105257 154.701639 \n",
       "L 507.120818 154.86431 \n",
       "L 507.628598 154.827045 \n",
       "L 509.659721 155.149907 \n",
       "L 510.167501 154.877303 \n",
       "L 510.675282 154.840292 \n",
       "L 511.183062 154.686018 \n",
       "L 511.690843 154.649308 \n",
       "L 512.198623 154.729758 \n",
       "L 512.706404 154.92696 \n",
       "L 513.214184 154.773317 \n",
       "L 513.721965 154.736667 \n",
       "L 514.229745 154.81665 \n",
       "L 514.737526 155.012847 \n",
       "L 515.245306 154.859804 \n",
       "L 515.753087 155.055489 \n",
       "L 516.260867 154.670758 \n",
       "L 516.768648 154.634483 \n",
       "L 517.784209 154.908996 \n",
       "L 518.79977 155.06692 \n",
       "L 519.30755 155.030405 \n",
       "L 519.815331 155.109052 \n",
       "L 520.323111 155.072567 \n",
       "L 521.338672 155.229215 \n",
       "L 521.846453 155.19273 \n",
       "L 522.354233 155.385156 \n",
       "L 523.369794 155.312082 \n",
       "L 524.385355 155.694922 \n",
       "L 527.432038 155.475564 \n",
       "L 527.939819 155.552288 \n",
       "L 528.447599 155.741725 \n",
       "L 528.95538 155.592483 \n",
       "L 531.494282 155.972513 \n",
       "L 532.509843 155.787702 \n",
       "L 533.017624 155.751488 \n",
       "L 533.525404 155.38047 \n",
       "L 534.540965 155.420559 \n",
       "L 535.048746 155.162389 \n",
       "L 535.556526 155.238197 \n",
       "L 536.064307 155.091809 \n",
       "L 536.572087 155.278422 \n",
       "L 537.587648 155.097171 \n",
       "L 539.11099 155.102503 \n",
       "L 540.126551 155.472845 \n",
       "L 541.142112 155.512169 \n",
       "L 542.665454 155.844434 \n",
       "L 543.681015 156.101162 \n",
       "L 544.696576 156.247791 \n",
       "L 545.204356 156.10316 \n",
       "L 545.712137 156.285057 \n",
       "L 546.727698 156.105158 \n",
       "L 547.235478 155.852906 \n",
       "L 547.743259 155.817758 \n",
       "L 548.75882 156.071783 \n",
       "L 552.313283 156.684268 \n",
       "L 553.328844 156.934012 \n",
       "L 554.852186 157.040011 \n",
       "L 560.437771 156.755149 \n",
       "L 563.992235 157.243371 \n",
       "L 565.007796 157.068189 \n",
       "L 568.562259 156.823717 \n",
       "L 569.57782 157.064975 \n",
       "L 571.608942 157.235351 \n",
       "L 572.624503 156.960012 \n",
       "L 573.640064 156.993733 \n",
       "L 574.147845 156.651644 \n",
       "L 575.163406 156.788224 \n",
       "L 577.194528 156.346295 \n",
       "L 579.22565 156.313971 \n",
       "L 580.241211 156.348548 \n",
       "L 580.748992 156.21398 \n",
       "L 581.764553 156.450671 \n",
       "L 582.272333 156.215497 \n",
       "L 582.780114 156.182408 \n",
       "L 582.780114 156.182408 \n",
       "\" clip-path=\"url(#p6070896839)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 75.507386 22.32 \n",
       "L 76.015167 123.120005 \n",
       "L 76.522947 123.120005 \n",
       "L 77.030728 148.32001 \n",
       "L 77.538508 143.28 \n",
       "L 78.046289 173.52 \n",
       "L 78.554069 137.520004 \n",
       "L 79.06185 110.520003 \n",
       "L 80.077411 133.20001 \n",
       "L 81.092972 148.32001 \n",
       "L 81.600752 146.381541 \n",
       "L 82.108533 151.920002 \n",
       "L 82.616313 163.44001 \n",
       "L 83.124094 148.32001 \n",
       "L 83.631874 146.83765 \n",
       "L 84.139655 139.920003 \n",
       "L 84.647436 149.646324 \n",
       "L 85.155216 153.360005 \n",
       "L 85.662997 151.920002 \n",
       "L 86.170777 159.774559 \n",
       "L 86.678558 162.563481 \n",
       "L 87.186338 160.920013 \n",
       "L 87.694119 163.44001 \n",
       "L 89.21746 159.120001 \n",
       "L 89.725241 154.402764 \n",
       "L 90.740802 158.887741 \n",
       "L 91.248582 157.770001 \n",
       "L 91.756363 153.665459 \n",
       "L 92.771924 163.44001 \n",
       "L 93.279704 165.120008 \n",
       "L 93.787485 161.260554 \n",
       "L 94.295265 162.909476 \n",
       "L 94.803046 167.058462 \n",
       "L 95.310826 168.480005 \n",
       "L 95.818607 164.91513 \n",
       "L 96.326387 163.920001 \n",
       "L 96.834168 167.659547 \n",
       "L 97.341948 164.356373 \n",
       "L 97.849729 165.680013 \n",
       "L 98.357509 169.137402 \n",
       "L 98.86529 168.158299 \n",
       "L 99.880851 162.205726 \n",
       "L 100.388631 161.424006 \n",
       "L 101.404192 167.704623 \n",
       "L 102.419753 166.053344 \n",
       "L 102.927534 167.105461 \n",
       "L 103.435314 166.320001 \n",
       "L 103.943095 167.330526 \n",
       "L 104.450875 166.568287 \n",
       "L 104.958656 169.248823 \n",
       "L 105.466436 165.120008 \n",
       "L 105.974217 166.083941 \n",
       "L 106.989778 161.520001 \n",
       "L 107.497558 157.770001 \n",
       "L 109.52868 161.661178 \n",
       "L 110.544241 160.560004 \n",
       "L 111.052022 161.452395 \n",
       "L 111.559802 159.520009 \n",
       "L 112.067583 159.021378 \n",
       "L 113.083144 163.44001 \n",
       "L 113.590924 161.583162 \n",
       "L 114.098705 161.083644 \n",
       "L 115.114266 162.674437 \n",
       "L 115.622046 164.700009 \n",
       "L 117.145388 166.840487 \n",
       "L 117.653169 166.320001 \n",
       "L 118.160949 164.625883 \n",
       "L 118.66873 166.487448 \n",
       "L 119.684291 163.210919 \n",
       "L 120.192071 162.760458 \n",
       "L 120.699852 163.44001 \n",
       "L 121.207632 162.996924 \n",
       "L 121.715413 160.372175 \n",
       "L 122.223193 156.720002 \n",
       "L 123.238754 160.256848 \n",
       "L 123.746535 157.770001 \n",
       "L 124.254315 157.412786 \n",
       "L 124.762096 155.005727 \n",
       "L 125.269876 156.720002 \n",
       "L 126.285437 158.050701 \n",
       "L 126.793218 156.720002 \n",
       "L 127.300998 156.393789 \n",
       "L 127.808779 158.012309 \n",
       "L 128.82434 159.255861 \n",
       "L 129.33212 160.802253 \n",
       "L 129.839901 161.38668 \n",
       "L 130.347681 160.110835 \n",
       "L 130.855462 160.690922 \n",
       "L 131.363242 160.352438 \n",
       "L 132.378803 161.477524 \n",
       "L 132.886584 161.141052 \n",
       "L 133.394364 161.686968 \n",
       "L 134.409925 159.304617 \n",
       "L 135.425486 160.39059 \n",
       "L 135.933267 160.080013 \n",
       "L 136.441047 160.607604 \n",
       "L 136.948828 160.300332 \n",
       "L 138.472169 161.827213 \n",
       "L 138.97995 163.120001 \n",
       "L 139.48773 162.805039 \n",
       "L 140.503291 163.752569 \n",
       "L 141.011072 164.990773 \n",
       "L 142.026633 165.883644 \n",
       "L 143.042194 165.245383 \n",
       "L 143.549974 166.426675 \n",
       "L 144.057755 166.849424 \n",
       "L 144.565535 166.530226 \n",
       "L 145.073316 164.754788 \n",
       "L 146.088877 164.160011 \n",
       "L 146.596657 164.583841 \n",
       "L 147.612218 162.594138 \n",
       "L 148.119999 162.320001 \n",
       "L 148.627779 161.354492 \n",
       "L 149.64334 162.205726 \n",
       "L 150.151121 161.260554 \n",
       "L 150.658902 161.004577 \n",
       "L 151.166682 160.080013 \n",
       "L 151.674463 159.835241 \n",
       "L 152.182243 158.930534 \n",
       "L 152.690024 158.696472 \n",
       "L 153.197804 157.810916 \n",
       "L 153.705585 158.887741 \n",
       "L 154.213365 158.658471 \n",
       "L 154.721146 159.074144 \n",
       "L 155.228926 158.846586 \n",
       "L 155.736707 157.987931 \n",
       "L 156.244487 158.4 \n",
       "L 156.752268 159.433056 \n",
       "L 157.260048 158.586673 \n",
       "L 157.767829 158.987492 \n",
       "L 158.275609 158.76878 \n",
       "L 158.78339 159.163636 \n",
       "L 159.29117 158.339288 \n",
       "L 159.798951 158.73198 \n",
       "L 160.306731 157.920009 \n",
       "L 161.322292 157.510599 \n",
       "L 161.830073 158.488425 \n",
       "L 163.353414 157.878628 \n",
       "L 164.368975 158.629091 \n",
       "L 164.876756 157.858996 \n",
       "L 165.384536 158.23012 \n",
       "L 165.892317 159.160226 \n",
       "L 167.415658 160.227694 \n",
       "L 167.923439 158.92328 \n",
       "L 168.431219 159.276529 \n",
       "L 168.939 160.170811 \n",
       "L 169.44678 159.429691 \n",
       "L 170.462341 160.115747 \n",
       "L 170.970122 160.986673 \n",
       "L 171.985683 161.645662 \n",
       "L 173.001244 160.201874 \n",
       "L 173.509024 161.049909 \n",
       "L 174.016805 160.855395 \n",
       "L 174.524585 161.691429 \n",
       "L 175.032366 161.495638 \n",
       "L 175.540146 161.810916 \n",
       "L 176.047927 161.616492 \n",
       "L 177.063488 162.236428 \n",
       "L 177.571268 162.042785 \n",
       "L 178.079049 161.354492 \n",
       "L 178.586829 162.155303 \n",
       "L 179.09461 162.456596 \n",
       "L 179.60239 161.776324 \n",
       "L 180.110171 162.076521 \n",
       "L 180.617951 161.889232 \n",
       "L 181.125732 160.739152 \n",
       "L 181.633512 161.520001 \n",
       "L 182.141293 160.860292 \n",
       "L 182.649074 160.68227 \n",
       "L 183.156854 160.979163 \n",
       "L 183.664635 161.7443 \n",
       "L 184.680196 161.38668 \n",
       "L 185.187976 160.745806 \n",
       "L 185.695757 161.497981 \n",
       "L 186.203537 159.941931 \n",
       "L 186.711318 160.690922 \n",
       "L 187.726879 158.53622 \n",
       "L 188.74244 159.120001 \n",
       "L 189.25022 159.856 \n",
       "L 189.758001 159.693464 \n",
       "L 190.265781 159.976388 \n",
       "L 190.773562 159.372644 \n",
       "L 191.281342 159.214329 \n",
       "L 191.789123 159.933925 \n",
       "L 192.296903 159.774559 \n",
       "L 193.312464 160.325161 \n",
       "L 193.820245 159.735386 \n",
       "L 194.328025 158.721706 \n",
       "L 194.835806 158.143737 \n",
       "L 195.343586 157.995952 \n",
       "L 196.359147 158.54762 \n",
       "L 196.866928 157.560001 \n",
       "L 197.374708 156.99884 \n",
       "L 197.882489 156.85885 \n",
       "L 198.390269 157.134819 \n",
       "L 198.89805 156.582295 \n",
       "L 199.40583 156.857153 \n",
       "L 199.913611 156.310246 \n",
       "L 200.421391 156.992066 \n",
       "L 201.436952 156.720002 \n",
       "L 201.944733 156.988806 \n",
       "L 202.960294 156.720002 \n",
       "L 203.468074 156.985622 \n",
       "L 203.975855 156.455433 \n",
       "L 204.483635 156.720002 \n",
       "L 204.991416 155.801251 \n",
       "L 205.499196 156.066315 \n",
       "L 206.514757 155.8119 \n",
       "L 207.022538 155.298474 \n",
       "L 208.038099 155.052831 \n",
       "L 208.545879 155.314681 \n",
       "L 210.069221 154.951578 \n",
       "L 211.084782 156.218517 \n",
       "L 211.592562 156.470198 \n",
       "L 212.100343 157.093333 \n",
       "L 213.115904 157.584709 \n",
       "L 213.623684 157.458463 \n",
       "L 214.131465 157.701027 \n",
       "L 214.639245 157.575277 \n",
       "L 215.147026 158.180883 \n",
       "L 216.162587 158.653814 \n",
       "L 217.685929 157.198295 \n",
       "L 218.193709 157.792351 \n",
       "L 218.70149 158.026008 \n",
       "L 219.20927 157.903111 \n",
       "L 219.717051 158.13474 \n",
       "L 220.224831 158.717215 \n",
       "L 220.732612 158.944398 \n",
       "L 222.763734 158.45197 \n",
       "L 223.271514 159.021378 \n",
       "L 223.779295 158.898842 \n",
       "L 224.287075 158.434292 \n",
       "L 224.794856 158.656278 \n",
       "L 225.302636 158.53622 \n",
       "L 225.810417 158.756373 \n",
       "L 226.318197 159.313299 \n",
       "L 228.349319 160.169009 \n",
       "L 228.8571 160.046743 \n",
       "L 229.872661 159.143613 \n",
       "L 230.380441 159.684707 \n",
       "L 231.396002 160.101823 \n",
       "L 231.903783 159.982141 \n",
       "L 232.411563 160.1884 \n",
       "L 232.919344 160.069199 \n",
       "L 233.934905 159.18902 \n",
       "L 235.458246 159.803548 \n",
       "L 235.966027 160.323794 \n",
       "L 236.981588 160.090543 \n",
       "L 237.997149 159.232159 \n",
       "L 239.52049 159.831111 \n",
       "L 240.028271 159.097846 \n",
       "L 241.043832 159.494324 \n",
       "L 241.551612 159.383429 \n",
       "L 243.582734 160.160973 \n",
       "L 244.090515 159.747027 \n",
       "L 245.613856 159.420003 \n",
       "L 246.121637 159.910508 \n",
       "L 246.629417 159.801656 \n",
       "L 247.137198 160.288151 \n",
       "L 247.644979 160.475304 \n",
       "L 248.66054 161.435797 \n",
       "L 249.16832 161.617964 \n",
       "L 249.676101 160.62698 \n",
       "L 250.183881 160.810439 \n",
       "L 250.691662 160.701511 \n",
       "L 251.199442 160.883693 \n",
       "L 251.707223 161.354492 \n",
       "L 252.215003 161.244933 \n",
       "L 252.722784 161.712007 \n",
       "L 253.230564 161.314883 \n",
       "L 253.738345 161.206376 \n",
       "L 254.246125 160.527365 \n",
       "L 254.753906 160.421697 \n",
       "L 255.261686 160.032684 \n",
       "L 255.769467 160.495282 \n",
       "L 256.277247 159.825884 \n",
       "L 257.800589 160.360008 \n",
       "L 258.308369 160.81529 \n",
       "L 258.81615 160.711169 \n",
       "L 259.32393 160.8853 \n",
       "L 259.831711 160.78154 \n",
       "L 260.339491 160.954529 \n",
       "L 260.847272 161.401971 \n",
       "L 261.355052 161.29767 \n",
       "L 261.862833 161.467835 \n",
       "L 262.370613 161.090734 \n",
       "L 262.878394 160.988115 \n",
       "L 263.386174 160.614348 \n",
       "L 264.401735 161.494271 \n",
       "L 264.909516 161.122142 \n",
       "L 265.417296 161.020799 \n",
       "L 265.925077 160.651914 \n",
       "L 266.432857 160.819737 \n",
       "L 266.940638 160.720001 \n",
       "L 267.448418 161.152723 \n",
       "L 267.956199 161.052642 \n",
       "L 268.463979 161.217641 \n",
       "L 269.47954 159.965964 \n",
       "L 269.987321 159.607503 \n",
       "L 270.495101 160.036364 \n",
       "L 271.002882 159.940729 \n",
       "L 272.018443 160.270517 \n",
       "L 272.526223 160.175017 \n",
       "L 273.541784 159.469886 \n",
       "L 274.557345 159.797871 \n",
       "L 275.065126 159.704774 \n",
       "L 276.080687 160.538195 \n",
       "L 276.588467 159.936133 \n",
       "L 277.096248 160.096896 \n",
       "L 277.604028 160.004221 \n",
       "L 278.111809 160.416004 \n",
       "L 279.63515 160.88874 \n",
       "L 280.142931 160.795253 \n",
       "L 280.650712 161.200007 \n",
       "L 281.158492 160.36139 \n",
       "L 281.666273 160.269886 \n",
       "L 282.174053 160.672942 \n",
       "L 283.697395 161.134609 \n",
       "L 284.205175 160.797672 \n",
       "L 284.712956 160.950519 \n",
       "L 285.220736 160.615655 \n",
       "L 285.728517 160.768202 \n",
       "L 286.236297 160.677704 \n",
       "L 286.744078 161.071087 \n",
       "L 287.251858 160.498 \n",
       "L 287.759639 160.408794 \n",
       "L 288.7752 160.710508 \n",
       "L 289.28298 160.621423 \n",
       "L 290.298541 160.920013 \n",
       "L 290.806322 161.30542 \n",
       "L 291.821883 161.598693 \n",
       "L 292.329663 161.97982 \n",
       "L 292.837444 162.124196 \n",
       "L 293.345224 162.033488 \n",
       "L 293.853005 161.709333 \n",
       "L 294.360785 161.853333 \n",
       "L 294.868566 162.229473 \n",
       "L 295.376346 162.139366 \n",
       "L 295.884127 162.281384 \n",
       "L 296.391907 162.191562 \n",
       "L 296.899688 162.563481 \n",
       "L 298.423029 162.294557 \n",
       "L 298.93081 162.434291 \n",
       "L 299.43859 162.801449 \n",
       "L 299.946371 162.711883 \n",
       "L 300.454151 163.076757 \n",
       "L 300.961932 163.213488 \n",
       "L 301.469712 163.575614 \n",
       "L 302.485273 163.395009 \n",
       "L 302.993054 163.080813 \n",
       "L 303.500834 163.216011 \n",
       "L 304.008615 163.574112 \n",
       "L 305.024176 163.840543 \n",
       "L 305.531956 163.52881 \n",
       "L 306.547517 164.235789 \n",
       "L 307.563078 164.056251 \n",
       "L 308.578639 164.316524 \n",
       "L 309.08642 164.227168 \n",
       "L 309.5942 163.70183 \n",
       "L 311.117542 164.090332 \n",
       "L 311.625322 164.002403 \n",
       "L 312.133103 164.130707 \n",
       "L 312.640883 163.827701 \n",
       "L 313.148664 163.311045 \n",
       "L 314.164225 163.568419 \n",
       "L 315.179786 164.249818 \n",
       "L 315.687567 164.375704 \n",
       "L 316.195347 163.864426 \n",
       "L 316.703128 163.567067 \n",
       "L 317.210908 163.693584 \n",
       "L 318.226469 163.524184 \n",
       "L 318.73425 162.810011 \n",
       "L 319.24203 162.937053 \n",
       "L 319.749811 162.854441 \n",
       "L 321.273152 163.232173 \n",
       "L 321.780933 162.942235 \n",
       "L 322.288713 163.274425 \n",
       "L 323.812055 163.028572 \n",
       "L 324.319835 163.152595 \n",
       "L 325.335396 162.581265 \n",
       "L 325.843177 162.909476 \n",
       "L 326.350957 162.829102 \n",
       "L 327.366518 163.07494 \n",
       "L 328.382079 162.914793 \n",
       "L 328.88986 163.036803 \n",
       "L 329.905421 162.877783 \n",
       "L 330.920982 163.120001 \n",
       "L 331.428762 162.641587 \n",
       "L 331.936543 162.961897 \n",
       "L 332.444323 163.082135 \n",
       "L 332.952104 163.003473 \n",
       "L 333.459884 162.727083 \n",
       "L 334.475445 162.966583 \n",
       "L 334.983226 163.282506 \n",
       "L 335.491006 163.400702 \n",
       "L 335.998787 163.714552 \n",
       "L 336.506567 163.831456 \n",
       "L 337.014348 163.752569 \n",
       "L 337.522128 163.479003 \n",
       "L 338.029909 163.401092 \n",
       "L 338.537689 163.711909 \n",
       "L 339.04547 163.633848 \n",
       "L 341.076592 164.094057 \n",
       "L 341.584372 163.824006 \n",
       "L 342.092153 163.938251 \n",
       "L 342.599933 163.860806 \n",
       "L 343.107714 164.165464 \n",
       "L 343.615494 164.278417 \n",
       "L 344.123275 164.200762 \n",
       "L 344.631055 164.313219 \n",
       "L 345.138836 164.046322 \n",
       "L 345.646617 164.15866 \n",
       "L 346.662178 164.005242 \n",
       "L 347.169958 164.117023 \n",
       "L 347.677739 163.852965 \n",
       "L 348.185519 163.777262 \n",
       "L 348.6933 163.514811 \n",
       "L 349.708861 163.365479 \n",
       "L 350.724422 163.588516 \n",
       "L 351.232202 163.884718 \n",
       "L 351.739983 163.809917 \n",
       "L 353.263324 164.138983 \n",
       "L 354.278885 163.989831 \n",
       "L 355.294446 164.572185 \n",
       "L 355.802227 164.314947 \n",
       "L 356.817788 164.529738 \n",
       "L 357.325568 163.911379 \n",
       "L 357.833349 163.83814 \n",
       "L 358.341129 163.584521 \n",
       "L 359.35669 164.160011 \n",
       "L 360.372251 164.37267 \n",
       "L 360.880032 164.2994 \n",
       "L 361.387812 164.405114 \n",
       "L 361.895593 163.975231 \n",
       "L 362.403373 164.081139 \n",
       "L 362.911154 163.653344 \n",
       "L 363.418934 163.404517 \n",
       "L 364.434495 163.970529 \n",
       "L 364.942276 163.898987 \n",
       "L 365.450056 164.180139 \n",
       "L 365.957837 163.932573 \n",
       "L 366.465617 163.861467 \n",
       "L 367.988959 163.125559 \n",
       "L 369.5123 162.918623 \n",
       "L 370.020081 163.197116 \n",
       "L 370.527861 162.608663 \n",
       "L 371.035642 162.540936 \n",
       "L 371.543422 162.818632 \n",
       "L 372.051203 162.406158 \n",
       "L 372.558983 162.511135 \n",
       "L 373.066764 162.444024 \n",
       "L 374.082325 162.652777 \n",
       "L 374.590105 162.92747 \n",
       "L 375.097886 163.03066 \n",
       "L 375.605666 162.963248 \n",
       "L 376.113447 162.726077 \n",
       "L 376.621227 162.829102 \n",
       "L 377.129008 163.101181 \n",
       "L 378.65235 163.406289 \n",
       "L 379.16013 163.170755 \n",
       "L 379.667911 163.104004 \n",
       "L 380.175691 162.869762 \n",
       "L 380.683472 162.803732 \n",
       "L 381.191252 163.072251 \n",
       "L 381.699033 163.006102 \n",
       "L 382.714594 163.207134 \n",
       "L 383.222374 163.47322 \n",
       "L 383.730155 163.406845 \n",
       "L 384.237935 163.009661 \n",
       "L 385.253496 163.538994 \n",
       "L 385.761277 163.47295 \n",
       "L 386.269057 163.078244 \n",
       "L 387.284618 163.276108 \n",
       "L 387.792399 163.210919 \n",
       "L 388.300179 162.982565 \n",
       "L 389.823521 163.27743 \n",
       "L 390.331301 163.050442 \n",
       "L 391.346862 163.245842 \n",
       "L 391.854643 163.02001 \n",
       "L 392.362423 163.278721 \n",
       "L 392.870204 163.214569 \n",
       "L 393.377984 163.472154 \n",
       "L 393.885765 163.568419 \n",
       "L 394.901326 163.120001 \n",
       "L 395.409106 163.216357 \n",
       "L 395.916887 163.152911 \n",
       "L 396.424667 162.771198 \n",
       "L 397.440228 162.963789 \n",
       "L 397.948009 162.742644 \n",
       "L 398.455789 162.996924 \n",
       "L 398.96357 162.934424 \n",
       "L 399.979131 163.125003 \n",
       "L 400.486911 162.905345 \n",
       "L 400.994692 163.157387 \n",
       "L 402.518033 163.44001 \n",
       "L 403.025814 163.221554 \n",
       "L 403.533594 163.15958 \n",
       "L 404.041375 163.253337 \n",
       "L 404.549155 163.191498 \n",
       "L 405.056936 163.44001 \n",
       "L 405.564716 163.378066 \n",
       "L 406.072497 163.007123 \n",
       "L 406.580277 163.254764 \n",
       "L 407.088058 163.039267 \n",
       "L 407.595838 163.132228 \n",
       "L 408.103619 162.917572 \n",
       "L 408.611399 163.010412 \n",
       "L 409.11918 162.490212 \n",
       "L 409.62696 162.583443 \n",
       "L 412.673644 162.22737 \n",
       "L 413.181424 162.320001 \n",
       "L 414.196985 162.202647 \n",
       "L 414.704766 161.993548 \n",
       "L 415.212546 162.085984 \n",
       "L 415.720327 162.328352 \n",
       "L 416.735888 162.511391 \n",
       "L 417.243668 162.303389 \n",
       "L 417.751449 161.94667 \n",
       "L 418.259229 162.187461 \n",
       "L 419.27479 162.072225 \n",
       "L 419.782571 162.163309 \n",
       "L 420.798132 162.640716 \n",
       "L 421.305912 162.730568 \n",
       "L 421.813693 162.672574 \n",
       "L 422.321473 162.467381 \n",
       "L 422.829254 162.409928 \n",
       "L 423.337034 162.205726 \n",
       "L 425.368156 162.563481 \n",
       "L 425.875937 162.506404 \n",
       "L 426.383717 162.740812 \n",
       "L 426.891498 162.683644 \n",
       "L 427.907059 162.859863 \n",
       "L 428.414839 162.368277 \n",
       "L 428.92262 162.167349 \n",
       "L 429.4304 162.111413 \n",
       "L 430.445961 162.576008 \n",
       "L 430.953742 162.519727 \n",
       "L 431.461522 162.607191 \n",
       "L 431.969303 162.551014 \n",
       "L 432.477083 162.351829 \n",
       "L 432.984864 162.010221 \n",
       "L 433.492644 162.09791 \n",
       "L 434.000425 161.900212 \n",
       "L 434.508205 161.987796 \n",
       "L 435.015986 161.932986 \n",
       "L 435.523766 161.73634 \n",
       "L 436.539327 161.911011 \n",
       "L 437.047108 161.856608 \n",
       "L 437.554888 161.661178 \n",
       "L 438.062669 161.607285 \n",
       "L 439.58601 161.867633 \n",
       "L 440.093791 161.813755 \n",
       "L 440.601571 161.620007 \n",
       "L 441.617132 161.792576 \n",
       "L 442.124913 162.017926 \n",
       "L 442.632693 162.103437 \n",
       "L 444.663816 161.889232 \n",
       "L 445.171596 161.974322 \n",
       "L 445.679377 161.921105 \n",
       "L 446.187157 162.143812 \n",
       "L 446.694938 162.228197 \n",
       "L 447.202718 162.174844 \n",
       "L 447.710499 161.84698 \n",
       "L 448.218279 162.068575 \n",
       "L 448.72606 162.152614 \n",
       "L 449.741621 162.046841 \n",
       "L 450.249401 162.130564 \n",
       "L 451.264962 162.025271 \n",
       "L 451.772743 161.836991 \n",
       "L 452.280523 162.056213 \n",
       "L 453.296084 161.951687 \n",
       "L 453.803865 162.169873 \n",
       "L 455.327206 162.417198 \n",
       "L 456.342767 162.312551 \n",
       "L 457.358328 162.476183 \n",
       "L 457.866109 162.691365 \n",
       "L 458.373889 162.77246 \n",
       "L 458.88167 162.720009 \n",
       "L 459.38945 162.534537 \n",
       "L 460.405011 162.430671 \n",
       "L 460.912792 162.511586 \n",
       "L 461.420572 162.459826 \n",
       "L 462.436133 162.620919 \n",
       "L 462.943914 162.43728 \n",
       "L 463.451694 162.122363 \n",
       "L 463.959475 162.203037 \n",
       "L 464.975036 162.101259 \n",
       "L 465.990597 162.261827 \n",
       "L 466.498377 162.472533 \n",
       "L 467.513938 162.109911 \n",
       "L 468.021719 161.79908 \n",
       "L 468.529499 161.619106 \n",
       "L 469.03728 161.829286 \n",
       "L 470.560621 162.068395 \n",
       "L 471.068402 162.018467 \n",
       "L 471.576182 162.09773 \n",
       "L 472.083963 161.918987 \n",
       "L 472.591743 161.611956 \n",
       "L 473.607304 161.513888 \n",
       "L 474.622865 161.032079 \n",
       "L 475.130646 161.111883 \n",
       "L 475.638426 161.319239 \n",
       "L 479.19289 161.869751 \n",
       "L 480.208451 161.772644 \n",
       "L 481.731793 160.998659 \n",
       "L 482.239573 160.825745 \n",
       "L 482.747354 161.029841 \n",
       "L 483.255134 160.857317 \n",
       "L 484.270695 161.0138 \n",
       "L 484.778476 160.966846 \n",
       "L 485.286256 161.169516 \n",
       "L 485.794037 161.247081 \n",
       "L 486.301817 161.200007 \n",
       "L 487.317378 161.354492 \n",
       "L 487.825159 161.307463 \n",
       "L 488.332939 161.384382 \n",
       "L 488.84072 161.337429 \n",
       "L 489.3485 161.53765 \n",
       "L 489.856281 161.490636 \n",
       "L 490.364061 161.690182 \n",
       "L 491.379622 161.841963 \n",
       "L 491.887403 161.549246 \n",
       "L 492.395183 161.625114 \n",
       "L 493.410744 161.28699 \n",
       "L 494.426305 161.682717 \n",
       "L 495.441866 161.589576 \n",
       "L 495.949647 161.664752 \n",
       "L 496.457427 161.618325 \n",
       "L 496.965208 161.693291 \n",
       "L 497.472988 161.889232 \n",
       "L 498.488549 162.037994 \n",
       "L 498.99633 161.991385 \n",
       "L 499.50411 162.186034 \n",
       "L 501.027452 162.406774 \n",
       "L 501.535232 162.36 \n",
       "L 502.043013 162.43321 \n",
       "L 502.550793 162.386511 \n",
       "L 503.058574 162.45951 \n",
       "L 503.566354 162.29346 \n",
       "L 504.074135 162.485691 \n",
       "L 504.581915 162.558299 \n",
       "L 505.089696 162.511736 \n",
       "L 505.597476 162.584164 \n",
       "L 506.105257 162.775133 \n",
       "L 507.628598 162.990435 \n",
       "L 508.136379 162.943692 \n",
       "L 508.64416 162.779024 \n",
       "L 510.167501 162.64019 \n",
       "L 510.675282 162.829102 \n",
       "L 511.690843 162.736756 \n",
       "L 512.198623 162.924886 \n",
       "L 513.214184 162.832631 \n",
       "L 514.229745 163.090411 \n",
       "L 515.245306 163.230731 \n",
       "L 516.260867 163.602395 \n",
       "L 517.276428 163.393717 \n",
       "L 518.79977 163.832132 \n",
       "L 519.30755 163.900805 \n",
       "L 519.815331 163.739186 \n",
       "L 520.830892 163.876277 \n",
       "L 521.338672 163.829909 \n",
       "L 522.354233 162.936572 \n",
       "L 522.862014 162.891436 \n",
       "L 524.385355 162.187131 \n",
       "L 525.400916 161.871748 \n",
       "L 525.908697 161.941623 \n",
       "L 526.416477 162.124736 \n",
       "L 526.924258 162.080907 \n",
       "L 527.432038 162.150316 \n",
       "L 528.447599 161.724263 \n",
       "L 528.95538 161.793838 \n",
       "L 529.46316 161.975869 \n",
       "L 529.970941 161.932506 \n",
       "L 530.478721 161.776865 \n",
       "L 531.494282 161.915112 \n",
       "L 532.002063 161.872004 \n",
       "L 533.017624 162.121342 \n",
       "L 534.540965 162.214813 \n",
       "L 536.064307 162.085638 \n",
       "L 536.572087 161.820994 \n",
       "L 537.079868 161.889232 \n",
       "L 538.095429 161.583162 \n",
       "L 538.603209 161.651444 \n",
       "L 539.11099 161.499003 \n",
       "L 540.126551 161.745337 \n",
       "L 540.634331 161.373447 \n",
       "L 541.649893 161.728935 \n",
       "L 542.157673 161.577394 \n",
       "L 542.665454 161.645091 \n",
       "L 543.173234 161.384653 \n",
       "L 543.681015 161.561608 \n",
       "L 544.696576 161.260554 \n",
       "L 546.219917 161.680359 \n",
       "L 547.235478 161.489044 \n",
       "L 548.75882 161.473708 \n",
       "L 551.297722 161.699104 \n",
       "L 553.836625 161.601442 \n",
       "L 556.375527 162.036462 \n",
       "L 556.883308 162.207889 \n",
       "L 557.391088 162.166748 \n",
       "L 557.898869 162.337665 \n",
       "L 558.91443 162.043822 \n",
       "L 563.484454 162.517925 \n",
       "L 565.515576 162.459135 \n",
       "L 566.531137 162.586117 \n",
       "L 567.038918 162.441366 \n",
       "L 569.07004 162.590503 \n",
       "L 570.085601 162.406158 \n",
       "L 570.593381 162.262623 \n",
       "L 571.608942 162.491789 \n",
       "L 573.640064 162.844648 \n",
       "L 574.655626 163.07123 \n",
       "L 577.702309 162.93091 \n",
       "L 578.210089 162.992463 \n",
       "L 579.22565 162.810641 \n",
       "L 580.748992 162.589887 \n",
       "L 581.256772 162.348089 \n",
       "L 582.272333 162.370455 \n",
       "L 582.780114 162.129603 \n",
       "L 582.780114 162.129603 \n",
       "\" clip-path=\"url(#p6070896839)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path d=\"M 75.507386 123.120005 \n",
       "L 76.015167 72.72001 \n",
       "L 76.522947 123.120005 \n",
       "L 77.030728 148.32001 \n",
       "L 77.538508 183.600005 \n",
       "L 78.046289 190.319998 \n",
       "L 78.554069 195.119998 \n",
       "L 79.06185 198.720005 \n",
       "L 79.56963 179.119999 \n",
       "L 80.077411 173.52 \n",
       "L 81.092972 198.720005 \n",
       "L 81.600752 185.150768 \n",
       "L 82.108533 180.719999 \n",
       "L 82.616313 163.44001 \n",
       "L 83.124094 167.220006 \n",
       "L 83.631874 164.625883 \n",
       "L 84.139655 156.720002 \n",
       "L 84.647436 144.341054 \n",
       "L 85.155216 138.240005 \n",
       "L 85.662997 147.120003 \n",
       "L 86.170777 150.610917 \n",
       "L 86.678558 140.650444 \n",
       "L 88.201899 138.627696 \n",
       "L 88.70968 134.320004 \n",
       "L 89.21746 133.920011 \n",
       "L 89.725241 130.071733 \n",
       "L 90.233021 136.560007 \n",
       "L 90.740802 132.874849 \n",
       "L 91.248582 132.570011 \n",
       "L 91.756363 138.392732 \n",
       "L 92.264143 140.908239 \n",
       "L 93.279704 139.920003 \n",
       "L 93.787485 136.741633 \n",
       "L 95.310826 135.720008 \n",
       "L 95.818607 137.871225 \n",
       "L 96.834168 146.561861 \n",
       "L 97.341948 148.32001 \n",
       "L 97.849729 145.520003 \n",
       "L 98.86529 144.56681 \n",
       "L 99.37307 148.32001 \n",
       "L 99.880851 143.691438 \n",
       "L 100.388631 143.28 \n",
       "L 100.896412 140.908239 \n",
       "L 101.404192 136.689242 \n",
       "L 101.911973 136.43322 \n",
       "L 102.419753 138.053347 \n",
       "L 102.927534 137.781824 \n",
       "L 103.943095 140.804222 \n",
       "L 104.450875 140.499324 \n",
       "L 104.958656 143.621698 \n",
       "L 105.466436 144.960013 \n",
       "L 105.974217 141.297056 \n",
       "L 106.481997 139.378068 \n",
       "L 106.989778 142.320003 \n",
       "L 107.497558 142.020001 \n",
       "L 108.005339 144.830778 \n",
       "L 108.513119 144.501832 \n",
       "L 109.0209 145.687164 \n",
       "L 109.52868 145.355304 \n",
       "L 110.036461 147.95479 \n",
       "L 110.544241 149.040011 \n",
       "L 111.052022 147.255216 \n",
       "L 111.559802 148.32001 \n",
       "L 112.067583 147.974797 \n",
       "L 112.575363 149.001094 \n",
       "L 113.083144 145.968 \n",
       "L 113.590924 148.32001 \n",
       "L 114.606485 147.673849 \n",
       "L 115.114266 148.638998 \n",
       "L 115.622046 147.060011 \n",
       "L 116.637607 148.934644 \n",
       "L 117.145388 146.194703 \n",
       "L 117.653169 145.92001 \n",
       "L 118.160949 148.023538 \n",
       "L 118.66873 146.561861 \n",
       "L 119.684291 148.32001 \n",
       "L 120.192071 150.302023 \n",
       "L 120.699852 151.120002 \n",
       "L 121.207632 149.704618 \n",
       "L 121.715413 151.606963 \n",
       "L 122.223193 151.300652 \n",
       "L 122.730974 148.856177 \n",
       "L 123.238754 150.707378 \n",
       "L 124.254315 150.138556 \n",
       "L 124.762096 151.920002 \n",
       "L 125.269876 152.647273 \n",
       "L 125.777657 152.352003 \n",
       "L 126.285437 153.060604 \n",
       "L 126.793218 151.778826 \n",
       "L 127.300998 153.457878 \n",
       "L 127.808779 154.135387 \n",
       "L 128.316559 150.960005 \n",
       "L 128.82434 150.697359 \n",
       "L 129.33212 147.613467 \n",
       "L 129.839901 149.253346 \n",
       "L 131.363242 145.822708 \n",
       "L 132.378803 145.420898 \n",
       "L 132.886584 146.109477 \n",
       "L 133.394364 145.909571 \n",
       "L 133.902145 146.582078 \n",
       "L 134.409925 148.104618 \n",
       "L 134.917706 146.184414 \n",
       "L 135.933267 145.800013 \n",
       "L 136.441047 146.445633 \n",
       "L 136.948828 145.428198 \n",
       "L 137.456608 146.885865 \n",
       "L 137.964389 146.694205 \n",
       "L 138.472169 147.312008 \n",
       "L 139.48773 146.931031 \n",
       "L 139.995511 147.532507 \n",
       "L 140.503291 148.90606 \n",
       "L 141.011072 148.707701 \n",
       "L 141.518852 147.742912 \n",
       "L 142.534413 150.404222 \n",
       "L 143.042194 148.69612 \n",
       "L 144.565535 148.136071 \n",
       "L 145.581096 149.226474 \n",
       "L 146.088877 149.040011 \n",
       "L 147.104438 151.514377 \n",
       "L 147.612218 151.315808 \n",
       "L 148.119999 150.420008 \n",
       "L 148.627779 150.231728 \n",
       "L 149.13556 149.355619 \n",
       "L 149.64334 149.177147 \n",
       "L 150.151121 150.363246 \n",
       "L 150.658902 150.180403 \n",
       "L 152.182243 151.635802 \n",
       "L 152.690024 150.79059 \n",
       "L 153.197804 151.265459 \n",
       "L 153.705585 151.083878 \n",
       "L 154.213365 152.196933 \n",
       "L 154.721146 152.653762 \n",
       "L 155.228926 152.466849 \n",
       "L 155.736707 152.916228 \n",
       "L 156.244487 152.730006 \n",
       "L 156.752268 153.172175 \n",
       "L 157.767829 152.80344 \n",
       "L 158.275609 151.393178 \n",
       "L 159.798951 154.50684 \n",
       "L 160.306731 154.320002 \n",
       "L 160.814512 155.328289 \n",
       "L 161.322292 155.731766 \n",
       "L 162.337853 154.180478 \n",
       "L 162.845634 154.583594 \n",
       "L 163.353414 154.402764 \n",
       "L 163.861195 153.648006 \n",
       "L 164.368975 153.47455 \n",
       "L 164.876756 153.872545 \n",
       "L 166.400097 153.360005 \n",
       "L 166.907878 153.749844 \n",
       "L 168.431219 153.250446 \n",
       "L 168.939 151.452974 \n",
       "L 169.954561 151.149953 \n",
       "L 170.462341 150.464694 \n",
       "L 171.985683 150.035186 \n",
       "L 173.001244 151.845396 \n",
       "L 173.509024 150.658156 \n",
       "L 174.524585 151.40572 \n",
       "L 175.032366 150.750471 \n",
       "L 175.540146 149.592731 \n",
       "L 176.047927 149.459711 \n",
       "L 176.555707 150.336014 \n",
       "L 177.063488 150.70209 \n",
       "L 177.571268 151.563569 \n",
       "L 178.079049 150.9269 \n",
       "L 179.09461 150.655617 \n",
       "L 180.110171 152.337403 \n",
       "L 180.617951 152.68155 \n",
       "L 181.125732 153.504696 \n",
       "L 181.633512 152.880014 \n",
       "L 182.649074 152.599253 \n",
       "L 183.156854 152.934087 \n",
       "L 184.172415 151.719075 \n",
       "L 184.680196 152.053338 \n",
       "L 185.187976 152.849042 \n",
       "L 185.695757 152.250285 \n",
       "L 186.711318 151.985461 \n",
       "L 187.219098 152.767061 \n",
       "L 188.234659 153.405201 \n",
       "L 188.74244 152.370012 \n",
       "L 190.265781 153.315605 \n",
       "L 190.773562 152.741061 \n",
       "L 191.789123 152.483491 \n",
       "L 192.296903 151.483645 \n",
       "L 193.820245 152.41231 \n",
       "L 194.328025 153.145544 \n",
       "L 194.835806 153.018307 \n",
       "L 195.343586 152.041532 \n",
       "L 195.851367 151.920002 \n",
       "L 196.359147 151.377752 \n",
       "L 196.866928 151.680007 \n",
       "L 197.374708 152.39801 \n",
       "L 198.390269 152.157039 \n",
       "L 198.89805 152.451152 \n",
       "L 199.40583 150.685718 \n",
       "L 200.421391 150.462516 \n",
       "L 200.929172 150.758717 \n",
       "L 202.960294 150.320002 \n",
       "L 203.468074 149.814071 \n",
       "L 203.975855 148.915282 \n",
       "L 204.483635 148.418829 \n",
       "L 204.991416 148.32001 \n",
       "L 206.006977 148.90606 \n",
       "L 206.514757 148.417297 \n",
       "L 208.038099 149.281839 \n",
       "L 208.545879 149.948909 \n",
       "L 209.05366 150.229099 \n",
       "L 210.069221 150.025273 \n",
       "L 211.592562 150.84938 \n",
       "L 212.100343 150.37334 \n",
       "L 213.623684 151.181541 \n",
       "L 214.131465 151.814905 \n",
       "L 215.147026 151.606963 \n",
       "L 215.654807 151.14022 \n",
       "L 216.162587 151.039433 \n",
       "L 216.670368 151.300652 \n",
       "L 217.178148 150.480014 \n",
       "L 217.685929 151.100085 \n",
       "L 218.193709 151.3583 \n",
       "L 218.70149 151.25852 \n",
       "L 219.20927 150.804514 \n",
       "L 220.224831 150.610917 \n",
       "L 220.732612 150.163911 \n",
       "L 221.240392 149.370009 \n",
       "L 221.748173 149.279181 \n",
       "L 222.255953 148.493796 \n",
       "L 222.763734 148.406602 \n",
       "L 223.271514 148.665208 \n",
       "L 223.779295 148.57803 \n",
       "L 224.794856 149.088828 \n",
       "L 225.302636 149.001094 \n",
       "L 225.810417 148.574545 \n",
       "L 226.318197 147.812622 \n",
       "L 226.825978 148.067157 \n",
       "L 227.333758 147.648014 \n",
       "L 227.841539 146.896755 \n",
       "L 229.36488 146.662107 \n",
       "L 229.872661 146.91541 \n",
       "L 230.380441 146.178826 \n",
       "L 230.888222 146.103724 \n",
       "L 231.396002 145.701824 \n",
       "L 231.903783 146.281175 \n",
       "L 233.427124 146.058468 \n",
       "L 234.950466 143.920003 \n",
       "L 235.966027 143.152808 \n",
       "L 236.473807 143.723777 \n",
       "L 236.981588 143.659189 \n",
       "L 237.489368 143.910014 \n",
       "L 238.504929 145.033057 \n",
       "L 239.01271 145.277289 \n",
       "L 239.52049 145.208901 \n",
       "L 240.028271 145.451089 \n",
       "L 241.043832 145.314509 \n",
       "L 241.551612 145.861476 \n",
       "L 242.059393 146.098738 \n",
       "L 242.567173 145.723649 \n",
       "L 243.074954 145.655351 \n",
       "L 243.582734 146.194703 \n",
       "L 244.090515 146.125414 \n",
       "L 244.598295 146.660124 \n",
       "L 245.106076 146.890747 \n",
       "L 245.613856 147.420004 \n",
       "L 246.121637 147.647007 \n",
       "L 246.629417 147.574444 \n",
       "L 247.137198 148.097003 \n",
       "L 247.644979 148.32001 \n",
       "L 248.152759 148.837312 \n",
       "L 249.16832 148.099601 \n",
       "L 249.676101 148.32001 \n",
       "L 250.183881 147.95479 \n",
       "L 250.691662 148.465678 \n",
       "L 251.707223 147.740689 \n",
       "L 252.215003 147.670154 \n",
       "L 252.722784 148.17601 \n",
       "L 253.230564 148.391807 \n",
       "L 253.738345 148.32001 \n",
       "L 254.246125 147.963066 \n",
       "L 254.753906 147.323393 \n",
       "L 255.261686 147.823106 \n",
       "L 255.769467 147.753712 \n",
       "L 256.277247 148.249414 \n",
       "L 256.785028 147.616095 \n",
       "L 257.292808 147.547858 \n",
       "L 257.800589 147.200001 \n",
       "L 258.308369 147.412524 \n",
       "L 258.81615 147.902324 \n",
       "L 259.32393 148.111738 \n",
       "L 259.831711 147.766164 \n",
       "L 260.339491 147.974797 \n",
       "L 260.847272 147.356078 \n",
       "L 261.862833 147.772187 \n",
       "L 262.370613 147.705376 \n",
       "L 262.878394 147.094062 \n",
       "L 263.386174 147.572837 \n",
       "L 263.893955 147.507108 \n",
       "L 264.401735 146.630999 \n",
       "L 265.417296 147.043203 \n",
       "L 265.925077 146.711494 \n",
       "L 266.432857 147.183674 \n",
       "L 266.940638 147.386674 \n",
       "L 267.448418 147.056677 \n",
       "L 267.956199 146.463162 \n",
       "L 268.463979 146.137325 \n",
       "L 268.97176 146.604819 \n",
       "L 271.002882 147.406021 \n",
       "L 271.510662 147.343265 \n",
       "L 272.018443 147.021033 \n",
       "L 272.526223 147.477848 \n",
       "L 273.034004 147.156923 \n",
       "L 274.049565 147.034296 \n",
       "L 274.557345 146.716961 \n",
       "L 275.065126 146.145391 \n",
       "L 275.572906 145.831901 \n",
       "L 276.080687 146.283638 \n",
       "L 277.096248 145.660714 \n",
       "L 278.111809 146.052009 \n",
       "L 278.619589 145.994827 \n",
       "L 279.12737 145.185679 \n",
       "L 281.158492 145.961391 \n",
       "L 282.681834 147.27258 \n",
       "L 283.697395 146.66454 \n",
       "L 285.220736 146.493924 \n",
       "L 285.728517 146.680492 \n",
       "L 286.236297 146.62385 \n",
       "L 286.744078 146.809216 \n",
       "L 287.251858 146.511392 \n",
       "L 287.759639 146.455562 \n",
       "L 288.267419 146.640012 \n",
       "L 290.298541 146.418116 \n",
       "L 291.314102 145.835506 \n",
       "L 292.329663 146.20038 \n",
       "L 292.837444 145.67665 \n",
       "L 293.345224 146.09303 \n",
       "L 293.853005 145.571973 \n",
       "L 294.360785 145.986671 \n",
       "L 294.868566 145.933859 \n",
       "L 295.376346 146.113563 \n",
       "L 295.884127 146.524145 \n",
       "L 296.899688 146.417034 \n",
       "L 297.407468 146.133705 \n",
       "L 297.915249 146.081284 \n",
       "L 298.93081 146.434293 \n",
       "L 299.946371 147.239189 \n",
       "L 301.469712 146.398935 \n",
       "L 301.977493 146.346859 \n",
       "L 302.485273 146.520014 \n",
       "L 303.500834 146.416013 \n",
       "L 304.008615 146.587861 \n",
       "L 304.516395 145.866913 \n",
       "L 305.024176 146.039212 \n",
       "L 306.039737 146.824627 \n",
       "L 306.547517 146.109477 \n",
       "L 307.055298 145.8386 \n",
       "L 307.563078 145.789003 \n",
       "L 308.070859 146.178826 \n",
       "L 309.08642 146.078791 \n",
       "L 309.5942 145.592731 \n",
       "L 310.101981 145.761906 \n",
       "L 310.609761 145.495865 \n",
       "L 311.117542 145.664529 \n",
       "L 311.625322 145.39984 \n",
       "L 312.640883 145.735395 \n",
       "L 313.148664 146.117017 \n",
       "L 313.656445 146.068096 \n",
       "L 314.164225 145.80536 \n",
       "L 314.672006 145.330175 \n",
       "L 315.179786 145.070113 \n",
       "L 315.687567 145.023805 \n",
       "L 316.195347 145.402108 \n",
       "L 316.703128 145.355304 \n",
       "L 317.718689 144.840511 \n",
       "L 318.73425 145.590013 \n",
       "L 319.749811 145.915023 \n",
       "L 320.257591 145.659136 \n",
       "L 320.765372 145.820831 \n",
       "L 321.273152 145.774027 \n",
       "L 321.780933 145.93482 \n",
       "L 322.288713 146.301933 \n",
       "L 322.796494 146.460984 \n",
       "L 323.304274 146.825528 \n",
       "L 323.812055 146.982867 \n",
       "L 324.827616 146.476109 \n",
       "L 325.335396 146.83765 \n",
       "L 325.843177 146.789645 \n",
       "L 326.350957 146.538189 \n",
       "L 326.858738 146.897431 \n",
       "L 327.874299 147.20676 \n",
       "L 328.88986 147.110405 \n",
       "L 329.905421 146.61323 \n",
       "L 331.428762 146.473676 \n",
       "L 331.936543 146.228303 \n",
       "L 332.444323 146.381541 \n",
       "L 332.952104 146.335759 \n",
       "L 333.459884 146.092114 \n",
       "L 333.967665 146.244706 \n",
       "L 335.491006 146.109477 \n",
       "L 335.998787 146.260868 \n",
       "L 336.506567 146.215927 \n",
       "L 337.014348 145.975826 \n",
       "L 337.522128 146.126585 \n",
       "L 338.537689 145.649493 \n",
       "L 339.04547 145.99385 \n",
       "L 339.55325 145.756468 \n",
       "L 340.061031 145.906207 \n",
       "L 340.568811 145.862647 \n",
       "L 341.076592 145.434507 \n",
       "L 341.584372 145.584005 \n",
       "L 342.599933 145.498749 \n",
       "L 343.107714 145.838194 \n",
       "L 344.123275 145.752458 \n",
       "L 344.631055 145.899673 \n",
       "L 345.138836 145.85685 \n",
       "L 345.646617 146.003313 \n",
       "L 346.662178 145.917757 \n",
       "L 347.677739 146.208281 \n",
       "L 348.185519 145.978004 \n",
       "L 348.6933 146.309624 \n",
       "L 349.20108 146.453339 \n",
       "L 350.216641 145.995277 \n",
       "L 350.724422 146.324434 \n",
       "L 351.232202 145.725887 \n",
       "L 351.739983 146.054322 \n",
       "L 352.755544 146.339018 \n",
       "L 353.263324 146.296646 \n",
       "L 354.278885 146.945466 \n",
       "L 354.786666 146.719274 \n",
       "L 355.294446 146.859144 \n",
       "L 355.802227 146.633928 \n",
       "L 356.310007 146.773438 \n",
       "L 356.817788 147.094062 \n",
       "L 357.833349 147.007981 \n",
       "L 358.84891 147.283154 \n",
       "L 359.35669 146.880007 \n",
       "L 359.864471 146.657976 \n",
       "L 360.372251 146.974816 \n",
       "L 361.387812 147.247661 \n",
       "L 362.403373 147.162405 \n",
       "L 363.418934 147.787613 \n",
       "L 363.926715 147.744264 \n",
       "L 364.434495 147.524216 \n",
       "L 364.942276 147.658017 \n",
       "L 365.450056 147.438885 \n",
       "L 365.957837 147.572356 \n",
       "L 366.465617 147.88098 \n",
       "L 366.973398 147.487311 \n",
       "L 367.988959 148.101629 \n",
       "L 368.496739 147.884014 \n",
       "L 369.00452 148.015337 \n",
       "L 369.5123 147.972424 \n",
       "L 370.020081 148.276631 \n",
       "L 370.527861 148.233403 \n",
       "L 371.035642 148.01744 \n",
       "L 371.543422 147.974797 \n",
       "L 372.051203 147.760006 \n",
       "L 372.558983 147.717964 \n",
       "L 373.066764 148.019497 \n",
       "L 373.574544 148.148583 \n",
       "L 374.082325 147.934948 \n",
       "L 374.590105 148.063732 \n",
       "L 375.605666 147.979468 \n",
       "L 376.113447 148.277517 \n",
       "L 376.621227 148.40486 \n",
       "L 377.129008 148.192953 \n",
       "L 377.636788 148.32001 \n",
       "L 378.144569 148.615491 \n",
       "L 378.65235 148.741406 \n",
       "L 379.16013 148.698643 \n",
       "L 380.175691 148.948958 \n",
       "L 381.191252 148.863297 \n",
       "L 381.699033 148.98756 \n",
       "L 382.206813 148.944797 \n",
       "L 382.714594 149.06852 \n",
       "L 383.730155 148.651585 \n",
       "L 384.237935 148.609663 \n",
       "L 384.745716 148.402637 \n",
       "L 385.253496 148.361256 \n",
       "L 385.761277 148.484708 \n",
       "L 386.269057 148.772213 \n",
       "L 386.776838 148.894599 \n",
       "L 387.284618 148.197083 \n",
       "L 388.300179 148.115793 \n",
       "L 388.80796 147.912237 \n",
       "L 389.31574 148.197879 \n",
       "L 389.823521 148.32001 \n",
       "L 390.331301 148.60406 \n",
       "L 390.839082 148.40103 \n",
       "L 391.854643 148.32001 \n",
       "L 392.870204 148.561538 \n",
       "L 393.377984 148.842494 \n",
       "L 393.885765 148.801533 \n",
       "L 394.901326 149.040011 \n",
       "L 395.409106 148.998931 \n",
       "L 395.916887 149.117471 \n",
       "L 396.424667 149.394883 \n",
       "L 396.932448 149.353442 \n",
       "L 397.440228 149.1534 \n",
       "L 397.948009 149.27095 \n",
       "L 398.96357 149.188968 \n",
       "L 399.47135 149.305917 \n",
       "L 399.979131 149.580009 \n",
       "L 400.994692 149.811593 \n",
       "L 402.010253 149.728695 \n",
       "L 402.518033 149.843721 \n",
       "L 403.025814 149.802355 \n",
       "L 404.041375 150.031116 \n",
       "L 404.549155 150.300281 \n",
       "L 405.564716 149.907107 \n",
       "L 406.072497 150.020616 \n",
       "L 406.580277 149.97943 \n",
       "L 407.088058 150.246613 \n",
       "L 407.595838 150.359085 \n",
       "L 408.611399 149.969322 \n",
       "L 409.62696 150.193756 \n",
       "L 410.134741 150.45819 \n",
       "L 410.642521 150.416824 \n",
       "L 411.658083 150.03041 \n",
       "L 412.673644 149.94948 \n",
       "L 414.196985 150.281685 \n",
       "L 414.704766 150.090416 \n",
       "L 415.212546 150.351049 \n",
       "L 415.720327 150.460698 \n",
       "L 416.228107 150.72001 \n",
       "L 417.751449 150.597338 \n",
       "L 418.259229 150.705801 \n",
       "L 419.27479 150.624435 \n",
       "L 420.290351 151.13648 \n",
       "L 420.798132 151.095339 \n",
       "L 421.305912 150.906518 \n",
       "L 422.829254 151.226286 \n",
       "L 423.337034 151.479184 \n",
       "L 423.844815 151.291189 \n",
       "L 424.352595 150.957211 \n",
       "L 424.860376 150.770508 \n",
       "L 425.875937 150.69048 \n",
       "L 426.891498 150.320002 \n",
       "L 427.399278 150.426061 \n",
       "L 427.907059 150.096694 \n",
       "L 428.414839 150.347595 \n",
       "L 429.4304 150.558406 \n",
       "L 429.938181 150.519142 \n",
       "L 430.445961 150.768 \n",
       "L 430.953742 150.728571 \n",
       "L 431.461522 150.545653 \n",
       "L 432.984864 150.857882 \n",
       "L 433.492644 150.67582 \n",
       "L 434.000425 150.921989 \n",
       "L 434.508205 150.882725 \n",
       "L 435.015986 150.985735 \n",
       "L 436.031547 150.623804 \n",
       "L 436.539327 150.726754 \n",
       "L 437.047108 150.970775 \n",
       "L 437.554888 150.931767 \n",
       "L 438.062669 150.751898 \n",
       "L 439.07823 150.674813 \n",
       "L 439.58601 150.355661 \n",
       "L 440.601571 150.280003 \n",
       "L 442.124913 149.749048 \n",
       "L 444.663816 149.56616 \n",
       "L 445.171596 149.806426 \n",
       "L 445.679377 149.631784 \n",
       "L 446.187157 149.595525 \n",
       "L 446.694938 149.421649 \n",
       "L 447.202718 149.660803 \n",
       "L 448.218279 149.862857 \n",
       "L 448.72606 149.826523 \n",
       "L 449.23384 149.927069 \n",
       "L 449.741621 149.754155 \n",
       "L 450.249401 149.854506 \n",
       "L 450.757182 149.818382 \n",
       "L 451.264962 150.054427 \n",
       "L 451.772743 150.153967 \n",
       "L 452.280523 150.117588 \n",
       "L 452.788304 149.945815 \n",
       "L 453.296084 149.909811 \n",
       "L 454.311645 150.377846 \n",
       "L 454.819426 150.476154 \n",
       "L 455.834987 150.940809 \n",
       "L 456.342767 150.903769 \n",
       "L 456.850548 151.134902 \n",
       "L 457.358328 150.963835 \n",
       "L 458.373889 150.89007 \n",
       "L 458.88167 151.120002 \n",
       "L 459.38945 151.216178 \n",
       "L 459.897231 151.445073 \n",
       "L 460.405011 151.407747 \n",
       "L 460.912792 151.503172 \n",
       "L 461.928353 151.428671 \n",
       "L 462.436133 151.523675 \n",
       "L 463.451694 151.976479 \n",
       "L 463.959475 151.675621 \n",
       "L 464.467255 151.638385 \n",
       "L 464.975036 151.863751 \n",
       "L 465.482816 151.957463 \n",
       "L 465.990597 151.920002 \n",
       "L 466.498377 152.013399 \n",
       "L 467.006158 151.714824 \n",
       "L 467.513938 151.677829 \n",
       "L 468.021719 151.901407 \n",
       "L 468.529499 151.7342 \n",
       "L 469.03728 151.957117 \n",
       "L 470.052841 152.142108 \n",
       "L 472.083963 151.993662 \n",
       "L 473.099524 152.177151 \n",
       "L 473.607304 152.396944 \n",
       "L 474.622865 152.322548 \n",
       "L 475.130646 152.029651 \n",
       "L 475.638426 152.120764 \n",
       "L 476.146207 151.956457 \n",
       "L 477.161768 152.392738 \n",
       "L 477.669549 152.482936 \n",
       "L 478.68511 152.409065 \n",
       "L 479.19289 152.625539 \n",
       "L 480.208451 152.551579 \n",
       "L 480.716232 152.640905 \n",
       "L 481.224012 152.856012 \n",
       "L 481.731793 152.693041 \n",
       "L 482.239573 152.781856 \n",
       "L 483.255134 152.708061 \n",
       "L 483.762915 152.921741 \n",
       "L 484.270695 152.884776 \n",
       "L 484.778476 152.972795 \n",
       "L 485.286256 152.935845 \n",
       "L 485.794037 152.774391 \n",
       "L 486.301817 152.737786 \n",
       "L 487.317378 152.416561 \n",
       "L 488.332939 152.592239 \n",
       "L 488.84072 152.80344 \n",
       "L 489.3485 152.890588 \n",
       "L 489.856281 152.730772 \n",
       "L 490.364061 152.8178 \n",
       "L 491.379622 152.376591 \n",
       "L 491.887403 152.340963 \n",
       "L 492.395183 152.428036 \n",
       "L 492.902964 152.392423 \n",
       "L 493.410744 152.234574 \n",
       "L 493.918525 152.321467 \n",
       "L 494.934086 151.885185 \n",
       "L 495.441866 152.093923 \n",
       "L 495.949647 151.937381 \n",
       "L 496.457427 152.024108 \n",
       "L 496.965208 151.989321 \n",
       "L 497.472988 152.196933 \n",
       "L 498.99633 152.092466 \n",
       "L 499.50411 151.937231 \n",
       "L 500.011891 152.023237 \n",
       "L 500.519671 151.868452 \n",
       "L 501.535232 151.800004 \n",
       "L 502.043013 151.885756 \n",
       "L 503.058574 151.817518 \n",
       "L 503.566354 151.902939 \n",
       "L 504.074135 151.630304 \n",
       "L 504.581915 151.715755 \n",
       "L 505.089696 151.681989 \n",
       "L 505.597476 151.76717 \n",
       "L 506.105257 151.970891 \n",
       "L 506.613037 151.818359 \n",
       "L 508.136379 152.071948 \n",
       "L 508.64416 152.038032 \n",
       "L 509.15194 152.240011 \n",
       "L 509.659721 152.20599 \n",
       "L 510.167501 152.289668 \n",
       "L 511.183062 152.221746 \n",
       "L 512.198623 152.388292 \n",
       "L 512.706404 152.237413 \n",
       "L 513.214184 152.320475 \n",
       "L 513.721965 152.286679 \n",
       "L 514.229745 152.136416 \n",
       "L 515.753087 152.03614 \n",
       "L 516.768648 152.201379 \n",
       "L 518.291989 151.755063 \n",
       "L 518.79977 151.722289 \n",
       "L 519.30755 151.804811 \n",
       "L 520.323111 151.624461 \n",
       "L 520.830892 151.706788 \n",
       "L 521.338672 151.674269 \n",
       "L 522.354233 151.380621 \n",
       "L 523.369794 151.659083 \n",
       "L 523.877575 151.62679 \n",
       "L 526.924258 152.227424 \n",
       "L 527.432038 151.855354 \n",
       "L 527.939819 152.049162 \n",
       "L 528.95538 151.871682 \n",
       "L 529.46316 151.952191 \n",
       "L 529.970941 151.920002 \n",
       "L 530.478721 151.775521 \n",
       "L 531.494282 151.936029 \n",
       "L 532.002063 151.904005 \n",
       "L 533.525404 152.031634 \n",
       "L 536.572087 152.617037 \n",
       "L 537.587648 152.44163 \n",
       "L 539.61877 153.084592 \n",
       "L 540.126551 152.831799 \n",
       "L 540.634331 153.019238 \n",
       "L 541.142112 152.876875 \n",
       "L 543.173234 153.07576 \n",
       "L 543.681015 153.043301 \n",
       "L 544.696576 153.305526 \n",
       "L 545.712137 153.34913 \n",
       "L 548.75882 153.694929 \n",
       "L 549.2666 153.122578 \n",
       "L 550.282161 153.166167 \n",
       "L 550.789942 153.241675 \n",
       "L 552.821064 152.792265 \n",
       "L 553.328844 152.974778 \n",
       "L 554.344405 152.591187 \n",
       "L 557.898869 152.480262 \n",
       "L 558.91443 152.735954 \n",
       "L 559.42221 152.599253 \n",
       "L 559.929991 152.673935 \n",
       "L 561.453332 152.055291 \n",
       "L 565.515576 151.711309 \n",
       "L 566.531137 151.548113 \n",
       "L 567.038918 151.726825 \n",
       "L 570.085601 151.654157 \n",
       "L 570.593381 151.728207 \n",
       "L 571.101162 151.492583 \n",
       "L 571.608942 151.669703 \n",
       "L 574.655626 151.495617 \n",
       "L 575.163406 151.671475 \n",
       "L 575.671187 151.642516 \n",
       "L 576.686748 151.992881 \n",
       "L 577.702309 151.832734 \n",
       "L 578.210089 151.702042 \n",
       "L 579.733431 151.920002 \n",
       "L 580.748992 151.760966 \n",
       "L 582.272333 151.67496 \n",
       "L 582.780114 151.84801 \n",
       "L 582.780114 151.84801 \n",
       "\" clip-path=\"url(#p6070896839)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_18\">\n",
       "    <path d=\"M 75.507386 123.120005 \n",
       "L 76.522947 123.120005 \n",
       "L 77.030728 173.52 \n",
       "L 77.538508 183.600005 \n",
       "L 78.046289 173.52 \n",
       "L 78.554069 195.119998 \n",
       "L 79.06185 186.120003 \n",
       "L 79.56963 167.920001 \n",
       "L 80.077411 173.52 \n",
       "L 80.585191 168.938186 \n",
       "L 81.092972 181.920007 \n",
       "L 81.600752 169.643077 \n",
       "L 82.616313 176.880012 \n",
       "L 83.631874 182.414117 \n",
       "L 84.647436 197.393691 \n",
       "L 85.155216 198.720005 \n",
       "L 85.662997 195.119998 \n",
       "L 86.678558 180.09392 \n",
       "L 87.186338 181.920007 \n",
       "L 87.694119 175.536004 \n",
       "L 88.201899 173.52 \n",
       "L 88.70968 175.386671 \n",
       "L 89.725241 171.782068 \n",
       "L 90.740802 175.14582 \n",
       "L 91.756363 171.992729 \n",
       "L 92.264143 167.590589 \n",
       "L 92.771924 160.560004 \n",
       "L 93.279704 159.520009 \n",
       "L 94.295265 168.214745 \n",
       "L 94.803046 167.058462 \n",
       "L 95.310826 168.480005 \n",
       "L 95.818607 167.373665 \n",
       "L 96.326387 163.920001 \n",
       "L 96.834168 162.971164 \n",
       "L 97.341948 157.483637 \n",
       "L 97.849729 154.480014 \n",
       "L 99.37307 158.82 \n",
       "L 99.880851 162.205726 \n",
       "L 100.896412 160.672942 \n",
       "L 101.404192 163.827701 \n",
       "L 101.911973 164.961513 \n",
       "L 102.419753 164.186673 \n",
       "L 102.927534 165.272735 \n",
       "L 103.435314 168.120012 \n",
       "L 103.943095 169.098949 \n",
       "L 105.974217 166.083941 \n",
       "L 106.481997 167.016781 \n",
       "L 106.989778 166.320001 \n",
       "L 107.497558 168.795012 \n",
       "L 108.005339 169.643077 \n",
       "L 108.513119 168.938186 \n",
       "L 109.0209 166.749854 \n",
       "L 110.036461 165.485229 \n",
       "L 112.067583 168.687136 \n",
       "L 112.575363 165.347026 \n",
       "L 113.083144 167.472003 \n",
       "L 114.098705 168.938186 \n",
       "L 114.606485 168.35077 \n",
       "L 115.114266 169.054189 \n",
       "L 116.129827 165.43111 \n",
       "L 116.637607 167.373665 \n",
       "L 118.160949 169.369421 \n",
       "L 118.66873 168.831632 \n",
       "L 119.684291 170.08364 \n",
       "L 120.699852 169.040009 \n",
       "L 121.207632 167.427693 \n",
       "L 122.730974 166.01363 \n",
       "L 123.238754 166.623157 \n",
       "L 123.746535 168.270005 \n",
       "L 124.254315 166.76537 \n",
       "L 125.269876 165.883644 \n",
       "L 125.777657 167.472003 \n",
       "L 126.285437 166.034854 \n",
       "L 126.793218 167.590589 \n",
       "L 127.300998 168.13748 \n",
       "L 127.808779 169.643077 \n",
       "L 128.316559 170.160003 \n",
       "L 128.82434 169.716227 \n",
       "L 129.33212 168.338693 \n",
       "L 129.839901 167.920001 \n",
       "L 130.347681 169.358531 \n",
       "L 130.855462 167.105461 \n",
       "L 131.363242 168.525412 \n",
       "L 131.871023 168.120012 \n",
       "L 132.378803 168.613806 \n",
       "L 132.886584 166.446322 \n",
       "L 133.394364 166.069566 \n",
       "L 134.409925 167.058462 \n",
       "L 134.917706 166.686107 \n",
       "L 135.425486 164.625883 \n",
       "L 135.933267 164.280009 \n",
       "L 136.441047 165.605962 \n",
       "L 136.948828 166.083941 \n",
       "L 137.456608 165.734642 \n",
       "L 137.964389 166.203878 \n",
       "L 138.472169 165.052807 \n",
       "L 138.97995 163.120001 \n",
       "L 139.48773 162.805039 \n",
       "L 139.995511 161.7075 \n",
       "L 140.503291 162.189775 \n",
       "L 141.011072 161.889232 \n",
       "L 142.026633 162.829102 \n",
       "L 142.534413 161.772644 \n",
       "L 143.549974 162.693332 \n",
       "L 144.565535 160.644088 \n",
       "L 145.581096 160.104181 \n",
       "L 146.088877 160.560004 \n",
       "L 147.104438 158.612959 \n",
       "L 147.612218 159.069653 \n",
       "L 148.627779 158.573801 \n",
       "L 149.13556 159.021378 \n",
       "L 149.64334 158.777146 \n",
       "L 150.151121 157.855136 \n",
       "L 151.166682 157.392013 \n",
       "L 151.674463 157.832591 \n",
       "L 152.182243 156.941056 \n",
       "L 152.690024 158.037649 \n",
       "L 153.197804 158.465459 \n",
       "L 153.705585 158.23742 \n",
       "L 154.213365 158.658471 \n",
       "L 154.721146 158.432114 \n",
       "L 155.228926 159.484561 \n",
       "L 155.736707 157.987931 \n",
       "L 156.244487 158.4 \n",
       "L 156.752268 156.92871 \n",
       "L 157.260048 157.964454 \n",
       "L 157.767829 158.369088 \n",
       "L 158.275609 159.383429 \n",
       "L 160.306731 160.920013 \n",
       "L 160.814512 160.696344 \n",
       "L 162.845634 162.158157 \n",
       "L 164.368975 161.492739 \n",
       "L 164.876756 161.845433 \n",
       "L 165.384536 162.760458 \n",
       "L 165.892317 161.975869 \n",
       "L 166.400097 160.640003 \n",
       "L 167.923439 161.677385 \n",
       "L 168.431219 161.467835 \n",
       "L 168.939 161.805418 \n",
       "L 169.44678 161.597431 \n",
       "L 169.954561 162.469739 \n",
       "L 170.462341 162.26043 \n",
       "L 171.477902 162.909476 \n",
       "L 171.985683 162.701158 \n",
       "L 172.493463 163.02001 \n",
       "L 173.001244 163.857832 \n",
       "L 173.509024 163.647847 \n",
       "L 174.016805 163.956936 \n",
       "L 175.032366 162.518991 \n",
       "L 175.540146 163.338187 \n",
       "L 176.047927 163.64262 \n",
       "L 176.555707 162.936002 \n",
       "L 177.571268 162.541792 \n",
       "L 178.079049 163.340695 \n",
       "L 178.586829 163.143538 \n",
       "L 179.09461 163.44001 \n",
       "L 180.110171 163.050442 \n",
       "L 181.125732 160.739152 \n",
       "L 181.633512 161.04001 \n",
       "L 182.141293 160.860292 \n",
       "L 182.649074 160.2068 \n",
       "L 183.156854 160.032684 \n",
       "L 183.664635 158.918143 \n",
       "L 184.172415 159.689303 \n",
       "L 184.680196 159.520009 \n",
       "L 185.187976 158.887741 \n",
       "L 185.695757 158.723674 \n",
       "L 186.203537 159.481647 \n",
       "L 187.219098 158.240364 \n",
       "L 187.726879 158.53622 \n",
       "L 188.234659 158.377409 \n",
       "L 188.74244 158.670006 \n",
       "L 189.25022 157.616012 \n",
       "L 189.758001 157.463375 \n",
       "L 190.265781 156.423965 \n",
       "L 190.773562 157.162111 \n",
       "L 191.281342 156.573283 \n",
       "L 194.328025 155.719149 \n",
       "L 195.343586 156.294685 \n",
       "L 195.851367 155.731766 \n",
       "L 196.359147 156.438835 \n",
       "L 197.374708 156.162325 \n",
       "L 197.882489 155.60926 \n",
       "L 198.390269 155.890382 \n",
       "L 199.40583 155.622869 \n",
       "L 199.913611 156.310246 \n",
       "L 200.929172 156.855485 \n",
       "L 201.436952 156.720002 \n",
       "L 202.452513 157.255463 \n",
       "L 202.960294 157.920009 \n",
       "L 203.975855 158.439699 \n",
       "L 204.483635 159.091778 \n",
       "L 204.991416 158.951263 \n",
       "L 205.499196 159.596267 \n",
       "L 206.514757 158.53622 \n",
       "L 209.05366 159.774559 \n",
       "L 209.56144 159.255861 \n",
       "L 210.069221 159.877899 \n",
       "L 210.577001 159.362701 \n",
       "L 211.084782 159.227472 \n",
       "L 211.592562 158.718522 \n",
       "L 212.100343 158.960004 \n",
       "L 213.623684 158.566155 \n",
       "L 214.639245 159.04146 \n",
       "L 215.147026 159.641749 \n",
       "L 215.654807 159.5099 \n",
       "L 216.162587 159.741589 \n",
       "L 216.670368 159.610326 \n",
       "L 217.178148 160.200011 \n",
       "L 217.685929 160.068042 \n",
       "L 218.193709 158.864685 \n",
       "L 218.70149 158.738379 \n",
       "L 219.20927 158.96789 \n",
       "L 219.717051 158.84211 \n",
       "L 220.224831 158.364762 \n",
       "L 221.240392 158.120005 \n",
       "L 222.255953 158.573801 \n",
       "L 222.763734 159.144755 \n",
       "L 223.271514 159.366576 \n",
       "L 223.779295 158.898842 \n",
       "L 224.794856 158.656278 \n",
       "L 225.302636 158.876762 \n",
       "L 225.810417 158.416973 \n",
       "L 226.318197 158.636781 \n",
       "L 226.825978 158.518 \n",
       "L 227.333758 159.072011 \n",
       "L 227.841539 159.287449 \n",
       "L 229.36488 157.935795 \n",
       "L 229.872661 157.821641 \n",
       "L 230.380441 157.049413 \n",
       "L 230.888222 157.267239 \n",
       "L 231.396002 156.829095 \n",
       "L 231.903783 157.046214 \n",
       "L 232.411563 156.611615 \n",
       "L 232.919344 157.152168 \n",
       "L 233.427124 157.366163 \n",
       "L 233.934905 157.900843 \n",
       "L 234.950466 158.320002 \n",
       "L 235.458246 158.846586 \n",
       "L 235.966027 158.415907 \n",
       "L 236.473807 158.938871 \n",
       "L 237.489368 159.345007 \n",
       "L 237.997149 159.232159 \n",
       "L 238.504929 159.433056 \n",
       "L 239.52049 158.586673 \n",
       "L 240.028271 158.47755 \n",
       "L 240.536051 158.059879 \n",
       "L 241.043832 157.336528 \n",
       "L 241.551612 157.539513 \n",
       "L 242.059393 157.128511 \n",
       "L 242.567173 157.33091 \n",
       "L 243.582734 156.517602 \n",
       "L 244.090515 156.720002 \n",
       "L 244.598295 156.317606 \n",
       "L 245.106076 156.820308 \n",
       "L 245.613856 157.020004 \n",
       "L 246.121637 156.919412 \n",
       "L 246.629417 157.415865 \n",
       "L 247.137198 157.017345 \n",
       "L 248.152759 156.818535 \n",
       "L 249.676101 155.645595 \n",
       "L 250.183881 156.135664 \n",
       "L 250.691662 156.33156 \n",
       "L 251.199442 156.816838 \n",
       "L 251.707223 156.430349 \n",
       "L 252.215003 156.623736 \n",
       "L 252.722784 155.95201 \n",
       "L 253.738345 156.338184 \n",
       "L 254.246125 156.815186 \n",
       "L 254.753906 157.004758 \n",
       "L 255.769467 156.81439 \n",
       "L 256.277247 156.437649 \n",
       "L 256.785028 156.344583 \n",
       "L 257.292808 156.532818 \n",
       "L 257.800589 156.440007 \n",
       "L 258.308369 156.068478 \n",
       "L 258.81615 155.97747 \n",
       "L 259.32393 156.164639 \n",
       "L 260.847272 155.893776 \n",
       "L 261.355052 156.079128 \n",
       "L 261.862833 156.537399 \n",
       "L 262.370613 156.173666 \n",
       "L 262.878394 156.629189 \n",
       "L 263.386174 156.538871 \n",
       "L 263.893955 156.178067 \n",
       "L 264.909516 156.540328 \n",
       "L 266.432857 156.274377 \n",
       "L 266.940638 155.920002 \n",
       "L 267.448418 155.833469 \n",
       "L 267.956199 156.012632 \n",
       "L 268.463979 155.92631 \n",
       "L 268.97176 156.104302 \n",
       "L 269.47954 155.754988 \n",
       "L 271.002882 155.501354 \n",
       "L 271.510662 155.678144 \n",
       "L 272.018443 155.59424 \n",
       "L 272.526223 154.992494 \n",
       "L 273.034004 155.427694 \n",
       "L 273.541784 155.602877 \n",
       "L 274.557345 155.437562 \n",
       "L 276.080687 155.956366 \n",
       "L 276.588467 155.61976 \n",
       "L 278.619589 155.295575 \n",
       "L 279.12737 155.466281 \n",
       "L 279.63515 155.135893 \n",
       "L 280.142931 155.555653 \n",
       "L 280.650712 155.226676 \n",
       "L 281.158492 155.147594 \n",
       "L 281.666273 155.564229 \n",
       "L 282.174053 155.484711 \n",
       "L 283.697395 155.984244 \n",
       "L 284.205175 156.393789 \n",
       "L 284.712956 156.557301 \n",
       "L 285.220736 156.476521 \n",
       "L 285.728517 155.910374 \n",
       "L 286.236297 156.073855 \n",
       "L 286.744078 155.511373 \n",
       "L 288.267419 156 \n",
       "L 288.7752 155.921909 \n",
       "L 289.28298 156.083033 \n",
       "L 289.790761 156.005107 \n",
       "L 290.298541 155.689815 \n",
       "L 290.806322 155.613181 \n",
       "L 291.314102 155.773523 \n",
       "L 291.821883 155.697054 \n",
       "L 292.329663 155.385427 \n",
       "L 292.837444 155.780147 \n",
       "L 293.345224 155.938612 \n",
       "L 293.853005 156.330208 \n",
       "L 294.360785 156.486675 \n",
       "L 294.868566 155.944019 \n",
       "L 295.376346 155.636132 \n",
       "L 295.884127 155.79311 \n",
       "L 296.391907 155.48698 \n",
       "L 297.407468 155.799463 \n",
       "L 297.915249 155.725022 \n",
       "L 298.423029 156.109093 \n",
       "L 298.93081 155.805727 \n",
       "L 299.43859 155.275658 \n",
       "L 299.946371 155.658152 \n",
       "L 300.454151 155.584867 \n",
       "L 300.961932 155.964958 \n",
       "L 301.469712 155.891313 \n",
       "L 302.485273 155.295004 \n",
       "L 302.993054 155.447851 \n",
       "L 303.500834 154.704013 \n",
       "L 304.008615 154.633973 \n",
       "L 305.024176 154.939878 \n",
       "L 306.039737 155.686165 \n",
       "L 306.547517 155.835798 \n",
       "L 307.055298 155.76421 \n",
       "L 307.563078 155.913017 \n",
       "L 308.070859 155.182757 \n",
       "L 308.578639 155.33218 \n",
       "L 309.08642 154.825002 \n",
       "L 309.5942 154.97456 \n",
       "L 310.609761 155.706217 \n",
       "L 312.640883 156.289232 \n",
       "L 313.656445 156.148086 \n",
       "L 314.672006 156.435261 \n",
       "L 315.179786 156.791048 \n",
       "L 315.687567 156.93266 \n",
       "L 316.195347 156.861479 \n",
       "L 317.718689 157.282349 \n",
       "L 318.226469 157.000597 \n",
       "L 318.73425 157.350001 \n",
       "L 319.24203 157.488399 \n",
       "L 319.749811 157.835354 \n",
       "L 320.257591 157.554789 \n",
       "L 320.765372 157.067107 \n",
       "L 321.273152 157.204949 \n",
       "L 321.780933 157.549637 \n",
       "L 322.288713 157.478936 \n",
       "L 322.796494 157.615096 \n",
       "L 323.304274 157.544545 \n",
       "L 324.827616 157.949269 \n",
       "L 325.335396 157.674171 \n",
       "L 325.843177 156.992066 \n",
       "L 326.350957 157.127279 \n",
       "L 327.366518 156.584788 \n",
       "L 327.874299 156.112773 \n",
       "L 328.88986 155.980804 \n",
       "L 329.39764 156.317606 \n",
       "L 329.905421 156.452279 \n",
       "L 330.413201 156.386009 \n",
       "L 330.920982 156.520005 \n",
       "L 331.428762 156.853082 \n",
       "L 331.936543 156.985622 \n",
       "L 332.444323 156.918826 \n",
       "L 332.952104 157.249139 \n",
       "L 333.967665 157.510599 \n",
       "L 334.475445 157.837818 \n",
       "L 335.491006 157.702469 \n",
       "L 335.998787 158.027405 \n",
       "L 338.537689 158.662211 \n",
       "L 339.04547 158.4 \n",
       "L 339.55325 158.332288 \n",
       "L 340.568811 158.583098 \n",
       "L 341.076592 158.900163 \n",
       "L 342.092153 158.764109 \n",
       "L 342.599933 158.505203 \n",
       "L 343.107714 158.629091 \n",
       "L 343.615494 158.371431 \n",
       "L 344.123275 158.495094 \n",
       "L 344.631055 158.428479 \n",
       "L 345.138836 158.741053 \n",
       "L 345.646617 158.295999 \n",
       "L 346.154397 158.607642 \n",
       "L 346.662178 158.352911 \n",
       "L 347.169958 158.287167 \n",
       "L 347.677739 158.409388 \n",
       "L 349.20108 157.653337 \n",
       "L 349.708861 157.775829 \n",
       "L 350.216641 158.083851 \n",
       "L 350.724422 158.205096 \n",
       "L 351.232202 157.955307 \n",
       "L 352.755544 157.764248 \n",
       "L 353.771105 158.00525 \n",
       "L 354.278885 157.758555 \n",
       "L 354.786666 157.878628 \n",
       "L 355.802227 157.752922 \n",
       "L 356.310007 157.508451 \n",
       "L 356.817788 157.80973 \n",
       "L 357.325568 157.928646 \n",
       "L 357.833349 158.228092 \n",
       "L 358.341129 158.345807 \n",
       "L 358.84891 158.643435 \n",
       "L 359.864471 158.876161 \n",
       "L 360.372251 158.812534 \n",
       "L 360.880032 159.107219 \n",
       "L 361.387812 159.043413 \n",
       "L 361.895593 159.33664 \n",
       "L 362.403373 159.272653 \n",
       "L 362.911154 159.564454 \n",
       "L 363.418934 159.677753 \n",
       "L 363.926715 159.967811 \n",
       "L 364.434495 160.080013 \n",
       "L 364.942276 160.368344 \n",
       "L 365.450056 160.126997 \n",
       "L 365.957837 159.710587 \n",
       "L 366.465617 159.646841 \n",
       "L 366.973398 159.933925 \n",
       "L 367.481178 159.870014 \n",
       "L 367.988959 159.631625 \n",
       "L 368.496739 159.742851 \n",
       "L 369.00452 159.679585 \n",
       "L 369.5123 159.268973 \n",
       "L 370.020081 159.380244 \n",
       "L 371.035642 159.947444 \n",
       "L 371.543422 159.53919 \n",
       "L 372.558983 160.102934 \n",
       "L 373.066764 159.868211 \n",
       "L 373.574544 159.462856 \n",
       "L 374.082325 159.572295 \n",
       "L 374.590105 159.168818 \n",
       "L 375.097886 159.10782 \n",
       "L 375.605666 159.217303 \n",
       "L 376.113447 159.156426 \n",
       "L 376.621227 159.435159 \n",
       "L 378.144569 159.252676 \n",
       "L 378.65235 158.855117 \n",
       "L 379.16013 158.290622 \n",
       "L 380.683472 158.617675 \n",
       "L 381.191252 158.224487 \n",
       "L 381.699033 157.999482 \n",
       "L 382.206813 157.608607 \n",
       "L 383.222374 157.827093 \n",
       "L 383.730155 157.604221 \n",
       "L 384.237935 157.878628 \n",
       "L 384.745716 157.821641 \n",
       "L 385.253496 157.434896 \n",
       "L 385.761277 157.708237 \n",
       "L 386.269057 157.816248 \n",
       "L 386.776838 158.088087 \n",
       "L 387.284618 158.195122 \n",
       "L 387.792399 158.138195 \n",
       "L 388.300179 158.408171 \n",
       "L 388.80796 158.351079 \n",
       "L 389.31574 158.457002 \n",
       "L 389.823521 158.725161 \n",
       "L 390.331301 158.505518 \n",
       "L 391.346862 159.039102 \n",
       "L 391.854643 158.981544 \n",
       "L 392.362423 159.08544 \n",
       "L 392.870204 158.866968 \n",
       "L 393.377984 158.970729 \n",
       "L 393.885765 158.592621 \n",
       "L 394.393545 158.53622 \n",
       "L 394.901326 158.64001 \n",
       "L 395.409106 158.423973 \n",
       "L 396.424667 158.312416 \n",
       "L 396.932448 158.574897 \n",
       "L 397.948009 158.463401 \n",
       "L 398.455789 158.249676 \n",
       "L 398.96357 158.352611 \n",
       "L 399.979131 157.612512 \n",
       "L 400.486911 157.558694 \n",
       "L 401.502472 157.765104 \n",
       "L 402.010253 157.711316 \n",
       "L 402.518033 157.970237 \n",
       "L 403.025814 157.916284 \n",
       "L 403.533594 157.70672 \n",
       "L 404.041375 157.653337 \n",
       "L 404.549155 157.289499 \n",
       "L 405.564716 157.184522 \n",
       "L 406.072497 156.977676 \n",
       "L 406.580277 156.925826 \n",
       "L 407.088058 157.182389 \n",
       "L 407.595838 157.284287 \n",
       "L 408.103619 157.539513 \n",
       "L 408.611399 157.487137 \n",
       "L 409.11918 157.588089 \n",
       "L 409.62696 157.382836 \n",
       "L 410.134741 157.483637 \n",
       "L 411.150302 157.379831 \n",
       "L 411.658083 157.632219 \n",
       "L 412.673644 157.831584 \n",
       "L 413.689205 157.122998 \n",
       "L 414.196985 157.072109 \n",
       "L 414.704766 157.322694 \n",
       "L 415.720327 156.619861 \n",
       "L 416.228107 156.42 \n",
       "L 417.243668 156.620296 \n",
       "L 417.751449 156.570669 \n",
       "L 418.76701 156.174071 \n",
       "L 419.27479 156.273987 \n",
       "L 419.782571 156.076709 \n",
       "L 420.290351 155.435294 \n",
       "L 420.798132 155.535871 \n",
       "L 421.813693 155.440942 \n",
       "L 422.321473 155.541053 \n",
       "L 422.829254 155.788033 \n",
       "L 423.337034 155.593474 \n",
       "L 424.860376 155.890982 \n",
       "L 425.368156 155.6974 \n",
       "L 425.875937 155.796129 \n",
       "L 426.891498 155.701831 \n",
       "L 427.907059 155.898132 \n",
       "L 428.414839 156.140696 \n",
       "L 428.92262 156.237938 \n",
       "L 429.4304 156.046083 \n",
       "L 429.938181 155.998979 \n",
       "L 430.445961 155.520009 \n",
       "L 431.461522 155.714884 \n",
       "L 431.969303 155.668516 \n",
       "L 432.477083 155.479094 \n",
       "L 433.492644 155.387424 \n",
       "L 434.000425 155.199219 \n",
       "L 434.508205 155.296281 \n",
       "L 435.015986 155.108722 \n",
       "L 435.523766 155.205648 \n",
       "L 436.031547 155.160512 \n",
       "L 436.539327 155.257093 \n",
       "L 437.047108 155.070645 \n",
       "L 437.554888 155.167061 \n",
       "L 438.062669 155.404202 \n",
       "L 438.570449 155.359111 \n",
       "L 439.07823 155.454731 \n",
       "L 439.58601 155.690475 \n",
       "L 440.093791 155.645174 \n",
       "L 440.601571 155.740013 \n",
       "L 441.617132 155.370421 \n",
       "L 442.632693 155.281336 \n",
       "L 443.140474 155.376009 \n",
       "L 443.648255 155.60926 \n",
       "L 444.156035 155.703228 \n",
       "L 444.663816 155.520009 \n",
       "L 445.679377 154.87891 \n",
       "L 446.187157 154.697569 \n",
       "L 446.694938 154.792137 \n",
       "L 447.202718 154.611412 \n",
       "L 447.710499 154.843161 \n",
       "L 448.72606 155.030871 \n",
       "L 449.741621 155.490735 \n",
       "L 450.249401 155.174135 \n",
       "L 450.757182 155.130816 \n",
       "L 451.264962 155.223657 \n",
       "L 451.772743 155.180384 \n",
       "L 452.788304 155.365164 \n",
       "L 453.296084 155.592483 \n",
       "L 453.803865 155.278722 \n",
       "L 454.311645 154.560974 \n",
       "L 456.342767 154.393512 \n",
       "L 456.850548 154.620004 \n",
       "L 457.358328 154.578172 \n",
       "L 457.866109 154.670142 \n",
       "L 458.373889 154.895372 \n",
       "L 459.897231 155.168563 \n",
       "L 460.405011 155.126325 \n",
       "L 461.420572 155.307126 \n",
       "L 461.928353 155.529457 \n",
       "L 462.943914 155.708485 \n",
       "L 463.451694 155.665887 \n",
       "L 464.467255 155.843863 \n",
       "L 464.975036 156.063762 \n",
       "L 465.482816 155.758758 \n",
       "L 465.990597 155.716371 \n",
       "L 466.498377 155.804826 \n",
       "L 467.006158 155.762498 \n",
       "L 467.513938 155.850668 \n",
       "L 468.021719 155.808385 \n",
       "L 468.529499 155.89627 \n",
       "L 469.03728 156.113825 \n",
       "L 469.54506 156.071362 \n",
       "L 470.052841 155.899439 \n",
       "L 470.560621 156.116153 \n",
       "L 471.068402 156.073855 \n",
       "L 472.083963 156.50518 \n",
       "L 472.591743 156.462538 \n",
       "L 473.099524 156.548574 \n",
       "L 473.607304 156.249173 \n",
       "L 474.115085 156.33527 \n",
       "L 474.622865 156.549235 \n",
       "L 475.638426 156.720002 \n",
       "L 476.146207 156.677479 \n",
       "L 477.161768 156.210916 \n",
       "L 477.669549 156.16919 \n",
       "L 478.177329 156.25452 \n",
       "L 478.68511 156.086052 \n",
       "L 480.208451 156.341053 \n",
       "L 480.716232 156.551804 \n",
       "L 481.731793 156.720002 \n",
       "L 482.239573 156.67811 \n",
       "L 482.747354 156.761848 \n",
       "L 483.762915 156.67826 \n",
       "L 484.270695 156.761698 \n",
       "L 485.286256 156.678425 \n",
       "L 486.809598 156.927163 \n",
       "L 487.317378 157.133798 \n",
       "L 487.825159 157.091966 \n",
       "L 488.332939 156.802569 \n",
       "L 488.84072 157.008603 \n",
       "L 489.856281 156.925631 \n",
       "L 490.871842 157.089232 \n",
       "L 491.379622 157.047806 \n",
       "L 491.887403 157.129262 \n",
       "L 492.395183 157.087896 \n",
       "L 492.902964 157.169096 \n",
       "L 493.918525 156.842192 \n",
       "L 495.441866 157.085222 \n",
       "L 496.457427 157.003376 \n",
       "L 496.965208 157.205205 \n",
       "L 497.472988 157.164244 \n",
       "L 497.980769 157.244378 \n",
       "L 498.488549 157.082593 \n",
       "L 498.99633 157.041918 \n",
       "L 499.50411 157.121917 \n",
       "L 500.011891 156.960868 \n",
       "L 501.027452 156.880194 \n",
       "L 501.535232 156.720002 \n",
       "L 502.043013 156.79991 \n",
       "L 502.550793 156.999335 \n",
       "L 503.058574 156.959156 \n",
       "L 504.074135 157.356219 \n",
       "L 505.597476 157.235095 \n",
       "L 506.105257 157.313652 \n",
       "L 506.613037 157.154826 \n",
       "L 507.120818 157.233278 \n",
       "L 507.628598 157.193248 \n",
       "L 508.136379 156.798784 \n",
       "L 509.15194 156.955791 \n",
       "L 510.167501 157.229688 \n",
       "L 511.183062 157.384968 \n",
       "L 512.198623 157.305376 \n",
       "L 512.706404 157.148773 \n",
       "L 513.214184 157.109345 \n",
       "L 514.229745 156.681159 \n",
       "L 514.737526 156.758799 \n",
       "L 515.245306 156.720002 \n",
       "L 515.753087 156.797432 \n",
       "L 516.260867 156.642677 \n",
       "L 516.768648 156.720002 \n",
       "L 517.784209 156.411754 \n",
       "L 518.291989 156.373617 \n",
       "L 518.79977 156.220229 \n",
       "L 519.30755 155.836804 \n",
       "L 519.815331 155.914534 \n",
       "L 520.323111 155.877134 \n",
       "L 520.830892 155.954624 \n",
       "L 521.338672 155.917283 \n",
       "L 522.354233 156.300483 \n",
       "L 522.862014 156.148582 \n",
       "L 523.877575 156.415929 \n",
       "L 524.385355 156.492203 \n",
       "L 524.893136 156.454547 \n",
       "L 525.400916 156.53061 \n",
       "L 526.416477 156.455433 \n",
       "L 526.924258 156.644494 \n",
       "L 527.432038 156.606868 \n",
       "L 527.939819 156.682346 \n",
       "L 529.970941 156.082507 \n",
       "L 532.002063 156.384011 \n",
       "L 532.509843 156.123332 \n",
       "L 534.033185 156.236826 \n",
       "L 536.064307 156.423965 \n",
       "L 537.079868 156.461546 \n",
       "L 538.603209 156.572802 \n",
       "L 539.11099 156.536197 \n",
       "L 539.61877 156.16919 \n",
       "L 542.665454 156.063326 \n",
       "L 543.173234 156.136926 \n",
       "L 544.188795 155.847273 \n",
       "L 545.204356 155.994308 \n",
       "L 545.712137 155.850097 \n",
       "L 546.219917 155.923456 \n",
       "L 547.235478 155.744519 \n",
       "L 549.774381 155.570057 \n",
       "L 550.789942 155.178071 \n",
       "L 551.297722 155.251355 \n",
       "L 551.805503 155.109788 \n",
       "L 552.313283 155.290213 \n",
       "L 554.344405 155.260683 \n",
       "L 554.852186 155.333336 \n",
       "L 555.867747 155.15886 \n",
       "L 557.391088 155.269905 \n",
       "L 558.406649 155.09648 \n",
       "L 558.91443 155.274471 \n",
       "L 559.929991 155.312683 \n",
       "L 560.437771 155.384435 \n",
       "L 562.976674 154.692117 \n",
       "L 566.023357 155.017428 \n",
       "L 568.054479 154.886015 \n",
       "L 568.562259 155.060746 \n",
       "L 569.07004 155.027927 \n",
       "L 570.085601 155.376009 \n",
       "L 571.608942 155.380125 \n",
       "L 573.640064 155.556674 \n",
       "L 577.194528 155.429001 \n",
       "L 578.71787 155.839357 \n",
       "L 579.22565 155.603387 \n",
       "L 579.733431 155.570718 \n",
       "L 580.748992 155.809166 \n",
       "L 582.272333 155.710993 \n",
       "L 582.780114 155.880003 \n",
       "L 582.780114 155.880003 \n",
       "\" clip-path=\"url(#p6070896839)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_19\">\n",
       "    <path d=\"M 75.507386 324.72 \n",
       "L 76.015167 274.320001 \n",
       "L 76.522947 257.519999 \n",
       "L 77.538508 143.28 \n",
       "L 78.046289 139.920003 \n",
       "L 78.554069 151.920002 \n",
       "L 79.06185 160.920013 \n",
       "L 79.56963 156.720002 \n",
       "L 80.077411 143.28 \n",
       "L 80.585191 150.610917 \n",
       "L 81.092972 139.920003 \n",
       "L 81.600752 154.135387 \n",
       "L 82.616313 163.44001 \n",
       "L 83.124094 160.920013 \n",
       "L 83.631874 164.625883 \n",
       "L 84.139655 173.52 \n",
       "L 84.647436 165.562118 \n",
       "L 85.662997 171.12 \n",
       "L 86.170777 164.356373 \n",
       "L 86.678558 166.946095 \n",
       "L 87.186338 165.120008 \n",
       "L 87.694119 167.472003 \n",
       "L 88.201899 173.52 \n",
       "L 88.70968 175.386671 \n",
       "L 89.21746 173.52 \n",
       "L 90.740802 178.39743 \n",
       "L 91.248582 170.370003 \n",
       "L 92.264143 167.590589 \n",
       "L 92.771924 172.080012 \n",
       "L 93.279704 173.52 \n",
       "L 93.787485 172.157848 \n",
       "L 94.295265 173.52 \n",
       "L 94.803046 177.396923 \n",
       "L 95.818607 179.66635 \n",
       "L 96.326387 178.32 \n",
       "L 96.834168 167.659547 \n",
       "L 97.341948 171.229093 \n",
       "L 97.849729 172.400006 \n",
       "L 98.357509 169.137402 \n",
       "L 98.86529 170.302982 \n",
       "L 99.37307 173.52 \n",
       "L 99.880851 172.491435 \n",
       "L 100.388631 173.52 \n",
       "L 100.896412 170.555294 \n",
       "L 102.419753 173.52 \n",
       "L 102.927534 170.770912 \n",
       "L 103.435314 169.920008 \n",
       "L 103.943095 165.562118 \n",
       "L 104.450875 164.830356 \n",
       "L 105.974217 167.736407 \n",
       "L 106.481997 170.26839 \n",
       "L 107.497558 171.945009 \n",
       "L 108.005339 171.193855 \n",
       "L 108.513119 165.883644 \n",
       "L 109.0209 163.740898 \n",
       "L 109.52868 163.143538 \n",
       "L 110.036461 164.024347 \n",
       "L 111.559802 162.320001 \n",
       "L 113.083144 152.688009 \n",
       "L 113.590924 150.972637 \n",
       "L 114.098705 150.610917 \n",
       "L 114.606485 148.966156 \n",
       "L 115.114266 148.638998 \n",
       "L 115.622046 149.580009 \n",
       "L 116.129827 148.008893 \n",
       "L 117.145388 149.838074 \n",
       "L 117.653169 148.32001 \n",
       "L 118.160949 150.395299 \n",
       "L 118.66873 150.078144 \n",
       "L 119.17651 148.609663 \n",
       "L 120.192071 150.302023 \n",
       "L 120.699852 147.760006 \n",
       "L 121.207632 148.596926 \n",
       "L 121.715413 148.32001 \n",
       "L 122.223193 150.216782 \n",
       "L 122.730974 149.928511 \n",
       "L 123.238754 148.58527 \n",
       "L 124.254315 150.138556 \n",
       "L 124.762096 151.920002 \n",
       "L 125.269876 152.647273 \n",
       "L 126.285437 152.062575 \n",
       "L 126.793218 153.755296 \n",
       "L 127.300998 151.500588 \n",
       "L 127.808779 150.258464 \n",
       "L 128.316559 150.960005 \n",
       "L 128.82434 150.697359 \n",
       "L 129.33212 152.32375 \n",
       "L 129.839901 148.32001 \n",
       "L 130.347681 149.013591 \n",
       "L 130.855462 150.610917 \n",
       "L 131.363242 150.363246 \n",
       "L 131.871023 151.020012 \n",
       "L 132.378803 148.989032 \n",
       "L 133.902145 150.9269 \n",
       "L 134.409925 150.689233 \n",
       "L 135.933267 152.520006 \n",
       "L 136.441047 150.610917 \n",
       "L 137.456608 150.163911 \n",
       "L 138.472169 148.118407 \n",
       "L 139.48773 149.312135 \n",
       "L 139.995511 148.32001 \n",
       "L 140.503291 148.124655 \n",
       "L 141.011072 147.156923 \n",
       "L 141.518852 148.512376 \n",
       "L 142.026633 148.32001 \n",
       "L 143.042194 149.44837 \n",
       "L 144.057755 147.57883 \n",
       "L 144.565535 148.871828 \n",
       "L 145.073316 149.415656 \n",
       "L 145.581096 150.676841 \n",
       "L 146.088877 150.480014 \n",
       "L 146.596657 151.000861 \n",
       "L 147.104438 150.094652 \n",
       "L 147.612218 149.906026 \n",
       "L 148.119999 150.420008 \n",
       "L 148.627779 151.622073 \n",
       "L 149.13556 150.736442 \n",
       "L 149.64334 150.548582 \n",
       "L 150.151121 151.04433 \n",
       "L 151.166682 150.672005 \n",
       "L 151.674463 149.821987 \n",
       "L 152.182243 149.646324 \n",
       "L 152.690024 150.131767 \n",
       "L 153.705585 149.783234 \n",
       "L 154.213365 147.673849 \n",
       "L 154.721146 147.517457 \n",
       "L 155.228926 146.725072 \n",
       "L 155.736707 147.210575 \n",
       "L 156.244487 148.32001 \n",
       "L 156.752268 148.163483 \n",
       "L 157.260048 147.386674 \n",
       "L 157.767829 146.000985 \n",
       "L 158.275609 145.861476 \n",
       "L 158.78339 145.11274 \n",
       "L 159.29117 144.980246 \n",
       "L 159.798951 143.642156 \n",
       "L 160.306731 144.120014 \n",
       "L 160.814512 143.399292 \n",
       "L 162.337853 143.045592 \n",
       "L 163.861195 144.432002 \n",
       "L 164.368975 145.456376 \n",
       "L 164.876756 145.899673 \n",
       "L 165.384536 145.771699 \n",
       "L 166.907878 147.066966 \n",
       "L 167.415658 145.827695 \n",
       "L 167.923439 146.254439 \n",
       "L 168.431219 147.224349 \n",
       "L 169.44678 148.049043 \n",
       "L 169.954561 147.376686 \n",
       "L 170.462341 147.783843 \n",
       "L 170.970122 147.120003 \n",
       "L 172.493463 148.32001 \n",
       "L 173.001244 147.144876 \n",
       "L 174.524585 145.2343 \n",
       "L 176.555707 144.792011 \n",
       "L 177.063488 143.681194 \n",
       "L 178.079049 144.471731 \n",
       "L 178.586829 143.872944 \n",
       "L 179.09461 142.788293 \n",
       "L 179.60239 143.182142 \n",
       "L 180.110171 142.11131 \n",
       "L 180.617951 142.504618 \n",
       "L 181.125732 143.376461 \n",
       "L 181.633512 143.28 \n",
       "L 182.649074 144.991706 \n",
       "L 184.172415 143.28 \n",
       "L 184.680196 143.653346 \n",
       "L 185.187976 144.487743 \n",
       "L 185.695757 144.852122 \n",
       "L 186.711318 144.654559 \n",
       "L 187.219098 145.469324 \n",
       "L 188.234659 146.172923 \n",
       "L 188.74244 146.070004 \n",
       "L 189.25022 145.520003 \n",
       "L 189.758001 145.866913 \n",
       "L 190.265781 146.654807 \n",
       "L 190.773562 146.551587 \n",
       "L 191.281342 147.329612 \n",
       "L 192.296903 147.992731 \n",
       "L 192.804684 147.451037 \n",
       "L 194.328025 148.42724 \n",
       "L 195.343586 148.213681 \n",
       "L 195.851367 148.955297 \n",
       "L 196.359147 148.84721 \n",
       "L 197.882489 149.777857 \n",
       "L 198.89805 147.90689 \n",
       "L 199.913611 149.344399 \n",
       "L 200.929172 148.32001 \n",
       "L 202.452513 148.018807 \n",
       "L 204.483635 149.209426 \n",
       "L 204.991416 149.895001 \n",
       "L 205.499196 149.79082 \n",
       "L 206.006977 150.078144 \n",
       "L 207.022538 151.421551 \n",
       "L 207.530318 151.699323 \n",
       "L 208.038099 151.205498 \n",
       "L 208.545879 149.948909 \n",
       "L 209.05366 149.847281 \n",
       "L 209.56144 150.126795 \n",
       "L 210.577001 149.924501 \n",
       "L 211.084782 149.44837 \n",
       "L 211.592562 149.350498 \n",
       "L 212.100343 149.626677 \n",
       "L 212.608123 149.1569 \n",
       "L 213.115904 149.06119 \n",
       "L 213.623684 149.335387 \n",
       "L 214.131465 148.503949 \n",
       "L 215.147026 148.32001 \n",
       "L 216.670368 149.132912 \n",
       "L 218.193709 148.856177 \n",
       "L 219.717051 149.646324 \n",
       "L 220.224831 148.848682 \n",
       "L 221.240392 150.070003 \n",
       "L 222.763734 149.792171 \n",
       "L 223.271514 149.010421 \n",
       "L 223.779295 148.922056 \n",
       "L 224.287075 149.177147 \n",
       "L 224.794856 149.772209 \n",
       "L 225.302636 149.682162 \n",
       "L 226.318197 150.180403 \n",
       "L 226.825978 150.089905 \n",
       "L 227.333758 149.664003 \n",
       "L 228.349319 149.488219 \n",
       "L 228.8571 149.733862 \n",
       "L 230.380441 151.449414 \n",
       "L 231.396002 150.610917 \n",
       "L 231.903783 150.521951 \n",
       "L 232.411563 151.083878 \n",
       "L 232.919344 150.669842 \n",
       "L 233.427124 151.227698 \n",
       "L 233.934905 151.459943 \n",
       "L 234.442685 152.011732 \n",
       "L 234.950466 152.240011 \n",
       "L 235.458246 152.147861 \n",
       "L 235.966027 152.692244 \n",
       "L 236.473807 151.965289 \n",
       "L 236.981588 151.874866 \n",
       "L 237.489368 152.100006 \n",
       "L 237.997149 152.637766 \n",
       "L 238.504929 151.920002 \n",
       "L 239.01271 152.142919 \n",
       "L 239.52049 152.053338 \n",
       "L 240.028271 152.584623 \n",
       "L 241.551612 153.237078 \n",
       "L 242.059393 153.145544 \n",
       "L 242.567173 153.360005 \n",
       "L 243.074954 152.964113 \n",
       "L 243.582734 152.266988 \n",
       "L 244.090515 152.48217 \n",
       "L 244.598295 151.790662 \n",
       "L 245.613856 151.020012 \n",
       "L 246.121637 150.937219 \n",
       "L 246.629417 151.153137 \n",
       "L 247.137198 150.475764 \n",
       "L 247.644979 150.395299 \n",
       "L 248.152759 149.724114 \n",
       "L 248.66054 149.646324 \n",
       "L 249.16832 149.862857 \n",
       "L 249.676101 150.371177 \n",
       "L 250.183881 149.707832 \n",
       "L 250.691662 149.922323 \n",
       "L 251.199442 149.554594 \n",
       "L 252.722784 149.328012 \n",
       "L 253.230564 148.966156 \n",
       "L 253.738345 148.892737 \n",
       "L 254.246125 149.390827 \n",
       "L 255.769467 150.018889 \n",
       "L 256.277247 150.508238 \n",
       "L 256.785028 150.713296 \n",
       "L 257.292808 150.636436 \n",
       "L 257.800589 150.280003 \n",
       "L 258.308369 150.483995 \n",
       "L 258.81615 150.965307 \n",
       "L 259.32393 150.610917 \n",
       "L 259.831711 150.81231 \n",
       "L 260.339491 150.184113 \n",
       "L 261.355052 150.036628 \n",
       "L 261.862833 149.415656 \n",
       "L 262.370613 149.890735 \n",
       "L 262.878394 150.090822 \n",
       "L 263.386174 150.561515 \n",
       "L 264.401735 150.414375 \n",
       "L 264.909516 150.071881 \n",
       "L 265.925077 151.000861 \n",
       "L 266.432857 150.659523 \n",
       "L 266.940638 150.586674 \n",
       "L 267.448418 150.780166 \n",
       "L 268.463979 150.634964 \n",
       "L 268.97176 150.299064 \n",
       "L 269.987321 150.682504 \n",
       "L 270.495101 150.610917 \n",
       "L 272.526223 151.364744 \n",
       "L 273.034004 150.77539 \n",
       "L 273.541784 150.704659 \n",
       "L 274.049565 151.148571 \n",
       "L 274.557345 150.820766 \n",
       "L 275.065126 151.006298 \n",
       "L 275.572906 150.935702 \n",
       "L 276.080687 150.610917 \n",
       "L 276.588467 151.049482 \n",
       "L 277.096248 150.726033 \n",
       "L 277.604028 150.909477 \n",
       "L 278.111809 150.840008 \n",
       "L 278.619589 150.519503 \n",
       "L 279.63515 150.883777 \n",
       "L 280.142931 150.815059 \n",
       "L 280.650712 150.995558 \n",
       "L 281.158492 151.423459 \n",
       "L 281.666273 151.106244 \n",
       "L 282.174053 151.037661 \n",
       "L 282.681834 150.476485 \n",
       "L 284.205175 150.277284 \n",
       "L 285.728517 150.809651 \n",
       "L 286.744078 150.676841 \n",
       "L 287.251858 150.852069 \n",
       "L 287.759639 151.267022 \n",
       "L 288.267419 151.200001 \n",
       "L 288.7752 150.654446 \n",
       "L 289.790761 150.524264 \n",
       "L 290.298541 150.935101 \n",
       "L 291.314102 150.804514 \n",
       "L 291.821883 151.211807 \n",
       "L 292.329663 150.910663 \n",
       "L 292.837444 150.84588 \n",
       "L 293.345224 150.312567 \n",
       "L 293.853005 150.717216 \n",
       "L 294.360785 150.186666 \n",
       "L 294.868566 150.124167 \n",
       "L 295.376346 150.294197 \n",
       "L 295.884127 150.000008 \n",
       "L 296.391907 150.169543 \n",
       "L 296.899688 150.107644 \n",
       "L 297.915249 150.44392 \n",
       "L 298.423029 150.381826 \n",
       "L 298.93081 150.091437 \n",
       "L 300.454151 149.909195 \n",
       "L 300.961932 150.075516 \n",
       "L 301.469712 150.467097 \n",
       "L 302.485273 150.345011 \n",
       "L 304.008615 151.504929 \n",
       "L 304.516395 151.665137 \n",
       "L 305.024176 151.602126 \n",
       "L 305.531956 151.31737 \n",
       "L 306.039737 151.476931 \n",
       "L 306.547517 151.414747 \n",
       "L 308.070859 152.547463 \n",
       "L 308.578639 152.264359 \n",
       "L 309.08642 152.638441 \n",
       "L 309.5942 152.792731 \n",
       "L 310.101981 152.293228 \n",
       "L 310.609761 152.013113 \n",
       "L 311.117542 152.167748 \n",
       "L 311.625322 152.105414 \n",
       "L 312.133103 152.475035 \n",
       "L 312.640883 152.41231 \n",
       "L 313.148664 152.564782 \n",
       "L 313.656445 152.287671 \n",
       "L 314.164225 152.653762 \n",
       "L 314.672006 152.377628 \n",
       "L 316.195347 152.192847 \n",
       "L 316.703128 152.343532 \n",
       "L 317.210908 152.704907 \n",
       "L 317.718689 152.643023 \n",
       "L 318.226469 152.370944 \n",
       "L 319.24203 152.249324 \n",
       "L 319.749811 151.979753 \n",
       "L 320.257591 152.128695 \n",
       "L 320.765372 151.860506 \n",
       "L 321.273152 151.801251 \n",
       "L 321.780933 151.53482 \n",
       "L 322.796494 151.831487 \n",
       "L 323.304274 151.360493 \n",
       "L 323.812055 151.30286 \n",
       "L 324.319835 151.040169 \n",
       "L 325.843177 151.482759 \n",
       "L 326.350957 151.832734 \n",
       "L 327.874299 151.659759 \n",
       "L 328.382079 152.00658 \n",
       "L 329.39764 152.293664 \n",
       "L 329.905421 152.637135 \n",
       "L 330.413201 152.57845 \n",
       "L 331.936543 151.806178 \n",
       "L 332.444323 151.749596 \n",
       "L 333.459884 152.033166 \n",
       "L 333.967665 151.778826 \n",
       "L 334.475445 151.32823 \n",
       "L 334.983226 151.666879 \n",
       "L 335.491006 151.611228 \n",
       "L 335.998787 151.751909 \n",
       "L 336.506567 151.696319 \n",
       "L 337.014348 151.836279 \n",
       "L 337.522128 152.170677 \n",
       "L 338.537689 152.447172 \n",
       "L 339.04547 152.390771 \n",
       "L 339.55325 152.528072 \n",
       "L 340.061031 152.085527 \n",
       "L 340.568811 151.837405 \n",
       "L 341.584372 152.112008 \n",
       "L 342.599933 152.001983 \n",
       "L 343.107714 151.75637 \n",
       "L 343.615494 152.083334 \n",
       "L 344.631055 151.594586 \n",
       "L 345.138836 151.920002 \n",
       "L 345.646617 151.865974 \n",
       "L 346.154397 151.62338 \n",
       "L 346.662178 151.758518 \n",
       "L 347.169958 151.705076 \n",
       "L 347.677739 151.276424 \n",
       "L 348.185519 151.598822 \n",
       "L 349.20108 151.86668 \n",
       "L 349.708861 152.186178 \n",
       "L 350.724422 152.079128 \n",
       "L 351.232202 152.211187 \n",
       "L 351.739983 151.972844 \n",
       "L 352.755544 151.867356 \n",
       "L 353.263324 151.447011 \n",
       "L 353.771105 151.395416 \n",
       "L 354.278885 151.160737 \n",
       "L 354.786666 150.378086 \n",
       "L 355.294446 150.328699 \n",
       "L 355.802227 150.644052 \n",
       "L 356.310007 150.776321 \n",
       "L 356.817788 150.726499 \n",
       "L 357.325568 150.858137 \n",
       "L 357.833349 150.446399 \n",
       "L 358.341129 150.758717 \n",
       "L 358.84891 150.70927 \n",
       "L 359.35669 150.840008 \n",
       "L 359.864471 150.610917 \n",
       "L 360.372251 150.023921 \n",
       "L 360.880032 149.976141 \n",
       "L 363.418934 150.627049 \n",
       "L 363.926715 150.578713 \n",
       "L 364.434495 150.707378 \n",
       "L 364.942276 150.129469 \n",
       "L 365.957837 150.73886 \n",
       "L 366.465617 150.866353 \n",
       "L 366.973398 151.168698 \n",
       "L 367.481178 150.945 \n",
       "L 367.988959 151.071486 \n",
       "L 368.496739 151.023121 \n",
       "L 369.00452 150.800834 \n",
       "L 369.5123 151.100701 \n",
       "L 370.020081 150.879045 \n",
       "L 370.527861 151.004541 \n",
       "L 371.035642 150.956716 \n",
       "L 371.543422 150.736442 \n",
       "L 372.558983 150.642189 \n",
       "L 373.066764 150.423583 \n",
       "L 373.574544 150.548582 \n",
       "L 374.590105 150.455606 \n",
       "L 375.097886 150.068231 \n",
       "L 375.605666 149.852433 \n",
       "L 376.113447 149.977343 \n",
       "L 376.621227 149.762431 \n",
       "L 377.129008 150.056485 \n",
       "L 378.144569 149.628555 \n",
       "L 378.65235 149.584214 \n",
       "L 379.16013 149.876601 \n",
       "L 379.667911 149.664003 \n",
       "L 380.175691 149.619843 \n",
       "L 380.683472 149.743265 \n",
       "L 381.191252 149.699105 \n",
       "L 381.699033 149.821987 \n",
       "L 382.714594 150.39922 \n",
       "L 383.222374 150.354279 \n",
       "L 385.761277 150.955304 \n",
       "L 386.776838 150.864626 \n",
       "L 387.284618 151.147324 \n",
       "L 387.792399 151.265459 \n",
       "L 388.300179 151.056481 \n",
       "L 388.80796 151.01127 \n",
       "L 389.823521 149.945815 \n",
       "L 390.331301 149.740291 \n",
       "L 390.839082 149.859553 \n",
       "L 391.346862 149.81664 \n",
       "L 391.854643 150.096935 \n",
       "L 392.870204 149.688696 \n",
       "L 393.377984 149.16402 \n",
       "L 393.885765 148.962041 \n",
       "L 394.393545 149.241465 \n",
       "L 394.901326 149.200008 \n",
       "L 395.409106 148.839189 \n",
       "L 396.424667 149.076406 \n",
       "L 396.932448 148.87647 \n",
       "L 397.440228 148.518429 \n",
       "L 397.948009 148.32001 \n",
       "L 398.455789 148.280446 \n",
       "L 398.96357 148.399002 \n",
       "L 399.47135 148.201695 \n",
       "L 399.979131 148.32001 \n",
       "L 400.486911 148.595198 \n",
       "L 400.994692 147.770475 \n",
       "L 401.502472 147.418608 \n",
       "L 402.518033 147.030707 \n",
       "L 403.025814 147.149728 \n",
       "L 403.533594 147.42418 \n",
       "L 404.041375 147.075557 \n",
       "L 404.549155 147.193963 \n",
       "L 405.056936 146.846782 \n",
       "L 405.564716 146.655498 \n",
       "L 406.580277 146.892129 \n",
       "L 408.103619 146.783426 \n",
       "L 409.11918 147.324265 \n",
       "L 409.62696 146.98162 \n",
       "L 410.642521 146.604428 \n",
       "L 411.150302 146.873488 \n",
       "L 412.165863 146.801931 \n",
       "L 412.673644 147.069474 \n",
       "L 414.196985 147.414612 \n",
       "L 414.704766 147.227624 \n",
       "L 415.212546 147.19165 \n",
       "L 416.228107 147.420004 \n",
       "L 416.735888 147.084344 \n",
       "L 417.243668 147.198349 \n",
       "L 417.751449 147.461341 \n",
       "L 418.259229 147.425337 \n",
       "L 418.76701 147.53832 \n",
       "L 419.782571 147.169495 \n",
       "L 420.290351 147.430594 \n",
       "L 421.813693 147.766569 \n",
       "L 422.321473 147.730536 \n",
       "L 422.829254 147.547452 \n",
       "L 423.337034 147.658783 \n",
       "L 423.844815 147.623065 \n",
       "L 424.352595 147.73396 \n",
       "L 424.860376 147.698242 \n",
       "L 425.875937 147.918846 \n",
       "L 426.383717 147.591687 \n",
       "L 426.891498 147.701832 \n",
       "L 427.907059 147.631086 \n",
       "L 428.414839 147.740689 \n",
       "L 428.92262 147.56076 \n",
       "L 429.4304 147.670154 \n",
       "L 430.445961 147.600009 \n",
       "L 431.461522 147.24308 \n",
       "L 431.969303 147.208773 \n",
       "L 432.984864 147.712346 \n",
       "L 433.492644 147.677514 \n",
       "L 434.508205 147.892891 \n",
       "L 435.015986 148.142289 \n",
       "L 435.523766 148.107051 \n",
       "L 436.539327 147.753712 \n",
       "L 437.047108 147.860537 \n",
       "L 437.554888 148.108238 \n",
       "L 438.062669 148.214267 \n",
       "L 438.570449 147.897667 \n",
       "L 439.07823 147.863105 \n",
       "L 440.093791 148.074667 \n",
       "L 440.601571 148.32001 \n",
       "L 441.109352 148.424867 \n",
       "L 441.617132 148.669038 \n",
       "L 442.124913 148.773114 \n",
       "L 442.632693 148.598458 \n",
       "L 443.140474 148.56331 \n",
       "L 443.648255 148.667116 \n",
       "L 444.156035 148.631968 \n",
       "L 444.663816 148.873856 \n",
       "L 445.171596 148.976791 \n",
       "L 445.679377 149.217537 \n",
       "L 446.187157 149.181834 \n",
       "L 446.694938 149.421649 \n",
       "L 447.202718 149.110727 \n",
       "L 447.710499 149.349987 \n",
       "L 448.72606 149.2787 \n",
       "L 449.741621 149.480979 \n",
       "L 450.249401 149.718106 \n",
       "L 450.757182 149.818382 \n",
       "L 451.264962 149.646324 \n",
       "L 451.772743 149.882264 \n",
       "L 452.280523 149.710581 \n",
       "L 452.788304 149.945815 \n",
       "L 453.803865 149.873897 \n",
       "L 454.311645 150.10796 \n",
       "L 454.819426 150.206644 \n",
       "L 455.327206 150.43964 \n",
       "L 455.834987 150.537602 \n",
       "L 456.342767 150.769547 \n",
       "L 456.850548 150.464694 \n",
       "L 457.358328 150.428374 \n",
       "L 457.866109 150.124783 \n",
       "L 458.88167 149.386667 \n",
       "L 459.38945 149.485125 \n",
       "L 459.897231 149.184387 \n",
       "L 460.405011 149.282846 \n",
       "L 460.912792 149.248434 \n",
       "L 461.420572 149.346547 \n",
       "L 462.436133 149.013591 \n",
       "L 463.451694 148.945894 \n",
       "L 463.959475 149.043767 \n",
       "L 464.975036 148.97625 \n",
       "L 465.482816 148.811552 \n",
       "L 465.990597 148.909094 \n",
       "L 466.498377 148.744906 \n",
       "L 468.021719 149.036286 \n",
       "L 468.529499 149.002851 \n",
       "L 469.03728 148.709698 \n",
       "L 469.54506 148.547028 \n",
       "L 470.052841 148.643909 \n",
       "L 471.068402 148.578466 \n",
       "L 471.576182 148.287746 \n",
       "L 472.083963 148.255558 \n",
       "L 472.591743 148.094719 \n",
       "L 473.099524 148.062861 \n",
       "L 473.607304 147.902684 \n",
       "L 474.115085 147.9994 \n",
       "L 474.622865 147.967782 \n",
       "L 475.130646 148.064168 \n",
       "L 475.638426 147.904802 \n",
       "L 476.146207 148.001022 \n",
       "L 476.653988 147.842137 \n",
       "L 477.161768 147.81091 \n",
       "L 477.669549 147.525553 \n",
       "L 478.177329 147.494821 \n",
       "L 478.68511 147.590951 \n",
       "L 479.19289 147.306946 \n",
       "L 479.700671 147.27659 \n",
       "L 480.716232 147.468445 \n",
       "L 481.224012 147.438014 \n",
       "L 481.731793 147.533483 \n",
       "L 482.239573 147.503052 \n",
       "L 482.747354 147.22163 \n",
       "L 483.255134 147.4424 \n",
       "L 483.762915 147.286969 \n",
       "L 484.270695 147.507108 \n",
       "L 484.778476 147.351977 \n",
       "L 485.794037 147.292076 \n",
       "L 486.301817 147.386674 \n",
       "L 486.809598 147.605341 \n",
       "L 487.317378 147.57518 \n",
       "L 487.825159 147.793065 \n",
       "L 488.332939 147.886597 \n",
       "L 488.84072 147.732518 \n",
       "L 489.3485 147.825885 \n",
       "L 489.856281 147.795649 \n",
       "L 490.364061 147.519034 \n",
       "L 491.887403 147.798202 \n",
       "L 492.902964 147.738226 \n",
       "L 493.410744 147.953017 \n",
       "L 493.918525 147.922916 \n",
       "L 494.426305 147.77085 \n",
       "L 494.934086 147.86294 \n",
       "L 495.441866 147.83305 \n",
       "L 495.949647 147.681644 \n",
       "L 496.457427 147.773494 \n",
       "L 496.965208 147.622539 \n",
       "L 497.472988 147.229621 \n",
       "L 498.99633 147.142999 \n",
       "L 499.50411 146.873113 \n",
       "L 500.011891 146.844739 \n",
       "L 500.519671 146.936709 \n",
       "L 501.027452 146.908321 \n",
       "L 501.535232 147.000005 \n",
       "L 502.550793 146.703848 \n",
       "L 503.058574 146.795457 \n",
       "L 503.566354 146.767399 \n",
       "L 504.074135 146.858708 \n",
       "L 504.581915 146.473195 \n",
       "L 505.597476 146.655858 \n",
       "L 506.613037 146.600478 \n",
       "L 507.628598 146.308737 \n",
       "L 508.64416 146.490499 \n",
       "L 509.659721 146.20038 \n",
       "L 510.167501 146.291073 \n",
       "L 510.675282 145.911614 \n",
       "L 511.183062 146.002427 \n",
       "L 511.690843 145.975826 \n",
       "L 513.721965 146.336675 \n",
       "L 514.229745 146.309834 \n",
       "L 514.737526 146.050267 \n",
       "L 515.245306 146.140074 \n",
       "L 515.753087 145.881303 \n",
       "L 517.276428 146.150092 \n",
       "L 517.784209 146.008075 \n",
       "L 518.291989 146.097326 \n",
       "L 518.79977 146.07104 \n",
       "L 520.323111 146.337336 \n",
       "L 521.338672 146.28451 \n",
       "L 522.354233 146.575169 \n",
       "L 522.862014 146.548583 \n",
       "L 523.877575 146.72363 \n",
       "L 524.385355 146.469155 \n",
       "L 524.893136 146.556574 \n",
       "L 525.400916 146.757441 \n",
       "L 525.908697 146.503792 \n",
       "L 526.416477 146.59088 \n",
       "L 526.924258 146.451236 \n",
       "L 528.95538 147.136119 \n",
       "L 529.970941 146.970009 \n",
       "L 530.478721 147.055791 \n",
       "L 530.986502 146.804635 \n",
       "L 531.494282 146.778289 \n",
       "L 532.002063 146.976002 \n",
       "L 533.017624 146.923116 \n",
       "L 535.048746 147.151801 \n",
       "L 537.587648 147.462482 \n",
       "L 539.11099 147.492868 \n",
       "L 541.142112 147.716086 \n",
       "L 542.665454 147.417075 \n",
       "L 544.696576 147.420981 \n",
       "L 545.204356 147.503593 \n",
       "L 546.219917 147.125185 \n",
       "L 547.235478 147.398721 \n",
       "L 549.774381 147.376686 \n",
       "L 553.328844 147.838472 \n",
       "L 554.852186 147.86668 \n",
       "L 555.867747 148.133727 \n",
       "L 556.375527 147.894693 \n",
       "L 556.883308 148.081021 \n",
       "L 560.945552 147.872358 \n",
       "L 562.468893 148.32001 \n",
       "L 563.484454 148.372401 \n",
       "L 563.992235 148.241499 \n",
       "L 565.007796 148.29389 \n",
       "L 566.531137 148.215874 \n",
       "L 567.546698 148.475876 \n",
       "L 569.07004 148.39771 \n",
       "L 570.085601 148.035704 \n",
       "L 575.671187 147.859966 \n",
       "L 576.178967 148.039159 \n",
       "L 576.686748 148.01394 \n",
       "L 577.194528 148.192607 \n",
       "L 578.210089 148.040286 \n",
       "L 579.22565 148.193118 \n",
       "L 579.733431 147.965079 \n",
       "L 581.256772 148.496935 \n",
       "L 581.764553 148.471505 \n",
       "L 582.272333 148.647935 \n",
       "L 582.780114 148.521613 \n",
       "L 582.780114 148.521613 \n",
       "\" clip-path=\"url(#p6070896839)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_20\">\n",
       "    <path d=\"M 50.14375 156.384007 \n",
       "L 608.14375 156.384007 \n",
       "\" clip-path=\"url(#p6070896839)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #000000; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 339.84 \n",
       "L 50.14375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 608.14375 339.84 \n",
       "L 608.14375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 339.84 \n",
       "L 608.14375 339.84 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 7.2 \n",
       "L 608.14375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 525.289062 103.26875 \n",
       "L 601.14375 103.26875 \n",
       "Q 603.14375 103.26875 603.14375 101.26875 \n",
       "L 603.14375 14.2 \n",
       "Q 603.14375 12.2 601.14375 12.2 \n",
       "L 525.289062 12.2 \n",
       "Q 523.289062 12.2 523.289062 14.2 \n",
       "L 523.289062 101.26875 \n",
       "Q 523.289062 103.26875 525.289062 103.26875 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_21\">\n",
       "     <path d=\"M 527.289062 20.298438 \n",
       "L 537.289062 20.298438 \n",
       "L 547.289062 20.298438 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_16\">\n",
       "     <!-- P(die=1) -->\n",
       "     <g transform=\"translate(555.289062 23.798438)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \n",
       "L 1259 2394 \n",
       "L 2053 2394 \n",
       "Q 2494 2394 2734 2622 \n",
       "Q 2975 2850 2975 3272 \n",
       "Q 2975 3691 2734 3919 \n",
       "Q 2494 4147 2053 4147 \n",
       "L 1259 4147 \n",
       "z\n",
       "M 628 4666 \n",
       "L 2053 4666 \n",
       "Q 2838 4666 3239 4311 \n",
       "Q 3641 3956 3641 3272 \n",
       "Q 3641 2581 3239 2228 \n",
       "Q 2838 1875 2053 1875 \n",
       "L 1259 1875 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-3d\" d=\"M 678 2906 \n",
       "L 4684 2906 \n",
       "L 4684 2381 \n",
       "L 678 2381 \n",
       "L 678 2906 \n",
       "z\n",
       "M 678 1631 \n",
       "L 4684 1631 \n",
       "L 4684 1100 \n",
       "L 678 1100 \n",
       "L 678 1631 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-31\" x=\"335.888672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_22\">\n",
       "     <path d=\"M 527.289062 34.976563 \n",
       "L 537.289062 34.976563 \n",
       "L 547.289062 34.976563 \n",
       "\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_17\">\n",
       "     <!-- P(die=2) -->\n",
       "     <g transform=\"translate(555.289062 38.476563)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-32\" x=\"335.888672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_23\">\n",
       "     <path d=\"M 527.289062 49.654688 \n",
       "L 537.289062 49.654688 \n",
       "L 547.289062 49.654688 \n",
       "\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_18\">\n",
       "     <!-- P(die=3) -->\n",
       "     <g transform=\"translate(555.289062 53.154688)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-33\" x=\"335.888672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_24\">\n",
       "     <path d=\"M 527.289062 64.332813 \n",
       "L 537.289062 64.332813 \n",
       "L 547.289062 64.332813 \n",
       "\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_19\">\n",
       "     <!-- P(die=4) -->\n",
       "     <g transform=\"translate(555.289062 67.832813)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-34\" x=\"335.888672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_25\">\n",
       "     <path d=\"M 527.289062 79.010938 \n",
       "L 537.289062 79.010938 \n",
       "L 547.289062 79.010938 \n",
       "\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_20\">\n",
       "     <!-- P(die=5) -->\n",
       "     <g transform=\"translate(555.289062 82.510938)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-35\" x=\"335.888672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_26\">\n",
       "     <path d=\"M 527.289062 93.689063 \n",
       "L 537.289062 93.689063 \n",
       "L 547.289062 93.689063 \n",
       "\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_21\">\n",
       "     <!-- P(die=6) -->\n",
       "     <g transform=\"translate(555.289062 97.189063)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" x=\"60.302734\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"99.316406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"162.792969\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"190.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3d\" x=\"252.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-36\" x=\"335.888672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" x=\"399.511719\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p6070896839\">\n",
       "   <rect x=\"50.14375\" y=\"7.2\" width=\"558\" height=\"332.64\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = multinomial.Multinomial(10, fair_probs).sample((1000,))\n",
    "cum_counts = counts.cumsum(dim=0)\n",
    "\n",
    "estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\n",
    "d2l.set_figsize((10, 6))\n",
    "for i in range(6):\n",
    "    d2l.plt.plot(estimates[:, i].numpy(),\n",
    "                 label=(\"P(die=\" + str(i + 1) + \")\"))\n",
    "d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Groups of experiments')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'CumulativeDistributionTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'SoftplusTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', 'Wishart', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull', 'wishart']\n",
      "Help on package torch.distributions in torch:\n",
      "\n",
      "NAME\n",
      "    torch.distributions\n",
      "\n",
      "DESCRIPTION\n",
      "    The ``distributions`` package contains parameterizable probability distributions\n",
      "    and sampling functions. This allows the construction of stochastic computation\n",
      "    graphs and stochastic gradient estimators for optimization. This package\n",
      "    generally follows the design of the `TensorFlow Distributions`_ package.\n",
      "    \n",
      "    .. _`TensorFlow Distributions`:\n",
      "        https://arxiv.org/abs/1711.10604\n",
      "    \n",
      "    It is not possible to directly backpropagate through random samples. However,\n",
      "    there are two main methods for creating surrogate functions that can be\n",
      "    backpropagated through. These are the score function estimator/likelihood ratio\n",
      "    estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly\n",
      "    seen as the basis for policy gradient methods in reinforcement learning, and the\n",
      "    pathwise derivative estimator is commonly seen in the reparameterization trick\n",
      "    in variational autoencoders. Whilst the score function only requires the value\n",
      "    of samples :math:`f(x)`, the pathwise derivative requires the derivative\n",
      "    :math:`f'(x)`. The next sections discuss these two in a reinforcement learning\n",
      "    example. For more details see\n",
      "    `Gradient Estimation Using Stochastic Computation Graphs`_ .\n",
      "    \n",
      "    .. _`Gradient Estimation Using Stochastic Computation Graphs`:\n",
      "         https://arxiv.org/abs/1506.05254\n",
      "    \n",
      "    Score function\n",
      "    ^^^^^^^^^^^^^^\n",
      "    \n",
      "    When the probability density function is differentiable with respect to its\n",
      "    parameters, we only need :meth:`~torch.distributions.Distribution.sample` and\n",
      "    :meth:`~torch.distributions.Distribution.log_prob` to implement REINFORCE:\n",
      "    \n",
      "    .. math::\n",
      "    \n",
      "        \\Delta\\theta  = \\alpha r \\frac{\\partial\\log p(a|\\pi^\\theta(s))}{\\partial\\theta}\n",
      "    \n",
      "    where :math:`\\theta` are the parameters, :math:`\\alpha` is the learning rate,\n",
      "    :math:`r` is the reward and :math:`p(a|\\pi^\\theta(s))` is the probability of\n",
      "    taking action :math:`a` in state :math:`s` given policy :math:`\\pi^\\theta`.\n",
      "    \n",
      "    In practice we would sample an action from the output of a network, apply this\n",
      "    action in an environment, and then use ``log_prob`` to construct an equivalent\n",
      "    loss function. Note that we use a negative because optimizers use gradient\n",
      "    descent, whilst the rule above assumes gradient ascent. With a categorical\n",
      "    policy, the code for implementing REINFORCE would be as follows::\n",
      "    \n",
      "        probs = policy_network(state)\n",
      "        # Note that this is equivalent to what used to be called multinomial\n",
      "        m = Categorical(probs)\n",
      "        action = m.sample()\n",
      "        next_state, reward = env.step(action)\n",
      "        loss = -m.log_prob(action) * reward\n",
      "        loss.backward()\n",
      "    \n",
      "    Pathwise derivative\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "    \n",
      "    The other way to implement these stochastic/policy gradients would be to use the\n",
      "    reparameterization trick from the\n",
      "    :meth:`~torch.distributions.Distribution.rsample` method, where the\n",
      "    parameterized random variable can be constructed via a parameterized\n",
      "    deterministic function of a parameter-free random variable. The reparameterized\n",
      "    sample therefore becomes differentiable. The code for implementing the pathwise\n",
      "    derivative would be as follows::\n",
      "    \n",
      "        params = policy_network(state)\n",
      "        m = Normal(*params)\n",
      "        # Any distribution with .has_rsample == True could work based on the application\n",
      "        action = m.rsample()\n",
      "        next_state, reward = env.step(action)  # Assuming that reward is differentiable\n",
      "        loss = -reward\n",
      "        loss.backward()\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    bernoulli\n",
      "    beta\n",
      "    binomial\n",
      "    categorical\n",
      "    cauchy\n",
      "    chi2\n",
      "    constraint_registry\n",
      "    constraints\n",
      "    continuous_bernoulli\n",
      "    dirichlet\n",
      "    distribution\n",
      "    exp_family\n",
      "    exponential\n",
      "    fishersnedecor\n",
      "    gamma\n",
      "    geometric\n",
      "    gumbel\n",
      "    half_cauchy\n",
      "    half_normal\n",
      "    independent\n",
      "    kl\n",
      "    kumaraswamy\n",
      "    laplace\n",
      "    lkj_cholesky\n",
      "    log_normal\n",
      "    logistic_normal\n",
      "    lowrank_multivariate_normal\n",
      "    mixture_same_family\n",
      "    multinomial\n",
      "    multivariate_normal\n",
      "    negative_binomial\n",
      "    normal\n",
      "    one_hot_categorical\n",
      "    pareto\n",
      "    poisson\n",
      "    relaxed_bernoulli\n",
      "    relaxed_categorical\n",
      "    studentT\n",
      "    transformed_distribution\n",
      "    transforms\n",
      "    uniform\n",
      "    utils\n",
      "    von_mises\n",
      "    weibull\n",
      "    wishart\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        torch.distributions.distribution.Distribution\n",
      "            torch.distributions.binomial.Binomial\n",
      "            torch.distributions.categorical.Categorical\n",
      "            torch.distributions.cauchy.Cauchy\n",
      "            torch.distributions.exp_family.ExponentialFamily\n",
      "                torch.distributions.bernoulli.Bernoulli\n",
      "                torch.distributions.beta.Beta\n",
      "                torch.distributions.continuous_bernoulli.ContinuousBernoulli\n",
      "                torch.distributions.dirichlet.Dirichlet\n",
      "                torch.distributions.exponential.Exponential\n",
      "                torch.distributions.gamma.Gamma\n",
      "                    torch.distributions.chi2.Chi2\n",
      "                torch.distributions.normal.Normal\n",
      "                torch.distributions.poisson.Poisson\n",
      "                torch.distributions.wishart.Wishart\n",
      "            torch.distributions.fishersnedecor.FisherSnedecor\n",
      "            torch.distributions.geometric.Geometric\n",
      "            torch.distributions.independent.Independent\n",
      "            torch.distributions.laplace.Laplace\n",
      "            torch.distributions.lkj_cholesky.LKJCholesky\n",
      "            torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal\n",
      "            torch.distributions.mixture_same_family.MixtureSameFamily\n",
      "            torch.distributions.multinomial.Multinomial\n",
      "            torch.distributions.multivariate_normal.MultivariateNormal\n",
      "            torch.distributions.negative_binomial.NegativeBinomial\n",
      "            torch.distributions.one_hot_categorical.OneHotCategorical\n",
      "                torch.distributions.one_hot_categorical.OneHotCategoricalStraightThrough\n",
      "            torch.distributions.studentT.StudentT\n",
      "            torch.distributions.transformed_distribution.TransformedDistribution\n",
      "                torch.distributions.gumbel.Gumbel\n",
      "                torch.distributions.half_cauchy.HalfCauchy\n",
      "                torch.distributions.half_normal.HalfNormal\n",
      "                torch.distributions.kumaraswamy.Kumaraswamy\n",
      "                torch.distributions.log_normal.LogNormal\n",
      "                torch.distributions.logistic_normal.LogisticNormal\n",
      "                torch.distributions.pareto.Pareto\n",
      "                torch.distributions.relaxed_bernoulli.RelaxedBernoulli\n",
      "                torch.distributions.relaxed_categorical.RelaxedOneHotCategorical\n",
      "                torch.distributions.weibull.Weibull\n",
      "            torch.distributions.uniform.Uniform\n",
      "            torch.distributions.von_mises.VonMises\n",
      "        torch.distributions.transforms.Transform\n",
      "            torch.distributions.transforms.AbsTransform\n",
      "            torch.distributions.transforms.AffineTransform\n",
      "            torch.distributions.transforms.CatTransform\n",
      "            torch.distributions.transforms.ComposeTransform\n",
      "            torch.distributions.transforms.CorrCholeskyTransform\n",
      "            torch.distributions.transforms.CumulativeDistributionTransform\n",
      "            torch.distributions.transforms.ExpTransform\n",
      "            torch.distributions.transforms.IndependentTransform\n",
      "            torch.distributions.transforms.LowerCholeskyTransform\n",
      "            torch.distributions.transforms.PowerTransform\n",
      "            torch.distributions.transforms.ReshapeTransform\n",
      "            torch.distributions.transforms.SigmoidTransform\n",
      "            torch.distributions.transforms.SoftmaxTransform\n",
      "            torch.distributions.transforms.SoftplusTransform\n",
      "            torch.distributions.transforms.StackTransform\n",
      "            torch.distributions.transforms.StickBreakingTransform\n",
      "            torch.distributions.transforms.TanhTransform\n",
      "    \n",
      "    class AbsTransform(Transform)\n",
      "     |  AbsTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`y = |x|`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AbsTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  codomain = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  domain = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "     |  \n",
      "     |  bijective = False\n",
      "    \n",
      "    class AffineTransform(Transform)\n",
      "     |  AffineTransform(loc, scale, event_dim=0, cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the pointwise affine mapping :math:`y = \\text{loc} + \\text{scale} \\times x`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (Tensor or float): Location parameter.\n",
      "     |      scale (Tensor or float): Scale parameter.\n",
      "     |      event_dim (int): Optional size of `event_shape`. This should be zero\n",
      "     |          for univariate random variables, 1 for distributions over vectors,\n",
      "     |          2 for distributions over matrices, etc.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AffineTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, event_dim=0, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "    \n",
      "    class Bernoulli(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Bernoulli(probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Bernoulli distribution parameterized by :attr:`probs`\n",
      "     |  or :attr:`logits` (but not both).\n",
      "     |  \n",
      "     |  Samples are binary (0 or 1). They take the value `1` with probability `p`\n",
      "     |  and `0` with probability `1 - p`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Bernoulli(torch.tensor([0.3]))\n",
      "     |      >>> m.sample()  # 30% chance 1; 70% chance 0\n",
      "     |      tensor([ 0.])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      probs (Number, Tensor): the probability of sampling `1`\n",
      "     |      logits (Number, Tensor): the log-odds of sampling `1`\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Bernoulli\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_enumerate_support = True\n",
      "     |  \n",
      "     |  support = Boolean()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class Beta(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Beta(concentration1, concentration0, validate_args=None)\n",
      "     |  \n",
      "     |  Beta distribution parameterized by :attr:`concentration1` and :attr:`concentration0`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n",
      "     |      >>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      concentration1 (float or Tensor): 1st concentration parameter of the distribution\n",
      "     |          (often referred to as alpha)\n",
      "     |      concentration0 (float or Tensor): 2nd concentration parameter of the distribution\n",
      "     |          (often referred to as beta)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Beta\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, concentration1, concentration0, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=())\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  concentration0\n",
      "     |  \n",
      "     |  concentration1\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'co...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Binomial(torch.distributions.distribution.Distribution)\n",
      "     |  Binomial(total_count=1, probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Binomial distribution parameterized by :attr:`total_count` and\n",
      "     |  either :attr:`probs` or :attr:`logits` (but not both). :attr:`total_count` must be\n",
      "     |  broadcastable with :attr:`probs`/:attr:`logits`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n",
      "     |      >>> x = m.sample()\n",
      "     |      tensor([   0.,   22.,   71.,  100.])\n",
      "     |  \n",
      "     |      >>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n",
      "     |      >>> x = m.sample()\n",
      "     |      tensor([[ 4.,  5.],\n",
      "     |              [ 7.,  6.]])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      total_count (int or Tensor): number of Bernoulli trials\n",
      "     |      probs (Tensor): Event probabilities\n",
      "     |      logits (Tensor): Event log-odds\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Binomial\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, total_count=1, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_enumerate_support = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class CatTransform(Transform)\n",
      "     |  CatTransform(tseq, dim=0, lengths=None, cache_size=0)\n",
      "     |  \n",
      "     |  Transform functor that applies a sequence of transforms `tseq`\n",
      "     |  component-wise to each submatrix at `dim`, of length `lengths[dim]`,\n",
      "     |  in a way compatible with :func:`torch.cat`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |     x0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0)\n",
      "     |     x = torch.cat([x0, x0], dim=0)\n",
      "     |     t0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10])\n",
      "     |     t = CatTransform([t0, t0], dim=0, lengths=[20, 20])\n",
      "     |     y = t(x)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CatTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tseq, dim=0, lengths=None, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  bijective\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  length\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'transforms': typing.List[torch.distributions.trans...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class Categorical(torch.distributions.distribution.Distribution)\n",
      "     |  Categorical(probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a categorical distribution parameterized by either :attr:`probs` or\n",
      "     |  :attr:`logits` (but not both).\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |      It is equivalent to the distribution that :func:`torch.multinomial`\n",
      "     |      samples from.\n",
      "     |  \n",
      "     |  Samples are integers from :math:`\\{0, \\ldots, K-1\\}` where `K` is ``probs.size(-1)``.\n",
      "     |  \n",
      "     |  If `probs` is 1-dimensional with length-`K`, each element is the relative probability\n",
      "     |  of sampling the class at that index.\n",
      "     |  \n",
      "     |  If `probs` is N-dimensional, the first N-1 dimensions are treated as a batch of\n",
      "     |  relative probability vectors.\n",
      "     |  \n",
      "     |  .. note:: The `probs` argument must be non-negative, finite and have a non-zero sum,\n",
      "     |            and it will be normalized to sum to 1 along the last dimension. :attr:`probs`\n",
      "     |            will return this normalized value.\n",
      "     |            The `logits` argument will be interpreted as unnormalized log probabilities\n",
      "     |            and can therefore be any real number. It will likewise be normalized so that\n",
      "     |            the resulting probabilities sum to 1 along the last dimension. :attr:`logits`\n",
      "     |            will return this normalized value.\n",
      "     |  \n",
      "     |  See also: :func:`torch.multinomial`\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
      "     |      >>> m.sample()  # equal probability of 0, 1, 2, 3\n",
      "     |      tensor(3)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      probs (Tensor): event probabilities\n",
      "     |      logits (Tensor): event log probabilities (unnormalized)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Categorical\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs'...\n",
      "     |  \n",
      "     |  has_enumerate_support = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class Cauchy(torch.distributions.distribution.Distribution)\n",
      "     |  Cauchy(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\n",
      "     |  independent normally distributed random variables with means `0` follows a\n",
      "     |  Cauchy distribution.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\n",
      "     |      tensor([ 2.3214])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): mode or median of the distribution.\n",
      "     |      scale (float or Tensor): half width at half maximum.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Cauchy\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Chi2(torch.distributions.gamma.Gamma)\n",
      "     |  Chi2(df, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Chi-squared distribution parameterized by shape parameter :attr:`df`.\n",
      "     |  This is exactly equivalent to ``Gamma(alpha=0.5*df, beta=0.5)``\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Chi2(torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # Chi2 distributed with shape df=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      df (float or Tensor): shape parameter of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Chi2\n",
      "     |      torch.distributions.gamma.Gamma\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  df\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'df': GreaterThan(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.gamma.Gamma:\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.gamma.Gamma:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.gamma.Gamma:\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThanEq(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class ComposeTransform(Transform)\n",
      "     |  ComposeTransform(parts: List[torch.distributions.transforms.Transform], cache_size=0)\n",
      "     |  \n",
      "     |  Composes multiple transforms in a chain.\n",
      "     |  The transforms being composed are responsible for caching.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      parts (list of :class:`Transform`): A list of transforms to compose.\n",
      "     |      cache_size (int): Size of cache. If zero, no caching is done. If one,\n",
      "     |          the latest single value is cached. Only 0 and 1 are supported.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ComposeTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, parts: List[torch.distributions.transforms.Transform], cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  bijective\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "    \n",
      "    class ContinuousBernoulli(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  ContinuousBernoulli(probs=None, logits=None, lims=(0.499, 0.501), validate_args=None)\n",
      "     |  \n",
      "     |  Creates a continuous Bernoulli distribution parameterized by :attr:`probs`\n",
      "     |  or :attr:`logits` (but not both).\n",
      "     |  \n",
      "     |  The distribution is supported in [0, 1] and parameterized by 'probs' (in\n",
      "     |  (0,1)) or 'logits' (real-valued). Note that, unlike the Bernoulli, 'probs'\n",
      "     |  does not correspond to a probability and 'logits' does not correspond to\n",
      "     |  log-odds, but the same names are used due to the similarity with the\n",
      "     |  Bernoulli. See [1] for more details.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = ContinuousBernoulli(torch.tensor([0.3]))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 0.2538])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      probs (Number, Tensor): (0,1) valued parameters\n",
      "     |      logits (Number, Tensor): real valued parameters whose sigmoid matches 'probs'\n",
      "     |  \n",
      "     |  [1] The continuous Bernoulli: fixing a pervasive error in variational\n",
      "     |  autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.\n",
      "     |  https://arxiv.org/abs/1907.06845\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ContinuousBernoulli\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, lims=(0.499, 0.501), validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class CorrCholeskyTransform(Transform)\n",
      "     |  CorrCholeskyTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transforms an uncontrained real vector :math:`x` with length :math:`D*(D-1)/2` into the\n",
      "     |  Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower\n",
      "     |  triangular matrix with positive diagonals and unit Euclidean norm for each row.\n",
      "     |  The transform is processed as follows:\n",
      "     |  \n",
      "     |      1. First we convert x into a lower triangular matrix in row order.\n",
      "     |      2. For each row :math:`X_i` of the lower triangular part, we apply a *signed* version of\n",
      "     |         class :class:`StickBreakingTransform` to transform :math:`X_i` into a\n",
      "     |         unit Euclidean length vector using the following steps:\n",
      "     |         - Scales into the interval :math:`(-1, 1)` domain: :math:`r_i = \\tanh(X_i)`.\n",
      "     |         - Transforms into an unsigned domain: :math:`z_i = r_i^2`.\n",
      "     |         - Applies :math:`s_i = StickBreakingTransform(z_i)`.\n",
      "     |         - Transforms back into signed domain: :math:`y_i = sign(r_i) * \\sqrt{s_i}`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CorrCholeskyTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y, intermediates=None)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = CorrCholesky()\n",
      "     |  \n",
      "     |  domain = IndependentConstraint(Real(), 1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class CumulativeDistributionTransform(Transform)\n",
      "     |  CumulativeDistributionTransform(distribution, cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the cumulative distribution function of a probability distribution.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      distribution (Distribution): Distribution whose cumulative distribution function to use for\n",
      "     |          the transformation.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      # Construct a Gaussian copula from a multivariate normal.\n",
      "     |      base_dist = MultivariateNormal(\n",
      "     |          loc=torch.zeros(2),\n",
      "     |          scale_tril=LKJCholesky(2).sample(),\n",
      "     |      )\n",
      "     |      transform = CumulativeDistributionTransform(Normal(0, 1))\n",
      "     |      copula = TransformedDistribution(base_dist, [transform])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CumulativeDistributionTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, distribution, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class Dirichlet(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Dirichlet(concentration, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Dirichlet distribution parameterized by concentration :attr:`concentration`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n",
      "     |      >>> m.sample()  # Dirichlet distributed with concentrarion concentration\n",
      "     |      tensor([ 0.1046,  0.8954])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      concentration (Tensor): concentration parameter of the distribution\n",
      "     |          (often referred to as alpha)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Dirichlet\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, concentration, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=())\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': IndependentConstraint(GreaterThan(...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Simplex()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Distribution(builtins.object)\n",
      "     |  Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)\n",
      "     |  \n",
      "     |  Distribution is the abstract base class for probability distributions.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  arg_constraints\n",
      "     |      Returns a dictionary from argument names to\n",
      "     |      :class:`~torch.distributions.constraints.Constraint` objects that\n",
      "     |      should be satisfied by each argument of this distribution. Args that\n",
      "     |      are not tensors need not appear in this dict.\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class ExpTransform(Transform)\n",
      "     |  ExpTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`y = \\exp(x)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExpTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  domain = Real()\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "    \n",
      "    class Exponential(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Exponential(rate, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Exponential distribution parameterized by :attr:`rate`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Exponential(torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # Exponential distributed with rate=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      rate (float or Tensor): rate = 1 / scale of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Exponential\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, rate, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'rate': GreaterThan(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThanEq(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class ExponentialFamily(torch.distributions.distribution.Distribution)\n",
      "     |  ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)\n",
      "     |  \n",
      "     |  ExponentialFamily is the abstract base class for probability distributions belonging to an\n",
      "     |  exponential family, whose probability mass/density function has the form is defined below\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |      p_{F}(x; \\theta) = \\exp(\\langle t(x), \\theta\\rangle - F(\\theta) + k(x))\n",
      "     |  \n",
      "     |  where :math:`\\theta` denotes the natural parameters, :math:`t(x)` denotes the sufficient statistic,\n",
      "     |  :math:`F(\\theta)` is the log normalizer function for a given family and :math:`k(x)` is the carrier\n",
      "     |  measure.\n",
      "     |  \n",
      "     |  Note:\n",
      "     |      This class is an intermediary between the `Distribution` class and distributions which belong\n",
      "     |      to an exponential family mainly to check the correctness of the `.entropy()` and analytic KL\n",
      "     |      divergence methods. We use this class to compute the entropy and KL divergence using the AD\n",
      "     |      framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and\n",
      "     |      Cross-entropies of Exponential Families).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __init__(self, batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  arg_constraints\n",
      "     |      Returns a dictionary from argument names to\n",
      "     |      :class:`~torch.distributions.constraints.Constraint` objects that\n",
      "     |      should be satisfied by each argument of this distribution. Args that\n",
      "     |      are not tensors need not appear in this dict.\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class FisherSnedecor(torch.distributions.distribution.Distribution)\n",
      "     |  FisherSnedecor(df1, df2, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Fisher-Snedecor distribution parameterized by :attr:`df1` and :attr:`df2`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n",
      "     |      >>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\n",
      "     |      tensor([ 0.2453])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      df1 (float or Tensor): degrees of freedom parameter 1\n",
      "     |      df2 (float or Tensor): degrees of freedom parameter 2\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FisherSnedecor\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df1, df2, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': Greater...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Gamma(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Gamma(concentration, rate, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Gamma distribution parameterized by shape :attr:`concentration` and :attr:`rate`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # Gamma distributed with concentration=1 and rate=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      concentration (float or Tensor): shape parameter of the distribution\n",
      "     |          (often referred to as alpha)\n",
      "     |      rate (float or Tensor): rate = 1 / scale of the distribution\n",
      "     |          (often referred to as beta)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Gamma\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, concentration, rate, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rat...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThanEq(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Geometric(torch.distributions.distribution.Distribution)\n",
      "     |  Geometric(probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Geometric distribution parameterized by :attr:`probs`,\n",
      "     |  where :attr:`probs` is the probability of success of Bernoulli trials.\n",
      "     |  It represents the probability that in :math:`k + 1` Bernoulli trials, the\n",
      "     |  first :math:`k` trials failed, before seeing a success.\n",
      "     |  \n",
      "     |  Samples are non-negative integers [0, :math:`\\inf`).\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Geometric(torch.tensor([0.3]))\n",
      "     |      >>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\n",
      "     |      tensor([ 2.])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      probs (Number, Tensor): the probability of sampling `1`. Must be in range (0, 1]\n",
      "     |      logits (Number, Tensor): the log-odds of sampling `1`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Geometric\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      "     |  \n",
      "     |  support = IntegerGreaterThan(lower_bound=0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class Gumbel(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  Gumbel(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Samples from a Gumbel Distribution.\n",
      "     |  \n",
      "     |  Examples::\n",
      "     |  \n",
      "     |      >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n",
      "     |      >>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\n",
      "     |      tensor([ 1.0124])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): Location parameter of the distribution\n",
      "     |      scale (float or Tensor): Scale parameter of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Gumbel\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  has_rsample\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class HalfCauchy(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  HalfCauchy(scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a half-Cauchy distribution parameterized by `scale` where::\n",
      "     |  \n",
      "     |      X ~ Cauchy(0, scale)\n",
      "     |      Y = |X| ~ HalfCauchy(scale)\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = HalfCauchy(torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # half-cauchy distributed with scale=1\n",
      "     |      tensor([ 2.3214])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      scale (float or Tensor): scale of the full Cauchy distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HalfCauchy\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, prob)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  scale\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'scale': GreaterThan(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThanEq(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class HalfNormal(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  HalfNormal(scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a half-normal distribution parameterized by `scale` where::\n",
      "     |  \n",
      "     |      X ~ Normal(0, scale)\n",
      "     |      Y = |X| ~ HalfNormal(scale)\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = HalfNormal(torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # half-normal distributed with scale=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      scale (float or Tensor): scale of the full Normal distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HalfNormal\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, prob)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  scale\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'scale': GreaterThan(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThanEq(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Independent(torch.distributions.distribution.Distribution)\n",
      "     |  Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None)\n",
      "     |  \n",
      "     |  Reinterprets some of the batch dims of a distribution as event dims.\n",
      "     |  \n",
      "     |  This is mainly useful for changing the shape of the result of\n",
      "     |  :meth:`log_prob`. For example to create a diagonal Normal distribution with\n",
      "     |  the same shape as a Multivariate Normal distribution (so they are\n",
      "     |  interchangeable), you can::\n",
      "     |  \n",
      "     |      >>> loc = torch.zeros(3)\n",
      "     |      >>> scale = torch.ones(3)\n",
      "     |      >>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n",
      "     |      >>> [mvn.batch_shape, mvn.event_shape]\n",
      "     |      [torch.Size(()), torch.Size((3,))]\n",
      "     |      >>> normal = Normal(loc, scale)\n",
      "     |      >>> [normal.batch_shape, normal.event_shape]\n",
      "     |      [torch.Size((3,)), torch.Size(())]\n",
      "     |      >>> diagn = Independent(normal, 1)\n",
      "     |      >>> [diagn.batch_shape, diagn.event_shape]\n",
      "     |      [torch.Size(()), torch.Size((3,))]\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      base_distribution (torch.distributions.distribution.Distribution): a\n",
      "     |          base distribution\n",
      "     |      reinterpreted_batch_ndims (int): the number of batch dims to\n",
      "     |          reinterpret as event dims\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Independent\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_distribution, reinterpreted_batch_ndims, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  has_enumerate_support\n",
      "     |  \n",
      "     |  has_rsample\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  arg_constraints = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class IndependentTransform(Transform)\n",
      "     |  IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0)\n",
      "     |  \n",
      "     |  Wrapper around another transform to treat\n",
      "     |  ``reinterpreted_batch_ndims``-many extra of the right most dimensions as\n",
      "     |  dependent. This has no effect on the forward or backward transforms, but\n",
      "     |  does sum out ``reinterpreted_batch_ndims``-many of the rightmost dimensions\n",
      "     |  in :meth:`log_abs_det_jacobian`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      base_transform (:class:`Transform`): A base transform.\n",
      "     |      reinterpreted_batch_ndims (int): The number of extra rightmost\n",
      "     |          dimensions to treat as dependent.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IndependentTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_transform, reinterpreted_batch_ndims, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  bijective\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class Kumaraswamy(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  Kumaraswamy(concentration1, concentration0, validate_args=None)\n",
      "     |  \n",
      "     |  Samples from a Kumaraswamy distribution.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\n",
      "     |      tensor([ 0.1729])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      concentration1 (float or Tensor): 1st concentration parameter of the distribution\n",
      "     |          (often referred to as alpha)\n",
      "     |      concentration0 (float or Tensor): 2nd concentration parameter of the distribution\n",
      "     |          (often referred to as beta)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Kumaraswamy\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, concentration1, concentration0, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'co...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class LKJCholesky(torch.distributions.distribution.Distribution)\n",
      "     |  LKJCholesky(dim, concentration=1.0, validate_args=None)\n",
      "     |  \n",
      "     |  LKJ distribution for lower Cholesky factor of correlation matrices.\n",
      "     |  The distribution is controlled by ``concentration`` parameter :math:`\\eta`\n",
      "     |  to make the probability of the correlation matrix :math:`M` generated from\n",
      "     |  a Cholesky factor propotional to :math:`\\det(M)^{\\eta - 1}`. Because of that,\n",
      "     |  when ``concentration == 1``, we have a uniform distribution over Cholesky\n",
      "     |  factors of correlation matrices. Note that this distribution samples the\n",
      "     |  Cholesky factor of correlation matrices and not the correlation matrices\n",
      "     |  themselves and thereby differs slightly from the derivations in [1] for\n",
      "     |  the `LKJCorr` distribution. For sampling, this uses the Onion method from\n",
      "     |  [1] Section 3.\n",
      "     |  \n",
      "     |      L ~ LKJCholesky(dim, concentration)\n",
      "     |      X = L @ L' ~ LKJCorr(dim, concentration)\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> l = LKJCholesky(3, 0.5)\n",
      "     |      >>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\n",
      "     |      tensor([[ 1.0000,  0.0000,  0.0000],\n",
      "     |              [ 0.3516,  0.9361,  0.0000],\n",
      "     |              [-0.1899,  0.4748,  0.8593]])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      dimension (dim): dimension of the matrices\n",
      "     |      concentration (float or Tensor): concentration/shape parameter of the\n",
      "     |          distribution (often referred to as eta)\n",
      "     |  \n",
      "     |  **References**\n",
      "     |  \n",
      "     |  [1] `Generating random correlation matrices based on vines and extended onion method`,\n",
      "     |  Daniel Lewandowski, Dorota Kurowicka, Harry Joe.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LKJCholesky\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dim, concentration=1.0, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': GreaterThan(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  support = CorrCholesky()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class Laplace(torch.distributions.distribution.Distribution)\n",
      "     |  Laplace(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Laplace distribution parameterized by :attr:`loc` and :attr:`scale`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # Laplace distributed with loc=0, scale=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): mean of the distribution\n",
      "     |      scale (float or Tensor): scale of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Laplace\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class LogNormal(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  LogNormal(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a log-normal distribution parameterized by\n",
      "     |  :attr:`loc` and :attr:`scale` where::\n",
      "     |  \n",
      "     |      X ~ Normal(loc, scale)\n",
      "     |      Y = exp(X) ~ LogNormal(loc, scale)\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # log-normal distributed with mean=0 and stddev=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): mean of log of distribution\n",
      "     |      scale (float or Tensor): standard deviation of log of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogNormal\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  loc\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  scale\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class LogisticNormal(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  LogisticNormal(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a logistic-normal distribution parameterized by :attr:`loc` and :attr:`scale`\n",
      "     |  that define the base `Normal` distribution transformed with the\n",
      "     |  `StickBreakingTransform` such that::\n",
      "     |  \n",
      "     |      X ~ LogisticNormal(loc, scale)\n",
      "     |      Y = log(X / (1 - X.cumsum(-1)))[..., :-1] ~ Normal(loc, scale)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): mean of the base distribution\n",
      "     |      scale (float or Tensor): standard deviation of the base distribution\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> # logistic-normal distributed with mean=(0, 0, 0) and stddev=(1, 1, 1)\n",
      "     |      >>> # of the base Normal distribution\n",
      "     |      >>> m = distributions.LogisticNormal(torch.tensor([0.0] * 3), torch.tensor([1.0] * 3))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 0.7653,  0.0341,  0.0579,  0.1427])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogisticNormal\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  loc\n",
      "     |  \n",
      "     |  scale\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Simplex()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class LowRankMultivariateNormal(torch.distributions.distribution.Distribution)\n",
      "     |  LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a multivariate normal distribution with covariance matrix having a low-rank form\n",
      "     |  parameterized by :attr:`cov_factor` and :attr:`cov_diag`::\n",
      "     |  \n",
      "     |      covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n",
      "     |  \n",
      "     |  Example:\n",
      "     |  \n",
      "     |      >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n",
      "     |      >>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\n",
      "     |      tensor([-0.2102, -0.5429])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (Tensor): mean of the distribution with shape `batch_shape + event_shape`\n",
      "     |      cov_factor (Tensor): factor part of low-rank form of covariance matrix with shape\n",
      "     |          `batch_shape + event_shape + (rank,)`\n",
      "     |      cov_diag (Tensor): diagonal part of low-rank form of covariance matrix with shape\n",
      "     |          `batch_shape + event_shape`\n",
      "     |  \n",
      "     |  Note:\n",
      "     |      The computation for determinant and inverse of covariance matrix is avoided when\n",
      "     |      `cov_factor.shape[1] << cov_factor.shape[0]` thanks to `Woodbury matrix identity\n",
      "     |      <https://en.wikipedia.org/wiki/Woodbury_matrix_identity>`_ and\n",
      "     |      `matrix determinant lemma <https://en.wikipedia.org/wiki/Matrix_determinant_lemma>`_.\n",
      "     |      Thanks to these formulas, we just need to compute the determinant and inverse of\n",
      "     |      the small size \"capacitance\" matrix::\n",
      "     |  \n",
      "     |          capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LowRankMultivariateNormal\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, cov_factor, cov_diag, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  covariance_matrix\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  precision_matrix\n",
      "     |  \n",
      "     |  scale_tril\n",
      "     |  \n",
      "     |  variance\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'cov_diag': IndependentConstraint(GreaterThan(lower...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = IndependentConstraint(Real(), 1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class LowerCholeskyTransform(Transform)\n",
      "     |  LowerCholeskyTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform from unconstrained matrices to lower-triangular matrices with\n",
      "     |  nonnegative diagonal entries.\n",
      "     |  \n",
      "     |  This is useful for parameterizing positive definite matrices in terms of\n",
      "     |  their Cholesky factorization.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LowerCholeskyTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  codomain = LowerCholesky()\n",
      "     |  \n",
      "     |  domain = IndependentConstraint(Real(), 2)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "     |  \n",
      "     |  bijective = False\n",
      "    \n",
      "    class MixtureSameFamily(torch.distributions.distribution.Distribution)\n",
      "     |  MixtureSameFamily(mixture_distribution, component_distribution, validate_args=None)\n",
      "     |  \n",
      "     |  The `MixtureSameFamily` distribution implements a (batch of) mixture\n",
      "     |  distribution where all component are from different parameterizations of\n",
      "     |  the same distribution type. It is parameterized by a `Categorical`\n",
      "     |  \"selecting distribution\" (over `k` component) and a component\n",
      "     |  distribution, i.e., a `Distribution` with a rightmost batch shape\n",
      "     |  (equal to `[k]`) which indexes each (batch of) component.\n",
      "     |  \n",
      "     |  Examples::\n",
      "     |  \n",
      "     |      # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n",
      "     |      # weighted normal distributions\n",
      "     |      >>> mix = D.Categorical(torch.ones(5,))\n",
      "     |      >>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n",
      "     |      >>> gmm = MixtureSameFamily(mix, comp)\n",
      "     |  \n",
      "     |      # Construct Gaussian Mixture Modle in 2D consisting of 5 equally\n",
      "     |      # weighted bivariate normal distributions\n",
      "     |      >>> mix = D.Categorical(torch.ones(5,))\n",
      "     |      >>> comp = D.Independent(D.Normal(\n",
      "     |                   torch.randn(5,2), torch.rand(5,2)), 1)\n",
      "     |      >>> gmm = MixtureSameFamily(mix, comp)\n",
      "     |  \n",
      "     |      # Construct a batch of 3 Gaussian Mixture Models in 2D each\n",
      "     |      # consisting of 5 random weighted bivariate normal distributions\n",
      "     |      >>> mix = D.Categorical(torch.rand(3,5))\n",
      "     |      >>> comp = D.Independent(D.Normal(\n",
      "     |                  torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n",
      "     |      >>> gmm = MixtureSameFamily(mix, comp)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      mixture_distribution: `torch.distributions.Categorical`-like\n",
      "     |          instance. Manages the probability of selecting component.\n",
      "     |          The number of categories must match the rightmost batch\n",
      "     |          dimension of the `component_distribution`. Must have either\n",
      "     |          scalar `batch_shape` or `batch_shape` matching\n",
      "     |          `component_distribution.batch_shape[:-1]`\n",
      "     |      component_distribution: `torch.distributions.Distribution`-like\n",
      "     |          instance. Right-most batch dimension indexes component.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MixtureSameFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, mixture_distribution, component_distribution, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, x)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, x)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  component_distribution\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mixture_distribution\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  arg_constraints = {}\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Multinomial(torch.distributions.distribution.Distribution)\n",
      "     |  Multinomial(total_count=1, probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Multinomial distribution parameterized by :attr:`total_count` and\n",
      "     |  either :attr:`probs` or :attr:`logits` (but not both). The innermost dimension of\n",
      "     |  :attr:`probs` indexes over categories. All other dimensions index over batches.\n",
      "     |  \n",
      "     |  Note that :attr:`total_count` need not be specified if only :meth:`log_prob` is\n",
      "     |  called (see example below)\n",
      "     |  \n",
      "     |  .. note:: The `probs` argument must be non-negative, finite and have a non-zero sum,\n",
      "     |            and it will be normalized to sum to 1 along the last dimension. :attr:`probs`\n",
      "     |            will return this normalized value.\n",
      "     |            The `logits` argument will be interpreted as unnormalized log probabilities\n",
      "     |            and can therefore be any real number. It will likewise be normalized so that\n",
      "     |            the resulting probabilities sum to 1 along the last dimension. :attr:`logits`\n",
      "     |            will return this normalized value.\n",
      "     |  \n",
      "     |  -   :meth:`sample` requires a single shared `total_count` for all\n",
      "     |      parameters and samples.\n",
      "     |  -   :meth:`log_prob` allows different `total_count` for each parameter and\n",
      "     |      sample.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n",
      "     |      >>> x = m.sample()  # equal probability of 0, 1, 2, 3\n",
      "     |      tensor([ 21.,  24.,  30.,  25.])\n",
      "     |  \n",
      "     |      >>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\n",
      "     |      tensor([-4.1338])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      total_count (int): number of trials\n",
      "     |      probs (Tensor): event probabilities\n",
      "     |      logits (Tensor): event log probabilities (unnormalized)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Multinomial\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, total_count=1, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'total_count': <class 'int'>}\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs'...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class MultivariateNormal(torch.distributions.distribution.Distribution)\n",
      "     |  MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a multivariate normal (also called Gaussian) distribution\n",
      "     |  parameterized by a mean vector and a covariance matrix.\n",
      "     |  \n",
      "     |  The multivariate normal distribution can be parameterized either\n",
      "     |  in terms of a positive definite covariance matrix :math:`\\mathbf{\\Sigma}`\n",
      "     |  or a positive definite precision matrix :math:`\\mathbf{\\Sigma}^{-1}`\n",
      "     |  or a lower-triangular matrix :math:`\\mathbf{L}` with positive-valued\n",
      "     |  diagonal entries, such that\n",
      "     |  :math:`\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top`. This triangular matrix\n",
      "     |  can be obtained via e.g. Cholesky decomposition of the covariance.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |  \n",
      "     |      >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
      "     |      >>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\n",
      "     |      tensor([-0.2102, -0.5429])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (Tensor): mean of the distribution\n",
      "     |      covariance_matrix (Tensor): positive-definite covariance matrix\n",
      "     |      precision_matrix (Tensor): positive-definite precision matrix\n",
      "     |      scale_tril (Tensor): lower-triangular factor of covariance, with positive-valued diagonal\n",
      "     |  \n",
      "     |  Note:\n",
      "     |      Only one of :attr:`covariance_matrix` or :attr:`precision_matrix` or\n",
      "     |      :attr:`scale_tril` can be specified.\n",
      "     |  \n",
      "     |      Using :attr:`scale_tril` will be more efficient: all computations internally\n",
      "     |      are based on :attr:`scale_tril`. If :attr:`covariance_matrix` or\n",
      "     |      :attr:`precision_matrix` is passed instead, it is only used to compute\n",
      "     |      the corresponding lower triangular matrices using a Cholesky decomposition.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultivariateNormal\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  covariance_matrix\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  precision_matrix\n",
      "     |  \n",
      "     |  scale_tril\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': Ind...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = IndependentConstraint(Real(), 1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class NegativeBinomial(torch.distributions.distribution.Distribution)\n",
      "     |  NegativeBinomial(total_count, probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Negative Binomial distribution, i.e. distribution\n",
      "     |  of the number of successful independent and identical Bernoulli trials\n",
      "     |  before :attr:`total_count` failures are achieved. The probability\n",
      "     |  of success of each Bernoulli trial is :attr:`probs`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      total_count (float or Tensor): non-negative number of negative Bernoulli\n",
      "     |          trials to stop, although the distribution is still valid for real\n",
      "     |          valued count\n",
      "     |      probs (Tensor): Event probabilities of success in the half open interval [0, 1)\n",
      "     |      logits (Tensor): Event log-odds for probabilities of success\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NegativeBinomial\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, total_count, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_b...\n",
      "     |  \n",
      "     |  support = IntegerGreaterThan(lower_bound=0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class Normal(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Normal(loc, scale, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a normal (also called Gaussian) distribution parameterized by\n",
      "     |  :attr:`loc` and :attr:`scale`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # normally distributed with loc=0 and scale=1\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      loc (float or Tensor): mean of the distribution (often referred to as mu)\n",
      "     |      scale (float or Tensor): standard deviation of the distribution\n",
      "     |          (often referred to as sigma)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Normal\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, scale, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class OneHotCategorical(torch.distributions.distribution.Distribution)\n",
      "     |  OneHotCategorical(probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a one-hot categorical distribution parameterized by :attr:`probs` or\n",
      "     |  :attr:`logits`.\n",
      "     |  \n",
      "     |  Samples are one-hot coded vectors of size ``probs.size(-1)``.\n",
      "     |  \n",
      "     |  .. note:: The `probs` argument must be non-negative, finite and have a non-zero sum,\n",
      "     |            and it will be normalized to sum to 1 along the last dimension. :attr:`probs`\n",
      "     |            will return this normalized value.\n",
      "     |            The `logits` argument will be interpreted as unnormalized log probabilities\n",
      "     |            and can therefore be any real number. It will likewise be normalized so that\n",
      "     |            the resulting probabilities sum to 1 along the last dimension. :attr:`logits`\n",
      "     |            will return this normalized value.\n",
      "     |  \n",
      "     |  See also: :func:`torch.distributions.Categorical` for specifications of\n",
      "     |  :attr:`probs` and :attr:`logits`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
      "     |      >>> m.sample()  # equal probability of 0, 1, 2, 3\n",
      "     |      tensor([ 0.,  0.,  0.,  1.])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      probs (Tensor): event probabilities\n",
      "     |      logits (Tensor): event log probabilities (unnormalized)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneHotCategorical\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs'...\n",
      "     |  \n",
      "     |  has_enumerate_support = True\n",
      "     |  \n",
      "     |  support = OneHot()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class OneHotCategoricalStraightThrough(OneHotCategorical)\n",
      "     |  OneHotCategoricalStraightThrough(probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a reparameterizable :class:`OneHotCategorical` distribution based on the straight-\n",
      "     |  through gradient estimator from [1].\n",
      "     |  \n",
      "     |  [1] Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation\n",
      "     |  (Bengio et al, 2013)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OneHotCategoricalStraightThrough\n",
      "     |      OneHotCategorical\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from OneHotCategorical:\n",
      "     |  \n",
      "     |  __init__(self, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from OneHotCategorical:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  param_shape\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from OneHotCategorical:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs'...\n",
      "     |  \n",
      "     |  has_enumerate_support = True\n",
      "     |  \n",
      "     |  support = OneHot()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Pareto(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  Pareto(scale, alpha, validate_args=None)\n",
      "     |  \n",
      "     |  Samples from a Pareto Type 1 distribution.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\n",
      "     |      tensor([ 1.5623])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      scale (float or Tensor): Scale parameter of the distribution\n",
      "     |      alpha (float or Tensor): Shape parameter of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Pareto\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, scale, alpha, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'alpha': GreaterThan(lower_bound=0.0), 'scale': Gre...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  has_rsample\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Poisson(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Poisson(rate, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Poisson distribution parameterized by :attr:`rate`, the rate parameter.\n",
      "     |  \n",
      "     |  Samples are nonnegative integers, with a pmf given by\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |    \\mathrm{rate}^k \\frac{e^{-\\mathrm{rate}}}{k!}\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Poisson(torch.tensor([4]))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 3.])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      rate (Number, Tensor): the rate parameter\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Poisson\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, rate, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'rate': GreaterThanEq(lower_bound=0.0)}\n",
      "     |  \n",
      "     |  support = IntegerGreaterThan(lower_bound=0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.exp_family.ExponentialFamily:\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "    \n",
      "    class PowerTransform(Transform)\n",
      "     |  PowerTransform(exponent, cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`y = x^{\\text{exponent}}`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PowerTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, exponent, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  domain = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "    \n",
      "    class RelaxedBernoulli(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a RelaxedBernoulli distribution, parametrized by\n",
      "     |  :attr:`temperature`, and either :attr:`probs` or :attr:`logits`\n",
      "     |  (but not both). This is a relaxed version of the `Bernoulli` distribution,\n",
      "     |  so the values are in (0, 1), and has reparametrizable samples.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = RelaxedBernoulli(torch.tensor([2.2]),\n",
      "     |                               torch.tensor([0.1, 0.2, 0.3, 0.99]))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      temperature (Tensor): relaxation temperature\n",
      "     |      probs (Number, Tensor): the probability of sampling `1`\n",
      "     |      logits (Number, Tensor): the log-odds of sampling `1`\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RelaxedBernoulli\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, temperature, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  temperature\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class RelaxedOneHotCategorical(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a RelaxedOneHotCategorical distribution parametrized by\n",
      "     |  :attr:`temperature`, and either :attr:`probs` or :attr:`logits`.\n",
      "     |  This is a relaxed version of the :class:`OneHotCategorical` distribution, so\n",
      "     |  its samples are on simplex, and are reparametrizable.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n",
      "     |                                       torch.tensor([0.1, 0.2, 0.3, 0.4]))\n",
      "     |      >>> m.sample()\n",
      "     |      tensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      temperature (Tensor): relaxation temperature\n",
      "     |      probs (Tensor): event probabilities\n",
      "     |      logits (Tensor): unnormalized log probability for each event\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RelaxedOneHotCategorical\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, temperature, probs=None, logits=None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  logits\n",
      "     |  \n",
      "     |  probs\n",
      "     |  \n",
      "     |  temperature\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs'...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Simplex()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class ReshapeTransform(Transform)\n",
      "     |  ReshapeTransform(in_shape, out_shape, cache_size=0)\n",
      "     |  \n",
      "     |  Unit Jacobian transform to reshape the rightmost part of a tensor.\n",
      "     |  \n",
      "     |  Note that ``in_shape`` and ``out_shape`` must have the same number of\n",
      "     |  elements, just as for :meth:`torch.Tensor.reshape`.\n",
      "     |  \n",
      "     |  Arguments:\n",
      "     |      in_shape (torch.Size): The input event shape.\n",
      "     |      out_shape (torch.Size): The output event shape.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ReshapeTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, in_shape, out_shape, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class SigmoidTransform(Transform)\n",
      "     |  SigmoidTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`y = \\frac{1}{1 + \\exp(-x)}` and :math:`x = \\text{logit}(y)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SigmoidTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = Interval(lower_bound=0.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  domain = Real()\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "    \n",
      "    class SoftmaxTransform(Transform)\n",
      "     |  SoftmaxTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform from unconstrained space to the simplex via :math:`y = \\exp(x)` then\n",
      "     |  normalizing.\n",
      "     |  \n",
      "     |  This is not bijective and cannot be used for HMC. However this acts mostly\n",
      "     |  coordinate-wise (except for the final normalization), and thus is\n",
      "     |  appropriate for coordinate-wise optimization algorithms.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SoftmaxTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  codomain = Simplex()\n",
      "     |  \n",
      "     |  domain = IndependentConstraint(Real(), 1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "     |  \n",
      "     |  bijective = False\n",
      "    \n",
      "    class SoftplusTransform(Transform)\n",
      "     |  SoftplusTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`\\text{Softplus}(x) = \\log(1 + \\exp(x))`.\n",
      "     |  The implementation reverts to the linear function when :math:`x > 20`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SoftplusTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  domain = Real()\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "    \n",
      "    class StackTransform(Transform)\n",
      "     |  StackTransform(tseq, dim=0, cache_size=0)\n",
      "     |  \n",
      "     |  Transform functor that applies a sequence of transforms `tseq`\n",
      "     |  component-wise to each submatrix at `dim`\n",
      "     |  in a way compatible with :func:`torch.stack`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |     x = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1)\n",
      "     |     t = StackTransform([ExpTransform(), identity_transform], dim=1)\n",
      "     |     y = t(x)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StackTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tseq, dim=0, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  bijective\n",
      "     |  \n",
      "     |  codomain\n",
      "     |  \n",
      "     |  domain\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'transforms': typing.List[torch.distributions.trans...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class StickBreakingTransform(Transform)\n",
      "     |  StickBreakingTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform from unconstrained space to the simplex of one additional\n",
      "     |  dimension via a stick-breaking process.\n",
      "     |  \n",
      "     |  This transform arises as an iterated sigmoid transform in a stick-breaking\n",
      "     |  construction of the `Dirichlet` distribution: the first logit is\n",
      "     |  transformed via sigmoid to the first probability and the probability of\n",
      "     |  everything else, and then the process recurses.\n",
      "     |  \n",
      "     |  This is bijective and appropriate for use in HMC; however it mixes\n",
      "     |  coordinates together and is less appropriate for optimization.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StickBreakingTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = Simplex()\n",
      "     |  \n",
      "     |  domain = IndependentConstraint(Real(), 1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "    \n",
      "    class StudentT(torch.distributions.distribution.Distribution)\n",
      "     |  StudentT(df, loc=0.0, scale=1.0, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Student's t-distribution parameterized by degree of\n",
      "     |  freedom :attr:`df`, mean :attr:`loc` and scale :attr:`scale`.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = StudentT(torch.tensor([2.0]))\n",
      "     |      >>> m.sample()  # Student's t-distributed with degrees of freedom=2\n",
      "     |      tensor([ 0.1046])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      df (float or Tensor): degrees of freedom\n",
      "     |      loc (float or Tensor): mean of the distribution\n",
      "     |      scale (float or Tensor): scale of the distribution\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StudentT\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df, loc=0.0, scale=1.0, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), ...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class TanhTransform(Transform)\n",
      "     |  TanhTransform(cache_size=0)\n",
      "     |  \n",
      "     |  Transform via the mapping :math:`y = \\tanh(x)`.\n",
      "     |  \n",
      "     |  It is equivalent to\n",
      "     |  ```\n",
      "     |  ComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])\n",
      "     |  ```\n",
      "     |  However this might not be numerically stable, thus it is recommended to use `TanhTransform`\n",
      "     |  instead.\n",
      "     |  \n",
      "     |  Note that one should use `cache_size=1` when it comes to `NaN/Inf` values.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TanhTransform\n",
      "     |      Transform\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = True\n",
      "     |  \n",
      "     |  codomain = Interval(lower_bound=-1.0, upper_bound=1.0)\n",
      "     |  \n",
      "     |  domain = Real()\n",
      "     |  \n",
      "     |  sign = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Transform:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from Transform:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Transform:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Transform:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "    \n",
      "    class Transform(builtins.object)\n",
      "     |  Transform(cache_size=0)\n",
      "     |  \n",
      "     |  Abstract class for invertable transformations with computable log\n",
      "     |  det jacobians. They are primarily used in\n",
      "     |  :class:`torch.distributions.TransformedDistribution`.\n",
      "     |  \n",
      "     |  Caching is useful for transforms whose inverses are either expensive or\n",
      "     |  numerically unstable. Note that care must be taken with memoized values\n",
      "     |  since the autograd graph may be reversed. For example while the following\n",
      "     |  works with or without caching::\n",
      "     |  \n",
      "     |      y = t(x)\n",
      "     |      t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n",
      "     |  \n",
      "     |  However the following will error when caching due to dependency reversal::\n",
      "     |  \n",
      "     |      y = t(x)\n",
      "     |      z = t.inv(y)\n",
      "     |      grad(z.sum(), [y])  # error because z is x\n",
      "     |  \n",
      "     |  Derived classes should implement one or both of :meth:`_call` or\n",
      "     |  :meth:`_inverse`. Derived classes that set `bijective=True` should also\n",
      "     |  implement :meth:`log_abs_det_jacobian`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      cache_size (int): Size of cache. If zero, no caching is done. If one,\n",
      "     |          the latest single value is cached. Only 0 and 1 are supported.\n",
      "     |  \n",
      "     |  Attributes:\n",
      "     |      domain (:class:`~torch.distributions.constraints.Constraint`):\n",
      "     |          The constraint representing valid inputs to this transform.\n",
      "     |      codomain (:class:`~torch.distributions.constraints.Constraint`):\n",
      "     |          The constraint representing valid outputs to this transform\n",
      "     |          which are inputs to the inverse transform.\n",
      "     |      bijective (bool): Whether this transform is bijective. A transform\n",
      "     |          ``t`` is bijective iff ``t.inv(t(x)) == x`` and\n",
      "     |          ``t(t.inv(y)) == y`` for every ``x`` in the domain and ``y`` in\n",
      "     |          the codomain. Transforms that are not bijective should at least\n",
      "     |          maintain the weaker pseudoinverse properties\n",
      "     |          ``t(t.inv(t(x)) == t(x)`` and ``t.inv(t(t.inv(y))) == t.inv(y)``.\n",
      "     |      sign (int or Tensor): For bijective univariate transforms, this\n",
      "     |          should be +1 or -1 depending on whether transform is monotone\n",
      "     |          increasing or decreasing.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Computes the transform `x => y`.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, cache_size=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  forward_shape(self, shape)\n",
      "     |      Infers the shape of the forward computation, given the input shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  inverse_shape(self, shape)\n",
      "     |      Infers the shapes of the inverse computation, given the output shape.\n",
      "     |      Defaults to preserving shape.\n",
      "     |  \n",
      "     |  log_abs_det_jacobian(self, x, y)\n",
      "     |      Computes the log det jacobian `log |dy/dx|` given input and output.\n",
      "     |  \n",
      "     |  with_cache(self, cache_size=1)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  event_dim\n",
      "     |  \n",
      "     |  inv\n",
      "     |      Returns the inverse :class:`Transform` of this transform.\n",
      "     |      This should satisfy ``t.inv.inv is t``.\n",
      "     |  \n",
      "     |  sign\n",
      "     |      Returns the sign of the determinant of the Jacobian, if applicable.\n",
      "     |      In general this only makes sense for bijective transforms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'codomain': <class 'torch.distributions.constraints...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  bijective = False\n",
      "    \n",
      "    class TransformedDistribution(torch.distributions.distribution.Distribution)\n",
      "     |  TransformedDistribution(base_distribution, transforms, validate_args=None)\n",
      "     |  \n",
      "     |  Extension of the Distribution class, which applies a sequence of Transforms\n",
      "     |  to a base distribution.  Let f be the composition of transforms applied::\n",
      "     |  \n",
      "     |      X ~ BaseDistribution\n",
      "     |      Y = f(X) ~ TransformedDistribution(BaseDistribution, f)\n",
      "     |      log p(Y) = log p(X) + log |det (dX/dY)|\n",
      "     |  \n",
      "     |  Note that the ``.event_shape`` of a :class:`TransformedDistribution` is the\n",
      "     |  maximum shape of its base distribution and its transforms, since transforms\n",
      "     |  can introduce correlations among events.\n",
      "     |  \n",
      "     |  An example for the usage of :class:`TransformedDistribution` would be::\n",
      "     |  \n",
      "     |      # Building a Logistic Distribution\n",
      "     |      # X ~ Uniform(0, 1)\n",
      "     |      # f = a + b * logit(X)\n",
      "     |      # Y ~ f(X) ~ Logistic(a, b)\n",
      "     |      base_distribution = Uniform(0, 1)\n",
      "     |      transforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\n",
      "     |      logistic = TransformedDistribution(base_distribution, transforms)\n",
      "     |  \n",
      "     |  For more examples, please look at the implementations of\n",
      "     |  :class:`~torch.distributions.gumbel.Gumbel`,\n",
      "     |  :class:`~torch.distributions.half_cauchy.HalfCauchy`,\n",
      "     |  :class:`~torch.distributions.half_normal.HalfNormal`,\n",
      "     |  :class:`~torch.distributions.log_normal.LogNormal`,\n",
      "     |  :class:`~torch.distributions.pareto.Pareto`,\n",
      "     |  :class:`~torch.distributions.weibull.Weibull`,\n",
      "     |  :class:`~torch.distributions.relaxed_bernoulli.RelaxedBernoulli` and\n",
      "     |  :class:`~torch.distributions.relaxed_categorical.RelaxedOneHotCategorical`\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_distribution, transforms, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  has_rsample\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  arg_constraints = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Uniform(torch.distributions.distribution.Distribution)\n",
      "     |  Uniform(low, high, validate_args=None)\n",
      "     |  \n",
      "     |  Generates uniformly distributed random samples from the half-open interval\n",
      "     |  ``[low, high)``.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |  \n",
      "     |      >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n",
      "     |      >>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\n",
      "     |      tensor([ 2.3418])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      low (float or Tensor): lower range (inclusive).\n",
      "     |      high (float or Tensor): upper range (exclusive).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Uniform\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, low, high, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  support\n",
      "     |      Returns a :class:`~torch.distributions.constraints.Constraint` object\n",
      "     |      representing this distribution's support.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'high': Dependent(), 'low': Dependent()}\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class VonMises(torch.distributions.distribution.Distribution)\n",
      "     |  VonMises(loc, concentration, validate_args=None)\n",
      "     |  \n",
      "     |  A circular von Mises distribution.\n",
      "     |  \n",
      "     |  This implementation uses polar coordinates. The ``loc`` and ``value`` args\n",
      "     |  can be any real number (to facilitate unconstrained optimization), but are\n",
      "     |  interpreted as angles modulo 2 pi.\n",
      "     |  \n",
      "     |  Example::\n",
      "     |      >>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample() # von Mises distributed with loc=1 and concentration=1\n",
      "     |      tensor([1.9777])\n",
      "     |  \n",
      "     |  :param torch.Tensor loc: an angle in radians.\n",
      "     |  :param torch.Tensor concentration: concentration parameter\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VonMises\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, loc, concentration, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  expand(self, batch_shape)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      The sampling algorithm for the von Mises distribution is based on the following paper:\n",
      "     |      Best, D. J., and Nicholas I. Fisher.\n",
      "     |      \"Efficient simulation of the von Mises distribution.\" Applied Statistics (1979): 152-157.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      The provided mean is the circular one.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      The provided variance is the circular one.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'loc...\n",
      "     |  \n",
      "     |  has_rsample = False\n",
      "     |  \n",
      "     |  support = Real()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Weibull(torch.distributions.transformed_distribution.TransformedDistribution)\n",
      "     |  Weibull(scale, concentration, validate_args=None)\n",
      "     |  \n",
      "     |  Samples from a two-parameter Weibull distribution.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |  \n",
      "     |      >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n",
      "     |      >>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\n",
      "     |      tensor([ 0.4784])\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      scale (float or Tensor): Scale parameter of distribution (lambda).\n",
      "     |      concentration (float or Tensor): Concentration parameter of distribution (k/shape).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Weibull\n",
      "     |      torch.distributions.transformed_distribution.TransformedDistribution\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, scale, concentration, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Returns entropy of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'sca...\n",
      "     |  \n",
      "     |  support = GreaterThan(lower_bound=0.0)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Computes the cumulative distribution function by inverting the\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Computes the inverse cumulative distribution function using\n",
      "     |      transform(s) and computing the score of the base distribution.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Scores the sample by inverting the transform(s) and computing the score\n",
      "     |      using the score of the base distribution and the log abs det jacobian.\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped reparameterized sample or sample_shape\n",
      "     |      shaped batch of reparameterized samples if the distribution parameters\n",
      "     |      are batched. Samples first from base distribution and applies\n",
      "     |      `transform()` for every transform in the list.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched. Samples first from\n",
      "     |      base distribution and applies `transform()` for every transform in the\n",
      "     |      list.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  has_rsample\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.transformed_distribution.TransformedDistribution:\n",
      "     |  \n",
      "     |  __annotations__ = {'arg_constraints': typing.Dict[str, torch.distribut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "    \n",
      "    class Wishart(torch.distributions.exp_family.ExponentialFamily)\n",
      "     |  Wishart(df: Union[torch.Tensor, numbers.Number], covariance_matrix: torch.Tensor = None, precision_matrix: torch.Tensor = None, scale_tril: torch.Tensor = None, validate_args=None)\n",
      "     |  \n",
      "     |  Creates a Wishart distribution parameterized by a symmetric positive definite matrix :math:`\\Sigma`,\n",
      "     |  or its Cholesky decomposition :math:`\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top`\n",
      "     |  \n",
      "     |  Example:\n",
      "     |      >>> m = Wishart(torch.eye(2), torch.Tensor([2]))\n",
      "     |      >>> m.sample()  # Wishart distributed with mean=`df * I` and\n",
      "     |                      # variance(x_ij)=`df` for i != j and variance(x_ij)=`2 * df` for i == j\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      covariance_matrix (Tensor): positive-definite covariance matrix\n",
      "     |      precision_matrix (Tensor): positive-definite precision matrix\n",
      "     |      scale_tril (Tensor): lower-triangular factor of covariance, with positive-valued diagonal\n",
      "     |      df (float or Tensor): real-valued parameter larger than the (dimension of Square matrix) - 1\n",
      "     |  Note:\n",
      "     |      Only one of :attr:`covariance_matrix` or :attr:`precision_matrix` or\n",
      "     |      :attr:`scale_tril` can be specified.\n",
      "     |      Using :attr:`scale_tril` will be more efficient: all computations internally\n",
      "     |      are based on :attr:`scale_tril`. If :attr:`covariance_matrix` or\n",
      "     |      :attr:`precision_matrix` is passed instead, it is only used to compute\n",
      "     |      the corresponding lower triangular matrices using a Cholesky decomposition.\n",
      "     |      'torch.distributions.LKJCholesky' is a restricted Wishart distribution.[1]\n",
      "     |  \n",
      "     |  **References**\n",
      "     |  \n",
      "     |  [1] `On equivalence of the LKJ distribution and the restricted Wishart distribution`,\n",
      "     |  Zhenxun Wang, Yunan Wu, Haitao Chu.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Wishart\n",
      "     |      torch.distributions.exp_family.ExponentialFamily\n",
      "     |      torch.distributions.distribution.Distribution\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, df: Union[torch.Tensor, numbers.Number], covariance_matrix: torch.Tensor = None, precision_matrix: torch.Tensor = None, scale_tril: torch.Tensor = None, validate_args=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  entropy(self)\n",
      "     |      Method to compute the entropy using Bregman divergence of the log normalizer.\n",
      "     |  \n",
      "     |  expand(self, batch_shape, _instance=None)\n",
      "     |      Returns a new distribution instance (or populates an existing instance\n",
      "     |      provided by a derived class) with batch dimensions expanded to\n",
      "     |      `batch_shape`. This method calls :class:`~torch.Tensor.expand` on\n",
      "     |      the distribution's parameters. As such, this does not allocate new\n",
      "     |      memory for the expanded distribution instance. Additionally,\n",
      "     |      this does not repeat any args checking or parameter broadcasting in\n",
      "     |      `__init__.py`, when an instance is first created.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          batch_shape (torch.Size): the desired expanded size.\n",
      "     |          _instance: new instance provided by subclasses that\n",
      "     |              need to override `.expand`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          New distribution instance with batch dimensions expanded to\n",
      "     |          `batch_size`.\n",
      "     |  \n",
      "     |  log_prob(self, value)\n",
      "     |      Returns the log of the probability density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  rsample(self, sample_shape=torch.Size([]), max_try_correction=None)\n",
      "     |      .. warning::\n",
      "     |          In some cases, sampling algorithn based on Bartlett decomposition may return singular matrix samples.\n",
      "     |          Several tries to correct singular samples are performed by default, but it may end up returning\n",
      "     |          singular matrix samples. Sigular samples may return `-inf` values in `.log_prob()`.\n",
      "     |          In those cases, the user should validate the samples and either fix the value of `df`\n",
      "     |          or adjust `max_try_correction` value for argument in `.rsample` accordingly.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  covariance_matrix\n",
      "     |  \n",
      "     |  mean\n",
      "     |      Returns the mean of the distribution.\n",
      "     |  \n",
      "     |  mode\n",
      "     |      Returns the mode of the distribution.\n",
      "     |  \n",
      "     |  precision_matrix\n",
      "     |  \n",
      "     |  scale_tril\n",
      "     |  \n",
      "     |  variance\n",
      "     |      Returns the variance of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  arg_constraints = {'covariance_matrix': PositiveDefinite(), 'df': Grea...\n",
      "     |  \n",
      "     |  has_rsample = True\n",
      "     |  \n",
      "     |  support = PositiveDefinite()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  cdf(self, value)\n",
      "     |      Returns the cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  enumerate_support(self, expand=True)\n",
      "     |      Returns tensor containing all values supported by a discrete\n",
      "     |      distribution. The result will enumerate over dimension 0, so the shape\n",
      "     |      of the result will be `(cardinality,) + batch_shape + event_shape`\n",
      "     |      (where `event_shape = ()` for univariate distributions).\n",
      "     |      \n",
      "     |      Note that this enumerates over all batched tensors in lock-step\n",
      "     |      `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens\n",
      "     |      along dim 0, but with the remaining batch dimensions being\n",
      "     |      singleton dimensions, `[[0], [1], ..`.\n",
      "     |      \n",
      "     |      To iterate over the full Cartesian product use\n",
      "     |      `itertools.product(m.enumerate_support())`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          expand (bool): whether to expand the support over the\n",
      "     |              batch dims to match the distribution's `batch_shape`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor iterating over dimension 0.\n",
      "     |  \n",
      "     |  icdf(self, value)\n",
      "     |      Returns the inverse cumulative density/mass function evaluated at\n",
      "     |      `value`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (Tensor):\n",
      "     |  \n",
      "     |  perplexity(self)\n",
      "     |      Returns perplexity of distribution, batched over batch_shape.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Tensor of shape batch_shape.\n",
      "     |  \n",
      "     |  sample(self, sample_shape=torch.Size([]))\n",
      "     |      Generates a sample_shape shaped sample or sample_shape shaped batch of\n",
      "     |      samples if the distribution parameters are batched.\n",
      "     |  \n",
      "     |  sample_n(self, n)\n",
      "     |      Generates n samples or n batches of samples if the distribution\n",
      "     |      parameters are batched.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  set_default_validate_args(value)\n",
      "     |      Sets whether validation is enabled or disabled.\n",
      "     |      \n",
      "     |      The default behavior mimics Python's ``assert`` statement: validation\n",
      "     |      is on by default, but is disabled if Python is run in optimized mode\n",
      "     |      (via ``python -O``). Validation may be expensive, so you may want to\n",
      "     |      disable it once a model is working.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          value (bool): Whether to enable validation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  batch_shape\n",
      "     |      Returns the shape over which parameters are batched.\n",
      "     |  \n",
      "     |  event_shape\n",
      "     |      Returns the shape of a single sample (without batching).\n",
      "     |  \n",
      "     |  stddev\n",
      "     |      Returns the standard deviation of the distribution.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.distributions.distribution.Distribution:\n",
      "     |  \n",
      "     |  has_enumerate_support = False\n",
      "\n",
      "FUNCTIONS\n",
      "    kl_divergence(p, q)\n",
      "        Compute Kullback-Leibler divergence :math:`KL(p \\| q)` between two distributions.\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            KL(p \\| q) = \\int p(x) \\log\\frac {p(x)} {q(x)} \\,dx\n",
      "        \n",
      "        Args:\n",
      "            p (Distribution): A :class:`~torch.distributions.Distribution` object.\n",
      "            q (Distribution): A :class:`~torch.distributions.Distribution` object.\n",
      "        \n",
      "        Returns:\n",
      "            Tensor: A batch of KL divergences of shape `batch_shape`.\n",
      "        \n",
      "        Raises:\n",
      "            NotImplementedError: If the distribution types have not been registered via\n",
      "                :meth:`register_kl`.\n",
      "        KL divergence is currently implemented for the following distribution pairs:\n",
      "            * :class:`~torch.distributions.Bernoulli` and :class:`~torch.distributions.Bernoulli`\n",
      "            * :class:`~torch.distributions.Bernoulli` and :class:`~torch.distributions.Poisson`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Beta` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.Binomial` and :class:`~torch.distributions.Binomial`\n",
      "            * :class:`~torch.distributions.Categorical` and :class:`~torch.distributions.Categorical`\n",
      "            * :class:`~torch.distributions.Cauchy` and :class:`~torch.distributions.Cauchy`\n",
      "            * :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.ContinuousBernoulli` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.Dirichlet` and :class:`~torch.distributions.Dirichlet`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Gumbel`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Exponential` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.ExponentialFamily` and :class:`~torch.distributions.ExponentialFamily`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Gumbel`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Gamma` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.Geometric` and :class:`~torch.distributions.Geometric`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Gumbel`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Gumbel` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.HalfNormal` and :class:`~torch.distributions.HalfNormal`\n",
      "            * :class:`~torch.distributions.Independent` and :class:`~torch.distributions.Independent`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Laplace`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Laplace` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.LowRankMultivariateNormal` and :class:`~torch.distributions.LowRankMultivariateNormal`\n",
      "            * :class:`~torch.distributions.LowRankMultivariateNormal` and :class:`~torch.distributions.MultivariateNormal`\n",
      "            * :class:`~torch.distributions.MultivariateNormal` and :class:`~torch.distributions.LowRankMultivariateNormal`\n",
      "            * :class:`~torch.distributions.MultivariateNormal` and :class:`~torch.distributions.MultivariateNormal`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Gumbel`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Laplace`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Normal` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.OneHotCategorical` and :class:`~torch.distributions.OneHotCategorical`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Pareto` and :class:`~torch.distributions.Uniform`\n",
      "            * :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Bernoulli`\n",
      "            * :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Binomial`\n",
      "            * :class:`~torch.distributions.Poisson` and :class:`~torch.distributions.Poisson`\n",
      "            * :class:`~torch.distributions.TransformedDistribution` and :class:`~torch.distributions.TransformedDistribution`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Beta`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.ContinuousBernoulli`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Exponential`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Gamma`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Gumbel`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Normal`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Pareto`\n",
      "            * :class:`~torch.distributions.Uniform` and :class:`~torch.distributions.Uniform`\n",
      "    \n",
      "    register_kl(type_p, type_q)\n",
      "        Decorator to register a pairwise function with :meth:`kl_divergence`.\n",
      "        Usage::\n",
      "        \n",
      "            @register_kl(Normal, Normal)\n",
      "            def kl_normal_normal(p, q):\n",
      "                # insert implementation here\n",
      "        \n",
      "        Lookup returns the most specific (type,type) match ordered by subclass. If\n",
      "        the match is ambiguous, a `RuntimeWarning` is raised. For example to\n",
      "        resolve the ambiguous situation::\n",
      "        \n",
      "            @register_kl(BaseP, DerivedQ)\n",
      "            def kl_version1(p, q): ...\n",
      "            @register_kl(DerivedP, BaseQ)\n",
      "            def kl_version2(p, q): ...\n",
      "        \n",
      "        you should register a third most-specific implementation, e.g.::\n",
      "        \n",
      "            register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n",
      "        \n",
      "        Args:\n",
      "            type_p (type): A subclass of :class:`~torch.distributions.Distribution`.\n",
      "            type_q (type): A subclass of :class:`~torch.distributions.Distribution`.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Bernoulli', 'Beta', 'Binomial', 'Categorical', 'Cauchy', '...\n",
      "    biject_to = <torch.distributions.constraint_registry.ConstraintRegistr...\n",
      "    identity_transform = ComposeTransform(\n",
      "        \n",
      "    )\n",
      "    transform_to = <torch.distributions.constraint_registry.ConstraintRegi...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\rambo\\miniconda3\\envs\\d2l\\lib\\site-packages\\torch\\distributions\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# help\n",
    "print(dir(torch.distributions))\n",
    "help(torch.distributions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
