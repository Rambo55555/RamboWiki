Key,Year,Url,Publication Title,Title,Abstract（英文）,Keywords,标题,Abstract（中文）,关键词,可用数据集,源代码,备注（如改进方向等）
C8VSYBES,2023,https://doi.org/10.1109/ASE56229.2023.00213,ASE 2023,CPA-DF: A Tool for Configurable Interval Analysis to Boost Program Verification,"Software verification is challenging, and auxiliary program invariants are used to improve the effectiveness of verification approaches. For instance, the k-induction implementation in CPACHECKER, an award-winning framework for program analysis, uses invariants produced by a configurable data-flow analysis to strengthen induction hypotheses. This invariant generator, CPA-DF, uses arithmetic expressions over intervals as its abstract domain and is able to prove some safe verification tasks alone. After extensively evaluating CPA-DF on SV-Benchmarks, the largest publicly available suite of C safety-verification tasks, we discover that its potential as a stand-alone analysis or a sub-analysis in a parallel portfolio for combined verification approaches has been significantly underestimated: (1) As a stand-alone analysis, CPA-DF finds almost as many proofs as the plain k-induction implementation without auxiliary invariants. (2) As a sub-analysis running in parallel to the plain k-induction implementation, CPA-DF boosts the portfolio verifier to solve a comparable amount of tasks as the heavily-optimized k-induction implementation with invariant injection. Our detailed analysis reveals that dynamic precision adjustment is crucial to the efficiency and effectiveness of CPA-DF. To generalize our results beyond CPACHECKER, we use CoVeriteam,a platform for cooperative verification, to compose three portfolio verifiers that execute CPA-DF and three other software verifiers in parallel, respectively. Surprisingly, running CPA-DF merely in parallel to these state-of-the-art tools further boosts the number of correct results up to more than 20 %. Demonstration video: https://youtu.be/l7UG-vhTL_4","Program Analysis,Data Flow Analysis,Software Model Checking,Interval Analysis,Dynamic Precision Adjustment,Invariant Generation",CPA-DF：一种可配置区间分析工具，用于增强程序验证,软件验证具有挑战性，使用辅助程序不变量来提高验证方法的有效性。例如，CPACHECKER中的k归纳实现是一个屡获殊荣的程序分析框架，它使用可配置数据流分析产生的不变量来加强归纳假设。这种不变生成器CPA-DF使用区间上的算术表达式作为其抽象域，并且能够单独证明一些安全的验证任务。在SV Benchmarks（最大的公开C安全验证任务套件）上对CPA-DF进行了广泛评估后，我们发现其作为独立分析或联合验证方法并行组合中的子分析的潜力被严重低估：（1）作为独立分析，CPA-DF发现的证明几乎与没有辅助不变量的普通k归纳实现一样多。（2） 作为与普通k归纳实现并行运行的子分析，CPA-DF增强了投资组合验证器，以解决与具有不变注入的高度优化的k归纳实现相当数量的任务。我们的详细分析表明，动态精度调整对CPA-DF的效率和有效性至关重要。为了将我们的结果推广到CPACHECKER之外，我们使用合作验证平台CoVeriteam来组成三个组合验证器，分别并行执行CPA-DF和其他三个软件验证器。令人惊讶的是，仅与这些最先进的工具并行运行CPA-DF，就可以将正确结果的数量进一步提高到20%以上。演示视频：https://youtu.be/l7UG-vhTL_4,程序分析，数据流分析，软件模型检查，区间分析，动态精度调整，不变量生成,,,
8PBLFHI9,2023,https://doi.org/10.1109/ASE56229.2023.00146,ASE 2023,Provengo: A Tool Suite for Scenario Driven Model-Based Testing,"We present Provengo, a comprehensive suite of tools designed to facilitate the implementation of Scenario-Driven Model-Based Testing (SDMBT), an innovative approach that utilizes scenarios to construct a model encompassing the user's perspective and the system's business value while also defining the desired outcomes. With the assistance of Provengo, testers gain the ability to effortlessly create natural user stories and seamlessly integrate them into a model capable of generating effective tests. The demonstration illustrates how SDMBT effectively addresses the bootstrapping challenge commonly encountered in model-based testing (MBT) by enabling incremental development, starting from simple models and gradually augmenting them with additional stories.","Testing,Software engineering,Business",Provengo：一个用于场景驱动的基于模型的测试的工具套件,我们介绍了Provengo，这是一套全面的工具，旨在促进场景驱动的基于模型的测试（SDMBT）的实施，这是是一种创新的方法，利用场景构建一个包含用户视角和系统业务价值的模型，同时定义所需的结果。在Provengo的帮助下，测试人员能够毫不费力地创建自然的用户故事，并将其无缝集成到能够生成有效测试的模型中。该演示展示了SDMBT如何通过实现增量开发，从简单的模型开始，并通过额外的故事逐渐增强，有效地解决基于模型的测试（MBT）中常见的自举挑战。,测试，软件工程，商业,,,
YXIZFW3F,2023,https://doi.org/10.1109/ASE56229.2023.00065,ASE 2023,"Better Patching Using LLM Prompting, via Self-Consistency","Large Language models (LLMs) can be induced to solve non-trivial problems with “few-shot” prompts including illustrative problem-solution examples. Now if the few-shots also include “chain of thought” ($\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a “explained” solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\mathcal{S}-C$ (or even $\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.","LLMs,Self-consistency,Program Repair",通过自一致性使用LLM提示进行更好的修补,大型语言模型（LLM）可以通过“少量”提示（包括说明性问题解决方案示例）来解决非琐碎的问题。现在，如果少数镜头也包括“思想链”（$\mathcal{C}oT$）解释，这是问题解释解决方案的形式，LLM将生成一个“已解释”的解决方案，并执行得更好。最近，一种令人兴奋的、实质上更好的技术，自洽[1]（$\mathcal{S}-C$）已经出现，基于对正确解决方案有许多看似合理的解释的直觉；当LLM被重复采样以生成一个解释-解决方案对池时，对于给定的问题，池中最频繁出现的解决方案（忽略解释）更有可能是正确的！不幸的是，使用这种高性能的$\mathcal{S}-C$（甚至$\mathcal{C}oT$）方法在软件工程环境中由于缺乏解释而受到阻碍；大多数软件数据集缺乏解释。在本文中，我们描述了$\mathcal{S}-C$方法进行程序修复，使用修复上的提交日志作为解释，仅在示例性的几个镜头中。我们在MODIT数据集上实现了最先进的结果，击败了以前基于提示的程序修复方法；我们还发现有证据表明，正确的提交消息有助于LLM学习生成更好的补丁。,LLM，自一致性，程序修复,,,
G9XMACX6,2023,https://doi.org/10.1109/ASE56229.2023.00216,ASE 2023,Optimizing Continuous Development by Detecting and Preventing Unnecessary Content Generation,"Continuous development (CD) helps developers quickly release and update their software. To enact CD, developers customize their CD builds to perform several tasks, including compiling, testing, static analysis checks, etc. However, as developers add more tasks to their builds, the builds take longer to run, therefore slowing down the entire CD process. Furthermore, developers may unknowingly include tasks into their builds whose results are not used (e.g., generating coverage files that are never read or uploaded anywhere), therefore wasting build runtime doing unnecessary tasks. We propose OptCD, a technique to dynamically detect unnecessary work within CD builds. Our intuition is that unnecessary work can be identified by the generation of files that are not used by any other task within the build. OptCD runs alongside a CD build, tracking the generated files during the build and which files are read/written. Files that are written to but are never read from are unnecessary content from a build. Based on the names of the unnecessary files, OptCD then maps the files to the specific build tasks responsible for generating or writing to those files. Finally, OptCD leverages ChatGPT to suggest changing the build configuration to disable generating these unnecessary files. Our evaluation of OptCD on 22 open-source projects finds that 95.6% of projects generate at least one unused directory, a directory whose contents are all unnecessarily generated. OptCD identifies the correct task that generates 92.0% of the unused directories. Further, OptCD can produce a patch for the CD configuration file to prevent generating 72.0% of the unused directories. Using the patches, we reduce the runtime by 7.0% on average for the projects we studied. We submitted 26 pull requests for the unused directories that we could disable. Developers have accepted 12 of them, with five rejected, and nine still pending.","Runtime,Static analysis,Writing,Chatbots,Software,Task analysis,Testing",通过检测和防止不必要的内容生成优化持续开发,持续开发（CD）帮助开发人员快速发布和更新软件。为了制定CD，开发人员自定义他们的CD构建以执行多项任务，包括编译、测试、静态分析检查等。然而，随着开发人员向构建中添加更多任务，构建运行所需的时间更长，因此会减慢整个CD过程。此外，开发人员可能会在不知情的情况下将结果未被使用的任务包括在构建中（例如，生成从未在任何地方读取或上传的覆盖文件），从而浪费构建运行时执行不必要的任务。我们提出了OptCD，这是一种在CD构建中动态检测不必要工作的技术。我们的直觉是，不必要的工作可以通过生成构建中任何其他任务都不使用的文件来识别。OptCD与CD构建一起运行，跟踪构建过程中生成的文件以及读取/写入的文件。写入但从未读取的文件是生成中不必要的内容。根据不必要文件的名称，OptCD将文件映射到负责生成或写入这些文件的特定构建任务。最后，OptCD利用ChatGPT建议更改构建配置，以禁止生成这些不必要的文件。我们对22个开源项目的OptCD评估发现，95.6%的项目至少生成了一个未使用的目录，该目录的内容都是不必要的生成。OptCD确定生成92.0%未使用目录的正确任务。此外，OptCD可以为CD配置文件生成一个补丁，以防止生成72.0%的未使用目录。使用这些补丁，我们研究的项目的运行时间平均减少了7.0%。我们提交了26个可以禁用的未使用目录的拉取请求。开发商已经接受了其中12家，其中5家被拒绝，9家仍在等待中。,聊天机器人，静态分析，写作，软件，任务分析，测试,,,
EUT579M7,2023,https://doi.org/10.1109/ASE56229.2023.00092,ASE 2023,Modeling Programmer Attention as Scanpath Prediction,"This paper launches a new effort at modeling programmer attention by predicting eye movement scanpaths. Programmer attention refers to what information people intake when performing programming tasks. Models of programmer attention refer to machine prediction of what information is important to people. Models of programmer attention are important because they help researchers build better interfaces, assistive technologies, and more human-like AI. For many years, researchers in SE have built these models based on features such as mouse clicks, key logging, and IDE interactions. Yet the holy grail in this area is scanpath prediction - the prediction of the sequence of eye fixations a person would take over a visual stimulus. A person's eye movements are considered the most concrete evidence that a person is taking in a piece of information. Scanpath prediction is a notoriously difficult problem, but we believe that the emergence of lower-cost, higheraccuracy eye tracking equipment and better large language models of source code brings a solution within grasp. We present an eye tracking experiment with 27 programmers and a prototype scanpath predictor to present preliminary results and obtain early community feedback.","scanpath prediction,human attention,eye tracking,neural networks,artificial intelligence",将程序员注意力建模为扫描路径预测,本文通过预测眼动扫描路径，对程序员的注意力进行了建模。程序员注意力是指人们在执行编程任务时所获取的信息。程序员注意力模型指的是机器对什么信息对人们来说很重要的预测。程序员注意力的模型很重要，因为它们可以帮助研究人员构建更好的界面、辅助技术和更人性化的人工智能。多年来，SE的研究人员一直基于鼠标点击、按键记录和IDE交互等功能构建这些模型。然而，这一领域的圣杯是扫描路径预测——预测一个人在视觉刺激下的眼睛注视顺序。一个人的眼球运动被认为是一个人接受信息的最具体证据。扫描路径预测是一个众所周知的难题，但我们相信，低成本、高精度的眼动追踪设备和更好的源代码大型语言模型的出现，将使解决方案触手可及。我们对27名程序员和一个原型扫描路径预测器进行了眼动追踪实验，以呈现初步结果并获得早期社区反馈。,扫描路径预测，人类注意力，眼睛跟踪，神经网络，人工智能,,,
NHRUFGX4,2023,https://doi.org/10.1109/ASE56229.2023.00116,ASE 2023,Minecraft: Automated Mining of Software Bug Fixes with Precise Code Context,"Repository mining of bug fixes from version control systems like GitHub is a challenging problem as far as the precision of the bug context is concerned, i.e., source codes immediately preceding and succeeding the fix location. Coupled with this, identification of the type of the bug fix goes a long way towards creating high quality datasets that can be used for several downstream tasks. However, existing bug fix datasets suffer from the following limitations that dilute the data quality. Firstly, they do not focus on multilingual projects in their entirety given that most open-source projects are now multilingual. Secondly, the granularity of the bug fixes are considered only at the function/method level without specifying line/statement level information. Thirdly, bug fixes lying within the scope of a source file but outside any of its constituent functions have not been examined. In this paper, we propose a solution to overcome the aforementioned limitations by introducing a novel and extensive dataset named Minecraft. With a size of 28.8GB (considering 416 GitHub projects encompassing programming languages such as C, C++, Java, and Python, 2.2M commits, 3.29M bug-fix pairs), Minecraft surpasses the existing datasets by 4-fold enlargement in terms of data availability. We believe Minecraft would serve as a valuable resource for various stakeholders in the software development and research communities, empowering them to improve software quality, develop innovative bug detection and auto-fix techniques, and advance the field of software engineering.","Training,Codes,Source coding,Computer bugs,Software quality,Data mining,Stakeholders",Minecraft：使用精确的代码上下文自动挖掘软件错误修复,从GitHub等版本控制系统中挖掘漏洞修复是一个具有挑战性的问题，就漏洞上下文的精度而言，即在修复位置之前和之后的源代码。再加上这一点，识别bug修复的类型将大大有助于创建可用于几个下游任务的高质量数据集。然而，现有的bug修复数据集存在以下限制，这些限制会降低数据质量。首先，考虑到大多数开源项目现在都是多语言的，他们并没有完全关注多语言项目。其次，只在函数/方法级别考虑错误修复的粒度，而不指定行/语句级别的信息。第三，位于源文件范围内但不在其任何组成函数范围内的错误修复尚未经过检查。在本文中，我们提出了一种解决方案，通过引入一个名为Minecraft的新颖而广泛的数据集来克服上述限制。Minecraft的大小为28.8GB（考虑到416个GitHub项目，包括C、C++、Java和Python等编程语言，220万次提交，329万次错误修复对），在数据可用性方面，它比现有数据集扩大了4倍。我们相信，Minecraft将成为软件开发和研究社区中各种利益相关者的宝贵资源，使他们能够提高软件质量，开发创新的错误检测和自动修复技术，并推进软件工程领域。,培训，代码，源代码，计算机错误，软件质量，数据挖掘，利益相关者,,,
Y3XS9VKB,2023,https://doi.org/10.1109/ASE56229.2023.00060,ASE 2023,SmartBugs 2.0: An Execution Framework for Weakness Detection in Ethereum Smart Contracts,"Smart contracts are blockchain programs that often handle valuable assets. Writing secure smart contracts is far from trivial, and any vulnerability may lead to significant financial losses. To support developers in identifying and eliminating vulnerabilities, methods and tools for the automated analysis of smart contracts have been proposed. However, the lack of commonly accepted benchmark suites and performance metrics makes it difficult to compare and evaluate such tools. Moreover, the tools are heterogeneous in their interfaces and reports as well as their runtime requirements, and installing several tools is time-consuming. In this paper, we present SmartBugs 2.0, a modular execution framework. It provides a uniform interface to 19 tools aimed at smart contract analysis and accepts both Solidity source code and EVM bytecode as input. After describing its architecture, we highlight the features of the framework. We evaluate the framework via its reception by the community and illustrate its scalability by describing its role in a study involving 3.25 million analyses.","Bytecode,EVM,Solidity,Security,Vulnerability",SmartBugs 2.0：以太坊智能合约弱点检测的执行框架,智能合约是区块链程序，通常处理有价值的资产。编写安全的智能合约绝非易事，任何漏洞都可能导致重大的财务损失。为了支持开发人员识别和消除漏洞，已经提出了智能合约自动化分析的方法和工具。然而，由于缺乏普遍接受的基准套件和性能指标，因此很难对此类工具进行比较和评估。此外，这些工具的接口和报告以及运行时要求都是异构的，安装几个工具非常耗时。在本文中，我们介绍了SmartBugs 2.0，一个模块化执行框架。它为19种智能合约分析工具提供了统一的接口，并接受Solidity源代码和EVM字节码作为输入。在描述了它的体系结构之后，我们重点介绍了该框架的特点。我们通过社区对该框架的接受程度来评估该框架，并通过描述其在一项涉及325万次分析的研究中的作用来说明其可扩展性。,字节码，EVM，稳固性，安全性，漏洞,,,
7RBWGQWZ,2023,https://doi.org/10.1109/ASE56229.2023.00208,ASE 2023,BUGSC++: A Highly Usable Real World Defect Benchmark for C/C++,"As software systems grow larger and more complex, debugging takes up an increasingly significant portion of developers' time and efforts during software maintenance. To aid software engineers in debugging, many automated debugging and repair techniques have been proposed. Both the development and evaluation of these automated techniques depend on benchmarks of bugs. While many different defect benchmarks have been developed, only a few benchmarks are widely used due to the origin of the collected bugs as well as the usability of the benchmarks themselves, risking a biased research landscape. This paper presents BUGSC++, a new benchmark that contains 209 real-world bugs collected from 22 open-source C/C++ projects. BugsC++ aims to provide high usability by providing a similar user interface to the widely used Defects4J. Further, BugsC++ ensures the replicability of the bugs in its collection by encapsulating each buggy program in a Docker container. By providing a highly usable real-world defect benchmark for C/C++, we hope to promote debugging research for C/C++.","software testing,bug,fault,defect benchmark",BUGSC++：一个高度可用的C/C现实世界缺陷基准++,随着软件系统变得越来越大、越来越复杂，调试在软件维护过程中占用了开发人员越来越大的时间和精力。为了帮助软件工程师进行调试，已经提出了许多自动调试和修复技术。这些自动化技术的开发和评估都依赖于bug的基准测试。虽然已经开发了许多不同的缺陷基准测试，但由于收集到的缺陷的来源以及基准测试本身的可用性，只有少数基准测试被广泛使用，这可能会导致研究领域存在偏见。本文介绍了BUGSC++，这是一个新的基准测试，包含了从22个开源C/C++项目中收集的209个真实世界的bug。BugsC++旨在通过提供与广泛使用的Defects4J类似的用户界面来提供高可用性。此外，BugsC++通过将每个Bug程序封装在Docker容器中，确保了其集合中Bug的可复制性。通过为C/C++提供一个高可用性的真实世界缺陷基准，我们希望促进C/C++的调试研究。,软件测试，bug，故障，缺陷基准,,,
MX3YP7PH,2023,https://doi.org/10.1109/ASE56229.2023.00095,ASE 2023,Systematically Detecting Packet Validation Vulnerabilities in Embedded Network Stacks,"Embedded Network Stacks (ENS) enable low-resource devices to communicate with the outside world, facilitating the development of Internet of Things and Cyber-Physical Systems. Some defects in ENS are thus high-severity cybersecurity vulnerabilities: they are remotely triggerable and can impact the physical world. While prior research has shed light on the characteristics of defects in many classes of software systems, no study has described the properties of ENS defects nor identified a systematic technique to expose them. The most common automated approach to detecting ENS defects is feedback-driven randomized dynamic analysis (“fuzzing”), a costly and unpredictable technique. This paper provides the first systematic characterization of cybersecurity vulnerabilities in ENS. We analyzed 61 vulnerabilities across 6 open-source ENS. Most of these ENS defects are concentrated in the transport and network layers of the network stack, require reaching different states in the network protocol, and can be triggered by only 1–2 modifications to a single packet. We therefore propose a novel systematic testing framework that focuses on the transport and network layers, uses seeds that cover a network protocol's states, and systematically modifies packet fields. We evaluate this framework on 4 ENS and replicated 12 of the 14 reported IP/TCP/UDP vulnerabilities. On recent versions of these ENSs, it discovered 7 novel defects (6 assigned CVES) during a bounded systematic test that covered all protocol states and made up to 3 modifications per packet. We found defects in 3 of the 4 ENS we tested that had not been found by prior fuzzing research. Our results suggest that fuzzing should be deferred until after systematic testing is employed.","Automated Testing,Validation,Cybersecurity,Embedded systems,IoT,Networking,Empirical Software Engineering,Fuzzing",系统检测嵌入式网络堆栈中的数据包验证漏洞,嵌入式网络堆栈（ENS）使低资源设备能够与外界通信，促进了物联网和网络物理系统的发展。因此，ENS中的一些缺陷是高度严重的网络安全漏洞：它们是可远程触发的，可以影响物理世界。虽然先前的研究已经阐明了许多类别软件系统中缺陷的特征，但没有研究描述ENS缺陷的特性，也没有确定暴露它们的系统技术。检测ENS缺陷最常见的自动化方法是反馈驱动的随机动态分析（“模糊化”），这是一种成本高昂且不可预测的技术。本文首次系统地描述了ENS中的网络安全漏洞。我们分析了6个开源ENS的61个漏洞。这些ENS缺陷大多集中在网络堆栈的传输层和网络层，需要达到网络协议中的不同状态，并且只需对单个数据包进行1-2次修改即可触发。因此，我们提出了一种新的系统测试框架，该框架侧重于传输层和网络层，使用覆盖网络协议状态的种子，并系统地修改数据包字段。我们在4个ENS上评估了该框架，并复制了14个报告的IP/TCP/UDP漏洞中的12个。在这些ENS的最新版本中，它在一次有界系统测试中发现了7个新缺陷（6个分配的CVES），该测试覆盖了所有协议状态，每个数据包最多有3个修改。我们在测试的4个ENS中发现了3个缺陷，这些缺陷是先前模糊研究没有发现的。我们的结果表明，模糊化应该推迟到系统测试之后。,自动化测试，验证，网络安全，嵌入式系统，物联网，网络，经验软件工程，引信,,,
3NHWYKIH,2023,https://doi.org/10.1109/ASE56229.2023.00190,ASE 2023,Government Mobile Apps: Analysing Citizen Feedback via App Reviews,"Governments worldwide are increasingly embracing digital transformation initiatives to enhance service delivery, engage citizens, and achieve better outcomes. However, obtaining continuous feedback on these initiatives poses a substantial challenge. This paper investigates the feasibility of leveraging mobile app reviews as a valuable source of citizen feedback on government digital services. We analyse 100,146 app reviews from 129 government mobile apps in Australia and identify several functional and usability issues. These include issues such as app instability, complexity, integration problems, navigation difficulties, inaccuracies, and challenges with ID verification and authentication processes. Furthermore, we uncover several factors that influence user satisfaction, including accuracy and reliability, convenience, dependability, user-centric design, and overall user-friendliness. These findings demonstrate a strong correlation between user feedback and the government's digital transformation strategy, underscoring the viability of mobile app reviews as a cost-effective avenue for collecting citizen feedback.","digital transformation,mobile apps,app reviews,user feedback",政府移动应用程序：通过应用程序评论分析公民反馈,世界各国政府越来越多地接受数字化转型举措，以加强服务提供，让公民参与进来，并取得更好的成果。然而，获得对这些举措的持续反馈是一项重大挑战。本文研究了利用移动应用程序评论作为公民对政府数字服务反馈的宝贵来源的可行性。我们分析了来自澳大利亚129个政府移动应用程序的100146条应用程序评论，并确定了几个功能和可用性问题。这些问题包括应用程序不稳定、复杂性、集成问题、导航困难、不准确以及身份验证和身份验证过程中的挑战。此外，我们还揭示了影响用户满意度的几个因素，包括准确性和可靠性、便利性、可靠性、以用户为中心的设计和整体用户友好性。这些发现表明，用户反馈与政府的数字化转型战略之间存在着强烈的相关性，突显了移动应用评论作为收集公民反馈的一种具有成本效益的途径的可行性。,数字化转型，移动应用，应用评论，用户反馈,,,
Z4FZGJ8Q,2023,https://doi.org/10.1109/ASE56229.2023.00194,ASE 2023,Semantic Data Augmentation for Deep Learning Testing Using Generative AI,"The performance of state-of-the-art Deep Learning models heavily depends on the availability of well-curated training and testing datasets that sufficiently capture the operational domain. Data augmentation is an effective technique in alleviating data scarcity, reducing the time-consuming and expensive data collection and labelling processes. Despite their potential, existing data augmentation techniques primarily focus on simple geometric and colour space transformations, like noise, flipping and resizing, producing datasets with limited diversity. When the augmented dataset is used for testing the Deep Learning models, the derived results are typically uninformative about the robustness of the models. We address this gap by introducing GENFUZZER, a novel coverage-guided data augmentation fuzzing technique for Deep Learning models underpinned by generative AI. We demonstrate our approach using widely-adopted datasets and models employed for image classification, illustrating its effectiveness in generating informative datasets leading up to a 26% increase in widely-used coverage criteria.","Generative AI,Deep Learning Testing,Coverage Guided Fuzzing,Data Augmentation,Safe AI",基于生成人工智能的深度学习测试语义数据增强,最先进的深度学习模型的性能在很大程度上取决于精心策划的训练和测试数据集的可用性，这些数据集能够充分捕捉操作领域。数据扩充是缓解数据短缺、减少耗时和昂贵的数据收集和标记过程的有效技术。尽管存在潜力，但现有的数据增强技术主要集中在简单的几何和颜色空间转换上，如噪声、翻转和调整大小，从而产生多样性有限的数据集。当增强数据集用于测试深度学习模型时，导出的结果通常对模型的稳健性没有信息。我们通过引入GENFUZZER来解决这一差距，这是一种新的覆盖率引导的数据增强模糊技术，用于生成人工智能支持的深度学习模型。我们使用广泛采用的数据集和图像分类模型展示了我们的方法，说明了它在生成信息数据集方面的有效性，从而使广泛使用的覆盖率标准提高了26%。,生成人工智能，深度学习测试，覆盖引导引信，数据增强，安全人工智能,,,
2BXWFWB7,2023,https://doi.org/10.1109/ASE56229.2023.00205,ASE 2023,Assessing the Impact of Refactoring Energy-Inefficient Code Patterns on Software Sustainability: An Industry Case Study,"Advances in technologies like artificial intelligence and metaverse have led to a proliferation of software systems in business and everyday life. With this widespread penetration, the carbon emissions of software are rapidly growing as well, thereby negatively impacting the long-term sustainability of our environment. Hence, optimizing software from a sustainability standpoint becomes more crucial than ever. We believe that the adoption of automated tools that can identify energy-inefficient patterns in the code and guide appropriate refactoring can significantly assist in this optimization. In this extended abstract, we present an industry case study that evaluates the sustainability impact of refactoring energy -inefficient code patterns identified by automated software sustainability assessment tools for a large application. Preliminary results highlight a positive impact on the application's sustainability post-refactoring, leading to a 29% decrease in per-user per-month energy consumption.","Industries,Energy consumption,Codes,Metaverse,Carbon dioxide,Software systems,Sustainable development",评估重构能源低效代码模式对软件可持续性的影响：一个行业案例研究,人工智能和元宇宙等技术的进步导致了商业和日常生活中软件系统的激增。随着这种广泛的渗透，软件的碳排放也在迅速增长，从而对我们环境的长期可持续性产生了负面影响。因此，从可持续性的角度优化软件变得比以往任何时候都更加重要。我们相信，采用自动化工具可以识别代码中能源效率低下的模式，并指导适当的重构，这将大大有助于优化。在这篇扩展摘要中，我们提出了一个行业案例研究，评估了大型应用程序的自动化软件可持续性评估工具所识别的重构能源效率低下的代码模式的可持续性影响。初步结果强调了重构后对应用程序可持续性的积极影响，导致每个用户每月的能源消耗减少了29%。,产业，能源消耗，代码，Metverse，二氧化碳，软件系统，可持续发展,,,
BKICC7KK,2023,https://doi.org/10.1109/ASE56229.2023.00103,ASE 2023,Towards Automatically Addressing Self-Admitted Technical Debt: How Far Are We?,"Upon evolving their software, organizations and individual developers have to spend a substantial effort to pay back technical debt, i.e, the fact that software is released in a shape not as good as it should be, e.g, in terms of functionality, reliability, or maintainability. This paper empirically investigates the extent to which technical debt can be automatically paid back by neural-based generative models, and in particular models exploiting different strategies for pre-training and fine-tuning. We start by extracting a dateset of 5,039 Self-Admitted Technical Debt (SATD) removals from 595 open-source projects. SATD refers to technical debt instances documented (e.g, via code comments) by developers. We use this dataset to experiment with seven different generative deep learning (DL) model configurations. Specifically, we compare transformers pre-trained and fine-tuned with different combinations of training objectives, including the fixing of generic code changes, SATD removals, and SATD-comment prompt tuning. Also, we investigate the applicability in this context of a recently-available Large Language Model (LLM)-based chat bot. Results of our study indicate that the automated repayment of SATD is a challenging task, with the best model we experimented with able to automatically fix ∼2% to 8% of test instances, depending on the number of attempts it is allowed to make. Given the limited size of the fine-tuning dataset (∼5k instances), the model's pre-training plays a fundamental role in boosting performance. Also, the ability to remove SATD steadily drops if the comment documenting the SATD is not provided as input to the model. Finally, we found general-purpose LLMs to not be a competitive approach for addressing SATD.","Self-Admitted Technical Debt,Pre-trained models,Machine Learning for Code",走向自动解决自我承认的技术债务：我们还有多远？,在开发软件时，组织和个人开发人员必须花费大量精力来偿还技术债务，即软件发布的形式不如预期，例如在功能、可靠性或可维护性方面。本文实证研究了基于神经的生成模型，特别是利用不同策略进行预训练和微调的模型，可以在多大程度上自动偿还技术债务。我们首先从595个开源项目中提取5039个自承认技术债务（SATD）删除的日期集。SATD是指由开发人员记录（例如，通过代码注释）的技术债务实例。我们使用这个数据集对七种不同的生成深度学习（DL）模型配置进行了实验。具体来说，我们将经过预训练和微调的transformer与不同的训练目标组合进行比较，包括修复通用代码更改、删除SATD和SATD注释提示调优。此外，我们还研究了最近推出的基于大型语言模型（LLM）的聊天机器人在这种情况下的适用性。我们的研究结果表明，SATD的自动还款是一项具有挑战性的任务，我们实验过的最佳模型能够自动修复约2%至8%的测试实例，这取决于它可以进行的尝试次数。鉴于微调数据集的大小有限（~5k个实例），模型的预训练在提高性能方面发挥着重要作用。此外，如果记录SATD的注释没有作为模型的输入提供，则删除SATD的能力会稳步下降。最后，我们发现通用LLM不是解决SATD的竞争方法。,自我承认的技术债务，预先培训的模型，代码的机器学习,,,
KUQXNWIK,2023,https://doi.org/10.1109/ASE56229.2023.00056,ASE 2023,Fine-Grained In-Context Permission Classification for Android Apps Using Control-Flow Graph Embedding,"Android is the most popular operating system for mobile devices nowadays. Permissions are a very important part of Android security architecture. Apps frequently need the users' permission, but many of them only ask for it once—when the user uses the app for the first time—and then they keep and abuse the given permissions. Longing to enhance Android permission security and users' private data protection is the driving factor behind our approach to explore fine-grained context-sensitive permission usage analysis and thereby identify misuses in Android apps. In this work, we propose an approach for classifying the fine-grained permission uses for each functionality of Android apps that a user interacts with. Our approach, named DroidGem, relies on mainly three technical components to provide an in-context classification for permission (mis)uses by Android apps for each functionality triggered by users: (1) static inter-procedural control-flow graphs and call graphs representing each functionality in an app that may be triggered by users' or systems' events through UI-linked event handlers, (2) graph embedding techniques converting graph structures into numerical encoding, and (3) supervised machine learning models classifying (mis)uses of permissions based on the embedding. We have implemented a prototype of DroidGem and evaluated it on 89 diverse apps. The results show that DroidGem can accurately classify whether permission used by the functionality of an app triggered by a UI-linked event handler is a misuse in relation to manually verified decisions, with up to 95% precision and recall. We believe that such a permission classification mechanism can be helpful in providing fine-grained permission notices in a context related to app users' actions, and improving their awareness of (mis)uses of permissions and private data in Android apps.","Privacy protection,Permission control,Android apps,Control flow graphs,Graph embedding,Classification",使用控制流图嵌入的Android应用程序上下文中的细粒度权限分类,安卓是当今最流行的移动设备操作系统。权限是安卓安全体系结构中非常重要的一部分。应用程序经常需要用户的权限，但其中许多应用程序只要求一次——当用户第一次使用应用程序时——然后保留并滥用给定的权限。渴望增强安卓权限安全和用户的私人数据保护是我们探索细粒度上下文敏感权限使用分析的方法背后的驱动因素，从而识别安卓应用程序中的滥用。在这项工作中，我们提出了一种方法，用于对用户交互的安卓应用程序的每个功能的细粒度权限使用进行分类。我们的方法名为DroidGem，主要依靠三个技术组件为Android应用程序对用户触发的每个功能的权限（错误）使用提供上下文中的分类：（1）静态过程间控制流图和调用图，表示应用程序中的每个功能，这些功能可能由用户或系统的事件通过UI链接的事件处理程序触发，（2）将图结构转换为数字编码的图嵌入技术，以及（3）基于嵌入对权限使用进行分类的监督机器学习模型。我们已经实现了DroidGem的原型，并在89个不同的应用程序上对其进行了评估。结果表明，DroidGem可以准确地分类由UI链接的事件处理程序触发的应用程序功能所使用的权限是否与手动验证的决策有关，准确率和召回率高达95%。我们相信，这样的权限分类机制有助于在与应用程序用户行为相关的上下文中提供细粒度的权限通知，并提高他们对安卓应用程序中权限和私人数据的（不当）使用的认识。,隐私保护，权限控制，安卓应用程序，控制流图，图形嵌入，分类,,,
2WDR4I95,2023,https://doi.org/10.1109/ASE56229.2023.00057,ASE 2023,An Industrial Practice for Securing Android Apps in the Banking Domain,"The emergence of mobile technology has significantly advanced the banking sector in terms of how consumers interact with their banks and manage their finances. The accessibility and ease of financial services have been improved by the switch from desktop banking to mobile banking. Mobile banking has a lot of advantages, but it also has security concerns. Illegal access to personal and financial information often occurs due to lapses in mobile security. In recent years, we have worked with banks from 10 countries and systematically analyzed 28 of their apps. We found several vulnerabilities in these apps by manual code reviews and by conducting 11 types of attacks. We then proposed and applied adequate security measures to protect these apps. Finally, we added these measures to our tool named AppProtect+ to effectively identify and thwart these threats. In this paper, we report our experience and practice of securing these Android apps.","Security,Privacy,Android apps,Banking apps,Industry practice",银行领域安卓应用程序安全的行业实践,移动技术的出现极大地推动了银行业在消费者与银行互动和财务管理方面的发展。从桌面银行向手机银行的转变提高了金融服务的可及性和便利性。手机银行有很多优点，但也有安全隐患。由于移动安全方面的失误，对个人和财务信息的非法访问经常发生。近年来，我们与来自10个国家的银行合作，系统分析了他们的28款应用程序。我们通过手动代码审查和进行11种类型的攻击，在这些应用程序中发现了几个漏洞。然后，我们提出并实施了足够的安全措施来保护这些应用程序。最后，我们将这些措施添加到名为AppProtect+的工具中，以有效识别和挫败这些威胁。在本文中，我们报告了我们保护这些安卓应用程序的经验和实践。,安全，隐私，安卓应用程序，银行应用程序，行业实践,,,
PSWYAR2D,2023,https://doi.org/10.1109/ASE56229.2023.00030,ASE 2023,Repeated Builds During Code Review: An Empirical Study of the OpenStack Community,"Code review is a popular practice where developers critique each others' changes. Since automated builds can identify low-level issues (e.g., syntactic errors, regression bugs), it is not uncommon for software organizations to incorporate automated builds in the code review process. In such code review deployment scenarios, submitted change sets must be approved for integration by both peer code reviewers and automated build bots. Since automated builds may produce an unreliable signal of the status of a change set (e.g., due to “flaky” or non-deterministic execution behaviour), code review tools, such as Gerrit, allow developers to request a “recheck”, which repeats the build process without updating the change set. We conjecture that an unconstrained recheck command will waste time and resources if it is not applied judiciously. To explore how the recheck command is applied in a practical setting, in this paper, we conduct an empirical study of 66,932 code reviews from the OpenStack community. We quantitatively analyze (i) how often build failures are rechecked; (ii) the extent to which invoking recheck changes build failure outcomes; and (iii) how much waste is generated by invoking recheck. We observe that (i) 55% of code reviews invoke the recheck command after a failing build is reported; (ii) invoking the recheck command only changes the outcome of a failing build in 42% of the cases; and (iii) invoking the recheck command increases review waiting time by an average of 2,200% and equates to 187.4 compute years of waste-enough compute resources to compete with the oldest land living animal on earth. Our observations indicate that the recheck command is frequently used after the builds fail, but does not achieve a high likelihood of build success. Based on a developer survey and our history-based quantitative findings, we encourage reviewer teams to think twice before rechecking and be considerate of waste. While recheck currently generates plenty of wasted computational resources and bloats waiting times, it also presents exciting future opportunities for researchers and tool builders to propose solutions that can reduce waste.","Code Review,Continuous Integration,Waste",代码审查过程中的重复构建：对OpenStack社区的实证研究,代码评审是一种流行的做法，开发人员会对彼此的更改进行批评。由于自动化构建可以识别低级问题（例如语法错误、回归错误），因此软件组织将自动化构建纳入代码审查过程并不罕见。在这种代码审查部署场景中，提交的更改集必须得到同行代码审查人员和自动构建机器人的批准才能进行集成。由于自动化构建可能会产生不可靠的变更集状态信号（例如，由于“不稳定”或不确定的执行行为），Gerrit等代码审查工具允许开发人员请求“重新检查”，即重复构建过程而不更新变更集。我们推测，如果不明智地应用无约束的复查命令，它将浪费时间和资源。为了探索recheck命令如何在实际环境中应用，在本文中，我们对来自OpenStack社区的66932条代码评审进行了实证研究。我们定量分析（i）重新检查构建失败的频率；（ii）调用重新检查更改构建失败结果的程度；以及（iii）通过调用重新检查产生了多少浪费。我们观察到（i）55%的代码审查在报告失败的构建后调用recheck命令；（ii）在42%的情况下，调用重新检查命令仅改变失败构建的结果；以及（iii）调用复查命令将复查等待时间平均增加2200%，相当于浪费了187.4计算年的计算资源，足以与地球上最古老的陆地动物竞争。我们的观察结果表明，在构建失败后经常使用recheck命令，但构建成功的可能性并不高。根据开发人员调查和我们基于历史的定量结果，我们鼓励审查团队在重新审查之前三思而后行，并考虑到浪费。虽然复查目前会产生大量浪费的计算资源和膨胀的等待时间，但它也为研究人员和工具构建者提供了令人兴奋的未来机会，可以提出减少浪费的解决方案。,代码审查，持续集成，浪费,,,
DA4MXFCY,2023,https://doi.org/10.1109/ASE56229.2023.00053,ASE 2023,A Comprehensive Study on Code Clones in Automated Driving Software,"With the continuous improvement of artificial intelligence technology, autonomous driving technology has been greatly developed. Hence automated driving software has drawn more and more attention from both researchers and practitioners. Code clone is a commonly used to speed up the development cycle in software development, but many studies have shown that code clones may affect software maintainability. Currently, there is little research investigating code clones in automated driving software. To bridge this gap, we conduct a comprehensive experience study on the code clones in automated driving software. Through the analysis of Apollo and Autoware, we have presented that code clones are prevalent in automated driving software. about 30% of code lines are involved in code clones and more than 50% of files contain code clones. Moreover, a notable portion of these code clones has caused bugs and co-modifications. Due to the high complexity of autonomous driving, the automated driving software is often designed to be modular, with each module responsible for a single task. When considering each module individually, we have found that Perception, Planning, Canbus, and Sensing modules are more likely to encounter code clones, and more likely to have bug-prone and co-modified clones. Finally, we have shown that there exist cross-module clones to propagate bugs and co-modifications in different modules, which undermine the software's modularity.","Automated Driving Software,Code Clone,Comodification,Bug-proneness,Software Modularity",自动驾驶软件中代码克隆的综合研究,随着人工智能技术的不断进步，自动驾驶技术得到了极大的发展。因此，自动驾驶软件越来越受到研究者和从业者的关注。代码克隆是软件开发中常用的加速开发周期的方法，但许多研究表明，代码克隆可能会影响软件的可维护性。目前，对自动驾驶软件中的代码克隆的研究很少。为了弥补这一差距，我们对自动驾驶软件中的代码克隆进行了全面的经验研究。通过对Apollo和Autoware的分析，我们发现代码克隆在自动驾驶软件中很普遍。大约30%的代码行涉及代码克隆，超过50%的文件包含代码克隆。此外，这些代码克隆中的一个显著部分导致了错误和共同修改。由于自动驾驶的高度复杂性，自动驾驶软件通常被设计成模块化的，每个模块负责一项任务。当单独考虑每个模块时，我们发现Perception、Planning、Canbus和Sensing模块更有可能遇到代码克隆，也更有可能出现易出错和共同修改的克隆。最后，我们已经证明，存在跨模块克隆来传播错误，并在不同的模块中进行共同修改，这会破坏软件的模块性。,自动驾驶软件，代码克隆，修改，漏洞倾向，软件模块化,,,
86IU98K8,2023,https://doi.org/10.1109/ASE56229.2023.00151,ASE 2023,Wemint:Tainting Sensitive Data Leaks in WeChat Mini-Programs,"Mini-programs (MiniApps), lightweight versions of full-featured mobile apps that run inside a host app such as WeChat, have become increasingly popular due to their simplified and convenient user experiences. However, MiniApps raise new security and privacy concerns as they can access partially or all of host apps' system resources, including sensitive personal data. While taint detection has been proven effective in addressing this kind of concerns, existing taint detection techniques for mobile apps cannot be directly applied to MiniApps. The main reason is that the key logics of MiniApps are usually written in J avaScript, and its intrinsic characteristics (function-level scope, dynamic types, synchronous programming, and code obfuscation) prevent existing taint detection techniques from precisely propagating the taints. To address this problem, we propose a novel taint detection technique, Wemint, that detects sensitive information leaks in MiniApps. Specifically, Wemint facilitates taint propagation via building a context-based model based on the operational prin-ciple of MiniApps and J avaScript, and addresses asynchronous function calls by modeling their callbacks explicitly in taint rules. In addition, due to the adoption of Abstract Syntax Trees (ASTs) for code representation during taint detection, Wemint exhibits better robustness against the commonly-applied code obfuscation. Our experimental results show that Wemint can effectively detect sensitive information leaks in WeChat MiniApps, as well as trace the path of sensitive data flows. By applying Wemint to over 20K suspicious MiniApps, we found that over 7.5K (36.5 %) of them have sensitive data leaks, and Wemint outperforms the state-of-the-art DoubleX based techniques in detecting these leaks.","WeChat Mini-programs,Taint detection,Secu-rity,Privacy",Wemint:污染微信小程序中的敏感数据泄露,迷你程序（MiniApps）是在微信等主机应用程序中运行的功能齐全的移动应用程序的轻量级版本，由于其简化和方便的用户体验而越来越受欢迎。然而，MiniApps提出了新的安全和隐私问题，因为它们可以访问主机应用程序的部分或全部系统资源，包括敏感的个人数据。虽然污点检测已被证明能有效解决这类问题，但现有的移动应用程序污点检测技术无法直接应用于MiniApps。主要原因是MiniApps的关键逻辑通常是用J avaScript编写的，其固有特性（功能级别范围、动态类型、同步编程和代码混淆）阻止了现有的污点检测技术准确地传播污点。为了解决这个问题，我们提出了一种新的污点检测技术Wemint，用于检测MiniApps中的敏感信息泄漏。具体而言，Wemint通过基于MiniApps和J avaScript的操作原理构建基于上下文的模型来促进污点传播，并通过在污点规则中显式地建模异步函数调用来解决异步函数调用问题。此外，由于在污点检测过程中采用了抽象语法树（AST）来表示代码，Wemint对常用的代码混淆表现出更好的鲁棒性。我们的实验结果表明，Wemint可以有效地检测微信应用程序中的敏感信息泄露，并跟踪敏感数据流的路径。通过将Wemint应用于超过2万个可疑的迷你应用程序，我们发现其中超过7.5万个（36.5%）存在敏感数据泄漏，并且Wemint在检测这些泄漏方面优于最先进的基于DoubleX的技术。,微信小程序，污点检测，安全，隐私,,,
RE8FJ7CU,2023,https://doi.org/10.1109/ASE56229.2023.00038,ASE 2023,DeepScaler: Holistic Autoscaling for Microservices Based on Spatiotemporal GNN with Adaptive Graph Learning,"Autoscaling functions provide the foundation for achieving elasticity in the modern cloud computing paradigm. It enables dynamic provisioning or de-provisioning resources for cloud software services and applications without human intervention to adapt to workload fluctuations. However, autoscaling microservice is challenging due to various factors. In particular, complex, time-varying service dependencies are difficult to quantify accurately and can lead to cascading effects when allocating resources. This paper presents DeepScaler, a deep learning-based holistic autoscaling approach for microservices that focus on coping with service dependencies to optimize service-level agreements (SLA) assurance and cost efficiency. DeepScaler employs (i) an expectation-maximization-based learning method to adaptively generate affinity matrices revealing service dependencies and (ii) an attention-based graph convolutional network to extract spatio-temporal features of microservices by aggregating neighbors' information of graph-structural data. Thus DeepScaler can capture more potential service dependencies and accurately estimate the resource requirements of all services under dynamic workloads. It allows DeepScaler to reconfigure the resources of the interacting services simultaneously in one resource provisioning operation, avoiding the cascading effect caused by service dependencies. Experimental results demonstrate that our method implements a more effective autoscaling mechanism for microservice that not only allocates resources accurately but also adapts to dependencies changes, significantly reducing SLA violations by an average of 41% at lower costs.","Cloud Computing,Microservice,QoS,Resource Management,Holistic Autoscaling,Graph Convolution,Container",DeepScaler：基于时空GNN和自适应图学习的微服务整体自动缩放,自动缩放功能为实现现代云计算范式中的弹性提供了基础。它实现了云软件服务和应用程序的动态资源调配或取消资源调配，而无需人工干预以适应工作负载波动。然而，由于各种因素，自动缩放微服务具有挑战性。特别是，复杂的、时变的服务依赖关系很难准确量化，并且在分配资源时可能导致级联效应。本文介绍了DeepScaler，这是一种基于深度学习的微服务整体自动缩放方法，专注于处理服务依赖性，以优化服务级别协议（SLA）保证和成本效率。DeepScaler采用（i）基于期望最大化的学习方法自适应生成揭示服务依赖性的亲和矩阵，以及（ii）基于注意力的图卷积网络，通过聚合图结构数据的邻居信息来提取微服务的时空特征。因此，DeepScaler可以捕获更多潜在的服务依赖关系，并准确估计动态工作负载下所有服务的资源需求。它允许DeepScaler在一次资源配置操作中同时重新配置交互服务的资源，避免了服务依赖性造成的级联效应。实验结果表明，我们的方法为微服务实现了一种更有效的自动缩放机制，不仅能准确地分配资源，还能适应依赖关系的变化，以更低的成本将SLA违规行为平均减少41%。,云计算，微服务，QoS，资源管理，整体自动缩放，图形卷积，容器,,,
RARGR53G,2023,https://doi.org/10.1109/ASE56229.2023.00184,ASE 2023,Improving Code Extraction from Coding Screencasts Using a Code-Aware Encoder-Decoder Model,"Accurate automatic code extraction from tutorial videos is crucial for software developers seeking to reuse the code contained in these videos. Current methods using optical character recognition (OCR) often yield inaccurate results due to code complexity and variations in screencast formats. To address this issue, we introduce CodeT5-OCRfix, an approach that leverages the pre-trained code-aware large language model CodeT5 to enhance code extraction accuracy by post-processing OCRed code. We first collect a large and diverse dataset of source code screenshots captured from more than 10K Java projects from GitHub. We then apply the most widely used OCR engine for the task of code extraction from videos, Tesseract, on these screenshots and collect the OCRed code along with the ground truth code extracted from the Java files. We built a training dataset of more than 585K pairs of OCRed and ground truth code pairs, which we then used to fine-tune CodeT5, obtaining our model CodeT5-OCRfix. An empirical evaluation on both screenshots and screencast frames shows that CodeT5-OCRfix outperforms baseline code extraction models and is also more time-efficient. Our approach therefore improves the state-of-the-art in code extraction techniques from screencasts and images.","code extraction,coding screencasts,code-aware model,optical character recognition",使用代码感知编码器-解码器模型改进从编码屏幕广播中提取代码,从教程视频中准确自动提取代码对于软件开发人员寻求重用这些视频中包含的代码至关重要。目前使用光学字符识别（OCR）的方法由于代码的复杂性和屏幕播放格式的变化而经常产生不准确的结果。为了解决这个问题，我们引入了CodeT5 OCRfix，这是一种利用预先训练的代码感知大型语言模型CodeT5的方法，通过对OCRed代码进行后处理来提高代码提取精度。我们首先从GitHub收集了一个庞大而多样的源代码截图数据集，这些截图来自于超过10K个Java项目。然后，我们在这些屏幕截图上应用最广泛使用的OCR引擎Tesseract从视频中提取代码，并收集OCRed代码以及从Java文件中提取的基本事实代码。我们建立了一个由585K对OCRed和地面实况码对组成的训练数据集，然后我们用它来微调CodeT5，获得我们的模型CodeT5 OCRfix。对屏幕截图和屏幕广播帧的经验评估表明，CodeT5 OCRfix优于基线代码提取模型，而且时间效率更高。因此，我们的方法改进了从屏幕广播和图像中提取代码的最先进技术。,代码提取，编码屏幕广播，代码感知模型，光学字符识别,,,
CRWXE9IC,2023,https://doi.org/10.1109/ASE56229.2023.00198,ASE 2023,Detecting Memory Errors in Python Native Code by Tracking Object Lifecycle with Reference Count,"Third-party Python modules are usually implemented as binary extensions by using native code (C/C++) to provide additional features and runtime acceleration. In native code, the heap-allocated PyObjects are managed by the reference counting mechanism provided in Python/C APIs for automatic reclaiming. Hence, improper refcount manipulations can lead to memory leaks and use-after-free problems, and cannot be detected by simply pairing the occurrence of source and sink points. To detect such problems, state-of-the-art approaches have made groundbreaking contributions to identifying inappropriate final refcount values before returning from native code to Python. However, not all problems can be exposed at the end of a path. To detect those hidden in the middle of a path in native code, it is also crucial to track the lifecycle state of PyObjects through the refcount and lifecycle operations in API calls. To achieve this goal, we propose the PyObject State Transition Model (PSTM) recording the lifecycle states and refcount values of PyObjects to describe the effects of Python/C API calls and pointer operations. We track state transitions of PyObjects with symbolic execution based on the model, and report problems when a statement triggers a transition to buggy states. The program state is also expanded to handle pointer nullity checks and smart pointers of PyObjects. We conduct experiments on 12 open-source projects and detect 259 real problems out of 280 reports, which is twice as many bugs as state-of-the-art approaches. We submit 168 real bugs to those active projects, and 106 issues are either confirmed or resolved.","Python Native Code,Static Analysis,Reference Counting,Memory Error",通过引用计数跟踪对象生命周期检测Python本机代码中的内存错误,第三方Python模块通常通过使用本机代码（C/C++）实现为二进制扩展，以提供额外的功能和运行时加速。在本机代码中，堆分配的PyObjects由Python/C API中提供的引用计数机制管理，用于自动回收。因此，不正确的refcount操作可能导致内存泄漏和释放后使用问题，并且不能通过简单地配对源点和汇点来检测。为了检测这些问题，最先进的方法在从本机代码返回Python之前，为识别不合适的最终refcount值做出了开创性的贡献。然而，并不是所有的问题都能在一条道路的尽头暴露出来。为了检测那些隐藏在本机代码中路径中间的对象，通过API调用中的refcount和生命周期操作来跟踪PyObject的生命周期状态也是至关重要的。为了实现这一目标，我们提出了PyObject状态转换模型（PSTM），该模型记录了PyObject的生命周期状态和refcount值，以描述Python/C API调用和指针操作的效果。我们使用基于模型的符号执行来跟踪PyObjects的状态转换，并在语句触发向错误状态的转换时报告问题。程序状态还扩展为处理指针无效性检查和PyObjects的智能指针。我们在12个开源项目上进行了实验，在280份报告中检测到259个真实问题，这是最先进方法的两倍。我们向那些活跃的项目提交了168个真正的错误，106个问题得到了确认或解决。,Python本机代码，静态分析，引用计数，内存错误,,,
6T6TB65X,2023,https://doi.org/10.1109/ASE56229.2023.00173,ASE 2023,SAT-Verifiable LTL Satisfiability Checking via Graph Representation Learning,"With the superior learning ability of neural networks, it is promising to obtain highly confident results for linear temporal logic (LTL) satisfiability checking in polynomial time. However, existing neural approaches are limited in inductive ability and in supporting with an arbitrary number of atomic propositions. Besides, there is no mechanism to verify the results for satisfiability checking. In this paper, we propose an approach to checking the satisfiability of an LTL formula and meanwhile generating a satisfiable trace if the LTL formula is satisfiable, where the satisfiable trace verifies the satisfiability result. The core contribution is a new graph representation for LTL formulae - one-step unfolded graph (OSUG) to incorporate the syntax and semantic features of LTL. Preliminary results show that our approach is superior to the state-of-the-art neural approaches on synthetic datasets and confirms the effectiveness of OSUG.","linear temporal logic,satisfiability checking,graph representation learning",基于图表示学习的SAT可验证LTL可满足性检查,凭借神经网络优越的学习能力，在多项式时间内获得线性时序逻辑（LTL）可满足性检验的高置信度结果是有希望的。然而，现有的神经方法在归纳能力和支持任意数量的原子命题方面受到限制。此外，没有机制来验证可满足性检查的结果。在本文中，我们提出了一种方法来检查LTL公式的可满足性，同时如果LTL公式是可满足的，则生成可满足的迹，其中可满足的迹线验证可满足性结果。核心贡献是LTL公式的一种新的图表示——一步展开图（OSUG），它融合了LTL的语法和语义特征。初步结果表明，在合成数据集上，我们的方法优于最先进的神经方法，并证实了OSUG的有效性。,线性时态逻辑，可满足性检验，图表示学习,,,
6D22BYZB,2023,https://doi.org/10.1109/ASE56229.2023.00195,ASE 2023,Enhancing Code Safety in Quantum Intermediate Representation,"Quantum Intermediate Representation (QIR) is an LLVM-based intermediate representation developed by Microsoft for quantum program compilers. QIR is designed to offer a universal solution for quantum program compilers, decoupled from both front-end languages and back-end hardware, thereby eliminating the need for redundant development of intermediate representations and compilers. However, the lack of a formal definition and reliance on natural language descriptions in the current state of QIR result in interpretational ambiguity and a dearth of rigor in implementing quantum functions. In this paper, we present formal definitions for QIR's data types and instruction sets to establish correctness and safety assurances for operations and intermediate code conversions within QIR. To demonstrate the effectiveness of our approach, we provide examples of unsafe QIR codes where errors can be identified with our method.","quantum programming language,Quantum Intermediate Representation,formal syntax and semantics",增强量子中介表示中的代码安全性,量子中间表示（QIR）是微软为量子程序编译器开发的一种基于LLVM的中间表示。QIR旨在为量子程序编译器提供通用解决方案，与前端语言和后端硬件解耦，从而消除对中间表示和编译器的冗余开发需求。然而，在目前的QIR状态下，缺乏正式的定义和对自然语言描述的依赖导致了解释的模糊性和实现量子函数的缺乏严谨性。在本文中，我们提出了QIR数据类型和指令集的形式化定义，以建立QIR中操作和中间代码转换的正确性和安全性保证。为了证明我们的方法的有效性，我们提供了不安全的QIR代码的例子，其中可以用我们的方法识别错误。,量子程序设计语言，量子中间表示，形式语法和语义,,,
TD46C4VU,2023,https://doi.org/10.1109/ASE56229.2023.00039,ASE 2023,Automatic Generation and Reuse of Precise Library Summaries for Object-Sensitive Pointer Analysis,"The extensive use of libraries in modern software impedes the scalability of pointer analysis. To address this issue, library summarization can be beneficial, but only if the resulting summary-based pointer analysis is faster without sacrificing much precision in the application code. However, currently, no library summarization approaches exist that meet this design objective. This paper presents a novel approach that solves this problem by using k-object-sensitive pointer analysis, k-obj, for Java. The approach involves applying k-obj, along with a set of summary-based inference rules, to generate a k-object-sensitive library summary. By replacing the program's library with this summary and applying k-obj, the efficiency of the program can be significantly improved while maintaining nearly the same or better precision in the application code. We validate our approach with an implementation in Soot and an evaluation using representative Java programs.","Java,Codes,Scalability,Libraries,Software,Software engineering",用于对象敏感指针分析的精确库摘要的自动生成和重用,现代软件中库的广泛使用阻碍了指针分析的可扩展性。为了解决这个问题，库摘要可能是有益的，但前提是生成的基于摘要的指针分析更快，而不会牺牲应用程序代码的精度。然而，目前还没有满足这一设计目标的图书馆摘要方法。本文提出了一种新的方法，通过使用Java中的k对象敏感指针分析k-obj来解决这个问题。该方法包括应用k-obj，以及一组基于摘要的推理规则，以生成k-对象敏感库摘要。通过用这个摘要替换程序库并应用k-obj，可以显著提高程序的效率，同时在应用程序代码中保持几乎相同或更好的精度。我们通过在Soot中的实现和使用代表性Java程序的评估来验证我们的方法。,Java，代码，可伸缩性，库，软件，软件工程,,,
N26DGILT,2023,https://doi.org/10.1109/ASE56229.2023.00159,ASE 2023,CodeGen4Libs: A Two-Stage Approach for Library-Oriented Code Generation,"Automated code generation has been extensively studied in recent literature. In this work, we first survey 66 participants to motivate a more pragmatic code generation scenario, i.e., library-oriented code generation, where the generated code should implement the functionally of the natural language query with the given library. We then revisit existing learning-based code generation techniques and find they have limited effectiveness in such a library-oriented code generation scenario. To address this limitation, we propose a novel library-oriented code generation technique, CodeGen4Libs, which incorporates two stages: import generation and code generation. The import generation stage generates import statements for the natural language query with the given third-party libraries, while the code generation stage generates concrete code based on the generated imports and the query. To evaluate the effectiveness of our approach, we conduct extensive experiments on a dataset of 403,780 data items. Our results demonstrate that CodeGen4Libs outperforms baseline models in both import generation and code generation stages, achieving improvements of up to 97.4% on EM (Exact Match), 54.5% on BLEU, and 53.5% on Hit@All. Overall, our proposed CodeGen4Libs approach shows promising results in generating high-quality code with specific third-party libraries, which can improve the efficiency and effectiveness of software development.","Code Generation,Third-party Library,Language Model",CodeGen4Libs：面向库的代码生成的两阶段方法,自动代码生成在最近的文献中得到了广泛的研究。在这项工作中，我们首先调查了66名参与者，以激发更实用的代码生成场景，即面向库的代码生成，其中生成的代码应该用给定的库实现自然语言查询的功能。然后，我们重新审视现有的基于学习的代码生成技术，发现它们在这种面向库的代码生成场景中的有效性有限。为了解决这一限制，我们提出了一种新的面向库的代码生成技术CodeGen4Libs，它包含两个阶段：导入生成和代码生成。导入生成阶段使用给定的第三方库为自然语言查询生成导入语句，而代码生成阶段则基于生成的导入和查询生成具体代码。为了评估我们方法的有效性，我们在403780个数据项的数据集上进行了广泛的实验。我们的结果表明，CodeGen4Libs在导入生成和代码生成阶段都优于基线模型，在EM（精确匹配）、BLEU和Hit@All.总体而言，我们提出的CodeGen4Libs方法在使用特定的第三方库生成高质量代码方面显示出了良好的效果，这可以提高软件开发的效率和有效性。,代码生成，第三方库，语言模型,,,
8EITXVVV,2023,https://doi.org/10.1109/ASE56229.2023.00100,ASE 2023,FLUX: Finding Bugs with LLVM IR Based Unit Test Crossovers,"Optimizing compilers are as ubiquitous as they are crucial to software development. However, bugs in compilers are not uncommon. Among the most serious are bugs in compiler optimizations, which can cause unexpected behavior in compiled binaries. Existing approaches for detecting such bugs have focused on end-to-end compiler fuzzing, which limits their ability for targeted exploration of a compiler's optimizations. This paper proposes FLUX (Finding bugs with LLVM IR based Unit test cross(X)overs), a fuzzer that is designed to generate test cases that stress compiler optimizations. Previous compiler fuzzers are overly constrained by having to construct well-formed inputs. FLUX sidesteps this constraint by using human-written unit test suites as a starting point, and then selecting random combinations of them to generate new tests. We hypothesize that tests generated this way will be able to explore new execution paths through compiler optimizations and find new bugs. Our evaluation of FLUX on LLVM indicates that it is able to increase path coverage over the baseline LLVM unit test suite and explores more edge coverage than previous work. Further, we demonstrate FLUX's ability to generate miscompiled and crash-producing IR on LLVM's optimizations. After a month of fuzzing, FLUX found 28 unique bugs in LLVM's active development branch. We have reported 11 of these bugs which led to 6 of them being patched by LLVM developers. 22 of these are crashes that are triggered by well-formed input programs, and 6 of these are miscompilation bugs that silently produced incorrect code.","compiler testing,fuzzing,miscompilation,compiler defect,crossover",FLUX：使用基于LLVM IR的单元测试交叉查找Bug,优化编译器无处不在，因为它们对软件开发至关重要。然而，编译器中的错误并不少见。其中最严重的是编译器优化中的错误，它可能会在编译的二进制文件中导致意外行为。现有的检测此类错误的方法侧重于端到端编译器模糊，这限制了它们有针对性地探索编译器优化的能力。本文提出了FLUX（Finding bugs with LLVM IR based Unit test cross（X）overs），这是一种模糊器，旨在生成强调编译器优化的测试用例。以前的编译器模糊器由于必须构造形式良好的输入而受到过度约束。FLUX通过使用人工编写的单元测试套件作为起点，然后选择它们的随机组合来生成新的测试，从而避开了这种限制。我们假设，以这种方式生成的测试将能够通过编译器优化来探索新的执行路径，并发现新的错误。我们对LLVM上FLUX的评估表明，它能够在基线LLVM单元测试套件上增加路径覆盖，并比以前的工作探索更多的边缘覆盖。此外，我们还展示了FLUX在LLVM优化上生成编译错误和崩溃的IR的能力。经过一个月的模糊处理，FLUX在LLVM的活跃开发分支中发现了28个独特的错误。我们已经报告了其中11个错误，导致LLVM开发人员修补了其中6个错误。其中22个是由格式良好的输入程序触发的崩溃，其中6个是无声地产生错误代码的错误编译错误。,编译器测试，模糊化，错误编译，编译器缺陷，交叉,,,
ZHIX87X8,2023,https://doi.org/10.1109/ASE56229.2023.00134,ASE 2023,CiD4HMOS: A Solution to HarmonyOS Compatibility Issues,"HarmonyOS is an operating system boasting a substantial global user base and provides multiple versions of its SDK. Various open-source applications continue to utilize older versions, leading to compatibility issues arising from system constraints. These prevalent issues can substantially affect the user experience. Although numerous solutions have been suggested for addressing compatibility issues in Android, the subject remains largely unexplored within the context of HarmonyOS. To bridge this gap, we investigate the evolution of APIs in HarmonyOS to pinpoint those potentially causing compatibility issues. Based on these insights, we implement CiD4HMOS, a tool designed to detect and categorize compatibility issues in HarmonyOS. We evaluate the feasibility of CiD4HMOS with open-source apps and subsequently apply it to commercially released apps, highlighting its effectiveness in accurately identifying HarmonyOS compatibility issues. The experimental results uncover that CiD4HMOS is effective in detecting compatibility issues in HarmonyOS apps, achieving an accuracy rate of 86.8 % in open-source apps. And, developers of commercially released apps have significantly endorsed our reports. Our research emphasizes the necessity of continuous exploration into compatibility issues within HarmonyOS, underlining the significant role tools like CiD4HMOS play in enhancing the overall user experience.","HarmonyOS,Framework Base,Compatibility Issue",CiD4HMOS:HarmonyOS兼容性问题的解决方案,HarmonyOS是一个拥有庞大全球用户群的操作系统，并提供多个版本的SDK。各种开源应用程序继续使用旧版本，导致系统限制导致兼容性问题。这些普遍存在的问题会严重影响用户体验。尽管已经提出了许多解决安卓兼容性问题的解决方案，但在HarmonyOS的背景下，这个主题在很大程度上仍未被探索。为了弥补这一差距，我们调查了HarmonyOS中API的演变，以找出可能导致兼容性问题的因素。基于这些见解，我们实现了CiD4HMOS，这是一个旨在检测和分类HarmonyOS中兼容性问题的工具。我们评估了CiD4HMOS与开源应用程序的可行性，并随后将其应用于商业发布的应用程序，强调了其在准确识别HarmonyOS兼容性问题方面的有效性。实验结果表明，CiD4HMOS在检测HarmonyOS应用程序中的兼容性问题方面是有效的，在开源应用程序中实现了86.8%的准确率。而且，商业发布的应用程序的开发人员对我们的报告表示了极大的支持。我们的研究强调了持续探索HarmonyOS兼容性问题的必要性，强调了CiD4HMOS等工具在增强整体用户体验方面发挥的重要作用。,HarmonyOS，框架基础，兼容性问题,,,
37AVI9F7,2023,https://doi.org/10.1109/ASE56229.2023.00147,ASE 2023,Fault Localization for Buggy Deep Learning Framework Conversions in Image Recognition,"When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs widely used for image recognition (MobileNetV2, ResNet101, and InceptionV3)converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 72%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose various strategies towards fault repair of the faults detected. We implement our technique on top of the Apache TVM deep learning compiler, and we test it by conducting a preliminary fault localization analysis for the conversion of InceptionV3 from TF to TFLite. Our approach detected a fault in a common DNN converter tool, which introduced precision errors in weights, reducing model accuracy. After our fault localization, we repaired the issue, reducing our conversion error to zero.","software engineering,software testing,fault localization,deep learning,image recognition,deep neural networks,deep learning framework conversions",图像识别中Buggy深度学习框架转换的故障定位,在部署深度神经网络（DNN）时，开发人员通常会将模型从一个深度学习框架转换到另一个（例如，TensorFlow到PyTorch）。然而，该过程容易出错，并可能影响目标模型的准确性。为了确定这种影响的程度，我们对在四个众所周知的深度学习框架（PyTorch、Keras、TensorFlow（TF）和TFLite）中转换的三个广泛用于图像识别的DNN（MobileNetV2、ResNet101和InceptionV3）进行了差分分析，并简要介绍了这三个DNN的差异分析，发现了许多模型崩溃和高达72%的输出标签差异。为了减少这种错误，我们提出了一种新的方法来定位和修复有缺陷的深度学习框架转换，重点是预训练的图像识别模型。我们的技术包括四个分析阶段：1）转换工具，2）模型参数，3）模型超参数，以及4）图形表示。此外，我们还提出了对检测到的故障进行故障修复的各种策略。我们在Apache TVM深度学习编译器的基础上实现了我们的技术，并通过对InceptionV3从TF到TFLite的转换进行初步的故障定位分析来测试它。我们的方法在一个常见的DNN转换器工具中检测到一个故障，该故障在权重中引入了精度误差，降低了模型精度。在故障定位之后，我们修复了问题，将转换错误降至零。,软件工程，软件测试，故障定位，深度学习，图像识别，深度神经网络，深度学习框架转换,,,
37XE3X6T,2023,https://doi.org/10.1109/ASE56229.2023.00074,ASE 2023,Enhancing Malware Detection for Android Apps: Detecting Fine-Granularity Malicious Components,"Existing Android malware detection systems primarily concentrate on detecting malware apps, leaving a gap in the research concerning the detection of malicious components in apps. In this work, we propose a novel approach to detect fine-granularity malicious components for Android apps and build a prototype called AMCDroid. For a given app, AMCDroid first models app behavior to a homogenous graph based on the call graph and code statements of the app. Then, the graph is converted to a statement tree sequence for malware detection through the AST-based Neural Network with Feature Mapping (ASTNNF) model. Finally, if the app is detected as malware, AMCDroid applies fine-granularity malicious component detection (MCD) algorithm which is based on many-objective genetic algorithm to the homogenous graph for detecting malicious component in the app adaptively. We evaluate AMCDroid on 95,134 samples. Compared with the other two state-of-the-art methods in malware detection, AMCDroid gets the highest performance on the test set with 0.9699 F1-Score, and shows better robustness in facing obfuscation. Moreover, AMCDroid is capable of detecting fine-granularity malicious components of (obfuscated) malware apps. Especially, its average F1-Score exceeds another state-of-the-art method by 50%.","Android,malware app,deep learning,many-objective genetic algorithm,malicious component",增强Android应用程序的恶意软件检测：检测细粒度恶意组件,现有的安卓恶意软件检测系统主要集中于检测恶意软件应用程序，在应用程序中恶意组件的检测研究方面存在空白。在这项工作中，我们提出了一种新的方法来检测Android应用程序的细粒度恶意组件，并构建了一个名为AMCDroid的原型。对于给定的应用程序，AMCDroid首先根据应用程序的调用图和代码语句将应用程序行为建模为同构图。然后，通过基于AST的具有特征映射的神经网络（ASTNNF）模型，将该图转换为用于恶意软件检测的语句树序列。最后，如果应用程序被检测为恶意软件，AMCDroid将基于多目标遗传算法的细粒度恶意组件检测（MCD）算法应用于同构图，以自适应地检测应用程序中的恶意组件。我们对95134个样本进行了AMCDroid评估。与其他两种最先进的恶意软件检测方法相比，AMCDroid在测试集上获得了最高的性能，F1得分为0.9699，并且在面对混淆时表现出更好的鲁棒性。此外，AMCDroid能够检测（模糊的）恶意软件应用程序的细粒度恶意组件。特别是，它的平均F1成绩超过了另一种最先进的方法50%。,安卓，恶意软件应用，深度学习，多目标遗传算法，恶意组件,,,
ANDUD9WV,2023,https://doi.org/10.1109/ASE56229.2023.00049,ASE 2023,PTDETECTOR: An Automated JavaScript Front-end Library Detector,"Identifying what front-end library runs on a web page is challenging. Although many mature detectors exist on the market, they suffer from false positives and the inability to detect libraries bundled by packers such as Webpack. Most importantly, the detection features they use are collected from developers' knowledge leading to an inefficient manual workflow and a large number of libraries that the existing detectors cannot detect. This paper introduces PTDETECTOR, which provides the first automated method for generating features and detecting libraries on web pages. We propose a novel data structure, the pTree, which we use as a detection feature. The pTree is well-suited for automation and addresses the limitations of existing detectors. We implement PTDETECTOR as a browser extension and test it on 200 top-traffic websites. Our experiments show that PTDETECTOR can identify packer-bundled libraries, and its detection results outperform existing tools.","JavaScript,Web,Library Detection,Front end",PTDETECTOR：一个自动化的JavaScript前端库检测器,识别在网页上运行的前端库是一项挑战。尽管市场上存在许多成熟的检测器，但它们存在误报，并且无法检测由打包器（如Webpack）捆绑的库。最重要的是，他们使用的检测功能是从开发人员的知识中收集的，这导致了低效的手动工作流程和大量现有检测器无法检测到的库。本文介绍了PTDETECTOR，它提供了第一种自动生成特征和检测网页库的方法。我们提出了一种新的数据结构，pTree，我们将其用作检测特征。pTree非常适合自动化，并解决了现有检测器的局限性。我们实现了PTDETECTOR作为浏览器扩展，并在200个顶级流量网站上进行了测试。我们的实验表明，PTDETECTOR可以识别打包器捆绑库，其检测结果优于现有工具。,JavaScript，Web，库检测，前端,,,
VFMH97SU,2023,https://doi.org/10.1109/ASE56229.2023.00125,ASE 2023,An Empirical Study of Parameter-Efficient Fine-Tuning Methods for Pre-Trained Code Models,"Pre-trained code models (e.g. CodeBERT and CodeT5) have demonstrated their code intelligence in various software engineering tasks, such as code summarization. And full fine-tuning has become the typical approach to adapting these models to downstream tasks. However, full fine-tuning these large models can be computationally expensive and memory-intensive, particularly when training for multiple tasks. To alleviate this issue, several parameter-efficient fine-tuning methods (e.g. Adapter and LoRA) have been proposed to only train a small number of additional parameters, while keeping the original pre-trained parameters frozen. Although these methods claim superiority over the prior techniques, they seldom make a comprehensive and fair comparison on multiple software engineering tasks. Moreover, besides their potential in reducing fine-tuning costs and maintaining approximate performance, the effectiveness of these methods in low-resource, cross-language, and cross-project scenarios is inadequately studied. To this end, we first conduct experiments by fine-tuning state-of-the-art code models with these methods on both code understanding tasks and code generation tasks. The results show that, by tuning only 0.5% additional parameters, these methods may achieve comparable or higher performance than full fine-tuning in code understanding tasks, but they may exhibit slightly weaker performance in code generation tasks. We also investigate the impact of these methods with varying numbers of training samples and find that, a considerable number of samples (e.g. 1000 for clone detection) may be required for them to approximate the performance of full fine-tuning. Our experimental results in cross-language and cross-project scenarios demonstrate that by freezing most pre-trained parameters and tuning only 0.5% additional parameters, these methods achieve consistent improvements in models' transfer learning ability in comparison to full fine-tuning. Our code and data are available at https://github.com/anonymous-ase23/ CodeModelParameterEfficientFinetuning.","pre-trained code models,fine-tuning,parameter-efficient,transfer learning",预训练代码模型参数有效微调方法的实证研究,经过预训练的代码模型（例如CodeBERT和CodeT5）已经在各种软件工程任务中展示了它们的代码智能，例如代码摘要。充分的微调已经成为使这些模型适应下游任务的典型方法。然而，对这些大型模型进行全面微调可能会耗费大量计算和内存，尤其是在针对多个任务进行训练时。为了缓解这个问题，已经提出了几种参数有效的微调方法（例如，Adapter和LoRA），以仅训练少量的附加参数，同时保持原始的预训练参数冻结。尽管这些方法声称优于现有技术，但它们很少对多个软件工程任务进行全面、公平的比较。此外，除了在降低微调成本和保持近似性能方面的潜力外，这些方法在低资源、跨语言和跨项目场景中的有效性还没有得到充分的研究。为此，我们首先在代码理解任务和代码生成任务中使用这些方法对最先进的代码模型进行微调，从而进行实验。结果表明，通过仅调整0.5%的附加参数，这些方法在代码理解任务中可以实现与完全微调相当或更高的性能，但在代码生成任务中可能表现出略弱的性能。我们还研究了这些方法对不同数量的训练样本的影响，发现它们可能需要相当多的样本（例如1000个用于克隆检测）才能接近完全微调的性能。我们在跨语言和跨项目场景中的实验结果表明，通过冻结大多数预先训练的参数，只调整0.5%的额外参数，与完全微调相比，这些方法在模型的迁移学习能力方面实现了一致的改进。我们的代码和数据可在https://github.com/anonymous-ase23/CodeModelParameterEfficient微调。,预先训练的代码模型，微调，参数高效，迁移学习,,,
PF5NWT9E,2023,https://doi.org/10.1109/ASE56229.2023.00098,ASE 2023,VALAR: Streamlining Alarm Ranking in Static Analysis with Value-Flow Assisted Active Learning,"Static analyzers play a critical role in program defects and security vulnerabilities detection. Despite their importance, the widespread adoption of static analysis techniques in industrial development faces numerous obstacles, among which the high rate of false alarms constitutes a significant one. To address this issue, we propose a novel approach called Valar, which performs alarm ranking for advanced value-flow analysis using the active learning technique. Active learning algorithms minimize the manual effort for alarm inspection by maximizing the effect of each user labeling in recognizing true/false alarms. Meanwhile, the value-flows provide Valar with a concise and comprehensive summary of the operational semantics about programs. Based on this, Valar is able to reason about the potential correlations between alarms and prioritize the most profitable unlabeled alarm. Additionally, the accuracy of Valar increases as more user labels are given and Valar's active learning model is further refined. We evaluate Valar on 20 real-world C/C++ programs using three value-flow based checkers. Our experimental results demonstrated that Valar significantly lowers the priorities of false alarms with most true alarms ranked high. Notably, Valar ranked all true alarms in the top 47% in 90% projects and ranked 90% true alarms in the top 22% in 75% projects. Furthermore, Valar has no requirement for pretraining and has a negligible computation time of less than 0.1s for each alarm prioritization.","Static analysis,alarm ranking,active learning",VALAR:利用价值流辅助的主动学习简化静态分析中的警报排序,静态分析器在程序缺陷和安全漏洞检测中发挥着关键作用。尽管静态分析技术很重要，但在工业发展中广泛采用静态分析技术面临着许多障碍，其中高误报率是一个重要障碍。为了解决这个问题，我们提出了一种称为Valar的新方法，该方法使用主动学习技术为高级价值流分析执行警报排序。主动学习算法通过最大限度地提高每个用户标记在识别真/假警报方面的效果，最大限度地减少了警报检查的手动工作量。同时，价值流为Valar提供了关于程序的操作语义的简明而全面的总结。基于此，Valar能够推断警报之间的潜在相关性，并优先考虑最有利可图的未标记警报。此外，随着给出更多的用户标签，以及Valar的主动学习模型得到进一步完善，Valar的准确性也随之提高。我们使用基于三个值流的检查器在20个真实世界的C/C++程序上评估Valar。我们的实验结果表明，Valar显著降低了假警报的优先级，大多数真实警报排名靠前。值得注意的是，Valar在90%的项目中将所有真实警报排在前47%，在75%的项目中将90%的真实警报列在前22%。此外，Valar不需要预训练，并且每个警报优先级的计算时间小于0.1s，可以忽略不计。,静态分析，报警排名，主动学习,,,
N39AYIAY,2023,https://doi.org/10.1109/ASE56229.2023.00132,ASE 2023,Automated Software Entity Matching Between Successive Versions,"Version control systems are widely used to manage the evolution of software applications. However, such version control systems take source code as lines of plain text, and thus they cannot present the evolution of software entities embedded in the source code. To this end, a few approaches have been proposed to match software entities before and after a given commit, known as software entity matching algorithms. However, the accuracy of such algorithms requires further improvement. In this paper, we propose an automated iterative algorithm (called ReMapper) to match software entities between two successive versions. The key insight of ReMapper is that the qualified name, the implementation, and the references of a software entity together can distinguish it from others. It matches software entities iteratively because the mapping depends on the reference-based similarity whereas the reference-based similarity depends on the mapping of entities as well. We evaluated ReMapper on a benchmark consisting of 215 commits from 21 real-world projects. Our evaluation results suggest that ReMapper substantially outperformed the state of the art, reducing the number of mistakes (false positives plus false negatives) substantially by 85.8%. We also evaluated to what extent it may improve the automated refactoring discovery (mining) that relies heavily on automated entity matching. Our evaluation results suggest that it substantially improved the state of the art in refactoring discovery, improving recall by 6.9% and reducing the number of false positives by 72.6%.","Entity Matching,Software Evolution,Software Refactoring,Entity Tracking",连续版本之间的自动化软件实体匹配,版本控制系统被广泛用于管理软件应用程序的演变。然而，这样的版本控制系统将源代码视为纯文本行，因此它们不能呈现嵌入源代码中的软件实体的演变。为此，已经提出了一些方法来在给定提交之前和之后匹配软件实体，称为软件实体匹配算法。然而，这种算法的准确性需要进一步提高。在本文中，我们提出了一种自动迭代算法（称为ReMapper）来匹配两个连续版本之间的软件实体。ReMapper的关键见解是，软件实体的限定名称、实现和引用加在一起可以将其与其他实体区分开来。它迭代地匹配软件实体，因为映射取决于基于引用的相似性，而基于引用的类似性也取决于实体的映射。我们在一个由21个真实世界项目的215个提交组成的基准上评估了ReMapper。我们的评估结果表明，ReMapper的性能大大优于现有技术，大大减少了85.8%的错误数量（假阳性加假阴性）。我们还评估了它在多大程度上可以改善严重依赖自动实体匹配的自动重构发现（挖掘）。我们的评估结果表明，它大大提高了重构发现的技术水平，召回率提高了6.9%，误报率减少了72.6%。,实体匹配，软件进化，软件重构，实体跟踪,,,
PYD3PM4Y,2023,https://doi.org/10.1109/ASE56229.2023.00107,ASE 2023,ASTER: Automatic Speech Recognition System Accessibility Testing for Stutterers,"The popularity of automatic speech recognition (ASR) systems nowadays leads to an increasing need for improving their accessibility. Handling stuttering speech is an important feature for accessible ASR systems. To improve the accessibility of ASR systems for stutterers, we need to expose and analyze the failures of ASR systems on stuttering speech. The speech datasets recorded from stutterers are not diverse enough to expose most of the failures. Furthermore, these datasets lack ground truth information about the non-stuttered text, rendering them unsuitable as comprehensive test suites. Therefore, a methodology for generating stuttering speech as test inputs to test and analyze the performance of ASR systems is needed. However, generating valid test inputs in this scenario is challenging. The reason is that although the generated test inputs should mimic how stutterers speak, they should also be diverse enough to trigger more failures. To address the challenge, we propose Aster, a technique for automatically testing the accessibility of ASR systems. Aster can generate valid test cases by injecting five different types of stuttering. The generated test cases can both simulate realistic stuttering speech and expose failures in ASR systems. Moreover, Aster can further enhance the quality of the test cases with a multi-objective optimization-based seed updating algorithm. We implemented Aster as a framework and evaluated it on four open-source ASR models and three commercial ASR systems. We conduct a comprehensive evaluation of Aster and find that it significantly increases the word error rate, match error rate, and word information loss in the evaluated ASR systems. Additionally, our user study demonstrates that the generated stuttering audio is indistinguishable from real-world stuttering audio clips.","Automatic Speech Recognition,Accessibility Testing",ASTER：口吃者自动语音识别系统可达性测试,如今，自动语音识别（ASR）系统的普及导致了对提高其可访问性的日益需求。处理结结巴巴的语音是可访问ASR系统的一个重要功能。为了提高ASR系统对口吃者的可访问性，我们需要揭露和分析ASR系统在口吃语音上的故障。从口吃者身上记录的语音数据集不够多样化，无法暴露大多数失败。此外，这些数据集缺乏关于非结结巴巴文本的基本事实信息，使它们不适合作为全面的测试套件。因此，需要一种生成口吃语音作为测试输入的方法来测试和分析ASR系统的性能。然而，在这种情况下生成有效的测试输入是具有挑战性的。原因是，尽管生成的测试输入应该模仿口吃者的说话方式，但它们也应该足够多样化，足以引发更多的失败。为了应对这一挑战，我们提出了Aster，这是一种自动测试ASR系统可访问性的技术。Aster可以通过注入五种不同类型的口吃来生成有效的测试用例。生成的测试用例既可以模拟真实的口吃语音，也可以暴露ASR系统中的故障。此外，Aster可以通过基于多目标优化的种子更新算法进一步提高测试用例的质量。我们实现了Aster作为一个框架，并在四个开源ASR模型和三个商业ASR系统上对其进行了评估。我们对Aster进行了全面评估，发现它显著提高了所评估ASR系统中的单词错误率、匹配错误率和单词信息损失。此外，我们的用户研究表明，生成的口吃音频与现实世界中的口吃音频片段无法区分。,自动语音识别，可访问性测试,,,
WT4C3G52,2023,https://doi.org/10.1109/ASE56229.2023.00051,ASE 2023,VD-Guard: DMA Guided Fuzzing for Hypervisor Virtual Device,"Virtualization has been widely used in various scenarios, such as cloud computing. As its core technology, virtualization hypervisor brings up the efficiency of sharing the physical machine's resources via virtual devices. However, virtualization hypervisor also introduces significant security risks due to defective design or implementation schemes on virtual devices. Although several methods have been proposed to detect vulnerabilities in virtual devices, they still cannot effectively discover them because of missing critical information related to the MMIO/PIO and DMA operations to guide their dynamic methods. In this paper, we propose a hybrid method, VD-GUARD, to detect vulnerabilities in virtual devices. Specifically, it first leverages static control flow analysis to track call traces from various data entry points of virtual devices (MMIO/PIO functions) to the critical dispatcher points (DMA functions), and generate seeds that can trigger this call trace via static analysis and limited fuzzing test. And then, it takes these seeds as input and leverages DMA guided fuzzing to discover bugs. To verify the effectiveness of Vd-guard, we build a dataset, including 10 bugs in QEMU, based on previous works, and Vd-guardoutperforms the state-of-the-art hypervisor fuzzer Morphuzz. Vd-guardalso has found 4 new vulnerabilities in QEMU and VirtualBox, all of which have been confirmed and fixed (have been assigned 3 CVE IDs).","virtual device,fuzzing,DMA",虚拟机监控程序虚拟设备的VD Guard:DMA引导引信,虚拟化已经广泛应用于各种场景中，例如云计算。虚拟化管理程序作为其核心技术，提高了通过虚拟设备共享物理机资源的效率。然而，由于虚拟设备上的设计或实现方案存在缺陷，虚拟化管理程序也会带来重大的安全风险。尽管已经提出了几种方法来检测虚拟设备中的漏洞，但由于缺少与MMIO/PIO和DMA操作相关的关键信息来指导其动态方法，它们仍然无法有效地发现漏洞。在本文中，我们提出了一种混合方法VD-GUARD来检测虚拟设备中的漏洞。具体而言，它首先利用静态控制流分析来跟踪从虚拟设备的各种数据入口点（MMIO/PIO函数）到关键调度器点（DMA函数）的调用跟踪，并通过静态分析和有限模糊测试生成可以触发此调用跟踪的种子。然后，它将这些种子作为输入，并利用DMA引导的模糊处理来发现错误。为了验证Vd-guard的有效性，我们在先前工作的基础上构建了一个数据集，其中包括QEMU中的10个bug，Vd-guardout执行了最先进的hypervisor fuzzer Morphuzz。Vd guardalso在QEMU和VirtualBox中发现了4个新漏洞，所有这些漏洞都已得到确认和修复（已分配3个CVE ID）。,虚拟设备，模糊，DMA,,,
BUVJ2I96,2023,https://doi.org/10.1109/ASE56229.2023.00013,ASE 2023,NRAgo: Solving SMT(NRA) Formulas with Gradient-Based Optimization,"The satisfiability problem modulo the nonlinear real arithmetic (NRA) theory serves as the foundation for a wide range of important applications, such as model checking, program analysis, and software testing. However, due to the high computational complexity, developing efficient solving algorithms for this problem has consistently presented a substantial challenge. We present a hybrid SMT(NRA) solver, called NRAgo, which combines the efficiency of gradient-based optimization method with the completeness of algebraic solving algorithm. With our approach, the practical performance on many satisfiable instances is substantially improved. The experimental evaluation shows that NRAgo achieves remarkable acceleration effects on a set of challenging SMT(NRA) benchmarks that are hard to solve for state-of-the-art SMT solvers.","satisfiability modulo theories,nonlinear real arithmetic,gradient-based optimization",NRAgo:SMT（NRA）公式的梯度优化求解,模非线性实数运算（NRA）理论的可满足性问题是一系列重要应用的基础，如模型检查、程序分析和软件测试。然而，由于计算复杂度高，开发高效的求解算法一直是一个巨大的挑战。我们提出了一种混合SMT（NRA）求解器，称为NRAgo，它将基于梯度的优化方法的效率与代数求解算法的完整性相结合。使用我们的方法，在许多可满足的实例上的实际性能得到了显著提高。实验评估表明，NRAgo在一组具有挑战性的SMT（NRA）基准上实现了显著的加速效果，这些基准对于最先进的SMT求解器来说很难解决。,可满足性模理论，非线性实数算法，基于梯度的优化,,,
M5LZZVEG,2023,https://doi.org/10.1109/ASE56229.2023.00077,ASE 2023,Prism: Revealing Hidden Functional Clusters from Massive Instances in Cloud Systems,"Ensuring the reliability of cloud systems is critical for both cloud vendors and customers. Cloud systems often rely on virtualization techniques to create instances of hardware resources, such as virtual machines. However, virtualization hinders the observability of cloud systems, making it challenging to diagnose platform-level issues. To improve system observability, we propose to infer functional clusters of instances, i.e., groups of instances having similar functionalities. We first conduct a pilot study on a large-scale cloud system, i.e., Huawei Cloud, demonstrating that instances having similar functionalities share similar communication and resource usage patterns. Motivated by these findings, we formulate the identification of functional clusters as a clustering problem and propose a non-intrusive solution called Prism. Prism adopts a coarse-to-fine clustering strategy. It first partitions instances into coarse-grained chunks based on communication patterns. Within each chunk, Prism further groups instances with similar resource usage patterns to produce fine-grained functional clusters. Such a design reduces noises in the data and allows Prism to process massive instances efficiently. We evaluate Prism on two datasets collected from the real-world production environment of Huawei Cloud. Our experiments show that Prism achieves a v-measure of ∼0.95, surpassing existing state-of-the-art solutions. Additionally, we illustrate the integration of Prism within monitoring systems for enhanced cloud reliability through two real-world use cases.","functional clusters,cloud observability,instances,cloud systems,software reliability",棱镜：从云系统中的大量实例中揭示隐藏的功能集群,确保云系统的可靠性对云供应商和客户都至关重要。云系统通常依赖虚拟化技术来创建硬件资源的实例，例如虚拟机。然而，虚拟化阻碍了云系统的可观察性，这使得诊断平台级问题具有挑战性。为了提高系统的可观察性，我们建议推断实例的功能簇，即具有相似功能的实例组。我们首先对一个大型云系统，即华为云进行了试点研究，证明具有类似功能的实例共享类似的通信和资源使用模式。受这些发现的启发，我们将功能聚类的识别公式化为聚类问题，并提出了一种称为Prism的非侵入性解决方案。Prism采用了从粗到细的聚类策略。它首先根据通信模式将实例划分为粗粒度的块。在每个区块中，Prism进一步将具有类似资源使用模式的实例分组，以生成细粒度的功能集群。这样的设计减少了数据中的噪声，并允许Prism有效地处理大量实例。我们在华为云真实生产环境中收集的两个数据集上对Prism进行了评估。我们的实验表明，Prism的v测量值为～0.95，超过了现有的最先进的解决方案。此外，我们通过两个真实世界的用例说明了Prism在监控系统中的集成，以增强云可靠性。,功能集群，云可观察性，实例，云系统，软件可靠性,,,
TQYYATVF,2023,https://doi.org/10.1109/ASE56229.2023.00017,ASE 2023,AutoDebloater: Automated Android App Debloating,"Android applications are getting bigger with an increasing number of features. However, not all the features are needed by a specific user. The unnecessary features can increase the attack surface and cost additional resources (e.g., storage and memory). Therefore, it is important to remove unnecessary features from Android applications. However, it is difficult for the end users to fully explore the apps to identify the unnecessary features, and there is no off-the-shelf tool available to assist users to debloat the apps by themselves. In this work, we propose AutoDebloater to debloat Android applications automatically for end users. AutoDebloater is a web application that can be accessed by end-users through a web browser. In particular, AutoDebloater can automatically explore an app and identify the transitions between activities. Then, AutoDebloater will present the Activity Transition Graph to users and ask them to select the activities they do not want to keep. Finally, AutoDebloater will remove the activities that are selected by users from the app. We conducted a user study on five Android apps downloaded from three categories (i.e., Finance, Tools, and Navigation) in Google Play and F-Droid. The results show that users are satisfied with AutoDebloater in terms of the stability of the debloated apps and the ability of AutoDebloater to identify features that are never noticed before. The tool is available at http://autodebloater.club. The code is available at https://github.com/jiakun-liu/autodebloater/ and the demonstration video can be found at https://youtu.be/Gmz0-p2n9D4.","Android,Software Debloating",AutoDebloater：自动安卓应用程序去浮动,Android应用程序越来越大，功能越来越多。然而，并不是所有的功能都是特定用户所需要的。不必要的功能可能会增加攻击面，并花费额外的资源（如存储和内存）。因此，从Android应用程序中删除不必要的功能是很重要的。然而，最终用户很难完全探索应用程序以识别不必要的功能，而且没有现成的工具可以帮助用户自行清除应用程序。在这项工作中，我们提出了AutoDebloater，以自动为最终用户卸载Android应用程序。AutoDebloater是一种网络应用程序，最终用户可以通过网络浏览器访问它。特别是，AutoDebloater可以自动探索应用程序并识别活动之间的转换。然后，AutoDebloater将向用户展示活动转换图，并要求他们选择不想保留的活动。最后，AutoDebloater将从应用程序中删除用户选择的活动。我们对Google Play和F-Droid中从三个类别（即金融、工具和导航）下载的五款Android应用程序进行了用户研究。结果表明，用户对AutoDebloater感到满意，因为它具有去浮动应用程序的稳定性，并且能够识别以前从未注意到的功能。该工具位于http://autodebloater.club.代码可在https://github.com/jiakun-liu/autodebloater/演示视频可在https://youtu.be/Gmz0-p2n9D4.,Android，软件去浮动,,,
N6MYTG89,2023,https://doi.org/10.1109/ASE56229.2023.00113,ASE 2023,ReuNify: A Step Towards Whole Program Analysis for React Native Android Apps,"React Native is a widely-used open-source frame-work that facilitates the development of cross-platform mobile apps. The framework enables JavaScript code to interact with native-side code, such as Objective-C/Swift for iOS and Java/Kotlin for Android, via a communication mechanism provided by React Native. However, previous research and tools have overlooked this mechanism, resulting in incomplete analysis of React Native app code. To address this limitation, we have developed REUNIFY, a prototype tool that integrates the JavaScript and native-side code of React Native apps into an intermediate language that can be processed by the Soot static analysis framework. By doing so, REUNIFY enables the generation of a comprehensive model of the app's behavior. Our evaluation indicates that, by leveraging REUNIFY, the Soot-based framework can improve its coverage of static analysis for the 1,007 most popular React Native Android apps, augmenting the number of lines of Jimple code by 70%. Additionally, we observed an average increase of 84% in new nodes reached in the callgraph for these apps, after integrating REUNIFY. When REUNIFY is used for taint flow analysis, an average of two additional privacy leaks were identified. Overall, our results demonstrate that REUNIFY significantly enhances the Soot-based framework's capability to analyze React Native Android apps.","react native,mobile apps,static analysis",ReuNify：React原生Android应用程序全程序分析的一步,React Native是一个广泛使用的开源框架，有助于跨平台移动应用程序的开发。该框架使JavaScript代码能够通过React native提供的通信机制与原生端代码交互，如iOS版的Objective-C/Swift和Android版的Java/Kotlin。然而，以前的研究和工具忽略了这一机制，导致对React Native应用程序代码的分析不完整。为了解决这一限制，我们开发了REUNIFY，这是一个原型工具，它将React native应用程序的JavaScript和本机端代码集成到一种可以由Soot静态分析框架处理的中间语言中。通过这样做，REUNIFY能够生成应用程序行为的综合模型。我们的评估表明，通过利用REUNIFY，基于Soot的框架可以提高1007个最受欢迎的React Native Android应用程序的静态分析覆盖率，将Jimple代码的行数增加70%。此外，我们观察到，在集成REUNIFY后，这些应用程序的调用图中达到的新节点平均增加了84%。当REUNIFY用于污染流分析时，平均发现另外两个隐私泄露。总体而言，我们的结果表明，REUNIFY显著增强了基于Soot的框架分析React Native Android应用程序的能力。,react原生，移动应用程序，静态分析,,,
8LN8BM3M,2023,https://doi.org/10.1109/ASE56229.2023.00048,ASE 2023,Automated Fixing of Web UI Tests via Iterative Element Matching,"Web UI test cases are used for the automatic testing of web applications. When a web application is updated, these UI tests should also be updated for regression testing of the new version of web application. With the rapid evolution, updating UI tests is a tedious and time-consuming task. To solve these problems, automatically repairing web UI tests has gained increasing attention recently. To repair web UI tests, the most important step is to match the UI elements before and after the web page update. Existing work matches UI elements according to visual information, attributes value, or Document Object Model (DOM) structures. However, they either achieve low element matching accuracy or only work on simple UI tests. To solve these problems, we proposed UITestFix, an approach based on a novel iterative matching algorithm for improving the accuracy of matching UI elements. UITestFix is designed based on two main insights: (1) beyond attribute and DOM structures, the relations between different elements can also guide the matching process, and (2) the matching results of previous iterations could guide the matching of the current iteration. Our evaluation of publicly available datasets and two industrial apps shows that UITestFix outperforms four existing approaches by achieving more accurate element matching and producing more correct fixes.","Iterative Element Matching,Web Testing,Test Repair",通过迭代元素匹配自动修复Web UI测试,Web UI测试用例用于Web应用程序的自动测试。更新web应用程序时，还应更新这些UI测试，以便对新版本的web应用程序进行回归测试。随着快速发展，更新UI测试是一项乏味而耗时的任务。为了解决这些问题，自动修复web UI测试近年来越来越受到关注。要修复web UI测试，最重要的步骤是在网页更新前后匹配UI元素。现有工作根据视觉信息、属性值或文档对象模型（DOM）结构匹配UI元素。然而，它们要么实现了较低的元素匹配精度，要么只适用于简单的UI测试。为了解决这些问题，我们提出了UITestFix，这是一种基于新的迭代匹配算法的方法，用于提高UI元素匹配的准确性。UITestFix的设计基于两个主要见解：（1）除了属性和DOM结构之外，不同元素之间的关系也可以指导匹配过程；（2）以前迭代的匹配结果可以指导当前迭代的匹配。我们对公开可用的数据集和两个工业应用程序的评估表明，UITestFix通过实现更准确的元素匹配和生成更正确的修复，优于四种现有方法。,迭代元素匹配，Web测试，测试修复,,,
PWX5AQ5C,2023,https://doi.org/10.1109/ASE56229.2023.00085,ASE 2023,A Needle is an Outlier in a Haystack: Hunting Malicious PyPI Packages with Code Clustering,"As the most popular Python software repository, PyPI has become an indispensable part of the Python ecosystem. Regrettably, the open nature of PyPI exposes end-users to substantial security risks stemming from malicious packages. Consequently, the timely and effective identification of malware within the vast number of newly-uploaded PyPI packages has emerged as a pressing concern. Existing detection methods are dependent on difficult-to-obtain explicit knowledge, such as taint sources, sinks, and malicious code patterns, rendering them susceptible to overlooking emergent malicious packages. In this paper, we present a lightweight and effective method, namely MPHunter, to detect malicious packages without requiring any explicit prior knowledge. MPHunter is founded upon two fundamental and insightful observations. First, malicious packages are considerably rarer than benign ones, and second, the functionality of installation scripts for malicious packages diverges significantly from those of benign packages, with the latter frequently forming clusters. Consequently, MPHunter utilizes clustering techniques to group the installation scripts of PyPI packages and identifies outliers. Subsequently, MPHunter ranks the outliers according to their outlierness and the distance between them and known malicious instances, thereby effectively highlighting potential evil packages. With MPHunter, we successfully identified 60 previously unknown malicious packages from a pool of 31,329 newly-uploaded packages over a two-month period. All of them have been confirmed by the PyPI official. Moreover, a manual analysis shows that MPHunter recognizes all potentially malicious installation scripts with a recall of 100% across all analyzed packages. We assert that MPHunter offers a valuable and advantageous supplement to existing detection techniques, augmenting the arsenal of software supply chain security analysis.","PyPI,malicious package detection,code clustering",Needle是Haystack中的异类：用代码聚类搜索恶意PyPI包,作为最流行的Python软件库，PyPI已成为Python生态系统中不可或缺的一部分。令人遗憾的是，PyPI的开放性使最终用户面临恶意软件包带来的巨大安全风险。因此，在大量新上传的PyPI包中及时有效地识别恶意软件已成为一个紧迫的问题。现有的检测方法依赖于难以获得的明确知识，如污染源、汇点和恶意代码模式，这使得它们很容易忽略出现的恶意包。在本文中，我们提出了一种轻量级且有效的方法，即MPHunter，来检测恶意软件包，而不需要任何明确的先验知识。MPHunter是建立在两个基本的和有见地的观察。首先，恶意包比良性包少得多，其次，恶意包的安装脚本的功能与良性包的功能明显不同，后者经常形成集群。因此，MPHunter利用集群技术对PyPI包的安装脚本进行分组，并识别异常值。随后，MPHunter根据异常值的寿命以及它们与已知恶意实例之间的距离对其进行排名，从而有效地突出潜在的恶意包。使用MPHunter，我们在两个月的时间里成功地从31329个新上传的包中识别了60个以前未知的恶意包。所有这些都得到了PyPI官员的确认。此外，手动分析显示，MPHunter识别所有潜在的恶意安装脚本，在所有分析的包中召回率为100%。我们断言，MPHunter为现有的检测技术提供了一个有价值和有利的补充，增强了软件供应链安全分析的武器库。,PyPI，恶意包检测，代码集群,,,
W9BDVT7V,2023,https://doi.org/10.1109/ASE56229.2023.00164,ASE 2023,Robin: A Novel Method to Produce Robust Interpreters for Deep Learning-Based Code Classifiers,"Deep learning has been widely used in source code classification tasks, such as code classification according to their functionalities, code authorship attribution, and vulnerability detection. Unfortunately, the black-box nature of deep learning makes it hard to interpret and understand why a classifier (i.e., classification model) makes a particular prediction on a given example. This lack of interpretability (or explainability) might have hindered their adoption by practitioners because it is not clear when they should or should not trust a classifier's prediction. The lack of interpretability has motivated a number of studies in recent years. However, existing methods are neither robust nor able to cope with out-of-distribution examples. In this paper, we propose a novel method to produce Robust interpreters for a given deep learning-based code classifier; the method is dubbed Robin. The key idea behind Robin is a novel hybrid structure combining an interpreter and two approximators, while leveraging the ideas of adversarial training and data augmentation. Experimental results show that on average the interpreter produced by Robin achieves a 6.11% higher fidelity (evaluated on the classifier), 67.22% higher fidelity (evaluated on the approximator), and 15.87x higher robustness than that of the three existing interpreters we evaluated. Moreover, the interpreter is 47.31% less affected by out-of-distribution examples than that of LEMNA.","Explainable AI,deep learning,code classification,robustness",Robin：一种为基于深度学习的代码分类器生成鲁棒解释器的新方法,深度学习已广泛应用于源代码分类任务，如根据功能进行代码分类、代码作者归属和漏洞检测。不幸的是，深度学习的黑匣子性质使得很难解释和理解分类器（即分类模型）为什么对给定的例子做出特定的预测。这种可解释性（或可解释性）的缺乏可能阻碍了从业者采用它们，因为不清楚他们何时应该或不应该相信分类器的预测。近年来，缺乏可解释性引发了许多研究。然而，现有的方法既不健壮，也不能处理分布外的例子。在本文中，我们提出了一种为给定的基于深度学习的代码分类器生成鲁棒解释器的新方法；这种方法被称为Robin。Robin背后的关键思想是一种新颖的混合结构，它结合了一个解释器和两个逼近器，同时利用了对抗性训练和数据扩充的思想。实验结果表明，与我们评估的三种现有解释器相比，Robin产生的解释器平均实现了6.11%的保真度（在分类器上评估）、67.22%的高保真度（在近似器上评估）和15.87倍的鲁棒性。此外，与LEMNA相比，口译员受分布外示例的影响小47.31%。,可解释的人工智能，深度学习，代码分类，稳健性,,,
UVFEQD3W,2023,https://doi.org/10.1109/ASE56229.2023.00089,ASE 2023,Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting,"Automated detection of software failures is an important but challenging software engineering task. It involves finding in a vast search space the failure-inducing test cases that contain an input triggering the software fault and an oracle asserting the incorrect execution. We are motivated to study how far this outstanding challenge can be solved by recent advances in large language models (LLMs) such as ChatGPT. However, our study reveals that ChatGPT has a relatively low success rate (28.8%) in finding correct failure-inducing test cases for buggy programs. A possible conjecture is that finding failure-inducing test cases requires analyzing the subtle differences (nuances) between the tokens of a program's correct version and those for its buggy version. When these two versions have similar sets of tokens and attentions, ChatGPT is weak in distinguishing their differences. We find that ChatGPT can successfully generate failure-inducing test cases when it is guided to focus on the nuances. Our solution is inspired by an interesting observation that ChatGPT could infer the intended functionality of buggy code if it is similar to the correct version. Driven by the inspiration, we develop a novel technique, called Differential Prompting, to effectively find failure-inducing test cases with the help of the compilable code synthesized by the inferred intention. Prompts are constructed based on the nuances between the given version and the synthesized code. We evaluate Differential Prompting on Quixbugs (a popular benchmark of buggy programs) and recent programs published at Codeforces (a popular programming contest portal, which is also an official benchmark of ChatGPT). We compare Differential Prompting with two baselines constructed using conventional ChatGPT prompting and Pynguin (the state-of-the-art unit test generation tool for Python programs). Our evaluation results show that for programs of Quixbugs, Differential Prompting can achieve a success rate of 75.0% in finding failure-inducing test cases, outperforming the best baseline by 2.6X. For programs of Codeforces, Differential Prompting's success rate is 66.7%, outperforming the best baseline by 4.0X.","failure-inducing test cases,large language models,program intention inference,program generation",Nuances是关键：解锁ChatGPT以发现具有差异提示的失败诱导测试,软件故障的自动检测是一项重要但具有挑战性的软件工程任务。它涉及在巨大的搜索空间中查找引发故障的测试用例，这些用例包含触发软件故障的输入和断言错误执行的预言机。我们有动力研究这一突出的挑战可以在多大程度上通过大型语言模型（LLM）（如ChatGPT）的最新进展来解决。然而，我们的研究表明，ChatGPT在为错误程序找到正确的失败诱导测试用例方面的成功率相对较低（28.8%）。一个可能的猜测是，要找到导致失败的测试用例，需要分析程序正确版本的令牌和错误版本的令牌之间的细微差别。当这两个版本具有相似的令牌集和注意力时，ChatGPT在区分它们的差异方面较弱。我们发现，当引导ChatGPT关注细微差别时，它可以成功地生成引发故障的测试用例。我们的解决方案受到了一个有趣的观察结果的启发，即如果ChatGPT与正确的版本相似，它可以推断出错误代码的预期功能。受此启发，我们开发了一种名为差分提示的新技术，借助推断意图合成的可编译代码，有效地找到导致失败的测试用例。提示是基于给定版本和合成代码之间的细微差别构建的。我们评估了Quixbugs（一个流行的bug程序基准）上的Differential Promotion和Codeforces（一种流行的编程竞赛门户网站，也是ChatGPT的官方基准）上最近发布的程序。我们将差分提示与使用传统的ChatGPT提示和Pynguin（Python程序的最先进的单元测试生成工具）构建的两个基线进行了比较。我们的评估结果表明，对于Quixbugs的程序，Differential Promoting在发现导致失败的测试用例方面可以实现75.0%的成功率，比最佳基线高出2.6X。对于Codeforces的程序来说，Different Promoting的成功率为66.7%，比最佳基准高出4.0X。,引发失败的测试用例，大型语言模型，程序意图推理，程序生成,,,
F384IGBA,2023,https://doi.org/10.1109/ASE56229.2023.00029,ASE 2023,Contextuality of Code Representation Learning,"Advanced machine learning models (ML) have been successfully leveraged in several software engineering (SE) applications. The existing SE techniques have used the embedding models ranging from static to contextualized ones to build the vectors for program units. The contextualized vectors address a phenomenon in natural language texts called polysemy, which is the coexistence of different meanings of a word/phrase. However, due to different nature, program units exhibit the nature of mixed polysemy. Some code tokens and statements exhibit polysemy while other tokens (e.g., keywords, separators, and operators) and statements maintain the same meaning in different contexts. A natural question is whether static or contextualized embeddings fit better with the nature of mixed polysemy in source code. The answer to this question is helpful for the SE researchers in selecting the right embedding model. We conducted experiments on 12 popular sequence-/tree-/graph-based embedding models and on the samples of a dataset of 10,222 Java projects with +14M methods. We present several contextuality evaluation metrics adapted from natural-language texts to code structures to evaluate the embeddings from those models. Among several findings, we found that the models with higher contextuality help a bug detection model perform better than the static ones. Neither static nor contextualized embedding models fit well with the mixed polysemy nature of source code. Thus, we develop Hycode, a hybrid embedding model that fits better with the nature of mixed polysemy in source code.","Code Representation Learning,Contextualized Embedding,Contextuality of Code Embedding",代码表示学习的语境性,先进的机器学习模型（ML）已经在几个软件工程（SE）应用中得到了成功的利用。现有的SE技术已经使用了从静态到上下文化的嵌入模型来构建程序单元的向量。语境化向量解决了自然语言文本中一种称为多义的现象，即一个词/短语的不同含义共存。然而，由于性质的不同，程序单元呈现出混合多义的性质。一些代码标记和语句表现出多义性，而其他标记（如关键字、分隔符和运算符）和语句在不同的上下文中保持相同的含义。一个自然的问题是，静态嵌入还是上下文嵌入更符合源代码中混合多义的性质。这个问题的答案有助于SE研究人员选择正确的嵌入模型。我们在12个流行的基于序列/树/图的嵌入模型上进行了实验，并在10222个Java项目的数据集样本上使用+14M方法进行了实验。我们提出了几个从自然语言文本到代码结构的上下文评估指标，以评估这些模型的嵌入。在一些发现中，我们发现具有较高上下文性的模型有助于错误检测模型比静态模型表现得更好。静态嵌入模型和上下文嵌入模型都不适合源代码的混合多义特性。因此，我们开发了Hycode，这是一种更适合源代码中混合多义的混合嵌入模型。,代码表示学习，上下文嵌入，代码嵌入的上下文,,,
DVUU97NN,2023,https://doi.org/10.1109/ASE56229.2023.00140,ASE 2023,A Large-Scale Empirical Study on Semantic Versioning in Golang Ecosystem,"Third-party libraries (TPLs) have become an essential component of software, accelerating development and reducing maintenance costs. However, breaking changes often occur during the upgrades of TPLs and prevent client programs from moving forward. Semantic versioning (SemVer) has been applied to standardize the versions of releases according to compatibility, but not all releases follow SemVer compliance. Lots of work focuses on SemVer compliance in ecosystems such as Java and JavaScript beyond Golang (Go for short). Due to the lack of tools to detect breaking changes and dataset for Go, developers of TPLs do not know if breaking changes occur and affect client programs, and developers of client programs may hesitate to upgrade dependencies in terms of breaking changes. To bridge this gap, we conduct the first large-scale empirical study in the Go ecosystem to study SemVer compliance in terms of breaking changes and their impact. In detail, we propose GoSVI (Go Semantic Versioning Insight) to detect breaking changes and analyze their impact by resolving identifiers in client programs and comparing their types with breaking changes. Moreover, we collect the first large-scale Go dataset with a dependency graph from GitHub, including 124K TPLs and 532K client programs. Based on the dataset, our results show that 86.3% of library upgrades follow SemVer compliance and 28.6% of no-major upgrades introduce breaking changes. Furthermore, the tendency to comply with SemVer has improved over time from 63.7% in 2018/09 to 92.2% in 2023/03. Finally, we find 33.3% of downstream client programs may be affected by breaking changes. These findings provide developers and users of TPLs with valuable insights to help make decisions related to SemVer.","Semantic Versioning,Breaking Change,Third-Party Library,Mining Software Repositories,Go Ecosystem",Golang生态系统语义版本控制的大规模实证研究,第三方库（TPL）已成为软件的重要组成部分，加速了开发并降低了维护成本。然而，在TPL的升级过程中经常会发生中断性更改，并阻止客户端程序向前移动。语义版本控制（SemVer）已被应用于根据兼容性标准化版本，但并非所有版本都遵循SemVer规范。许多工作都集中在生态系统中的SemVer合规性上，如Golang之外的Java和JavaScript（简称Go）。由于缺乏检测中断更改的工具和Go的数据集，TPL的开发人员不知道中断更改是否发生并影响客户端程序，客户端程序的开发人员可能会犹豫是否升级中断更改方面的依赖关系。为了弥补这一差距，我们在Go生态系统中进行了第一次大规模的实证研究，以研究SemVer在打破变化及其影响方面的合规性。详细地说，我们提出了GoSVI（Go Semantic Versioning Insight）来检测中断更改，并通过解析客户端程序中的标识符并将其类型与中断更改进行比较来分析其影响。此外，我们从GitHub收集了第一个带有依赖图的大型Go数据集，包括124K个TPL和532K个客户端程序。基于数据集，我们的结果显示，86.3%的库升级遵循SemVer合规性，28.6%的无重大升级引入了突破性变化。此外，随着时间的推移，遵守SemVer的趋势从2018/09年的63.7%提高到2023/03年的92.2%。最后，我们发现33.3%的下游客户端程序可能会受到中断更改的影响。这些发现为TPL的开发人员和用户提供了有价值的见解，以帮助他们做出与SemVer相关的决策。,语义版本控制，突破性变革，第三方库，挖掘软件存储库，Go生态系统,,,
BYGAP4RL,2023,https://doi.org/10.1109/ASE56229.2023.00210,ASE 2023,ZC\(^\mbox3\): Zero-Shot Cross-Language Code Clone Detection,"Developers introduce code clones to improve programming productivity. Many existing studies have achieved impressive performance in monolingual code clone detection. However, during software development, more and more developers write semantically equivalent programs with different languages to support different platforms and help developers translate projects from one language to another. Considering that collecting cross-language parallel data, especially for low-resource languages, is expensive and time-consuming, how designing an effective cross-language model that does not rely on any parallel data is a significant problem. In this paper, we propose a novel method named ZC3 for Z_ero-shot Cross-language Code Clone detection. ZC3 designs the contrastive snippet prediction to form an isomorphic representation space among different programming languages. Based on this, ZC3 exploits domain-aware learning and cycle consistency learning to further constrain the model to generate representations that are aligned among different languages meanwhile are diacritical for different types of clones. To evaluate our approach, we conduct extensive experiments on four representative cross-language clone detection datasets. Experimental results show that ZC3 outperforms the state-of-the-art baselines by 67.12%, 51.39%, 14.85%, and 53.01% on the MAP score, respectively. We further investigate the representational distribution of different languages and discuss the effectiveness of our method.","code clone detection,zero-shot learning,cross-language,deep neural networks",ZC\（^\mbox3\）：零样本跨语言代码克隆检测,开发人员引入代码克隆来提高编程效率。许多现有的研究在单语代码克隆检测方面取得了令人印象深刻的性能。然而，在软件开发过程中，越来越多的开发人员用不同的语言编写语义等效的程序，以支持不同的平台，并帮助开发人员将项目从一种语言翻译到另一种语言。考虑到收集跨语言并行数据，特别是对于低资源语言，既昂贵又耗时，如何设计一个不依赖任何并行数据的有效跨语言模型是一个重大问题。本文提出了一种新的Z_ero-shot跨语言代码克隆检测方法ZC3。ZC3设计了对比片段预测，以形成不同编程语言之间的同构表示空间。基于此，ZC3利用领域感知学习和循环一致性学习来进一步约束模型，以生成在不同语言之间对齐的表示，同时对不同类型的克隆是至关重要的。为了评估我们的方法，我们在四个具有代表性的跨语言克隆检测数据集上进行了广泛的实验。实验结果表明，ZC3在MAP得分上分别优于最先进的基线67.12%、51.39%、14.85%和53.01%。我们进一步研究了不同语言的表征分布，并讨论了我们的方法的有效性。,代码克隆检测，零样本学习，交叉语言，深度神经网络,,,
RZHNKE48,2023,https://doi.org/10.1109/ASE56229.2023.00150,ASE 2023,LiSum: Open Source Software License Summarization with Multi-Task Learning,"Open source software (OSS) licenses regulate the conditions under which users can reuse, modify, and distribute the software legally. However, there exist various OSS licenses in the community, written in a formal language, which are typically long and complicated to understand. In this paper, we conducted a 661-participants online survey to investigate the perspectives and practices of developers towards OSS licenses. The user study revealed an indeed need for an automated tool to facilitate license understanding. Motivated by the user study and the fast growth of licenses in the community, we propose the first study towards automated license summarization. Specifically, we released the first high quality text summarization dataset and designed two tasks, i.e., license text summarization (LTS), aiming at generating a relatively short summary for an arbitrary license, and license term classification (LTC), focusing on the attitude inference towards a predefined set of key license terms (e.g., Distribute). Aiming at the two tasks, we present LiSum, a multi-task learning method to help developers overcome the obstacles of understanding OSS licenses. Comprehensive experiments demonstrated that the proposed jointly training objective boosted the performance on both tasks, surpassing state-of-the-art baselines with gains of at least 5 points w.r.t. F1 scores of four summarization metrics and achieving 95.13% micro average F1 score for classification simultaneously. We released all the datasets, the replication package, and the questionnaires for the community.","Open Source Software Licenses,Multi-Task Learning,License comprehension",LiSum：基于多任务学习的开源软件许可证综述,开放源码软件（OSS）许可证规定了用户可以合法地重用、修改和分发软件的条件。然而，社区中存在各种以正式语言编写的OSS许可证，这些许可证通常很长，很难理解。在本文中，我们进行了一项661名参与者的在线调查，以调查开发人员对OSS许可证的看法和做法。用户研究表明，确实需要一个自动化工具来促进对许可证的理解。受用户研究和社区中许可证快速增长的激励，我们提出了第一项关于自动许可证摘要的研究。具体而言，我们发布了第一个高质量文本摘要数据集，并设计了两个任务，即许可证文本摘要（LTS）和许可证术语分类（LTC），前者旨在为任意许可证生成相对较短的摘要，后者侧重于对预定义的一组关键许可证术语的态度推断（例如，Distribute）。针对这两个任务，我们提出了LiSum，一种多任务学习方法，帮助开发人员克服理解OSS许可证的障碍。综合实验表明，所提出的联合训练目标提高了两项任务的性能，超过了最先进的基线，与四个汇总指标的F1得分相比，至少提高了5分，同时达到了95.13%的微观平均F1得分。我们为社区发布了所有数据集、复制包和问卷。,开源软件许可证，多任务学习，许可证理解,,,
78SXWAC8,2023,https://doi.org/10.1109/ASE56229.2023.00153,ASE 2023,Generative Model-Based Testing on Decision-Making Policies,"The reliability of decision-making policies is urgently important today as they have established the fundamentals of many critical applications, such as autonomous driving and robotics. To ensure reliability, there have been a number of research efforts on testing decision-making policies that solve Markov decision processes (MDPs). However, due to the deep neural network (DNN)-based inherit and infinite state space, developing scalable and effective testing frameworks for decision-making policies still remains open and challenging. In this paper, we present an effective testing framework for decision-making policies. The framework adopts a generative diffusion model-based test case generator that can easily adapt to different search spaces, ensuring the practicality and validity of test cases. Then, we propose a termination state novelty-based guidance to diversify agent behaviors and improve the test effectiveness. Finally, we evaluate the framework on five widely used benchmarks, including autonomous driving, aircraft collision avoidance, and gaming scenarios. The results demonstrate that our approach identifies more diverse and influential failure-triggering test cases compared to current state-of-the-art techniques. Moreover, we employ the detected failure cases to repair the evaluated models, achieving better robustness enhancement compared to the baseline method.","generative model,testing,decision-making policies",基于生成模型的决策策略测试,决策政策的可靠性在今天非常重要，因为它们已经奠定了自动驾驶和机器人等许多关键应用的基础。为了确保可靠性，已经有许多研究工作在测试解决马尔可夫决策过程（MDP）的决策策略。然而，由于基于深度神经网络（DNN）的继承和无限的状态空间，开发可扩展和有效的决策策略测试框架仍然是开放和具有挑战性的。在本文中，我们提出了一个有效的决策政策测试框架。该框架采用了基于生成扩散模型的测试用例生成器，可以很容易地适应不同的搜索空间，确保了测试用例的实用性和有效性。然后，我们提出了一种基于终止状态新颖性的指导，以使代理行为多样化，并提高测试的有效性。最后，我们在五个广泛使用的基准上评估了该框架，包括自动驾驶、飞机防撞和游戏场景。结果表明，与当前最先进的技术相比，我们的方法确定了更多样、更具影响力的故障触发测试用例。此外，我们使用检测到的故障案例来修复评估的模型，与基线方法相比，实现了更好的鲁棒性增强。,生成模型，测试，决策策略,,,
KSPJSJAP,2023,https://doi.org/10.1109/ASE56229.2023.00073,ASE 2023,"MalWuKong: Towards Fast, Accurate, and Multilingual Detection of Malicious Code Poisoning in OSS Supply Chains","In the face of increased threats within software registries and management systems, we address the critical need for effective malicious code detection. In this paper, we propose an innovative approach that integrates source code slicing, inter-procedural analysis, and cross-file inter-procedural analysis, thereby enhancing the detection precision and reducing false positives. This approach has been encapsulated within a multi-analysis-based framework for automatic detection of malicious code in real-world software packages. In its application to major third-party software registries like PyPI and NPM, our framework has proven effective, identifying 130 malicious packages from a total of 169,640 monitored over a continuous period of five weeks. This work advances the current state-of-the-art solution to malicious code detection, demonstrating significant practical impact in strengthening the software supply chain defense.","Malware Detection,Software Supply Chain Security,Security Tools",MalWuKong：实现OSS供应链中恶意代码中毒的快速、准确和多语言检测,面对软件注册中心和管理系统内日益增长的威胁，我们迫切需要有效的恶意代码检测。在本文中，我们提出了一种创新的方法，它集成了源代码切片、过程间分析和跨文件过程间分析，从而提高了检测精度并减少了误报。这种方法被封装在一个基于多分析的框架中，用于自动检测真实世界软件包中的恶意代码。在PyPI和NPM等主要第三方软件注册中心的应用中，我们的框架已被证明是有效的，在连续五周的监控中，从总共169640个恶意软件包中识别出130个。这项工作推进了当前最先进的恶意代码检测解决方案，在加强软件供应链防御方面发挥了重大的实际影响。,恶意软件检测，软件供应链安全，安全工具,,,
2KBKGCTX,2023,https://doi.org/10.1109/ASE56229.2023.00033,ASE 2023,HOBAT: Batch Verification for Homogeneous Structural Neural Networks,"The rapid development of deep learning has significantly transformed the ecology of the software engineering field. As new data continues to grow and evolve at an explosive rate, the challenge of iteratively updating software built on neural networks has become a critical issue. While the continuous learning paradigm enables networks to incorporate new data and update accordingly without losing previous memories, resulting in a batch of new networks as candidates for software updating, these approaches merely select from these networks by empirically testing their accuracy; they lack formal guarantees for such a batch of networks, especially in the presence of adversarial samples. Existing verification techniques, based on constraint solving, interval propagation, and linear approximation, provide formal guarantees but are designed to verify the properties of individual networks rather than a batch of networks. To address this issue, we analyze the batch verification problem corresponding to several non-traditional machine learning paradigms and further propose a framework named HOBAT (BATch verification for HOmogeneous structural neural networks) to enhance batch verification under reasonable assumptions about the representation of homogeneous structure neural networks, increasing scalability in practical applications. Our method involves abstracting the neurons at the same position in a batch of networks into a single neuron, followed by an iterative refinement process on the abstracted neuron to restore the precision until the desired properties for verification are met. Our method is orthogonal to boundary propagation verification on a single neural network. To assess our methodology, we integrate it with boundary propagation verification and observe significant improvements compared to the vanilla approach. Our experiments demonstrate the enormous potential for verifying large batches of networks in the era of big data.","Batch verification,Neural networks,Homogeneous structure,Abstraction",HOBAT：齐次结构神经网络的批量验证,深度学习的快速发展极大地改变了软件工程领域的生态。随着新数据以爆炸性的速度不断增长和发展，迭代更新基于神经网络的软件的挑战已成为一个关键问题。虽然连续学习范式使网络能够在不丢失先前记忆的情况下结合新数据并相应更新，从而产生一批新网络作为软件更新的候选者，但这些方法只是通过经验测试其准确性来从这些网络中进行选择；它们对这样一批网络缺乏形式上的保证，尤其是在存在对抗性样本的情况下。现有的基于约束求解、区间传播和线性近似的验证技术提供了形式上的保证，但旨在验证单个网络的性质，而不是一批网络的性质。为了解决这个问题，我们分析了与几种非传统机器学习范式相对应的批量验证问题，并进一步提出了一个名为HOBAT（HOhomogeneous structure neural networks的batch verification）的框架，以在关于同构结构神经网络表示的合理假设下增强批量验证，提高了实际应用中的可扩展性。我们的方法包括将一批网络中相同位置的神经元抽象为一个神经元，然后对抽象的神经元进行迭代细化，以恢复精度，直到达到所需的验证特性。我们的方法与单个神经网络上的边界传播验证正交。为了评估我们的方法，我们将其与边界传播验证相结合，并观察到与普通方法相比有显著改进。我们的实验证明了在大数据时代验证大批量网络的巨大潜力。,批量验证，神经网络，同构结构，抽象,,,
XGKT34YB,2023,https://doi.org/10.1109/ASE56229.2023.00035,ASE 2023,On Automated Assistants for Software Development: The Role of LLMs,"Software developers handle many complex tasks that include gathering and applying domain knowledge, coordinating subtasks, designing interfaces, turning ideas into elegant code, and more. They must switch contexts between these tasks, incurring more cognitive costs. Recent advances in large language models (LLMs) open up new possibilities for moving beyond the support provided by automated assistants (AAs) available today. In this paper, we explore if a human memory model can provide a framework for the systematic investigation of AAs for software development based on LLMs and other new technologies.","automation,machine learning,artificial intelligence,large language models,software development productivity",论软件开发的自动化助手：LLM的作用,软件开发人员处理许多复杂的任务，包括收集和应用领域知识、协调子任务、设计接口、将想法转化为优雅的代码等等。他们必须在这些任务之间切换上下文，从而产生更多的认知成本。大型语言模型（LLM）的最新进展为超越当今可用的自动化助理（AA）提供的支持开辟了新的可能性。在本文中，我们探讨了人类记忆模型是否可以为基于LLM和其他新技术的软件开发的自动化系统研究提供一个框架。,自动化，机器学习，人工智能，大型语言模型，软件开发生产力,,,
VSWCDDFW,2023,https://doi.org/10.1109/ASE56229.2023.00079,ASE 2023,Fuzzing for CPS Mutation Testing,"Mutation testing can help reduce the risks of releasing faulty software. For such reason, it is a desired practice for the development of embedded software running in safety-critical cyber-physical systems (CPS). Unfortunately, state-of-the-art test data generation techniques for mutation testing of C and C++ software, two typical languages for CPS software, rely on symbolic execution, whose limitations often prevent its application (e.g., it cannot test black-box components). We propose a mutation testing approach that leverages fuzz testing, which has proved effective with C and C++ software. Fuzz testing automatically generates diverse test inputs that exercise program branches in a varied number of ways and, therefore, exercise statements in different program states, thus maximizing the likelihood of killing mutants, our objective. We performed an empirical assessment of our approach with software components used in satellite systems currently in orbit. Our empirical evaluation shows that mutation testing based on fuzz testing kills a significantly higher proportion of live mutants than symbolic execution (i.e., up to an additional 47 percentage points). Further, when symbolic execution cannot be applied, fuzz testing provides significant benefits (i.e., up to 41% mutants killed). Our study is the first one comparing fuzz testing and symbolic execution for mutation testing; our results provide guidance towards the development of fuzz testing tools dedicated to mutation testing.","Mutation testing,Fuzzing,Test data generation",CPS突变检测的模糊化,突变测试可以帮助降低发布错误软件的风险。因此，开发运行在安全关键网络物理系统（CPS）中的嵌入式软件是一种理想的实践。不幸的是，用于C和C++软件的突变测试的最先进的测试数据生成技术，这两种典型的CPS软件语言，依赖于符号执行，其局限性往往阻碍了其应用（例如，它不能测试黑盒组件）。我们提出了一种利用模糊测试的突变测试方法，该方法已在C和C++软件中被证明是有效的。模糊测试自动生成不同的测试输入，这些测试输入以多种方式锻炼程序分支，因此，在不同的程序状态下锻炼语句，从而最大限度地提高杀死突变体的可能性，这是我们的目标。我们使用目前在轨卫星系统中使用的软件组件对我们的方法进行了实证评估。我们的经验评估表明，基于模糊测试的突变测试杀死的活突变体比例明显高于符号执行（即，高达47个百分点）。此外，当不能应用符号执行时，模糊测试提供了显著的好处（即，高达41%的突变体被杀死）。我们的研究是第一个比较模糊测试和符号执行的突变测试；我们的研究结果为专门用于突变测试的模糊测试工具的开发提供了指导。,突变测试，模糊化，测试数据生成,,,
PJH6Z5GV,2023,https://doi.org/10.1109/ASE56229.2023.00172,ASE 2023,Towards Self-Adaptive Machine Learning-Enabled Systems Through QoS-Aware Model Switching,"Machine Learning (ML), particularly deep learning, has seen vast advancements, leading to the rise of Machine Learning-Enabled Systems (MLS). However, numerous software engineering challenges persist in propelling these MLS into production, largely due to various run-time uncertainties that impact the overall Quality of Service (QoS). These uncertainties emanate from ML models, software components, and environmental factors. Self-adaptation techniques present potential in managing run-time uncertainties, but their application in MLS remains largely unexplored. As a solution, we propose the concept of a Machine Learning Model Balancer, focusing on managing uncertainties related to ML models by using multiple models. Subsequently, we introduce AdaMLS, a novel self-adaptation approach that leverages this concept and extends the traditional MAPE-K loop for continuous MLS adaptation. AdaMLS employs lightweight unsupervised learning for dynamic model switching, thereby ensuring consistent QoS. Through a self-adaptive object detection system prototype, we demonstrate AdaMLS's effectiveness in balancing system and model performance. Preliminary results suggest AdaMLS surpasses naive and single state-of-the-art models in QoS guarantees, heralding the advancement towards self-adaptive MLS with optimal OoS in dynamic environments.","Self Adaptation,Self-adaptive systems,Software Architecture,ML-Enabled Systems,ML4SA,Unsupervised Learning,Object Detection",通过QoS感知模型切换实现自适应机器学习系统,机器学习（ML），特别是深度学习，已经取得了巨大的进步，导致了机器学习支持系统（MLS）的兴起。然而，在推动这些MLS投入生产的过程中，仍然存在许多软件工程挑战，这主要是由于影响整体服务质量（QoS）的各种运行时不确定性。这些不确定性源于ML模型、软件组件和环境因素。自适应技术在管理运行时的不确定性方面具有潜力，但其在MLS中的应用在很大程度上尚未探索。作为一种解决方案，我们提出了机器学习模型均衡器的概念，重点是通过使用多个模型来管理与ML模型相关的不确定性。随后，我们介绍了AdaMLS，这是一种新颖的自适应方法，利用了这一概念，并扩展了传统的MAPE-K循环，用于连续MLS自适应。AdaMLS采用轻量级无监督学习进行动态模型切换，从而确保一致的QoS。通过一个自适应目标检测系统原型，我们展示了AdaMLS在平衡系统和模型性能方面的有效性。初步结果表明，AdaMLS在QoS保证方面超越了原始和单一的最先进模型，预示着在动态环境中向具有最优OoS的自适应MLS迈进。,自适应，自适应系统，软件体系结构，ML启用系统，ML4SA，无监督学习，对象检测,,,
YWAKXIUA,2023,https://doi.org/10.1109/ASE56229.2023.00136,ASE 2023,Are They All Good? Studying Practitioners' Expectations on the Readability of Log Messages,"Developers write logging statements to generate logs that provide run-time information for various tasks. The readability of log messages in the logging statements (i.e., the descriptive text) is rather crucial to the value of the generated logs. Immature log messages may slow down or even obstruct the process of log analysis. Despite the importance of log messages, there is still a lack of standards on what constitutes good readability of log messages and how to write them. In this paper, we conduct a series of interviews with 17 industrial practitioners to investigate their expectations on the readability of log messages. Through the interviews, we derive three aspects related to the readability of log messages, including Structure, Information, and Wording, along with several specific practices to improve each aspect. We validate our findings through a series of online questionnaire surveys and receive positive feedback from the participants. We then manually investigate the readability of log messages in large-scale open source systems and find that a large portion (38.1%) of the log messages have inadequate readability. Motivated by such observation, we further explore the potential of automatically classifying the readability of log messages using deep learning and machine learning models. We find that both deep learning and machine learning models can effectively classify the readability of log messages with a balanced accuracy above 80.0% on average. Our study provides comprehensive guidelines for composing log messages to further improve practitioners' logging practices.","software logging,log messages,empirical study",它们都好吗？研究从业者对日志信息可读性的期望,开发人员编写日志记录语句来生成日志，这些日志为各种任务提供运行时信息。日志记录语句中日志消息（即描述性文本）的可读性对生成的日志的价值至关重要。不成熟的日志消息可能会减慢甚至阻碍日志分析的进程。尽管日志消息很重要，但对于什么构成日志消息的良好可读性以及如何编写日志消息，仍然缺乏标准。在本文中，我们对17名行业从业者进行了一系列采访，以调查他们对日志消息可读性的期望。通过访谈，我们得出了与日志消息可读性相关的三个方面，包括结构、信息和措辞，以及改进每个方面的几个具体实践。我们通过一系列在线问卷调查验证了我们的发现，并收到了参与者的积极反馈。然后，我们手动研究了大规模开源系统中日志消息的可读性，发现很大一部分（38.1%）的日志消息可读性不足。受此启发，我们进一步探索了使用深度学习和机器学习模型自动对日志消息可读性进行分类的潜力。我们发现，深度学习和机器学习模型都可以有效地对日志消息的可读性进行分类，平均准确率在80.0%以上。我们的研究为编写日志消息提供了全面的指导，以进一步改进从业者的日志实践。,软件日志，日志消息，实证研究,,,
WNN5Y3VY,2023,https://doi.org/10.1109/ASE56229.2023.00082,ASE 2023,Maat: Performance Metric Anomaly Anticipation for Cloud Services with Conditional Diffusion,"Ensuring the reliability and user satisfaction of cloud services necessitates prompt anomaly detection followed by diagnosis. Existing techniques for anomaly detection focus solely on real-time detection, meaning that anomaly alerts are issued as soon as anomalies occur. However, anomalies can propagate and escalate into failures, making faster-than-real-time anomaly detection highly desirable for expediting downstream analysis and intervention. This paper proposes Maat, the first work to address anomaly anticipation of performance metrics in cloud services. Maat adopts a novel two-stage paradigm for anomaly anticipation, consisting of metric forecasting and anomaly detection on forecasts. The metric forecasting stage employs a conditional denoising diffusion model to enable multi-step forecasting in an auto-regressive manner. The detection stage extracts anomaly-indicating features based on domain knowledge and applies isolation forest with incremental learning to detect upcoming anomalies. Thus, our method can uncover anomalies that better conform to human expertise. Evaluation on three publicly available datasets demonstrates that Maat can anticipate anomalies faster than real-time comparatively or more effectively compared with state-of-the-art real-time anomaly detectors. We also present cases highlighting Maat's success in forecasting abnormal metrics and discovering anomalies.","Could Computing,Anomaly Anticipation,Denosing Diffusion Model,Performance Metric",Maat：具有条件扩散的云服务的性能度量异常预测,为了确保云服务的可靠性和用户满意度，必须在进行诊断后立即进行异常检测。现有的异常检测技术只关注实时检测，这意味着一旦发生异常，就会发出异常警报。然而，异常可能传播并升级为故障，因此非常需要比实时更快的异常检测来加快下游分析和干预。本文提出了Maat，这是第一个解决云服务中性能指标异常预期的工作。Maat采用了一种新的两阶段异常预测范式，包括度量预测和预测异常检测。度量预测阶段采用条件去噪扩散模型，以实现自回归方式的多步骤预测。检测阶段基于领域知识提取异常指示特征，并应用具有增量学习的隔离森林来检测即将到来的异常。因此，我们的方法可以发现更符合人类专业知识的异常现象。对三个公开可用数据集的评估表明，与最先进的实时异常检测器相比，Maat可以比实时更快或更有效地预测异常。我们还介绍了突出Maat在预测异常指标和发现异常方面的成功案例。,Can计算，异常预测，去噪扩散模型，性能度量,,,
7N2XKLZR,2023,https://doi.org/10.1109/ASE56229.2023.00206,ASE 2023,Log Parsing: How Far Can ChatGPT Go?,"Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, ChatGPT, the current cutting-edge large language model (LLM), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate ChatGPT's ability to undertake log parsing by addressing two research questions. (1) Can ChatGPT effectively parse logs? (2) How does ChatGPT perform with different prompting methods? Our results show that ChatGPT can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.","Log analytics,Log parsing,Large language model,ChatGPT",日志分析：ChatGPT能走多远？,软件日志在确保大型软件系统的可靠性和可维护性方面发挥着至关重要的作用，因为它们通常是运行时信息的唯一来源。日志解析将原始日志消息转换为结构化数据，是实现下游日志分析的重要初始步骤。在最近的研究中，当前最前沿的大型语言模型（LLM）ChatGPT已被广泛应用于各种软件工程任务。然而，它在自动日志解析方面的性能仍不清楚。在本文中，我们通过解决两个研究问题来评估ChatGPT进行日志解析的能力。（1） ChatGPT能否有效地解析日志？（2） ChatGPT如何使用不同的提示方法执行？我们的结果表明，ChatGPT在适当的提示下，尤其是在很少的快照提示下，可以在日志解析中取得有希望的结果。基于我们的发现，我们概述了基于ChatGPT的日志解析的几个挑战和机遇。,日志分析，日志解析，大型语言模型，ChatGPT,,,
BBQ5ZC3W,2023,https://doi.org/10.1109/ASE56229.2023.00124,ASE 2023,Thunderkaller: Profiling and Improving the Performance of Syzkaller,"Fuzzing is widely adopted to discover vulnerabilities in software, including the kernel. One of the most popular and state-of-the-art fuzzers for kernels is Syzkaller. However, Syzkaller has a much lower testing throughput compared to other user-space fuzzers, which affects the efficiency of both Syzkaller and other Syzkaller-based fuzzers. In this paper, we profiled the performance of Syzkaller, recognized that the major cost comes from program isolation and kernel instrumentation, and then proposed kernel image duplication and three optimization techniques to mitigate such overheads and present the solution Thunderkaller. Our solution does not change or depend on the fuzzing algorithm in any way, orthogonal to other refinements to Syzkaller. Our evaluation shows that, in 24 hours, Thunderkaller speeds up 2.8× compared to vanilla Syzkaller, achieves 25.8% more basic block coverage, detects 21 more unique bugs, and triggers the common bugs 6.3× faster. In a long time of fuzzing, we have found 6 unique Linux kernel bugs and obtained a CVE.","Fuzzing,OS kernel,Measurement,Performance",Thunderkaller：剖析和改进Syzkaller的性能,模糊化被广泛用于发现软件中的漏洞，包括内核。Syzkaller是最流行和最先进的内核模糊器之一。然而，与其他用户空间模糊器相比，Syzkaller的测试吞吐量要低得多，这影响了Syzkaler和其他基于Syzaller的模糊器的效率。在本文中，我们介绍了Syzkaller的性能，认识到主要成本来自程序隔离和内核检测，然后提出了内核映像复制和三种优化技术来减轻这种开销，并提出了Thunderkaller解决方案。我们的解决方案不会以任何方式改变或依赖于模糊算法，与Syzkaller的其他改进正交。我们的评估表明，与香草Syzkaller相比，Thunderkaller在24小时内的速度提高了2.8倍，基本块覆盖率提高了25.8%，检测到了21个独特的错误，触发常见错误的速度加快了6.3倍。在长期的模糊处理中，我们发现了6个独特的Linux内核错误，并获得了CVE。,引信，操作系统内核，测量，性能,,,
VBCR9WN2,2023,https://doi.org/10.1109/ASE56229.2023.00161,ASE 2023,Scalable Industrial Control System Analysis via XAI-Based Gray-Box Fuzzing,"Conventional approaches to analyzing industrial control systems have relied on either white-box analysis or black-box fuzzing. However, white-box methods rely on sophisticated domain expertise, while black-box methods suffers from state explosion and thus scales poorly when analyzing real ICS involving a large number of sensors and actuators. To address these limitations, we propose XAI-based gray-box fuzzing, a novel approach that leverages explainable AI and machine learning modeling of ICS to accurately identify a small set of actuators critical to ICS safety, which result in significant reduction of state space without relying on domain expertise. Experiment results show that our method accurately explains the ICS model and significantly speeds-up fuzzing by 64x when compared to conventional black-box methods.","Fuzzing,Industrial Control Systems,Learning based Approaches,Explainable AI,Security Attack",基于XAI的灰盒引信可扩展工业控制系统分析,分析工业控制系统的传统方法依赖于白盒分析或黑盒模糊化。然而，白盒方法依赖于复杂的领域专业知识，而黑盒方法受到状态爆炸的影响，因此在分析涉及大量传感器和致动器的真实ICS时，其规模很小。为了解决这些限制，我们提出了基于XAI的灰盒模糊化，这是一种新的方法，利用可解释的人工智能和ICS的机器学习建模来准确识别对ICS安全至关重要的一小部分执行器，这在不依赖领域专业知识的情况下显著减少了状态空间。实验结果表明，与传统的黑盒方法相比，我们的方法准确地解释了ICS模型，并将模糊处理速度显著提高了64倍。,引信，工业控制系统，基于学习的方法，可解释的人工智能，安全攻击,,,
X4H37V4D,2023,https://doi.org/10.1109/ASE56229.2023.00015,ASE 2023,Bus Factor Explorer,"Bus factor (BF) is a metric that tracks knowledge distribution in a project. It is the minimal number of engineers that have to leave for a project to stall. Despite the fact that there are several algorithms for calculating the bus factor, only a few tools allow easy calculation of bus factor and convenient analysis of results for projects hosted on Git-based providers. We introduce Bus Factor Explorer, a web application that provides an interface and an API to compute, export, and explore the Bus Factor metric via treemap visualization, simulation mode, and chart editor. It supports repositories hosted on GitHub and enables functionality to search repositories in the interface and process many repositories at the same time. Our tool allows users to identify the files and subsystems at risk of stalling in the event of developer turnover by analyzing the VCS history. The application and its source code are publicly available on GitHub at https://github.com/JetBrains-Research/bus-factor-explorer. The demonstration video can be found on YouTube: https://youtu.be/uIoV79N14z8","bus factor,truck factor,knowledge management,intelligent collaboration tools",总线系数资源管理器,总线因子（BF）是跟踪项目中知识分布的度量。这是为了让一个项目停滞而不得不离开的工程师的最低数量。尽管有几种算法可以计算总线因子，但只有少数工具可以轻松计算总线因子并方便地分析托管在基于Git的提供商上的项目的结果。我们介绍了Bus Factor Explorer，这是一个web应用程序，它提供了一个界面和一个API，用于通过树图可视化、模拟模式和图表编辑器计算、导出和探索Bus Factor度量。它支持托管在GitHub上的存储库，并支持在界面中搜索存储库并同时处理多个存储库的功能。我们的工具允许用户通过分析VCS历史来识别在开发人员流失的情况下有停滞风险的文件和子系统。该应用程序及其源代码可在GitHub上公开获取，网址为https://github.com/JetBrains-Research/bus-factor-explorer.演示视频可以在YouTube上找到：https://youtu.be/uIoV79N14z8,公交车因素，卡车因素，知识管理，智能协作工具,,,
SQLET6CR,2023,https://doi.org/10.1109/ASE56229.2023.00218,ASE 2023,Adaptive REST API Testing with Reinforcement Learning,"Modern web services increasingly rely on REST APIs. Effectively testing these APIs is challenging due to the vast search space to be explored, which involves selecting API operations for sequence creation, choosing parameters for each operation from a potentially large set of parameters, and sampling values from the virtually infinite parameter input space. Current testing tools lack efficient exploration mechanisms, treating all operations and parameters equally (i.e., not considering their importance or complexity) and lacking prioritization strategies. Furthermore, these tools struggle when response schemas are absent in the specification or exhibit variants. To address these limitations, we present an adaptive REST API testing technique that incorporates reinforcement learning to prioritize operations and parameters during exploration. Our approach dynamically analyzes request and response data to inform dependent parameters and adopts a sampling-based strategy for efficient processing of dynamic API feedback. We evaluated our technique on ten RESTful services, comparing it against state-of-the-art REST testing tools with respect to code coverage achieved, requests generated, operations covered, and service failures triggered. Additionally, we performed an ablation study on prioritization, dynamic feedback analysis, and sampling to assess their individual effects. Our findings demonstrate that our approach outperforms existing REST API testing tools in terms of effectiveness, efficiency, and fault-finding ability.","Reinforcement Learning for Testing,Automated REST API Testing",带强化学习的自适应REST API测试,现代web服务越来越依赖RESTAPI。有效测试这些API是具有挑战性的，因为需要探索巨大的搜索空间，其中包括选择用于序列创建的API操作，从潜在的大量参数中选择每个操作的参数，以及从几乎无限的参数输入空间中采样值。目前的测试工具缺乏有效的探索机制，对所有操作和参数一视同仁（即不考虑其重要性或复杂性），也缺乏优先顺序策略。此外，当规范中没有响应模式或出现变体时，这些工具会遇到困难。为了解决这些限制，我们提出了一种自适应REST API测试技术，该技术结合了强化学习，以在探索过程中对操作和参数进行优先级排序。我们的方法动态分析请求和响应数据以通知相关参数，并采用基于采样的策略来有效处理动态API反馈。我们在十个RESTful服务上评估了我们的技术，并将其与最先进的REST测试工具在实现的代码覆盖率、生成的请求、覆盖的操作和触发的服务故障方面进行了比较。此外，我们对优先级、动态反馈分析和采样进行了消融研究，以评估其个体影响。我们的研究结果表明，我们的方法在有效性、效率和故障修复能力方面优于现有的REST API测试工具。,用于测试的强化学习，自动化REST API测试,,,
RFIM2BPJ,2023,https://doi.org/10.1109/ASE56229.2023.00187,ASE 2023,Towards Safe Automated Refactoring of Imperative Deep Learning Programs to Graph Execution,"Efficiency is essential to support responsiveness w.r.t. ever-growing datasets, especially for Deep Learning (DL) systems. DL frameworks have traditionally embraced deferred execution-style DL code-supporting symbolic, graph-based Deep Neural Network (DNN) computation. While scalable, such development is error-prone, non-intuitive, and difficult to debug. Consequently, more natural, imperative DL frameworks encouraging eager execution have emerged at the expense of run-time performance. Though hybrid approaches aim for the “best of both worlds,” using them effectively requires subtle considerations to make code amenable to safe, accurate, and efficient graph execution. We present our ongoing work on automated refactoring that assists developers in specifying whether and how their otherwise eagerly-executed imperative DL code could be reliably and efficiently executed as graphs while preserving semantics. The approach, based on a novel imperative tensor analysis, will automatically determine when it is safe and potentially advantageous to migrate imperative DL code to graph execution and modify decorator parameters or eagerly executing code already running as graphs. The approach is being implemented as a PyDev Eclipse IDE plug-in and uses the WALA Ariadne analysis framework. We discuss our ongoing work towards optimizing imperative DL code to its full potential.","deep learning,refactoring,graph execution",面向图执行的强制性深度学习程序的安全自动重构,效率对于支持不断增长的数据集的响应能力至关重要，尤其是对于深度学习（DL）系统。DL框架传统上采用了延迟执行风格的DL代码，支持符号、基于图的深度神经网络（DNN）计算。虽然这种开发是可扩展的，但它容易出错、不直观，而且很难调试。因此，以牺牲运行时性能为代价，出现了更自然、更迫切的DL框架，鼓励渴望执行。尽管混合方法的目标是“两全其美”，但有效地使用它们需要微妙的考虑，以使代码能够安全、准确和高效地执行图。我们介绍了我们正在进行的自动化重构工作，该工作有助于开发人员指定他们在其他方面热切执行的命令式DL代码是否以及如何作为图可靠高效地执行，同时保留语义。该方法基于一种新的命令式张量分析，将自动确定何时将命令式DL代码迁移到图执行中是安全的，并且可能是有利的，并修改装饰器参数或急切地执行已经作为图运行的代码。该方法被实现为PyDev Eclipse IDE插件，并使用WALA Ariadne分析框架。我们讨论了我们正在进行的优化势在必行的DL代码以充分发挥其潜力的工作。,深度学习，重构，图形执行,,,
8U6BD4YC,2023,https://doi.org/10.1109/ASE56229.2023.00186,ASE 2023,Pluggable Type Inference for Free,"A pluggable type system extends a host programming language with type qualifiers. It lets programmers write types like unsigned int, secret string, and nonnull object. Typechecking with pluggable types detects and prevents more errors than the host type system. However, programmers must write type qualifiers; this is the biggest obstacle to use of pluggable types in practice. Type inference can solve this problem. Traditional approaches to type inference are type-system-specific: for each new pluggable type system, the type inference algorithm must be extended to build and then solve a system of constraints corresponding to the rules of the underlying type system. We propose a novel type inference algorithm that can infer type qualifiers for any pluggable type system with little to no new type-system-specific code-that is, “for free”. The key insight is that extant practical pluggable type systems are flow-sensitive and therefore already implement local type inference. Using this insight, we can derive a global inference algorithm by re-using existing implementations of local inference. Our algorithm runs iteratively in rounds. Each round uses the results of local type inference to produce summaries (specifications) for procedures and fields. These summaries enable improved inference throughout the program in subsequent rounds. The algorithm terminates when the inferred summaries reach a fixed point. In practice, many pluggable type systems are built on frameworks. By implementing our algorithm once, at the framework level, it can be reused by any typechecker built using that frame-work. Using that insight, we have implemented our algorithm for the open-source Checker Framework project, which is widely used in industry and on which dozens of specialized pluggable typecheckers have been built. In experiments with 11 distinct pluggable type systems and 12 projects, our algorithm reduced, by 45 % on average, the number of warnings that developers must resolve by writing annotations.","Pluggable type systems,type qualifiers,type-checking,type inference,static analysis",免费的可插类型推理,可插拔类型系统使用类型限定符扩展主机编程语言。它允许程序员编写诸如unsigned int、secret string和nonnull对象之类的类型。与主机类型系统相比，使用可插拔类型的类型检查可以检测并防止更多错误。但是，程序员必须编写类型限定符；这是在实践中使用可插拔类型的最大障碍。类型推理可以解决这个问题。传统的类型推理方法是特定于类型系统的：对于每个新的可插拔类型系统，必须扩展类型推理算法，以构建并解决与底层类型系统的规则相对应的约束系统。我们提出了一种新的类型推断算法，该算法可以推断任何可插拔类型系统的类型限定符，而几乎没有新的类型系统特定代码，也就是说，“免费”。关键的见解是，现有的实用可插拔类型系统是流敏感的，因此已经实现了本地类型推断。利用这一见解，我们可以通过重新使用现有的局部推理实现来导出全局推理算法。我们的算法循环迭代运行。每一轮都使用局部类型推断的结果来生成过程和字段的摘要（规范）。这些总结能够在随后的几轮中改进整个程序的推理。当推断出的摘要达到固定点时，算法终止。在实践中，许多可插拔类型的系统都是建立在框架上的。通过在框架级别实现一次我们的算法，任何使用该框架构建的类型检查器都可以重用它。利用这一见解，我们为开源Checker Framework项目实现了我们的算法，该项目在工业中广泛使用，并在此基础上构建了数十个专门的可插拔类型检查器。在对11个不同的可插拔类型系统和12个项目进行的实验中，我们的算法平均将开发人员必须通过编写注释来解决的警告数量减少了45%。,可插拔类型系统，类型限定符，类型检查，类型推理，静态分析,,,
HP8HT2FH,2023,https://doi.org/10.1109/ASE56229.2023.00040,ASE 2023,SCPatcher: Mining Crowd Security Discussions to Enrich Secure Coding Practices,"Secure coding practices (SCPs) have been proposed to guide software developers to write code securely to prevent potential security vulnerabilities. Yet, they are typically one-sentence principles without detailed specifications, e.g., “Properly free allocated memory upon the completion of functions and at all exit points.”, which makes them difficult to follow in practice, especially for software developers who are not yet experienced in secure programming. To address this problem, this paper proposes SCPatcher, an automated approach to enrich secure coding practices by mining crowd security discussions on online knowledge-sharing platforms, such as Stack Overflow. In particular, for each security post, SCPatcher first extracts the area of coding examples and coding explanations with a fix-prompt tuned Large Language Model (LLM) via Prompt Learning. Then, it hierarchically slices the lengthy code into coding examples and summarizes the coding explanations with the areas. Finally, SCPatcher matches the CWE and Public SCP, integrating them with extracted coding examples and explanations to form the SCP specifications, which are the wild SCPs with details, proposed by the developers. To evaluate the performance of SCPatcher, we conduct experiments on 3,907 security posts from Stack Overflow. The experimental results show that SCPatcher outperforms all baselines in extracting the coding examples with 2.73 % MLine on average, as well as coding explanations with 3.97 % F1 on average. Moreover, we apply SCPatcher on 447 new security posts to further evaluate its practicality, and the extracted SCP specifications enrich the public SCPs with 3,074 lines of code and 1,967 sentences.","Software Security,Secure Coding Practice,Artificial Intelligence,Large Language Model",SCPatcher：挖掘群组安全讨论以丰富安全编码实践,已经提出了安全编码实践（SCP）来指导软件开发人员安全地编写代码，以防止潜在的安全漏洞。然而，它们通常是一句话的原则，没有详细的规范，例如，“在完成功能和所有退出点时正确释放分配的内存。”，这使得它们在实践中很难遵循，尤其是对于还没有安全编程经验的软件开发人员来说。为了解决这个问题，本文提出了SCPatcher，这是一种通过挖掘在线知识共享平台（如Stack Overflow）上的人群安全讨论来丰富安全编码实践的自动化方法。特别是，对于每个安全帖子，SCPatcher首先通过prompt Learning使用修复提示调优的大型语言模型（LLM）来提取编码示例和编码解释的区域。然后，它将冗长的代码分层划分为编码示例，并用区域总结编码解释。最后，SCPatcher将CWE和Public SCP进行匹配，将它们与提取的编码示例和解释相结合，形成SCP规范，这是开发人员提出的带有细节的野生SCP。为了评估SCPatcher的性能，我们对来自Stack Overflow的3907个安全帖子进行了实验。实验结果表明，SCPatcher在提取平均2.73%MLine的编码示例和平均3.97%F1的编码解释方面优于所有基线。此外，我们将SCPatcher应用于447个新的安全岗位，以进一步评估其实用性，提取的SCP规范用3074行代码和1967句句子丰富了公共SCP。,软件安全，安全编码实践，人工智能，大型语言模型,,,
TWVK9JVL,2023,https://doi.org/10.1109/ASE56229.2023.00183,ASE 2023,PSMT: Satisfiability Modulo Theories Meets Probability Distribution,"SMT (Satisfiability Modulo Theories) has been widely used in program verification, analysis, and test generation. But sometimes, SMT solver outputs incomprehensible solutions, especially for practical instances. Besides, due to the design of the deterministic algorithms, for a given formula, the result of each run is the same. In this paper, we concentrate on combining SMT solving with probability, which will instruct the SMT solver to give some plausible solutions. We define a special problem: PSMT, which allows solving an SMT instance with variables conforming to a certain distribution. We define distribution under constraint for PSMT, which is based on MCSAT (Model Constructing Satisfiability), a mainstream SMT-solving algorithm. We propose the Prob-MCSAT algorithm, which combines the MCSAT algorithm and introduces the probability to variables. The visualized examples show that the resulting assignments will form a clear trend based on Prob-SMT.","Satisfiability Modulo Theories,Model Constructing Satisfiability,Probability Distribution",PSMT：可满足性模理论满足概率分布,SMT（可满足性模理论）已广泛应用于程序验证、分析和测试生成。但有时，SMT求解器会输出无法理解的解决方案，尤其是在实际情况下。此外，由于确定性算法的设计，对于给定的公式，每次运行的结果是相同的。在本文中，我们专注于将SMT求解与概率相结合，这将指导SMT求解器给出一些合理的解。我们定义了一个特殊的问题：PSMT，它允许使用符合特定分布的变量来解决SMT实例。我们定义了PSMT的约束分布，它是基于主流SMT求解算法MCSAT（模型构造可满足性）的。我们提出了Prob-MCSAT算法，它结合了MCSAT算法并将概率引入变量。可视化的例子表明，基于Prob-SMT的结果分配将形成一个明显的趋势。,可满足性模理论，可满足性模型构建，概率分布,,,
4WRZXS8L,2023,https://doi.org/10.1109/ASE56229.2023.00068,ASE 2023,Vision-Based Widget Mapping for Test Migration Across Mobile Platforms: Are We There Yet?,"Automated GUI testing through the reuse of existing tests has recently gained prominence in research. Cross-platform migration of GUI tests between different platform versions of an application offers a promising opportunity for test reuse. Widget mapping, identifying similarities between source and target application widgets and connecting semantically analogous pairs, is central to these approaches. Vision-based widget mapping approaches are supposed to provide platform-agnostic solutions more suitable for cross-platform migration, considering that different platform versions frequently display strong resemblances in the appearance of their semantically similar widgets. However, the efficacy of vision-based widget mapping for cross-platform migration remains limited and the reasons remain unclear. In this paper, we present the first comprehensive investigation of vision-based widget mapping for cross-platform GUI test migration. We devote considerable effort to constructing a dataset consisting of 6,730 bi-directional mapped widget pairs across the iOS and Android platforms, and categorize the mapped widgets into eight classifications to thoroughly assess the capabilities of various approaches. We implement 89 configurations, derived from five distinct vision-based widget mapping methodologies, and evaluate their performance utilizing our dataset. Our findings reveal valuable insights that can be employed to advance vision-based widget mapping techniques: (1) The current approach exhibits potential for improvement, as certain configurations demonstrate superior performance in comparison to existing methods; (2) Some features can adversely impact the mapping, requiring more consideration; (3) A substantial proportion of mapped widgets display varying inconsistent contents in their appearance, which require more sophisticated vision algorithms.","Test migration,GUI testing,vision-based GUI analysis",用于跨移动平台测试迁移的基于视觉的Widget映射：我们已经实现了吗？,通过重用现有测试实现的自动化GUI测试最近在研究中获得了突出地位。GUI测试在应用程序的不同平台版本之间的跨平台迁移为测试重用提供了一个很好的机会。小部件映射，识别源和目标应用程序小部件之间的相似性，并连接语义相似的对，是这些方法的核心。考虑到不同的平台版本在语义相似的小部件的外观上经常显示出强烈的相似性，基于视觉的小部件映射方法应该提供更适合跨平台迁移的平台无关解决方案。然而，基于视觉的小部件映射在跨平台迁移中的效果仍然有限，原因尚不清楚。在本文中，我们首次全面研究了用于跨平台GUI测试迁移的基于视觉的窗口小部件映射。我们花了大量精力构建了一个由6730个跨iOS和Android平台的双向映射小部件对组成的数据集，并将映射的小部件分为八类，以彻底评估各种方法的能力。我们实现了89种配置，这些配置源自五种不同的基于视觉的窗口小部件映射方法，并利用我们的数据集评估了它们的性能。我们的研究结果揭示了可用于推进基于视觉的窗口小部件映射技术的有价值的见解：（1）当前的方法显示出改进的潜力，因为与现有方法相比，某些配置表现出优异的性能；（2） 有些特征可能会对映射产生不利影响，需要更多的考虑；（3） 相当一部分映射的小部件在外观上显示不同的不一致内容，这需要更复杂的视觉算法。,测试迁移，GUI测试，基于视觉的GUI分析,,,
VX5AHKIY,2023,https://doi.org/10.1109/ASE56229.2023.00105,ASE 2023,Causality-Aided Trade-Off Analysis for Machine Learning Fairness,"There has been an increasing interest in enhancing the fairness of machine learning (ML). Despite the growing number of fairness-improving methods, we lack a systematic understanding of the trade-offs among factors considered in the ML pipeline when fairness-improving methods are applied. This understanding is essential for developers to make informed decisions regarding the provision of fair ML services. Nonetheless, it is extremely difficult to analyze the trade-offs when there are multiple fairness parameters and other crucial metrics involved, coupled, and even in conflict with one another. This paper uses causality analysis as a principled method for analyzing trade-offs between fairness parameters and other crucial metrics in ML pipelines. To practically and effectively conduct causality analysis, we propose a set of domain-specific optimizations to facilitate accurate causal discovery and a unified, novel interface for trade-off analysis based on well-established causal inference methods. We conduct a comprehensive empirical study using three real-world datasets on a collection of widely-used fairness-improving techniques. Our study obtains actionable suggestions for users and developers of fair ML. We further demonstrate the versatile usage of our approach in selecting the optimal fairness-improving method, paving the way for more ethical and socially responsible AI technologies.","Machine Learning,Causality Analysis,Fairness",机器学习公平性的因果关系辅助权衡分析,人们对增强机器学习（ML）的公平性越来越感兴趣。尽管公平性改进方法的数量越来越多，但当应用公平性改善方法时，我们对ML管道中考虑的因素之间的权衡缺乏系统的了解。这种理解对于开发人员就公平ML服务的提供做出明智的决定至关重要。尽管如此，当涉及多个公平性参数和其他关键指标，相互耦合，甚至相互冲突时，分析权衡是极其困难的。本文使用因果关系分析作为一种原则性方法来分析ML管道中公平性参数和其他关键度量之间的权衡。为了切实有效地进行因果关系分析，我们提出了一组特定领域的优化，以促进准确的因果关系发现，并基于公认的因果推断方法，为权衡分析提供了一个统一、新颖的界面。我们使用三个真实世界的数据集，对广泛使用的公平性改进技术进行了全面的实证研究。我们的研究为公平ML的用户和开发人员提供了可操作的建议。我们进一步证明了我们的方法在选择最佳公平改进方法方面的多用途，为更合乎道德和对社会负责的人工智能技术铺平了道路。,机器学习，因果关系分析，公平,,,
YZW5EY7B,2023,https://doi.org/10.1109/ASE56229.2023.00106,ASE 2023,Perfce: Performance Debugging on Databases with Chaos Engineering-Enhanced Causality Analysis,"Debugging performance anomalies in databases is challenging. Causal inference techniques enable qualitative and quantitative root cause analysis of performance downgrades. Nevertheless, causality analysis is challenging in practice, particularly due to limited observability. Recently, chaos engineering (CE) has been applied to test complex software systems. CE frameworks mutate chaos variables to inject catastrophic events (e.g., network slowdowns) to stress-test these software systems. The systems under chaos stress are then tested (e.g., via differential testing) to check if they retain normal functionality, such as returning correct SQL query outputs even under stress. To date, CE is mainly employed to aid software testing. This paper identifies the novel usage of CE in diagnosing performance anomalies in databases. Our framework, PERFCE, has two phases - offline and online. The offline phase learns statistical models of a database using both passive observations and proactive chaos experiments. The online phase diagnoses the root cause of performance anomalies from both qualitative and quantitative aspects on-the-fly. In evaluation, Perfce outperformed previous works on synthetic datasets and is highly accurate and moderately expensive when analyzing real-world (distributed) databases like MySQL and TiDB.","Performance Debugging,Chaos Engineering,Causality Analysis",Perfce:利用混沌工程增强因果关系分析对数据库进行性能调试,调试数据库中的性能异常是一项挑战。因果推理技术能够对性能降级进行定性和定量的根本原因分析。然而，因果关系分析在实践中具有挑战性，特别是由于可观察性有限。近年来，混沌工程（CE）已被应用于复杂软件系统的测试。CE框架变异混沌变量以注入灾难性事件（例如，网络减速）来对这些软件系统进行压力测试。然后对处于混乱压力下的系统进行测试（例如，通过差分测试），以检查它们是否保持正常功能，例如即使在压力下也返回正确的SQL查询输出。到目前为止，CE主要用于辅助软件测试。本文介绍了CE在诊断数据库性能异常方面的新用途。我们的框架，PERFCE，有两个阶段-离线和在线。离线阶段使用被动观测和主动混沌实验来学习数据库的统计模型。在线阶段从定性和定量两个方面实时诊断性能异常的根本原因。在评估中，Perfce在合成数据集上的表现优于以前的工作，并且在分析MySQL和TiDB等真实世界（分布式）数据库时高度准确，成本适中。,性能调试，混沌工程，因果关系分析,,,
FRT4WCJY,2023,https://doi.org/10.1109/ASE56229.2023.00114,ASE 2023,On the Evaluation of Neural Code Translation: Taxonomy and Benchmark,"In recent years, neural code translation has gained increasing attention. While most of the research focuses on improving model architectures and training processes, we notice that the evaluation process and benchmark for code translation models are severely limited: they primarily treat source code as natural languages and provide a holistic accuracy score while disregarding the full spectrum of model capabilities across different translation types and complexity. In this paper, we present a comprehensive investigation of four state-of-the-art models and analyze in-depth the advantages and limitations of three existing benchmarks. Based on the empirical results, we develop a taxonomy that categorizes code translation tasks into four primary types according to their complexity and knowledge dependence: token level (type 1), syntactic level (type 2), library level (type 3), and algorithm level (type 4). We then conduct a thorough analysis of how existing approaches perform across these four categories. Our findings indicate that while state-of-the-art code translation models excel in type-1 and type-2 translations, they struggle with knowledge-dependent ones such as type-3 and type-4. Existing benchmarks are biased towards trivial translations, such as keyword mapping. To overcome these limitations, we construct G-TransEval, a new benchmark by manually curating type-3 and type-4 translation pairs and unit test cases. Results on our new benchmark suggest that G-TransEval can exhibit more comprehensive and finer-grained capability of code translation models and thus provide a more rigorous evaluation. Our studies also provide more insightful findings and suggestions for future research, such as building type-3 and type-4 training data and ensembling multiple pretraining approaches.","Code Translation,Empirical Study,Benchmark,Evaluation",论神经编码翻译的评价：分类与基准,近年来，神经代码的翻译越来越受到人们的关注。虽然大多数研究都集中在改进模型架构和训练过程上，但我们注意到，代码翻译模型的评估过程和基准受到严重限制：它们主要将源代码视为自然语言，并提供整体的准确性分数，而忽略了不同翻译类型和复杂性的全方位模型能力。在本文中，我们对四个最先进的模型进行了全面的研究，并深入分析了三个现有基准的优势和局限性。基于经验结果，我们开发了一种分类法，根据代码翻译任务的复杂性和知识依赖性将其分为四种主要类型：标记级别（类型1）、句法级别（类型2）、库级别（类型3）和算法级别（类型4）。然后，我们对现有方法在这四个类别中的表现进行了彻底的分析。我们的研究结果表明，虽然最先进的代码翻译模型在类型1和类型2的翻译中表现出色，但它们在类型3和类型4等依赖知识的翻译中却举步维艰。现有的基准测试偏向于琐碎的翻译，例如关键字映射。为了克服这些限制，我们通过手动管理类型3和类型4的翻译对和单元测试用例，构建了G-TransEval，这是一个新的基准。我们新基准测试的结果表明，G-TransEval可以展示出更全面、更细粒度的代码翻译模型能力，从而提供更严格的评估。我们的研究还为未来的研究提供了更具洞察力的发现和建议，例如构建3型和4型训练数据，以及整合多种预训练方法。,代码翻译，实证研究，基准，评估,,,
FSCVL8ZS,2023,https://doi.org/10.1109/ASE56229.2023.00088,ASE 2023,Revealing Performance Issues in Server-Side WebAssembly Runtimes Via Differential Testing,"WebAssembly (Wasm) is a bytecode format originally serving as a compilation target for Web applications. It has recently been used increasingly on the server side, e.g., providing a safer, faster, and more portable alternative to Linux containers. With the popularity of server-side Wasm applications, it is essential to study performance issues (i.e., abnormal latency) in Wasm runtimes, as they may cause a significant impact on server-side applications. However, there is still a lack of attention to performance issues in server-side Wasm runtimes. In this paper, we design a novel differential testing approach WarpDiff to identify performance issues in server-side Wasm runtimes. The key insight is that in normal cases, the execution time of the same test case on different Wasm runtimes should follow an oracle ratio. We identify abnormal cases where the execution time ratio significantly deviates from the oracle ratio and subsequently locate the Wasm runtimes that cause the performance issues. We apply WarpDiff to test five popular server-side Wasm runtimes using 123 test cases from the LLVM test suite and demonstrate the top 10 abnormal cases we identified. We further conduct an in-depth analysis of these abnormal cases and summarize seven performance issues, all of which have been confirmed by the developers. We hope our work can inspire future investigation on improving Wasm runtime implementation and thus promoting the development of server-side Wasm applications.","WebAssembly,performance issues,differential testing",通过差异测试揭示服务器端WebAssembly运行时的性能问题,WebAssembly（Wasm）是一种字节码格式，最初用作Web应用程序的编译目标。它最近越来越多地用于服务器端，例如，为Linux容器提供了一种更安全、更快、更便携的替代方案。随着服务器端Wasm应用程序的普及，研究Wasm运行时的性能问题（即异常延迟）至关重要，因为它们可能会对服务器端应用程序造成重大影响。然而，服务器端Wasm运行时的性能问题仍然缺乏关注。在本文中，我们设计了一种新的差分测试方法WarpDiff来识别服务器端Wasm运行时的性能问题。关键的见解是，在正常情况下，同一测试用例在不同Wasm运行时上的执行时间应该遵循oracle比率。我们确定了执行时间比率显著偏离oracle比率的异常情况，并随后定位了导致性能问题的Wasm运行时。我们使用LLVM测试套件中的123个测试案例，将WarpDiff应用于测试五个流行的服务器端Wasm运行时，并展示了我们发现的前10个异常案例。我们进一步对这些异常情况进行了深入分析，总结出7个性能问题，这些问题都得到了开发人员的确认。我们希望我们的工作能够启发未来对改进Wasm运行时实现的研究，从而促进服务器端Wasm应用程序的开发。,WebAssembly，性能问题，差异测试,,,
A4LNP3BF,2023,https://doi.org/10.1109/ASE56229.2023.00086,ASE 2023,Effective Concurrency Testing for Go via Directional Primitive-Constrained Interleaving Exploration,"The Go language (Go/Golang) has been attracting increasing attention from the industry over recent years due to its strong concurrency support and ease of deployment. This programming language encourages developers to use channel-based concurrency, which simplifies the development of concurrent programs. Unfortunately, it also introduces new concurrency problems that differ from those caused by the mechanism of shared memory concurrency. However, there are only few works that aim to detect such Go-specific concurrency issues. Even state-of-the-art testing tools will miss critical concurrent bugs that require fine-grained and effective interleaving exploration. This paper presents GoPie, a novel testing approach for detecting Go concurrency bugs through primitive-constrained interleaving exploration. GoPie utilizes execution histories to identify new interleavings instead of relying on exhaustive exploration or random scheduling. To evaluate its performance, we applied GoPie to existing benchmarks and large-scale open-source projects. Results show that GoPie can effectively explore concurrent interleavings and detect significantly more bugs in the benchmark. Furthermore, it uncovered 11 unique previously unknown concurrent bugs, and 9 of which have been confirmed.","Go,Concurrency Testing,Fuzzing",基于定向基元约束交错探索的围棋有效并发测试,Go语言（Go/Golang）由于其强大的并发支持和易于部署的特点，近年来越来越受到业界的关注。这种编程语言鼓励开发人员使用基于通道的并发，这简化了并发程序的开发。不幸的是，它还引入了与共享内存并发机制不同的新并发问题。然而，只有少数工作旨在检测这种Go特定的并发问题。即使是最先进的测试工具也会错过关键的并发错误，这些错误需要细粒度和有效的交错探索。本文提出了GoPie，这是一种通过基元约束交错探索来检测Go并发错误的新测试方法。GoPie利用执行历史来识别新的穿插，而不是依赖于详尽的探索或随机调度。为了评估其性能，我们将GoPie应用于现有的基准测试和大型开源项目。结果表明，GoPie可以有效地探索并发穿插，并在基准测试中检测到更多的bug。此外，它还发现了11个以前未知的独特并发错误，其中9个已被确认。,Go，并发测试，模糊化,,,
NISNBC7B,2023,https://doi.org/10.1109/ASE56229.2023.00138,ASE 2023,Twin Graph-Based Anomaly Detection via Attentive Multi-Modal Learning for Microservice System,"Microservice architecture has sprung up over recent years for managing enterprise applications, due to its ability to independently deploy and scale services. Despite its benefits, ensuring the reliability and safety of a microservice system remains highly challenging. Existing anomaly detection algorithms based on a single data modality (i.e., metrics, logs, or traces) fail to fully account for the complex correlations and interactions between different modalities, leading to false negatives and false alarms, whereas incorporating more data modalities can offer opportunities for further performance gain. As a fresh attempt, we propose in this paper a semi-supervised graph-based anomaly detection method, MSTGAD, which seamlessly integrates all available data modalities via attentive multi-modal learning. First, we extract and normalize features from the three modalities, and further integrate them using a graph, namely MST (microservice system twin) graph, where each node represents a service instance and the edge indicates the scheduling relationship between different service instances. The MST graph provides a virtual representation of the status and scheduling relationships among service instances of a real-world microservice system. Second, we construct a transformer-based neural network with both spatial and temporal attention mechanisms to model the inter-correlations between different modalities and temporal dependencies between the data points. This enables us to detect anomalies automatically and accurately in real-time. Extensive experiments on two real-world datasets verify the effectiveness of our proposed MSTGAD method, achieving competitive performance against state-of-the-art approaches, with a 0.961 F1-score and an average increase of 4.85%. The source code of MST-GAD is publicly available at https://github.com/ant-research/microservice_system_twin_graph_based_anomaly_detection.","anomaly detection,multi-modal learning,system twin graph",基于双模学习的微服务系统异常检测,近年来，由于能够独立部署和扩展服务，微服务架构在管理企业应用程序方面如雨后春笋般涌现。尽管有好处，但确保微服务系统的可靠性和安全性仍然极具挑战性。现有的基于单一数据模式（即度量、日志或跟踪）的异常检测算法无法充分考虑不同模式之间的复杂相关性和交互作用，导致假阴性和假警报，而结合更多的数据模式可以提供进一步提高性能的机会。作为一种新的尝试，我们在本文中提出了一种基于半监督图的异常检测方法MSTGAD，该方法通过专注的多模态学习无缝集成所有可用的数据模态。首先，我们从这三种模式中提取并归一化特征，并使用一个图将它们进一步集成，即MST（微服务系统双胞胎）图，其中每个节点表示一个服务实例，边缘表示不同服务实例之间的调度关系。MST图提供了真实世界微服务系统的服务实例之间的状态和调度关系的虚拟表示。其次，我们构建了一个具有空间和时间注意力机制的基于变换器的神经网络，以对不同模态之间的相互关联和数据点之间的时间依赖性进行建模。这使我们能够实时自动准确地检测异常。在两个真实世界的数据集上进行的大量实验验证了我们提出的MSTGAD方法的有效性，与最先进的方法相比实现了竞争性能，F1得分为0.961，平均增长4.85%。MST-GAD的源代码可在https://github.com/ant-research/microservice_system_twin_graph_based_anomaly_detection.,异常检测，多模态学习，系统双图,,,
7XURVUKU,2023,https://doi.org/10.1109/ASE56229.2023.00075,ASE 2023,"Let's Chat to Find the APIs: Connecting Human, LLM and Knowledge Graph through AI Chain","API recommendation methods have evolved from literal and semantic keyword matching to query expansion and query clarification. The latest query clarification method is knowledge graph (KG)-based, but limitations include out-of-vocabulary (OOV) failures and rigid question templates. To address these limitations, we propose a novel knowledge-guided query clarification approach for API recommendation that leverages a large language model (LLM) guided by KG. We utilize the LLM as a neural knowledge base to overcome OOV failures, generating fluent and appropriate clarification questions and options. We also leverage the structured API knowledge and entity relationships stored in the KG to filter out noise, and transfer the optimal clarification path from KG to the LLM, increasing the efficiency of the clarification process. Our approach is designed as an AI chain that consists of five steps, each handled by a separate LLM call, to improve accuracy, efficiency, and fluency for query clarification in API recommendation. We verify the usefulness of each unit in our AI chain, which all received high scores close to a perfect 5. When compared to the baselines, our approach shows a significant improvement in MRR, with a maximum increase of 63.9% higher when the query statement is covered in KG and 37.2% when it is not. Ablation experiments reveal that the guidance of knowledge in the KG and the knowledge-guided pathfinding strategy are crucial for our approach's performance, resulting in a 19.0% and 22.2% increase in MAP, respectively. Our approach demonstrates a way to bridge the gap between KG and LLM, effectively compensating for the strengths and weaknesses of both.","API recommendation,query clarification,knowledge graph,large language model,out-of-vocabulary",让我们聊天寻找API：通过AI链连接人类、LLM和知识图,API推荐方法已经从字面和语义关键字匹配发展到查询扩展和查询澄清。最新的查询澄清方法是基于知识图（KG）的，但其局限性包括词汇表外（OOV）故障和僵化的问题模板。为了解决这些限制，我们为API推荐提出了一种新的知识引导查询澄清方法，该方法利用了KG指导的大型语言模型（LLM）。我们将LLM作为神经知识库来克服OOV失败，生成流畅且适当的澄清问题和选项。我们还利用存储在KG中的结构化API知识和实体关系来过滤噪声，并将最佳澄清路径从KG传递到LLM，从而提高澄清过程的效率。我们的方法被设计为一个AI链，由五个步骤组成，每个步骤由一个单独的LLM调用处理，以提高API建议中查询澄清的准确性、效率和流畅性。我们验证了人工智能链中每个单元的有用性，这些单元都获得了接近完美5的高分。与基线相比，我们的方法显示出MRR的显著提高，当查询语句包含在KG中时，最大提高了63.9%，当不包含时，最大增加了37.2%。消融实验表明，KG中的知识引导和知识引导的寻路策略对我们的方法的性能至关重要，分别导致MAP增加19.0%和22.2%。我们的方法展示了一种弥合KG和LLM之间差距的方法，有效地弥补了两者的优势和劣势。,API推荐，查询澄清，知识图谱，大型语言模型，词汇表外,,,
IC7FB5TU,2023,https://doi.org/10.1109/ASE56229.2023.00181,ASE 2023,An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair,"The advent of large language models (LLMs) has opened up new opportunities for automated program repair (APR). In particular, some recent studies have explored how to leverage large language models of code (LLMCs) for program repair tasks and show promising results. However, most of them adopt the zero/few-shot learning paradigm for APR, which directly use LLMCs to generate the possibly correct code given its surrounding context. Though effective, the repair capabilities of LLMCs based on the fine-tuning paradigm have yet to be extensively explored. Also, it remains unknown whether LLMCs have the potential to repair more complicated bugs (e.g., multi-hunk bugs). To fill the gap, in this work, we conduct a comprehensive study on the program repair capability of LLMCs in the fine-tuning paradigm. We select 5 popular LLMCs with representative pre-training architectures, including CodeBERT, GraphCode-BERT, PLBART, CodeT5, and UniX coder. We consider 3 typical program repair scenarios (i.e., bugs, vulnerabilities, and errors) involving 3 programming languages (i.e., Java, $\mathrm{C}/\mathrm{C}++$, and JavaScript). Notably, we take both single-hunk and multi-hunk bugs/vulnerabilities into account. We then fine-tune them on widely-used datasets and compare them with existing state-of-the-art APR tools. We also investigate the impact of different design choices, which include code abstractions, code representations, and model evaluation metrics. Our experimental results show that LLMCs in the fine-tuning paradigm can significantly outperform previous state-of-the-art APR tools. Through in-depth analysis, we provide insights into choosing appropriate strategies to guide LLMCs for better performance. Lastly, we reveal several limitations of LLMCs for APR and make suggestions for future research on LLMC-based APR.","Automated Program Repair,Large Language Models of Code,Neural Machine Translation,Fine-Tuning",用于程序自动修复的代码大语言模型微调的实证研究,大型语言模型（LLM）的出现为自动程序修复（APR）开辟了新的机会。特别是，最近的一些研究探索了如何利用大型语言代码模型（LLMC）进行程序修复任务，并显示出了有希望的结果。然而，它们中的大多数都采用了APR的零/少镜头学习范式，该范式直接使用LLMC在给定其周围环境的情况下生成可能正确的代码。尽管有效，但基于微调范式的LLMC的修复能力仍有待广泛探索。此外，LLMC是否有潜力修复更复杂的bug（例如，多块bug）仍然未知。为了填补这一空白，在这项工作中，我们在微调范式中对LLMC的程序修复能力进行了全面的研究。我们选择了5种具有代表性预训练架构的流行LLMC，包括CodeBERT、GraphCode-BERT、PLBART、CodeT5和UniX编码器。我们考虑了3种典型的程序修复场景（即bug、漏洞和错误），涉及3种编程语言（即Java、$\mathrm{C}/\mathrm{C}++$和JavaScript）。值得注意的是，我们同时考虑了单块和多块错误/漏洞。然后，我们在广泛使用的数据集上对它们进行微调，并将它们与现有的最先进的APR工具进行比较。我们还研究了不同设计选择的影响，包括代码抽象、代码表示和模型评估指标。我们的实验结果表明，微调范式中的LLMC可以显著优于以前最先进的APR工具。通过深入分析，我们提供了选择适当策略的见解，以指导LLMC获得更好的绩效。最后，我们揭示了LLMC对APR的几个局限性，并对未来基于LLMC的APR研究提出了建议。,自动程序修复，代码的大型语言模型，神经机器翻译，微调,,,
FKTTGPAQ,2023,https://doi.org/10.1109/ASE56229.2023.00123,ASE 2023,RocketHA: A High Availability Design Paradigm for Distributed Log-Based Storage System,"As a team from Alibaba Cloud, we have developed and open-sourced RocketMQ, a cloud-native “messaging, eventing, streaming” real-time data processing platform that covers cloud-edge-device collaboration scenarios. During the development of RocketMQ, we also formulated RocketHA, a log-based storage high availability design theory that provides a robust solution for distributed log storage software used in industrial applications. RocketHA comprises six fundamental components that enable automatic cluster recovery from failures such as crashes and partitions. This design paradigm has been successfully implemented in the open-source RocketMQ. Our evaluation demonstrates that RocketHA ensures high availability, fast recovery, high throughput, and data loss prevention. We hope that RocketHA will inspire and guide the development of high-availability solutions for all log-based storage systems.","high-availability,log-based software,automatic recovery",RocketHA：分布式日志存储系统的高可用性设计范式,作为阿里云的一个团队，我们开发并开源了RocketMQ，这是一个云原生的“消息、事件、流”实时数据处理平台，涵盖云边缘设备协作场景。在RocketMQ的开发过程中，我们还制定了RocketHA，这是一种基于日志的存储高可用性设计理论，为工业应用中使用的分布式日志存储软件提供了强大的解决方案。RocketHA包括六个基本组件，它们能够从崩溃和分区等故障中自动恢复集群。这种设计范式已经在开源RocketMQ中成功实现。我们的评估表明，RocketHA可确保高可用性、快速恢复、高吞吐量和数据丢失预防。我们希望RocketHA将激励和指导所有基于日志的存储系统的高可用性解决方案的开发。,高可用性，基于日志的软件，自动恢复,,,
BPLNA7IV,2023,https://doi.org/10.1109/ASE56229.2023.00133,ASE 2023,AutoLog: A Log Sequence Synthesis Framework for Anomaly Detection,"The rapid progress of modern computing systems has led to a growing interest in informative run-time logs. Various log-based anomaly detection techniques have been proposed to ensure software reliability. However, their implementation in the industry has been limited due to the lack of high-quality public log resources as training datasets. While some log datasets are available for anomaly detection, they suffer from limitations in (1) comprehensiveness of log events; (2) scalability over diverse systems; and (3) flexibility of log utility. To address these limitations, we propose AUTOLOG, the first automated log generation methodology for anomaly detection. AUTOLOG uses program analysis to generate runtime log sequences without actually running the system. AUTOLOG starts with probing comprehensive logging statements associated with the call graphs of an application. Then, it constructs execution graphs for each method after pruning the call graphs to find log-related execution paths in a scalable manner. Finally, AUTOLOG propagates the anomaly label to each acquired execution path based on human knowledge. It generates flexible log sequences by walking along the log execution paths with controllable parameters. Experiments on 50 popular Java projects show that AUTOLOG acquires significantly more (9x-58x) log events than existing log datasets from the same system, and generates log messages much faster (15x) with a single machine than existing passive data collection approaches. AUTOLOG also provides hyper-parameters to adjust the data size, anomaly rate, and component indicator for simulating different real-world scenarios. We further demonstrate AUTOLOG's practicality by showing that AUTOLOG enables log-based anomaly detectors to achieve better performance (1.93%) compared to existing log datasets. We hope AUTOLOG can facilitate the benchmarking and adoption of automated log analysis techniques.","Legged locomotion,Training,Industries,Java,Runtime,Scalability,Software algorithms",AutoLog:一种用于异常检测的日志序列合成框架,现代计算系统的快速发展引起了人们对信息运行时日志的兴趣。已经提出了各种基于日志的异常检测技术来确保软件的可靠性。然而，由于缺乏高质量的公共日志资源作为训练数据集，它们在行业中的实施受到了限制。虽然一些日志数据集可用于异常检测，但它们在以下方面受到限制：（1）日志事件的全面性；（2） 在不同系统上的可扩展性；（3）日志实用程序的灵活性。为了解决这些限制，我们提出了AUTOLOG，这是第一种用于异常检测的自动日志生成方法。AUTOLOG使用程序分析生成运行时日志序列，而无需实际运行系统。AUTOLOG从探测与应用程序的调用图相关联的全面日志记录语句开始。然后，它在修剪调用图之后为每个方法构建执行图，以可扩展的方式找到与日志相关的执行路径。最后，AUTOLOG基于人类知识将异常标签传播到每个获取的执行路径。它通过在具有可控参数的日志执行路径上行走来生成灵活的日志序列。在50个流行的Java项目上的实验表明，AUTOLOG从同一系统中获取的日志事件比现有的日志数据集多得多（9x-58x），并且用一台机器生成日志消息的速度比现有的被动数据收集方法快得多（15x）。AUTOLOG还提供超参数来调整数据大小、异常率和组件指标，以模拟不同的真实世界场景。我们进一步证明了AUTOLOG的实用性，表明与现有的日志数据集相比，AUTOLOG能够使基于日志的异常检测器获得更好的性能（1.93%）。我们希望AUTOLOG能够促进自动化日志分析技术的基准测试和采用。,步行，训练，工业，Java，运行时，可伸缩性，软件算法,,,
ZEN6FGW8,2023,https://doi.org/10.1109/ASE56229.2023.00034,ASE 2023,NaturalFuzz: Natural Input Generation for Big Data Analytics,"Fuzzing applies input mutations iteratively with the only goal of finding more bugs, resulting in synthetic tests that tend to lack realism. Big data analytics are expected to ingest real-world data as input. Therefore, when synthetic test data are not easily comprehensible, they are less likely to facilitate the downstream task of fixing errors. Our position is that fuzzing in this domain must achieve both high naturalness and high code coverage. We propose a new natural synthetic test generation tool for big data analytics, called NaturalFuzz. It generates both unstructured, semi-structured, and structured data with corresponding semantics such as ‘zipcode’ and ‘age.’ The key insights behind NaturalFuzz are two-fold. First, though existing test data may be small and lack coverage, we can grow this data to increase code coverage. Second, we can strategically mix constituent parts across different rows and columns to construct new realistic synthetic data by leveraging fine-grained data provenance. On commercial big data application benchmarks, NaturalFuzz achieves an additional 19.9% coverage and detects 1.9× more faults than a machine learning-based synthetic data generator (SDV) when generating comparably sized inputs. This is because an ML-based synthetic data generator does not consider which code branches are exercised by which input rows from which tables, while NaturalFuzz is able to select input rows that have a high potential to increase code coverage and mutate the selected data towards unseen, new program behavior. NaturalFuzz's test data is more realistic than the test data generated by two baseline fuzzers (BigFuzz and Jazzer), while increasing code coverage and fault detection potential. NaturalFuzz is the first fuzzing methodology with three benefits: (1) exclusively generate natural inputs, (2) fuzz multiple input sources simultaneously, and (3) find deeper semantics faults.","Fuzzing,Testing,Data Analytics",NaturalFuzz：大数据分析的自然输入生成,模糊化迭代地应用输入突变，唯一的目标是发现更多的错误，导致合成测试往往缺乏真实性。大数据分析预计将吸收真实世界的数据作为输入。因此，当合成测试数据不容易理解时，它们不太可能促进修复错误的下游任务。我们的立场是，该领域中的模糊处理必须同时实现高自然度和高代码覆盖率。我们提出了一种新的用于大数据分析的自然合成测试生成工具，称为NaturalFuzz。它生成具有相应语义的非结构化、半结构化和结构化数据，如“zipcode”和“age”NaturalFuzz背后的关键见解有两个方面。首先，尽管现有的测试数据可能很小并且缺乏覆盖率，但我们可以增加这些数据以增加代码覆盖率。其次，我们可以战略性地混合不同行和列的组成部分，通过利用细粒度的数据来源来构建新的真实合成数据。在商业大数据应用基准测试中，NaturalFuzz在生成大小相当的输入时，实现了19.9%的额外覆盖率，检测到的故障比基于机器学习的合成数据生成器（SDV）多1.9倍。这是因为基于ML的合成数据生成器不考虑哪些代码分支由哪些表中的哪些输入行执行，而NaturalFuzz能够选择具有增加代码覆盖率的高潜力的输入行，并将所选数据变异为看不见的新程序行为。NaturalFuzz的测试数据比两个基线模糊器（BigFuzz和Jazzer）生成的测试数据更真实，同时增加了代码覆盖率和故障检测潜力。NaturalFuzz是第一种模糊化方法，有三个优点：（1）只生成自然输入，（2）同时模糊多个输入源，（3）发现更深层次的语义错误。,引信，测试，数据分析,,,
R63GFVVD,2023,https://doi.org/10.1109/ASE56229.2023.00167,ASE 2023,Fixing Privilege Escalations in Cloud Access Control with MaxSAT and Graph Neural Networks,"Identity and Access Management (IAM) is an access control service employed within cloud platforms. Customers must configure IAM to establish secure access control rules for their cloud organizations. However, IAM misconfigurations can be exploited to conduct Privilege Escalation (PE) attacks, resulting in significant financial losses. Consequently, addressing these PEs is crucial for improving security assurance for cloud customers. Nevertheless, the area of repairing IAM PEs due to IAM mis-configurations is relatively underexplored. To our knowledge, the only existing IAM repair tool called IAM-Deescalate focuses on a limited number of IAM PE patterns, indicating the potential for further enhancements. We propose a novel IAM Privilege Escalation Repair Engine called IAMPERE that efficiently generates an approximately minimal patch for repairing a broader range of IAM PEs. To achieve this, we first formulate the IAM repair problem into a MaxSAT problem. Despite the remarkable success of modern MaxSAT solvers, their scalability for solving complex repair problems remains a challenge due to the state explosion. To improve scalability, we employ deep learning to prune the search space. Specifically, we apply a carefully designed GNN model to generate an intermediate patch that is relatively small, but not necessarily minimal. We then apply a MaxSAT solver to search for a minimum repair within the space defined by the intermediate patch, as the final approximately minimum patch. Experimental results on both synthesized and real-world IAM misconfigurations show that, compared to IAM-Deescalate, IAMPERE repairs a significantly larger number of IAM misconfigurations with markedly smaller patch sizes.","Cloud Access Control,MaxSAT,Graph Neural Networks",用MaxSAT和图神经网络修复云访问控制中的权限提升,身份和访问管理（IAM）是一种在云平台中使用的访问控制服务。客户必须配置IAM以为其云组织建立安全访问控制规则。然而，IAM的错误配置可能会被用来进行特权升级（PE）攻击，从而导致重大的财务损失。因此，解决这些PE对于提高云客户的安全保障至关重要。尽管如此，由于IAM错误配置而导致的IAM PE修复领域的开发相对不足。据我们所知，目前唯一一个名为IAM Deescalate的IAM修复工具专注于数量有限的IAM PE模式，这表明了进一步增强的潜力。我们提出了一种新的IAM特权升级修复引擎，称为IAMPERE，它可以有效地生成近似最小的补丁，用于修复更广泛的IAM PE。为了实现这一点，我们首先将IAM修复问题公式化为MaxSAT问题。尽管现代MaxSAT解决方案取得了显著的成功，但由于状态爆炸，其解决复杂维修问题的可扩展性仍然是一个挑战。为了提高可扩展性，我们使用深度学习来修剪搜索空间。具体来说，我们应用精心设计的GNN模型来生成一个相对较小但不一定最小的中间补丁。然后，我们应用MaxSAT求解器在由中间补丁定义的空间内搜索最小修复，作为最终的近似最小补丁。对合成和真实世界IAM错误配置的实验结果表明，与IAM Deescalate相比，IAMPERE以明显较小的补丁大小修复了大量IAM错误配置。,云访问控制，MaxSAT，图神经网络,,,
AGZMX7SU,2023,https://doi.org/10.1109/ASE56229.2023.00128,ASE 2023,Predicting Compilation Resources for Adaptive Build in an Industrial Setting,"Development teams in large companies often maintain a huge codebase whose build time can be painfully long in a single machine. To reduce the build time, tools such as Bazel and distcc are used to build the code base in a distributed way. However, in the process of distributed build, certain remote slave machines can crash due to two types of errors: Out Of Memory (OOM) and Deadline Exceeded (DE) errors. These crashes lead to time-consuming rebuilds, as suffered by the WeiXin Group (WXG) of Tencent Inc. (the vendor of WeChat, a highly popular mobile app in China). Aiming to prevent these two types of errors, in this paper, we propose a new approach named PCRLINEAR, which predicts the memory and time requirements of the given C++ file, allowing the underlying distributed build system to schedule compilation resources adaptively according to the prediction results. Our experiments show that PCRLINEAR reduces the number of OOM and DE errors from 5% to 0.2% and, at the same time, achieves substantial build-performance improvement of 30% on average.","distributed compilation,linear regression,compilation resources",预测工业环境中适应性建设的编译资源,大公司的开发团队经常维护一个庞大的代码库，其构建时间在一台机器中可能非常长。为了减少构建时间，使用Bazel和distcc等工具以分布式方式构建代码库。然而，在分布式构建过程中，某些远程从机可能会由于两种类型的错误而崩溃：内存不足（OOM）和超过最后期限（DE）错误。这些崩溃导致了耗时的重建，腾讯股份有限公司旗下的微信集团（WXG）就是如此。为了防止这两种类型的错误，本文提出了一种新的方法PCRLINER，它预测给定C++文件的内存和时间需求，允许底层分布式构建系统根据预测结果自适应地调度编译资源。我们的实验表明，PCRLINER将OOM和DE错误的数量从5%减少到0.2%，同时实现了平均30%的显著构建性能改进。,分布式编译，线性回归，编译资源,,,
FCCUJRG8,2023,https://doi.org/10.1109/ASE56229.2023.00165,ASE 2023,Identify and Update Test Cases When Production Code Changes: A Transformer-Based Approach,"Software testing is one of the most essential parts of the software lifecycle and requires a substantial amount of time and effort. During the software evolution, test cases should co-evolve with the production code. However, the co-evolution of test cases often fails due to tight project schedules and other reasons. Obsolete test cases improve the cost of software maintenance and may fail to reveal faults and even lead to future bugs. Therefore, it is essential to detect and update these obsolete test cases in time. In this paper, we propose a novel approach Ceprot (Co-Evolution of Production-Test Code) to identify outdated test cases and update them automatically according to changes in the production code. Ceprot consists of two stages, i.e., obsolete test identification and updating. Specifically, given a production code change and a corresponding test case, Ceprot first identifies whether the test case should be updated. If the test is identified as obsolete, Ceprot will update it to a new version of test case. To evaluate the effectiveness of the two stages, we construct two datasets. Our dataset focuses on method-level production code changes and updates on their obsolete test cases. The experimental results show that Ceprot can effectively identify obsolete test cases with precision and recall of 98.3% and 90.0%, respectively. In addition, test cases generated by Ceprot are identical to the ground truth for 12.3% of samples that are identified as obsolete by Ceprot. We also conduct dynamic evaluation and human evaluation to measure the effectiveness of the updated test cases by Ceprot. 48.0% of updated test cases can be compiled and the average coverage of updated cases is 34.2% which achieves 89% coverage improvement over the obsolete tests. We believe that this study can motivate the co-evolution of production and test code.","Test code maintenance,Mining Software Repositories,Software Evolution",当生产代码更改时识别和更新测试用例：一种基于Transformer的方法,软件测试是软件生命周期中最重要的部分之一，需要大量的时间和精力。在软件进化过程中，测试用例应该与生产代码共同进化。然而，由于项目进度紧张和其他原因，测试用例的协同进化往往会失败。过时的测试用例会提高软件维护成本，并且可能无法揭示故障，甚至导致未来的错误。因此，及时检测和更新这些过时的测试用例是至关重要的。在本文中，我们提出了一种新的方法Ceprot（生产测试代码的协同进化）来识别过时的测试用例，并根据生产代码的变化自动更新它们。Ceprot由两个阶段组成，即过时的测试识别和更新。具体来说，给定生产代码更改和相应的测试用例，Ceprot首先确定是否应该更新测试用例。如果测试被确定为过时，Ceprot会将其更新为测试用例的新版本。为了评估这两个阶段的有效性，我们构建了两个数据集。我们的数据集中在方法级别的生产代码更改和过时测试用例的更新上。实验结果表明，Ceprot能够有效识别过时的测试用例，准确率和召回率分别为98.3%和90.0%。此外，Ceprot生成的测试用例与Ceprot确定为过时的12.3%样本的基本事实相同。我们还进行了动态评估和人为评估，以衡量Ceprot更新的测试用例的有效性。48.0%的更新测试用例可以被编译，更新用例的平均覆盖率为34.2%，与过时的测试相比，覆盖率提高了89%。我们相信，这项研究可以激励生产代码和测试代码的共同进化。,测试代码维护，挖掘软件存储库，软件进化,,,
IG3WNZMW,2023,https://doi.org/10.1109/ASE56229.2023.00156,ASE 2023,ATOM: Automated Black-Box Testing of Multi-Label Image Classification Systems,"Multi-label Image Classification Systems (MICSs) developed based on Deep Neural Networks (DNNs) are extensively used in people's daily life. Currently, although there are a variety of approaches to test DNN-based systems, they typically rely on the internals of DNNs to design test cases, and do not take the core specification of MICS (i.e., correctly recognizing multiple objects in a given image) into account. In this paper, we propose ATOM, an automated and systematic black-box testing framework for testing MICS. Specifically, ATOM exploits the label combination as the testing adequacy criteria, hoping to systematically examine the impact of correlations between a fixed number of labels on the classification ability of MICS. Then, ATOM leverages image search engine and natural language processing to find test images that are not only common to the real-world, but also relevant to target label combinations. Finally, ATOM combines metamorphic testing and label information to realize test oracle identification, based on which the ability of MICS in classifying different label combinations is evaluated. To evaluate the effectiveness of ATOM, we have performed experiments on two popular datasets of MICS, VOC and COCO (each with five state-of-the-art DNN models), and one real-world photo tagging application from our industrial partner. The experimental results reveal that the performance of current DNN-based MICSs remains less satisfactory even in recognizing correlations between only two labels, as ATOM triggers a total number of 6,049 such label combination related errors for all MICSs studied. In particular, ATOM reports 587 error-revealing images for the industrial MICS, in which 92% of them are confirmed by the developers.","Multi-label Image Classification Testing,Black-box Testing,Metamorphic Testing",ATOM：多标签图像分类系统的自动黑匣子测试,基于深度神经网络（DNN）开发的多标签图像分类系统（MICS）在人们的日常生活中得到了广泛的应用。目前，尽管有多种方法来测试基于DNN的系统，但它们通常依赖于DNN的内部结构来设计测试用例，并且没有考虑MICS的核心规范（即，正确识别给定图像中的多个对象）。在本文中，我们提出了ATOM，这是一个用于测试MICS的自动化和系统的黑盒测试框架。具体而言，ATOM利用标签组合作为测试充分性标准，希望系统地研究固定数量标签之间的相关性对MICS分类能力的影响。然后，ATOM利用图像搜索引擎和自然语言处理来找到不仅在现实世界中常见，而且与目标标签组合相关的测试图像。最后，ATOM将变形测试和标签信息相结合，实现了测试预言机识别，并在此基础上评估了MICS对不同标签组合进行分类的能力。为了评估ATOM的有效性，我们在MICS、VOC和COCO的两个流行数据集上进行了实验（每个数据集都有五个最先进的DNN模型），以及我们的工业合作伙伴的一个真实世界的照片标记应用程序。实验结果表明，即使在仅识别两个标签之间的相关性时，当前基于DNN的MICS的性能仍然不太令人满意，因为ATOM对所研究的所有MICS总共触发了6049个这样的标签组合相关错误。特别是，ATOM报告了587张工业MICS的错误揭示图像，其中92%得到了开发人员的确认。,多标签图像分类测试，黑盒测试，变形测试,,,
RJI2522P,2023,https://doi.org/10.1109/ASE56229.2023.00041,ASE 2023,The MAP Metric in Information Retrieval Fault Localization,"The MAP (Mean Average Precision) metric is one of the most popular performance metrics in the field of Information Retrieval Fault Localization (IRFL). However, there are problematic implementations of this MAP metric used in IRFL research. These implementations deviate from the text book definitions of MAP, rendering the metric sensitive to the truncation of retrieval results and inaccuracies and impurities of the used datasets. The application of such a deviating metric can lead to performance overestimation. This can pose a problem for comparability, transferability, and validity of IRFL performance results. In this paper, we discuss the definition and mathematical properties of MAP and common deviations and pitfalls in its implementation. We investigate and discuss the conditions enabling such overestimation: the truncation of retrieval results in combination with ground truths spanning multiple files and improper handling of undefined AP results. We demonstrate the overestimation effects using the Bench4BL benchmark and five well known IRFL techniques. Our results indicate that a flawed implementation of the MAP metric can lead to an overestimation of the IRFL performance, in extreme cases by up to 70 %. We argue for a strict adherence to the text book version of MAP with the extension of undefined AP values to be set to 0 for all IRFL experiments. We hope that this work will help to improve comparability and transferability in IRFL research.","information retrieval,fault localization,MAP,mean average precision",信息检索故障定位中的MAP度量,MAP（Mean Average Precision）度量是信息检索故障定位（IRFL）领域中最流行的性能度量之一。然而，在IRFL研究中使用的这种MAP度量的实现存在问题。这些实现偏离了MAP的教科书定义，使得度量对检索结果的截断以及所用数据集的不准确和杂质敏感。这种偏离度量的应用可能会导致对性能的高估。这可能会给IRFL绩效结果的可比性、可转移性和有效性带来问题。在本文中，我们讨论了MAP的定义和数学性质，以及它在实现中的常见偏差和陷阱。我们调查并讨论了导致这种高估的条件：检索结果的截断与跨越多个文件的基本事实相结合，以及对未定义的AP结果的不当处理。我们使用Bench4BL基准和五种众所周知的IRFL技术来证明高估效应。我们的结果表明，MAP度量的错误实现可能导致对IRFL性能的高估，在极端情况下高达70%。我们主张严格遵守MAP的教科书版本，并将所有IRFL实验的未定义AP值扩展为0。我们希望这项工作将有助于提高IRFL研究的可比性和可转移性。,信息检索，故障定位，MAP，平均精度,,,
Z2CGINVL,2023,https://doi.org/10.1109/ASE56229.2023.00135,ASE 2023,An Empirical Study of Malicious Code In PyPI Ecosystem,"PyPI provides a convenient and accessible package management platform to developers, enabling them to quickly implement specific functions and improve work efficiency. However, the rapid development of the PyPI ecosystem has led to a severe problem of malicious package propagation. Malicious developers disguise malicious packages as normal, posing a significant security risk to end-users. To this end, we conducted an empirical study to understand the characteristics and current state of the malicious code lifecycle in the PyPI ecosystem. We first built an automated data collection framework and collated a multi-source malicious code dataset containing 4,669 malicious package files. We preliminarily classified these malicious code into five categories based on malicious behaviour characteristics. Our research found that over 50 % of malicious code exhibits multiple malicious behaviours, with information stealing and command execution being particularly prevalent. In addition, we observed several novel attack vectors and anti-detection techniques. Our analysis revealed that 74.81 % of all malicious packages successfully entered end-user projects through source code installation, thereby increasing security risks. A real-world investigation showed that many reported malicious packages persist in PyPI mirror servers globally, with over 72 % remaining for an extended period after being discovered. Finally, we sketched a portrait of the malicious code lifecycle in the PyPI ecosystem, effectively reflecting the characteristics of malicious code at different stages. We also present some suggested mitigations to improve the security of the Python open-source ecosystem.","PyPI Ecosystem,Package Management,Malicious Code,Mirror Source,Lifecycle Portrait",PyPI生态系统中恶意代码的实证研究,PyPI为开发人员提供了一个方便、可访问的包管理平台，使他们能够快速实现特定功能，提高工作效率。然而，PyPI生态系统的快速发展导致了严重的恶意包传播问题。恶意开发人员将恶意软件包伪装成正常软件包，对最终用户构成重大安全风险。为此，我们进行了一项实证研究，以了解PyPI生态系统中恶意代码生命周期的特征和现状。我们首先构建了一个自动数据收集框架，并整理了一个包含4669个恶意包文件的多源恶意代码数据集。根据恶意行为特征，我们将这些恶意代码初步分为五类。我们的研究发现，超过50%的恶意代码表现出多种恶意行为，其中信息窃取和命令执行尤为普遍。此外，我们还观察到了几种新的攻击载体和反检测技术。我们的分析显示，74.81%的恶意软件包通过源代码安装成功进入最终用户项目，从而增加了安全风险。一项真实世界的调查显示，许多报告的恶意软件包在全球PyPI镜像服务器中持续存在，其中超过72%的软件包在被发现后会长时间保留。最后，我们描绘了PyPI生态系统中恶意代码的生命周期，有效地反映了恶意代码在不同阶段的特征。我们还提出了一些建议的缓解措施，以提高Python开源生态系统的安全性。,PyPI生态系统，包管理，恶意代码，镜像源，生命周期画像,,,
79CV3S9B,2023,https://doi.org/10.1109/ASE56229.2023.00152,ASE 2023,Towards a Formal Framework for Normative Requirements Elicitation,"As software and cyber-physical systems interacting with humans become prevalent in domains such as healthcare, education and customer service, software engineers need to consider normative (i.e., social, legal, ethical, empathetic and cultural) requirements. However, their elicitation is challenging, as they must reflect the often conflicting or redundant views of stakeholders ranging from users and operators to lawyers, ethicists and regulators. To address this challenge, we introduce a tool-supported Formal framework for normaTive requirements elicitation (FormaTive). It allows specification of normative rules for a software system in an intuitive high-level language, and automates: (i) the mapping of the rules to an internal formal representation; (ii) their analysis to identify rule conflicts, redundancies, and concerns; and (iii) the synthesis of feedback enabling users to understand and resolve problems.","formal methods,requirements elicitation,traceability analysis",规范性需求引出的形式化框架,随着与人类互动的软件和网络物理系统在医疗保健、教育和客户服务等领域变得普遍，软件工程师需要考虑规范性（即社会、法律、道德、同理心和文化）要求。然而，它们的启发是具有挑战性的，因为它们必须反映从用户和运营商到律师、伦理学家和监管机构等利益相关者经常相互矛盾或多余的观点。为了应对这一挑战，我们引入了一个支持工具的规范需求引出形式化框架（FormaTive）。它允许用直观的高级语言规范软件系统的规范性规则，并自动化：（i）将规则映射到内部形式表示；（ii）他们的分析，以确定规则冲突、冗余和关注点；以及（iii）使用户能够理解和解决问题的反馈的综合。,形式化方法，需求引出，可追溯性分析,,,
E5NGNP5G,2023,https://doi.org/10.1109/ASE56229.2023.00042,ASE 2023,MUTEN: Mutant-Based Ensembles for Boosting Gradient-Based Adversarial Attack,"Mutation testing (MT) for deep learning (DL) has gained huge attention in the past few years. However, how MT can really help DL is still unclear. In this paper, we introduce one promising direction for the usage of mutants. Specifically, since mutants can be seen as one kind of ensemble model and ensemble model can be used to boost the adversarial attack, we propose MUTEN, which applies the attack on mutants to improve the success rate of well-known attacks against gradient-masking models. Experimental results on MNIST, SVHN, and CIFAR-10 show that MUTEN can increase the success rate of four attacks by up to 45%. Furthermore, experiments on four defense approaches, bit-depth reduction, JPEG compression, Defensive distillation, and Label smoothing, demonstrate that MUTEN can break the defense models effectively by enhancing the attacks with the success rate of up to 96%.","Deep learning,Smoothing methods,Transform coding,Boosting,Testing,Software engineering",MUTEN：用于增强基于梯度的对抗性攻击的基于突变体的集合,用于深度学习（DL）的突变测试（MT）在过去几年中引起了极大的关注。然而，MT如何真正帮助DL还不清楚。在这篇文章中，我们介绍了一个很有前途的方向，利用突变体。具体来说，由于突变体可以被视为一种集成模型，并且集成模型可以用来增强对抗性攻击，我们提出了MUTEN，它将对突变体的攻击应用于提高已知的针对梯度掩蔽模型的攻击的成功率。在MNIST、SVHN和CIFAR-10上的实验结果表明，MUTEN可以将四次攻击的成功率提高45%。此外，在比特深度缩减、JPEG压缩、防御蒸馏和标签平滑四种防御方法上的实验表明，MUTEN可以通过增强攻击来有效地打破防御模型，成功率高达96%。,深度学习，平滑方法，转换编码，增强，测试，软件工程,,,
WH9ELXIN,2023,https://doi.org/10.1109/ASE56229.2023.00044,ASE 2023,Understanding and Enhancing Issue Prioritization in GitHub,"GitHub has become a prominent platform for open source software development, facilitating collaboration and communication among a diverse group of contributors. Efficient issue tracking is a crucial aspect of managing projects on GitHub, and labels serve as one of the primary mechanisms for issue prioritization, while various other issue features are also utilized by issue handlers for the same purpose. However, in large projects, prioritizing issues remains a challenge, and the efficacy of using labels or other issue features for prioritization is not well understood. To address this knowledge gap, we conduct a comprehensive empirical study that investigates the role of labels in GitHub issue prioritization, examines the influence of various issue features on prioritization, and assesses the performance of different ranking algorithms based on these impactful features. Our study, conducted on a dataset comprising data from over 1.5 million issues across diverse GitHub projects, provides valuable insights for issue handling in open source platforms and offers guidance for future research in this domain. Specifically, the study reveals the limited effectiveness of labels in issue prioritization, highlights the significance of certain issue features in the prioritization process, and compares the performance of various ranking algorithms for issue prioritization to support issue handlers.","GitHub,issue tracking,issue prioritization",理解和增强GitHub中的问题优先级,GitHub已经成为开源软件开发的一个突出平台，促进了不同贡献者之间的协作和沟通。有效的问题跟踪是GitHub上管理项目的一个关键方面，标签是问题优先级的主要机制之一，而问题处理程序也会出于同样的目的使用各种其他问题功能。然而，在大型项目中，对问题进行优先级排序仍然是一个挑战，并且使用标签或其他问题特征进行优先级排序的有效性还没有得到很好的理解。为了解决这一知识差距，我们进行了一项全面的实证研究，调查了标签在GitHub问题优先级中的作用，考察了各种问题特征对优先级的影响，并基于这些有影响力的特征评估了不同排名算法的性能。我们的研究是在一个数据集上进行的，该数据集包括来自不同GitHub项目的150多万个问题的数据，为开源平台中的问题处理提供了有价值的见解，并为该领域的未来研究提供了指导。具体而言，该研究揭示了标签在问题优先级排序中的有限有效性，强调了某些问题特征在优先级排序过程中的重要性，并比较了各种问题优先级排序算法的性能，以支持问题处理程序。,GitHub，问题跟踪，问题优先级,,,
DBKZ6AQK,2023,https://doi.org/10.1109/ASE56229.2023.00142,ASE 2023,SmartCoCo: Checking Comment-Code Inconsistency in Smart Contracts via Constraint Propagation and Binding,"Smart contracts are programs running on the blockchain. Comments in source code provide meaningful information for developers to facilitate code writing and understanding. Given various kinds of token standards in smart contracts (e.g., ERC-20, ERC-721), developers often copy&paste code from other projects as templates, and then implement their own logic as add-ons to such templates. In many cases, the consistency between code and comment is not well-aligned, leading to comment-code inconsistencies (as we call CCIs). Such inconsistencies can mislead developers and users, and even introduce vulnerabilities to the contracts. In this paper, we present SmartCoCo, a novel framework to detect comment-code inconsistencies in smart contracts. In particular, our research focuses on comments related to roles, parameters, and events that may lead to security implications. To achieve this, SmartCoCo takes the original smart contract source code as input and automatically analyzes the comment and code to find potential inconsistencies. SmartCoCo associates comment constraints and code facts via a set of propagation and binding strategies, allowing it to effectively discover inconsistencies with more contextual information. We evaluated SmartCoCo on 101,780 unique smart contracts on Ethereum. The evaluation result shows that SmartCoCo achieves good effectiveness and efficiency. In particular, SmartCoCo reports 4,732 inconsistencies from 1,745 smart contracts, with a precision of over 79% on 439 manual-labeled comment-code inconsistencies. Meanwhile, it only takes 2.64 seconds to check a smart contract on average.","Smart contract,program comprehension,text analytics,comment-code inconsistency",SmartCoCo：通过约束传播和绑定检查智能合约中的注释代码不一致,智能合约是在区块链上运行的程序。源代码中的注释为开发人员提供了有意义的信息，以便于编写和理解代码。考虑到智能合约中的各种代币标准（例如，ERC-20、ERC-721），开发人员经常将其他项目的代码作为模板进行复制和粘贴，然后将自己的逻辑作为这些模板的附加组件来实现。在许多情况下，代码和注释之间的一致性没有很好地协调，导致注释代码不一致（我们称之为CCI）。这种不一致可能误导开发人员和用户，甚至给合同带来漏洞。在本文中，我们提出了SmartCoCo，这是一种检测智能合约中注释代码不一致的新框架。特别是，我们的研究重点是与可能导致安全影响的角色、参数和事件相关的评论。为了实现这一点，SmartCoCo将原始智能合约源代码作为输入，并自动分析注释和代码以发现潜在的不一致之处。SmartCoCo通过一组传播和绑定策略将注释约束和代码事实关联起来，使其能够有效地发现与更多上下文信息的不一致。我们评估了SmartCoCo在以太坊上的101780个独特智能合约。评估结果表明，SmartCoCo取得了良好的效果和效率。特别是，SmartCoCo报告了1745个智能合约中的4732个不一致，439个手动标记的注释代码不一致的精度超过79%。同时，检查智能合约平均只需2.64秒。,智能合约，程序理解，文本分析，评论代码不一致,,,
PJEXUAJQ,2023,https://doi.org/10.1109/ASE56229.2023.00021,ASE 2023,Hot Patching Hot Fixes: Reflection and Perspectives,"With our reliance on software continuously increasing, it is of utmost importance that it be reliable. However, complete prevention of bugs in live systems is unfortunately an impossible task due to time constraints, incomplete testing, and developers not having knowledge of the full stack. As a result, mitigating risks for systems in production through hot patching and hot fixing has become an integral part of software development. In this paper, we first give an overview of the terminology used in the literature for research on this topic. Subsequently, we build upon these findings and present our vision for an automated framework for predicting and mitigating critical software issues at runtime. Our framework combines hot patching and hot fixing research from multiple fields, in particular: software defect and vulnerability prediction, automated test generation and repair, as well as runtime patching. We hope that our vision inspires research collaboration between the different communities.","Software Engineering,Software maintenance,Predictive maintenance,Prediction methods,Repair",热修补热修复：反思与展望,随着我们对软件的依赖不断增加，它的可靠性至关重要。然而，不幸的是，由于时间限制、测试不完整以及开发人员不了解完整的堆栈，完全防止实时系统中的错误是一项不可能完成的任务。因此，通过热修补和热修复来降低生产中系统的风险已成为软件开发不可或缺的一部分。在本文中，我们首先概述了文献中用于该主题研究的术语。随后，我们在这些发现的基础上，提出了在运行时预测和缓解关键软件问题的自动化框架的愿景。我们的框架结合了来自多个领域的热修补和热修复研究，特别是：软件缺陷和漏洞预测、自动测试生成和修复，以及运行时修补。我们希望我们的愿景能够激励不同社区之间的研究合作。,软件工程，软件维护，预测性维护，预测方法，修复,,,
ZPDSEVA7,2023,https://doi.org/10.1109/ASE56229.2023.00027,ASE 2023,Merge-Replay: Efficient IFDS-Based Taint Analysis by Consolidating Equivalent Value Flows,"The IFDS-based taint analysis employs two mutually iterative passes: a forward pass that identifies taints and a backward pass that detects aliases. This approach ensures both flow and context sensitivity, leading to remarkable precision. To preserve flow sensitivity, the IFDS-based taint analysis enhances data abstractions with activation statements that pinpoint the moment they acquire taint. Nonetheless, this mechanism can inadvertently introduce equivalent, yet redundant, value flows. This occurs when distinct activation statements are linked with the same data abstraction, resulting in unnecessary computational and memory-intensive demands on the analysis process. We introduce MergeDroid, a novel approach to improve the efficiency of IFDS-based taint analysis by consolidating equivalent value flows. This involves merging activation statements linked to the same data abstraction from various reachable data facts that are reachable at a given program point during the backward pass. This process generates a representative symbolic activation statement applicable to all equivalent data facts, reducing them to a single symbolic data fact. During the forward pass, when this symbolic data fact returns to its point of creation, the analysis reverts to the original data facts alongside their initial activation statements. This merge-and-replay strategy eliminates redundant value flow propagation, resulting in performance gains. Furthermore, we also improve analysis efficiency and precision by leveraging context-sensitive insights from activation statements. Our evaluation on 40 Android apps demonstrates that MergeDroid significantly enhances IFDS-based taint analysis performance. On average, MergeDroid accelerates analysis by 9.0× while effectively handling 6 more apps scalably. Additionally, it reduces false positives by significantly decreasing reported leak warnings, achieving an average reduction of 19.2%.","Taint analysis,IFDS,scalability,precision",合并回放：基于等价价值流合并的有效IFDS污染分析,基于IFDS的污点分析采用两个相互迭代的过程：一个是识别污点的前向过程，另一个是检测别名的后向过程。这种方法确保了流量和上下文的敏感性，从而实现了显著的精度。为了保持流的敏感性，基于IFDS的污染分析通过激活语句来增强数据抽象，这些激活语句可以精确定位它们获得污染的时刻。尽管如此，这种机制可能会无意中引入等价但冗余的价值流。当不同的激活语句与相同的数据抽象链接时，就会发生这种情况，从而对分析过程产生不必要的计算和内存密集型需求。我们介绍了MergeDroid，这是一种通过合并等值流来提高基于IFDS的污染分析效率的新方法。这涉及到将链接到同一数据抽象的激活语句从反向传递期间在给定程序点可访问的各种可访问数据事实中合并。该过程生成适用于所有等效数据事实的代表性符号激活语句，将它们简化为单个符号数据事实。在前向传递过程中，当该符号数据事实返回到其创建点时，分析将返回到原始数据事实及其初始激活语句。这种合并和重放策略消除了冗余的价值流传播，从而提高了性能。此外，我们还通过利用激活语句中对上下文敏感的见解来提高分析效率和准确性。我们对40个Android应用程序的评估表明，MergeDroid显著提高了基于IFDS的污染分析性能。MergeDroid平均可将分析速度提高9.0倍，同时可扩展地有效处理6个以上的应用程序。此外，它通过显著减少报告的泄漏警告来减少误报，平均减少19.2%。,污染分析，IFDS，可扩展性，精度,,,
7WFVWJ4E,2023,https://doi.org/10.1109/ASE56229.2023.00171,ASE 2023,Mutation-based Fault Localization of Deep Neural Networks,"Deep neural networks (DNNs) are susceptible to bugs, just like other types of software systems. A significant uptick in using DNN, and its applications in wide-ranging areas, including safety-critical systems, warrant extensive research on software engineering tools for improving the reliability of DNN-based systems. One such tool that has gained significant attention in the recent years is DNN fault localization. This paper revisits mutation-based fault localization in the context of DNN models and proposes a novel technique, named deepmufl, applicable to a wide range of DNN models. We have implemented deepmufl and have evaluated its effectiveness using 109 bugs obtained from StackOverflow. Our results show that deepmufl detects 53/109 of the bugs by ranking the buggy layer in top-1 position, outperforming state-of-the-art static and dynamic DNN fault localization systems that are also designed to target the class of bugs supported by deepmufl. Moreover, we observed that we can halve the fault localization time for a pre-trained model using mutation selection, yet losing only 7.55% of the bugs localized in ton-1 position.","Deep Neural Network,Mutation,Fault Localization",基于变异的深度神经网络故障定位,深度神经网络（DNN）和其他类型的软件系统一样，容易受到漏洞的影响。DNN的使用及其在包括安全关键系统在内的广泛领域的应用的显著增加，需要对软件工程工具进行广泛研究，以提高基于DNN的系统的可靠性。DNN故障定位是近年来备受关注的一种工具。本文在DNN模型的背景下重新审视了基于突变的故障定位，并提出了一种新的技术，称为deepmufl，适用于广泛的DNN模型。我们已经实现了deepmufl，并使用从StackOverflow获得的109个bug评估了它的有效性。我们的结果表明，deepmufl通过将漏洞层排名在前1位来检测53/109个漏洞，优于最先进的静态和动态DNN故障定位系统，这些系统也是针对deepmuf支持的漏洞类别而设计的。此外，我们观察到，使用突变选择，我们可以将预训练模型的故障定位时间减半，但只损失了7.55%定位在ton-1位置的错误。,深度神经网络，突变，故障定位,,,
99MDXN35,2023,https://doi.org/10.1109/ASE56229.2023.00109,ASE 2023,What Makes Good In-Context Demonstrations for Code Intelligence Tasks with LLMs?,"Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%, 175.96%, and 50.81% on code summarization, bug fixing, and program synthesis, respectively.","Codes,Source coding,Computer bugs,Predictive models,Natural language processing,Task analysis,Software engineering",是什么使LLM的代码智能任务具有良好的上下文演示？,预训练的源代码模型在许多代码智能任务中获得了广泛的流行。近年来，随着模型和语料库规模的扩大，大型语言模型已经显示出上下文学习（ICL）的能力。ICL使用任务指令和一些示例作为演示，然后将演示输入到语言模型中进行预测。这种新的学习范式是免费训练的，在各种自然语言处理和代码智能任务中表现出了令人印象深刻的性能。然而，ICL的性能在很大程度上取决于演示的质量，例如所选示例。系统地研究如何为代码相关任务构建良好的演示是很重要的。在本文中，我们实证研究了三个关键因素对ICL在代码智能任务中的性能的影响：示例的选择、顺序和数量。我们在三个代码智能任务上进行了广泛的实验，包括代码摘要、错误修复和程序合成。我们的实验结果表明，上述三个因素都会显著影响ICL在代码智能任务中的性能。此外，我们总结了我们的发现，并就如何构建有效的演示提供了建议，同时考虑到这三个角度。我们还表明，基于我们的发现精心设计的演示可以比广泛使用的演示构建方法带来实质性的改进，例如，在代码摘要、错误修复和程序合成方面，BLEU-4、EM和EM分别提高至少9.90%、175.96%和50.81%。,代码，源代码，计算机错误，预测模型，自然语言处理，任务分析，软件工程,,,
6NNBM7LI,2023,https://doi.org/10.1109/ASE56229.2023.00148,ASE 2023,Towards Autonomous Testing Agents via Conversational Large Language Models,"Software testing is an important part of the development cycle, yet it requires specialized expertise and substantial developer effort to adequately test software. Recent discoveries of the capabilities of large language models (LLMs) suggest that they can be used as automated testing assistants, and thus provide helpful information and even drive the testing process. To highlight the potential of this technology, we present a taxonomy of LLM-based testing agents based on their level of autonomy, and describe how a greater level of autonomy can benefit developers in practice. An example use of LLMs as a testing assistant is provided to demonstrate how a conversational framework for testing can help developers. This also highlights how the often criticized “hallucination” of LLMs can be beneficial for testing. We identify other tangible benefits that LLM-driven testing agents can bestow, and also discuss potential limitations.","software testing,machine learning,large language model,artificial intelligence, test automation",基于会话大语言模型的自主测试代理,软件测试是开发周期的重要组成部分，但它需要专门的专业知识和大量的开发人员努力来充分测试软件。最近对大型语言模型（LLM）功能的发现表明，它们可以用作自动化测试助手，从而提供有用的信息，甚至推动测试过程。为了突出这项技术的潜力，我们根据其自主权水平对基于LLM的测试代理进行了分类，并描述了更高水平的自主权如何在实践中使开发人员受益。提供了LLM作为测试助手的示例，以演示用于测试的会话框架如何帮助开发人员。这也突显了LLM经常被批评的“幻觉”如何对测试有益。我们确定了LLM驱动的测试代理可以带来的其他实际好处，并讨论了潜在的局限性。,软件测试，机器学习，大型语言模型，人工智能，测试自动化,,,
ISZHABU6,2023,https://doi.org/10.1109/ASE56229.2023.00108,ASE 2023,RPCover: Recovering gRPC Dependency in Multilingual Projects,"The advent of microservice architecture has led to a significant shift in the development of service-oriented software. In particular, the use of Remote Procedure Call (RPC), a mode of Inter-Process Communication (IPC) prevalent in microservices, has noticeably increased. To figure out the relationships between services and obtain a high-level understanding of service-oriented software, a line of recent work focuses on the dynamic construction of service call graphs, which relies on the preliminary deployment of services and only captures the calling relationships within a specific time frame. Meanwhile, static methods avoid the need for pre-deployment and often provide a more stable and complete graph compared to dynamic techniques. However, research and practical applications of static call graph construction remain relatively unexplored. This paper introduces RPCover, a novel gRPC dependency recovery framework that facilitates the interconnection of services across various programming languages using their static gRPC calls. In addition, due to the lack of a multilingual microservice benchmark that uses gRPC, we build the first multilingual benchmark RPCoverBench that contains complex gRPC call relations. RPCover has been evaluated on a single language benchmark (DeathStarBench) and our multilingual benchmark (RPCoverBench). The results show that RPCover effectively recovers 99.33% of the use cases of gRPC calls with less than 200% of the overhead compared with a single-language semantic dependency analyzer.","gRPC,dependency recovery,microservice",RPCover:恢复多语言项目中的gRPC依赖关系,微服务架构的出现导致了面向服务软件开发的重大转变。特别是，远程过程调用（RPC）的使用显著增加，这是微服务中流行的进程间通信（IPC）模式。为了弄清楚服务之间的关系并获得对面向服务软件的高级理解，最近的一系列工作集中在服务调用图的动态构建上，它依赖于服务的初步部署，并且只捕获特定时间范围内的调用关系。同时，与动态技术相比，静态方法避免了预部署的需要，并且通常提供更稳定和完整的图。然而，静态调用图构造的研究和实际应用仍然相对未被探索。本文介绍了RPCover，这是一种新的gRPC依赖恢复框架，它使用静态gRPC调用促进了各种编程语言之间的服务互连。此外，由于缺乏使用gRPC的多语言微服务基准，我们构建了第一个包含复杂gRPC调用关系的多语言基准RPCoverBench。RPCover已经在单一语言基准（DeathStarBench）和我们的多语言基准（RPCoverBench）上进行了评估。结果表明，与单语言语义依赖分析器相比，RPCover有效地恢复了99.33%的gRPC调用用例，开销不到200%。,gRPC，依赖恢复，微服务,,,
EJQTURDE,2023,https://doi.org/10.1109/ASE56229.2023.00155,ASE 2023,Merge Conflict Resolution: Classification or Generation?,"Collaborative development is critical to improve the productivity. Multiple contributors work simultaneously on the same project and might make changes to the same code locations. This can cause conflicts and require manual intervention from developers to resolve them. To alleviate the human efforts of manual conflict resolution, researchers have proposed various automatic techniques. More recently, deep learning models have been adopted to solve this problem and achieved state-of-the-art performance. However, these techniques leverage classification to combine the existing elements of input. The classification- based models cannot generate new tokens or produce flexible combinations, and have a wrong hypothesis that fine-grained conflicts of one single coarse-grained conflict are independent. In this work, we propose to generate the resolutions of merge conflicts from a totally new perspective, that is, generation, and we present a conflict resolution technique, MergeGen. First, we design a structural and fine-grained conflict-aware representation for the merge conflicts. Then, we propose to leverage an encoder- decoder-based generative model to process the designed conflict representation and generate the resolutions auto-regressively. We further perform a comprehensive study to evaluate the effectiveness of MergeGen. The quantitative results show that MergeGen outperforms the state-of-the-art (SOTA) techniques from both precision and accuracy. Our evaluation on multiple programming languages verifies the good generalization ability of MergeGen. In addition, the ablation study shows that the major component of our technique makes a positive contribution to the performance of MergeGen, and the granularity analysis reveals the high tolerance of MergeGen to coarse-grained conflicts. Moreover, the analysis on generating new tokens further proves the advance of generative models.","Merge Conflict Resolution,Generative Models,Conflict Representation",合并冲突解决：分类还是生成？,协作开发对于提高生产力至关重要。多个贡献者同时在同一个项目上工作，并可能对相同的代码位置进行更改。这可能会导致冲突，需要开发人员手动干预才能解决。为了减轻人工解决冲突的工作量，研究人员提出了各种自动技术。最近，深度学习模型被用来解决这个问题，并取得了最先进的性能。然而，这些技术利用分类来组合输入的现有元素。基于分类的模型不能生成新的令牌或产生灵活的组合，并且有一个错误的假设，即单个粗粒度冲突的细粒度冲突是独立的。在这项工作中，我们建议从一个全新的角度生成合并冲突的解决方案，即生成，并提出了一种冲突解决技术MergeGen。首先，我们为合并冲突设计了一个结构化的、细粒度的冲突感知表示。然后，我们提出利用基于编码器-解码器的生成模型来处理设计的冲突表示，并自动回归生成解决方案。我们进一步进行了一项全面的研究，以评估MergeGen的有效性。定量结果表明，MergeGen在精度和准确性方面都优于最先进的（SOTA）技术。我们对多种编程语言的评估验证了MergeGen良好的泛化能力。此外，消融研究表明，我们技术的主要组件对MergeGen的性能做出了积极贡献，粒度分析揭示了MergeGen对粗粒度冲突的高容忍度。此外，对生成新令牌的分析进一步证明了生成模型的先进性。,合并冲突解决，生成模型，冲突表示,,,
VYB8E3K6,2023,https://doi.org/10.1109/ASE56229.2023.00182,ASE 2023,Challenges of Accurate and Efficient AutoML,"Embedded Artificial Intelligence (AI) is becoming increasingly important in the field of healthcare where such AI enabled devices are utilized to assist physicians, clinicians, and surgeons in their diagnosis, rehabilitation and therapy planning. However, it is still a challenging task to come up with an accurate and efficient machine learning model for resource-limited devices that work $24\times 7$.. It requires both intuition and experience. This dependence on human expertise and reliance on trial-and-error-based design methods create impediments to the standard processes of effort estimation, design phase planning, and generating service-level agreements for projects that involve AI-enabled MedTech devices. In this paper, we present AutoML search from an algorithmic perspective, instead of a more prevalent optimization or black-box tool view. We briefly present and point to case studies that demonstrate the efficacy of the automation approach in terms of productivity improvements. We believe that our proposed method can make AutoML more amenable to the applications of software engineering principles and also accelerate biomedical device engineering, where there is a high dependence on skilled human resources.","automation,software engineering,IoT,Deep Learning,AutoML",准确高效的AutoML面临的挑战,嵌入式人工智能（AI）在医疗保健领域正变得越来越重要，这种支持AI的设备被用来帮助医生、临床医生和外科医生进行诊断、康复和治疗规划。然而，为工作成本为24美元乘以7美元的资源有限设备提供准确高效的机器学习模型仍然是一项具有挑战性的任务。。它需要直觉和经验。这种对人类专业知识的依赖和对基于试错的设计方法的依赖，阻碍了人工智能医疗技术设备项目的工作量估计、设计阶段规划和服务水平协议的标准流程。在本文中，我们从算法的角度介绍了AutoML搜索，而不是更普遍的优化或黑盒工具视图。我们简要介绍并指出案例研究，这些案例研究证明了自动化方法在提高生产力方面的有效性。我们相信，我们提出的方法可以使AutoML更适合软件工程原理的应用，也可以加速生物医学设备工程，因为生物医学设备工程高度依赖熟练的人力资源。,自动化，软件工程，物联网，深度学习，AutoML,,,
2RHW4RFX,2023,https://doi.org/10.1109/ASE56229.2023.00010,ASE 2023,COMEX: A Tool for Generating Customized Source Code Representations,"Learning effective representations of source code is critical for any Machine Learning for Software Engineering (ML4SE) system. Inspired by natural language processing, large language models (LLMs) like Codex and CodeGen treat code as generic sequences of text and are trained on huge corpora of code data, achieving state of the art performance on several software engineering (SE) tasks. However, valid source code, unlike natural language, follows a strict structure and pattern governed by the underlying grammar of the programming language. Current LLMs do not exploit this property of the source code as they treat code like a sequence of tokens and overlook key structural and semantic properties of code that can be extracted from code-views like the Control Flow Graph (CFG), Data Flow Graph (DFG), Abstract Syntax Tree (AST), etc. Unfortunately, the process of generating and integrating code-views for every programming language is cumbersome and time consuming. To overcome this barrier, we propose our tool COMEX - a framework that allows researchers and developers to create and combine multiple code-views which can be used by machine learning (ML) models for various SE tasks. Some salient features of our tool are: (i) it works directly on source code (which need not be compilable), (ii) it currently supports Java and C#, (iii) it can analyze both method-level snippets and program-level snippets by using both intra-procedural and inter-procedural analysis, and (iv) it is easily extendable to other languages as it is built on tree-sitter - a widely used incremental parser that supports over 40 languages. We believe this easy-to-use code-view generation and customization tool will give impetus to research in source code representation learning methods and ML4SE. The source code and demonstration of our tool can be found at https://github.com/IBM/tree-sitter-codeviews and https://youtu.be/GER6U87FVbU, respectively.","Representation Learning,Static Analysis",COMEX：生成自定义源代码表示的工具,学习源代码的有效表示对于任何用于软件工程的机器学习（ML4SE）系统来说都是至关重要的。受自然语言处理的启发，像Codex和CodeGen这样的大型语言模型（LLM）将代码视为通用的文本序列，并在庞大的代码数据语料库上进行训练，从而在几个软件工程（SE）任务中实现最先进的性能。然而，与自然语言不同，有效的源代码遵循由编程语言的底层语法控制的严格结构和模式。当前的LLM不利用源代码的这一特性，因为它们将代码视为令牌序列，并忽略了可以从代码视图（如控制流图（CFG）、数据流图（DFG）、抽象语法树（AST））中提取的代码的关键结构和语义特性。不幸的是，为每种编程语言生成和集成代码视图的过程既繁琐又耗时。为了克服这一障碍，我们提出了我们的工具COMEX，这是一个允许研究人员和开发人员创建和组合多个代码视图的框架，机器学习（ML）模型可以将其用于各种SE任务。我们的工具的一些显著特征是：（i）它直接处理源代码（不需要可编译），（ii）它目前支持Java和C#，（iii）它可以通过使用过程内和过程间分析来分析方法级代码段和程序级代码段，（iv）它是建立在tree-sitter之上的，可以很容易地扩展到其他语言，tree-sitter是一种广泛使用的增量解析器，支持40多种语言。我们相信，这个易于使用的代码视图生成和定制工具将推动对源代码表示学习方法和ML4SE的研究。我们的工具的源代码和演示可以在https://github.com/IBM/tree-sitter-codeviews和https://youtu.be/GER6U87FVbU分别地,表征学习，静态分析,,,
7FM8PQAR,2023,https://doi.org/10.1109/ASE56229.2023.00078,ASE 2023,From Commit Message Generation to History-Aware Commit Message Completion,"Commit messages are crucial to software development, allowing developers to track changes and collaborate effectively. Despite their utility, most commit messages lack important information since writing high-quality commit messages is tedious and time-consuming. The active research on commit message generation (CMG) has not yet led to wide adoption in practice. We argue that if we could shift the focus from commit message generation to commit message completion and use previous commit history as additional context, we could significantly improve the quality and the personal nature of the resulting commit messages. In this paper, we propose and evaluate both of these novel ideas. Since the existing datasets lack historical data, we collect and share a novel dataset called CommitChronicle, containing 10.7M commits across 20 programming languages. We use this dataset to evaluate the completion setting and the usefulness of the historical context for state-of-the-art CMG models and GPT-3.5-turbo. Our results show that in some contexts, commit message completion shows better results than generation, and that while in general GPT-3.5-turbo performs worse, it shows potential for long and detailed messages. As for the history, the results show that historical information improves the performance of CMG models in the generation task, and the performance of GPT-3.5-turbo in both generation and completion.","Computer languages,Filtering,Focusing,Writing,Software,History,Reliability",从提交消息生成到历史感知提交消息完成,提交消息对软件开发至关重要，使开发人员能够跟踪更改并进行有效协作。尽管它们很有用，但大多数提交消息都缺乏重要信息，因为编写高质量的提交消息既乏味又耗时。对提交消息生成（CMG）的积极研究尚未在实践中得到广泛采用。我们认为，如果我们能够将重点从提交消息生成转移到提交消息完成，并使用以前的提交历史记录作为额外的上下文，我们就可以显著提高最终提交消息的质量和个人性质。在本文中，我们提出并评价了这两种新颖的思想。由于现有的数据集缺乏历史数据，我们收集并共享了一个名为CommitChristle的新数据集，该数据集包含20种编程语言的107M个提交。我们使用此数据集来评估最先进的CMG模型和GPT-3.5-turbo的完成设置和历史上下文的有用性。我们的结果表明，在某些上下文中，提交消息完成显示出比生成更好的结果，并且虽然GPT-3.5-turbo通常表现更差，但它显示出长而详细的消息的潜力。对于历史，结果表明，历史信息提高了CMG模型在生成任务中的性能，以及GPT-3.5-turbo在生成和完成任务中的表现。,计算机语言，过滤，聚焦，写作，软件，历史，可靠性,,,
VLUJD8WB,2023,https://doi.org/10.1109/ASE56229.2023.00080,ASE 2023,Persisting and Reusing Results of Static Program Analyses on a Large Scale,"Static Program Analysis (SPA) has long been established as an important technique for gaining insights into software systems. Over the last years, analysis designers increasingly produced analyses that are compositional, collaborative, or incremental in nature - thus relying on some form of existing results to increase performance or even precision. However, systematic result reuse is still rare in this field even though the analyzed software is mainly composed of reusable software components. For this work, we study 40 state-of-the-art SPA implementations and find that there is a tremendous potential for reusing analysis results. We attribute this to the fact that there is no systematic process in place for persisting and sharing analysis results and propose such a process here to fill this gap. In this paper, we present SPARRI, a prototype implementation providing an HTTP API to publish, search, and reuse SPA results. Our evaluation shows that reusing existing results with SPARRI can improve analysis performance by up to 92%. Furthermore, we see potential in applying it to other research areas like empirical software studies. benchmark creation. and artifact evaluation.","Static Program Analysis,Modular Analysis,Result Reuse,Repository Mining",大规模静态程序分析结果的持久化与重用,长期以来，静态程序分析（SPA）一直被认为是深入了解软件系统的一项重要技术。在过去的几年里，分析设计者越来越多地进行组合、协作或增量分析，从而依赖于某种形式的现有结果来提高性能甚至精度。然而，尽管所分析的软件主要由可重用的软件组件组成，但系统的结果重用在该领域仍然很少见。在这项工作中，我们研究了40种最先进的SPA实现，发现重用分析结果的潜力巨大。我们将此归因于没有系统的过程来保存和共享分析结果，并在这里提出了这样一个过程来填补这一空白。在本文中，我们介绍了SPARRI，一个原型实现，它提供了一个HTTP API来发布、搜索和重用SPA结果。我们的评估表明，使用SPARRI重用现有结果可以将分析性能提高92%。此外，我们看到了将其应用于其他研究领域的潜力，如实证软件研究。基准创建。以及伪影评估。,静态程序分析，模块化分析，结果重用，存储库挖掘,,,
TB88XNVT,2023,https://doi.org/10.1109/ASE56229.2023.00217,ASE 2023,A Comparative Study of Transformer-Based Neural Text Representation Techniques on Bug Triaging,"Bug report management has been shown to be an important and time consuming software maintenance task. Often, the first step in managing bug reports is related to triaging a bug to the appropriate developer who is best suited to understand, localize, and fix the target bug. Additionally, assigning a given bug to a particular part of a software project can help to expedite the fixing process. However, despite the importance of these activities, they are quite challenging, where days can be spent on the manual triaging process. Past studies have attempted to leverage the limited textual data of bug reports to train text classification models that automate this process - to varying degrees of success. However, the textual representations and machine learning models used in prior work are limited by their expressiveness, often failing to capture nuanced textual patterns that might otherwise aid in the triaging process. Recently, large, transformer-based, pre-tained neural text representation techniques (i.e., large language models or LLMs) such as BERT and CodeBERT have achieved greater performance with simplified training procedures in several natural language processing tasks, including text classification. However, the potential for using these techniques to improve upon prior approaches for automated bug triaging is not well studied or understood. Therefore, in this paper we offer one of the first investigations that fine-tunes transformer-based language models for the task of bug triaging on four open source datasets, spanning a collective 53 years of development history with over 400 developers and over 150 software project components. Our study includes both a quantitative and qualitative analysis of effectiveness. Our findings illustrate that DeBERTa is the most effective technique across the triaging tasks of developer and component assignment, and the measured performance delta is statistically significant compared to other techniques. However, through our qualitative analysis, we also observe that each technique possesses unique abilities best suited to certain types of bug reports.","Bug Triaging,Transformer,LLMs,Text-Embedding,DL4SE",基于Transformer的Bug分类神经文本表示技术的比较研究,Bug报告管理已被证明是一项重要且耗时的软件维护任务。通常，管理bug报告的第一步是将bug测试给最适合理解、本地化和修复目标bug的适当开发人员。此外，将给定的bug分配给软件项目的特定部分可以帮助加快修复过程。然而，尽管这些活动很重要，但它们具有相当大的挑战性，可以花几天时间进行手动试验。过去的研究试图利用错误报告的有限文本数据来训练文本分类模型，使这一过程自动化，并取得了不同程度的成功。然而，先前工作中使用的文本表示和机器学习模型受到其表达能力的限制，通常无法捕捉到细微的文本模式，否则可能有助于试验过程。最近，诸如BERT和CodeBERT的基于变换器的大型预保持神经文本表示技术（即，大型语言模型或LLM）在包括文本分类在内的几个自然语言处理任务中通过简化训练过程获得了更高的性能。然而，使用这些技术来改进现有的自动错误分类方法的潜力还没有得到很好的研究或理解。因此，在本文中，我们提供了第一批研究之一，对基于转换器的语言模型进行微调，以在四个开源数据集上进行错误测试，这四个数据集跨越了53年的开发历史，共有400多名开发人员和150多个软件项目组件。我们的研究包括有效性的定量和定性分析。我们的研究结果表明，在开发人员和组件分配的试验任务中，DeBERTa是最有效的技术，与其他技术相比，测量的性能增量具有统计学意义。然而，通过我们的定性分析，我们也观察到每种技术都具有最适合某些类型的错误报告的独特能力。,Bug Triaging，Transformer，LLM，文本嵌入，DL4SE,,,
P2G3EN3H,2023,https://doi.org/10.1109/ASE56229.2023.00145,ASE 2023,Detection of Java Basic Thread Misuses Based on Static Event Analysis,"The fundamental asynchronous thread (java.lang. Thread) in Java can be easily misused, due to the lack of deep understanding for garbage collection and thread interruption mechanism. For example, a careless implementation of asynchronous thread may cause no response to the interrupt mechanism in time, resulting in unexpected thread-related behaviors, especially resource leak/waste. Currently, few works aim at these misuses and related works adopt either the dynamic approach which lacks effective inputs or the static path-sensitive approach with high time consumption due to the path explosion, causing false negatives. We have found that the behavior of threads and the interaction between threads and its referencing objects can be abstracted. In this paper, we propose an event analysis approach to detect the defects in Java programs and Android apps, which focuses on the existence or the order of the events to reduce the false negatives. We extract the misuse-related events, containing the thread events and the destroy events of the object referenced by the thread. Then we analyze the events with loop identification, happens-before relationship construction and alias determination. Finally, we implement an automatic tool named Leopard and evaluate it on real world Java programs and Android apps. Experiments show that it is efficient when comparing with the existing approach (misuse: 723 vs 47, time: 60s vs 30min), which also outperforms the existing work in precision. The manual check indicates that Leopard is more efficient and effective than existing work. Besides, 66 issues reported by us have been confirmed and 21 of them have been fixed by developers.","Thread Misuse,Basic Thread,Runnable,Static Analysis,Java Program",基于静态事件分析的Java基本线程误用检测,java中的基本异步线程（java.lang.thread）很容易被误用，这是由于对垃圾收集和线程中断机制缺乏深入的理解。例如，异步线程的粗心实现可能会导致对中断机制没有及时响应，从而导致意外的线程相关行为，尤其是资源泄漏/浪费。目前，针对这些误用的工作很少，相关工作要么采用缺乏有效输入的动态方法，要么采用由于路径爆炸而导致高时间消耗的静态路径敏感方法，从而导致假阴性。我们发现，线程的行为以及线程与其引用对象之间的交互是可以抽象的。在本文中，我们提出了一种事件分析方法来检测Java程序和Android应用程序中的缺陷，该方法关注事件的存在或顺序，以减少误报。我们提取与误用相关的事件，其中包含线程引用的对象的线程事件和销毁事件。然后，我们用循环识别、关系构建和别名确定之前发生的事件进行分析。最后，我们实现了一个名为Leopard的自动工具，并在现实世界的Java程序和Android应用程序上对其进行了评估。实验表明，与现有方法（误用：723 vs 47，时间：60s vs 30min）相比，该方法是有效的，在精度上也优于现有方法。手动检查表明Leopard比现有工作更高效。此外，我们报告的66个问题已得到确认，其中21个问题已由开发商解决。,线程误用，基本线程，可运行，静态分析，Java程序,,,
2EC9EBG3,2023,https://doi.org/10.1109/ASE56229.2023.00177,ASE 2023,Dynamic Graph Neural Networks-Based Alert Link Prediction for Online Service Systems,"A fault in large online service systems often triggers numerous alerts due to the complex business and component dependencies among services, which is known as “alert storm”. In a short time, an online service system may generate a huge amount of alert data. This poses a challenge for on-call engineers to identify alerts that are associated with a system failure for root cause analysis. In this paper, we propose DyAlert, a dynamic graph neural networks-based approach for linking alerts that might be triggered by a same fault to reduce the burden of on-call engineers in the fault analysis. Our insight is that alerts are often triggered by alert propagation when a system failure occurs, e.g., alert $a$ would lead to the occurrence of alert $b$. Whether two alerts should be linked depends on if one alert is triggered by the propagation of the other. Leveraging this insight, we design a dynamic graph (namely Alert-Metric Dynamic Graph) that describes the propagation process of alerts. Based on the dynamic graph, we train a neural networks-based model to predict alert links. We evaluate DyAlert with real-world data collected from an online service system running 85 business units and about 30,000 different services in a large enterprise. The results show that DyAlert is effective in predicting alert links and it outperforms the state-of-the-art approaches with an average increase of 0.259 in F1-score.","Linked Alerts,Online Service Systems,Graph Neural Networks",基于动态图神经网络的在线服务系统告警链路预测,由于服务之间的业务和组件依赖关系复杂，大型在线服务系统中的故障通常会引发大量警报，这被称为“警报风暴”。在短时间内，在线服务系统可能会生成大量的警报数据。这给随叫随到的工程师识别与系统故障相关的警报以进行根本原因分析带来了挑战。在本文中，我们提出了DyAlert，这是一种基于动态图神经网络的方法，用于链接可能由同一故障触发的警报，以减轻随叫随到的工程师在故障分析中的负担。我们的见解是，当系统故障发生时，警报通常由警报传播触发，例如，警报$a$将导致警报$b$的发生。是否应链接两个警报取决于一个警报是否由另一个警报的传播触发。利用这一见解，我们设计了一个描述警报传播过程的动态图（即警报度量动态图）。基于动态图，我们训练了一个基于神经网络的模型来预测警报链接。我们使用从一个在线服务系统收集的真实世界数据来评估DyAlert，该系统在一个大型企业中运行着85个业务单元和大约30000种不同的服务。结果表明，DyAlert在预测警报链接方面是有效的，并且它优于最先进的方法，F1得分平均增加了0.259。,链接警报，在线服务系统，图形神经网络,,,
JPL8WMF7,2023,https://doi.org/10.1109/ASE56229.2023.00129,ASE 2023,Fast and Reliable Program Synthesis via User Interaction,"The performance of programming-by-example systems varies significantly across different tasks and even across different examples in one task. The key issue is that the search space depends on the given examples in a complex way. In particular, scalable synthesizers typically rely on a combination of machine learning to prioritize search order and deduction to prune search space, making it hard to quantitatively reason about how much an example speeds up the search. We propose a novel approach for quantifying the effectiveness of an example at reducing synthesis time. Based on this technique, we devise an algorithm that actively queries the user to obtain additional examples that significantly reduce synthesis time. We evaluate our approach on 30 challenging benchmarks across two different data science domains. Even with ineffective initial user-provided examples for pruning, our approach on average achieves a 6.0× speed-up in synthesis time compared to state-of-the-art synthesizers.","Machine learning algorithms,Synthesizers,Scalability,Machine learning,Reliability theory,Data science,Reliability engineering",通过用户交互快速可靠地合成程序,示例系统编程的性能在不同任务之间，甚至在一个任务中的不同示例之间都有显著差异。关键问题是搜索空间以复杂的方式依赖于给定的示例。特别是，可伸缩合成器通常依靠机器学习来区分搜索顺序的优先级，以及推理来修剪搜索空间，这使得很难定量地推断一个例子在多大程度上加快了搜索。我们提出了一种新的方法来量化实例在减少合成时间方面的有效性。基于这一技术，我们设计了一种算法，该算法可以主动查询用户，以获得显著减少合成时间的额外示例。我们在两个不同数据科学领域的30个具有挑战性的基准上评估了我们的方法。即使最初用户提供的修剪示例无效，与最先进的合成器相比，我们的方法平均也能在合成时间上提高6.0倍。,机器学习算法，综合器，可伸缩性，机器学习，可靠性理论，数据科学，可靠性工程,,,
3H8KXPBN,2023,https://doi.org/10.1109/ASE56229.2023.00093,ASE 2023,MUT4SLX: Fast Mutant Generation for Simulink,"Several experience reports illustrate that mutation testing is capable of supporting a “shift-left” testing strategy for software systems coded in textual programming languages like C++. For graphical modelling languages like Simulink, such experience reports are missing, primarily because of a lack of adequate tool support. In this paper, we present a proof-of-concept (named MUT4S LX) for automatic mutant generation and test execution of Simulink models. MUT14SLX features 15 mutation operators which are modelled after realistic faults (mined from an industrial bug database) and are fast to inject (because we only replace parameter values within blocks). An experimental evaluation on a sample project (a Helicopter Control System) demonstrates that MUT4SLX is capable of injecting 70 mutants in less than a second, resulting in a total analysis time of 8.14 hours.","software testing,mutation testing,mutation analysis,Simulink models,cyber-physical systems",MUT4SLX：用于Simulink的快速突变体生成,一些经验报告表明，突变测试能够支持用C++等文本编程语言编码的软件系统的“左移”测试策略。对于像Simulink这样的图形建模语言，缺少这样的经验报告，主要是因为缺乏足够的工具支持。在本文中，我们提出了一个用于Simulink模型的自动突变体生成和测试执行的概念验证（名为MUT4S-LX）。MUT14SLX具有15个突变算子，这些算子是根据实际故障建模的（从工业错误数据库中挖掘），并且注入速度快（因为我们只替换块中的参数值）。对一个样本项目（直升机控制系统）的实验评估表明，MUT4SLX能够在不到一秒钟的时间内注射70个突变体，总分析时间为8.14小时。,软件测试，变异测试，变异分析，Simulink模型，网络物理系统,,,
GNLQUNXR,2023,https://doi.org/10.1109/ASE56229.2023.00101,ASE 2023,An Integrated Program Analysis Framework for Graduate Courses in Programming Languages and Software Engineering,"Program analysis, verification and testing are important topics in programming languages and software engineering. They aim to produce engineers who are not only capable of empirically evaluating but, also formally reasoning on the correctness of software systems. We propose a specialized framework, Chiron, designed to teach graduate-level courses on these topics. Chiron has a small code base for easy understanding, uses a unified intermediate representation across all its analysis modules, maintains a modular architecture for plugging in new algorithms and uses a “fun” programming language to provide a gamified experience. Currently, it packages a dataflow analysis engine for driving compiler optimizations, an abstract interpretation engine for verification, a symbolic execution engine, a fuzzer and an evolutionary test generator for program testing, and a spectrum based statistical bug localization module. Within Chiron, program analysis tasks are posed in an unconventional setting (as adventures of a turtle) to provide a gamified experience; the accompanying animations (showing the movements of the turtle) allow the student to understand the underlying concepts better, and the detailed logs allow the teaching assistants in their grading activities. Chiron has been used in two offerings of a graduate level course on program analysis, verification and testing. In response to our survey questionnaire, all the students unanimously held the opinion that Chiron was extremely helpful in aiding their learning, and recommended its use in similar courses.","program analysis verification and testing,programming languages,software engineering,graduate level course,education",程序设计语言与软件工程研究生课程的集成程序分析框架,程序分析、验证和测试是编程语言和软件工程中的重要课题。他们的目标是培养出不仅能够凭经验进行评估，而且能够对软件系统的正确性进行形式推理的工程师。我们提出了一个专门的框架，Chiron，旨在教授这些主题的研究生课程。Chiron的代码库很小，易于理解，在所有分析模块中使用统一的中间表示，维护用于插入新算法的模块化架构，并使用“有趣”的编程语言提供游戏化体验。目前，它封装了一个用于驱动编译器优化的数据流分析引擎、一个用于验证的抽象解释引擎、一种符号执行引擎、一款用于程序测试的模糊器和进化测试生成器，以及一个基于频谱的统计错误定位模块。在Chiron中，程序分析任务是在非传统的环境中提出的（比如乌龟的冒险），以提供游戏化的体验；随附的动画（展示乌龟的动作）使学生更好地理解基本概念，详细的日志使助教能够进行评分活动。Chiron已经在两个研究生级别的课程中被用于程序分析、验证和测试。在回答我们的调查问卷时，所有学生一致认为Chiron在帮助他们学习方面非常有帮助，并建议在类似的课程中使用它。,程序分析验证和测试，编程语言，软件工程，研究生课程，教育,,,
TVD64VVD,2023,https://doi.org/10.1109/ASE56229.2023.00207,ASE 2023,RJoules: An Energy Measurement Tool for R,"With the exponential growth of data, the demand for effective data analysis tools has increased significantly. R language, known for its statistical modeling and data analysis capabilities, has become one of the most popular programming languages among data scientists and researchers. As the importance of energy-aware software systems continues to rise, several studies investigate the impact of source code and different stages of machine learning model training on energy consumption. However, existing studies in this domain primarily focus on programming languages like Python and Java, resulting in a lack of energy measuring tools for other programming languages such as R. To address this gap, we propose “RJoules”, a tool designed to measure the energy consumption of R code snippets. We evaluate the correctness and performance of RJoules by applying it to four machine learning algorithms on three different systems. Our aim is to support developers and practitioners in building energy-aware systems in R. The demonstration of the tool is available at https://youtu.be/yMKFuvAM-DE and related artifacts at https://rishalab.github.io/RJoules/.","energy measurement,R,Intel-RAPL",RJoules：R的能量测量工具,随着数据的指数级增长，对有效数据分析工具的需求显著增加。R语言以其统计建模和数据分析能力而闻名，已成为数据科学家和研究人员中最受欢迎的编程语言之一。随着能量感知软件系统的重要性不断提高，一些研究调查了源代码和机器学习模型训练的不同阶段对能量消耗的影响。然而，该领域现有的研究主要集中在Python和Java等编程语言上，导致缺乏用于R等其他编程语言的能量测量工具。为了解决这一差距，我们提出了“RJoules”，这是一种旨在测量R代码片段能量消耗的工具。我们通过将RJoules应用于三个不同系统上的四种机器学习算法来评估其正确性和性能。我们的目标是支持开发人员和从业者在R中构建能源感知系统。该工具的演示可在https://youtu.be/yMKFuvAM-DE和相关工件https://rishalab.github.io/RJoules/.,能量测量，R，Intel RAPL,,,
VD8AXYR8,2023,https://doi.org/10.1109/ASE56229.2023.00025,ASE 2023,Characterizing Flaky Tests in Node.js Applications,"Regression testing is an important means of assessing the quality of Node.js applications. However, non-deterministic executions inside Node.js framework could make test cases intermittently pass or fail on the same version of code, which are called flaky tests. Flaky tests can cause unreliable test results, and make developers waste a significant amount of time debugging the bugs that do not belong to the target application. In this paper, we conduct an empirical study on 87 flaky tests from 7 popular Node.js applications, and analyze the non-determinism that causes these flaky tests. Through this study, there is a wide range of non-determinism to cause flaky tests, including non-deterministic event triggering order, non-deterministic function calls, non-deterministic process/thread scheduling order, non-deterministic execution of asynchronous tasks and non-deterministic event triggering data. The result reveals that, existing approaches on event race detection are not sufficient for flaky test detection. In future, researchers can design flaky test detection approaches targeted at different categories of non-determinism.","flaky tests,non-determinism",Node.js应用程序中的Flaky测试特性,回归测试是评估Node.js应用程序质量的重要手段。然而，Node.js框架内的非确定性执行可能会使测试用例在同一版本的代码上间歇性地通过或失败，这被称为flaky测试。缺陷测试可能会导致测试结果不可靠，并使开发人员浪费大量时间调试不属于目标应用程序的错误。在本文中，我们对7个流行的Node.js应用程序中的87个片状测试进行了实证研究，并分析了导致这些片状测试的非确定性。通过这项研究，存在广泛的不确定性导致了片状测试，包括不确定性事件触发顺序、不确定性函数调用、不确定性进程/线程调度顺序、异步任务的不确定性执行和不确定性事件触发器数据。结果表明，现有的事件竞赛检测方法不足以进行片状测试检测。未来，研究人员可以针对不同类别的非决定论设计片状测试检测方法。,零散的测试，非决定论,,,
KDNJSRF2,2023,https://doi.org/10.1109/ASE56229.2023.00131,ASE 2023,ESRO: Experience Assisted Service Reliability against Outages,"Modern cloud services are prone to failures due to their complex architecture, making diagnosis a critical process. Site Reliability Engineers (SREs) spend hours leveraging multiple sources of data, including the alerts, error logs, and domain expertise through past experiences to locate the root cause(s). These experiences are documented as natural language text in outage reports for previous outages. However, utilizing the raw yet rich semi-structured information in the reports systematically is time-consuming. Structured information, on the other hand, such as alerts that are often used during fault diagnosis, is voluminous and requires expert knowledge to discern. Several strategies have been proposed to use each source of data separately for root cause analysis. In this work, we build a diagnostic service called ESRO that recommends root causes and remediation for failures by utilizing structured as well as semi-structured sources of data systematically. ESRO constructs a causal graph using alerts and a knowledge graph using outage reports, and merges them in a novel way to form a unified graph during training. A retrieval based mechanism is then used to search the unified graph and rank the likely root causes and remediation techniques based on the alerts fired during an outage at inference time. Not only the individual alerts, but their respective importance in predicting an outage group is taken into account during recommendation. We evaluated our model on several cloud service outages of a large SaaS enterprise over the course of ~2 years, and obtained an average improvement of 27% in rouge scores after comparing the likely root causes against the ground truth over state-of-the-art baselines. We further establish the effectiveness of ESRO through qualitative analysis on multiple real outage examples.","System Monitoring,Cloud Services,Causal Graph,Knowledge Graph",ESRO：针对停机的体验辅助服务可靠性,现代云服务由于其复杂的架构而容易出现故障，因此诊断是一个关键过程。现场可靠性工程师（SRE）花费数小时利用多个数据源，包括警报、错误日志和领域专业知识，通过过去的经验来定位根本原因。这些经验以自然语言文本的形式记录在以前停机的停机报告中。然而，系统地利用报告中原始但丰富的半结构化信息是耗时的。另一方面，结构化信息，如故障诊断期间经常使用的警报，数量庞大，需要专家知识才能辨别。已经提出了几种策略，分别使用每个数据源进行根本原因分析。在这项工作中，我们构建了一个名为ESRO的诊断服务，该服务通过系统地利用结构化和半结构化数据源来建议故障的根本原因和补救措施。ESRO使用警报构建因果图，使用停机报告构建知识图，并以一种新颖的方式将它们合并，以在训练期间形成统一的图。然后使用基于检索的机制来搜索统一图，并根据推理时停机期间发出的警报对可能的根本原因和补救技术进行排序。在推荐过程中，不仅要考虑单个警报，还要考虑它们在预测停机组中各自的重要性。我们对一家大型SaaS企业在约2年内的几次云服务中断进行了评估，并在将可能的根本原因与最先进的基线的基本事实进行比较后，获得了27%的胭脂评分平均改善。我们通过对多个实际停机实例的定性分析，进一步确定了ESRO的有效性。,系统监控，云服务，因果图，知识图,,,
9EA6IMHN,2023,https://doi.org/10.1109/ASE56229.2023.00005,ASE 2023,Message from the Chairs: ASE 2023,"On behalf of the entire conference organizing committee, it is our great pleasure to welcome you to the 38th edition of the IEEE/ACM International Conference on Automated Software Engineering, ASE 2023, in the Grand Duchy of Luxembourg. The ASE conference series is the premier research forum for automated software engineering research and practice. Each year it brings together researchers and practitioners from academia and industry to discuss foundations, techniques, and tools for automated analysis, design, implementation, testing, and maintenance of large software systems.",,主席致辞：ASE 2023,我们非常高兴地代表整个会议组委会欢迎您参加在卢森堡大公国举行的第38届IEEE/ACM自动化软件工程国际会议ASE 2023。ASE系列会议是自动化软件工程研究和实践的首要研究论坛。每年，它都会聚集学术界和行业的研究人员和从业者，讨论大型软件系统的自动化分析、设计、实现、测试和维护的基础、技术和工具。,,,,
DPFIU9T4,2023,https://doi.org/10.1109/ASE56229.2023.00214,ASE 2023,LIV: Loop-Invariant Validation Using Straight-Line Programs,"Validation of program invariants (a.k.a. correctness witnesses) is an established procedure in software verification. There are steady advances in verification of more and more complex software systems, but coming up with good loop invariants remains the central task of many verifiers. While it often requires large amounts of computation to construct safe and inductive invariants, they are more easy to automatically validate. We propose LIV, a new tool for loop-invariant validation, which makes it more practical to check if the invariant produced by a verifier is sufficient to establish an inductive safety proof. The main idea is to apply divide-and-conquer on the program level: We split the program into smaller, loop-free programs (a.k.a. straight-line programs) that form simpler verification tasks. Because the verification conditions are not encoded in logic syntax (such as SMT), but as programs in the language of the original program, any off-the-shelf verifier can be used to verify the generated straight-line programs. In case the validation fails, useful information can be extracted about which part of the proof failed (which straight-line programs are wrong). We show that our approach works by evaluating it on a suitable benchmark. Supplementary website: https://www.sosy-lab.org/research/liv/","Program Analysis,Software Verification,Formal Methods,Program Correctness,Automatic Verification,Verification Tools,Provers,Program Invariants",LIV：使用直线程序的循环不变量验证,程序不变量（也称为正确性见证人）的验证是软件验证中的一个既定过程。在越来越复杂的软件系统的验证方面取得了稳步的进展，但找到好的循环不变量仍然是许多验证器的中心任务。虽然构造安全和归纳不变量通常需要大量计算，但它们更容易自动验证。我们提出了LIV，这是一种用于循环不变量验证的新工具，它使检查验证器产生的不变量是否足以建立归纳安全证明变得更加实用。主要思想是在程序级别上应用分而治之：我们将程序拆分为更小的无循环程序（也称为直线程序），这些程序形成更简单的验证任务。由于验证条件不是用逻辑语法（如SMT）编码的，而是作为原始程序语言的程序，任何现成的验证器都可以用来验证生成的直线程序。如果验证失败，可以提取关于证明的哪一部分失败的有用信息（哪些直线程序是错误的）。我们通过在合适的基准上对其进行评估来证明我们的方法是有效的。补充网站：https://www.sosy-lab.org/research/liv/,程序分析，软件验证，形式化方法，程序正确性，自动验证，验证工具，校验器，程序不变量,,,
Q68EY38Z,2023,https://doi.org/10.1109/ASE56229.2023.00154,ASE 2023,TEASER: Simulation-Based CAN Bus Regression Testing for Self-Driving Cars Software,"Safety-critical systems such as self-driving cars (SDCs) must be rigorously tested. Especially electronic control units (ECUs) of SDCs should be tested with realistic input data. In this context, a communication protocol called Controller Area Network (CAN) is typically used to transfer sensor data to the SDC control units. A challenge for SDC maintainers and testers is the need to manually define the CAN inputs that realistically represent the state of the SDC in the real world. To address this challenge, we developed TEASER; a tool that generates realistic CAN signals for SDCs obtained from sensors from state-of-the-art car simulators. We evaluated TEASER based on its integration capability into a DevOps pipeline of aicas GmbH, a company in the automotive sector. Concretely, we integrated TEASER in a Continous Integration (CI) pipeline configured with Jenkins. The pipeline executes the test cases in simulation environments and sends the sensor data over the CAN bus to a physical CAN device, the test subject. Our evaluation shows the ability of TEASER to generate and execute CI test cases that expose simulation-based faults (using regression strategies); the tool produces CAN inputs that realistically represent the state of the SDC in the real world. This result is critically important for increasing the automation and effectiveness of simulation-based CAN bus regression testing for SDCs.","Autonomous systems,Regression Testing,Simulation Environment,CAN Bus",TEASER：基于仿真的自动驾驶汽车CAN总线回归测试软件,自动驾驶汽车（SDCs）等安全关键系统必须经过严格测试。特别是SDCs的电子控制单元（ECU）应使用真实的输入数据进行测试。在这种情况下，称为控制器局域网（CAN）的通信协议通常用于将传感器数据传输到SDC控制单元。SDC维护人员和测试人员面临的挑战是需要手动定义CAN输入，以真实地表示SDC在现实世界中的状态。为了应对这一挑战，我们开发了TEASER；一种为SDCs生成真实CAN信号的工具，该信号是从最先进的汽车模拟器的传感器中获得的。我们根据TEASER与汽车行业公司aicas GmbH的DevOps管道的集成能力对其进行了评估。具体地说，我们将TEASER集成在由Jenkins配置的连续集成（CI）管道中。管道在模拟环境中执行测试用例，并通过CAN总线将传感器数据发送到测试对象的物理CAN设备。我们的评估显示了TEASER生成和执行暴露基于模拟的故障的CI测试用例的能力（使用回归策略）；该工具产生真实地表示SDC在现实世界中的状态的CAN输入。这一结果对于提高基于模拟的SDCs CAN总线回归测试的自动化和有效性至关重要。,自主系统，回归测试，仿真环境，CAN总线,,,
KH2TBJ5Y,2023,https://doi.org/10.1109/ASE56229.2023.00215,ASE 2023,cegar-pt: A Tool for Abstraction by Program Transformation,"Abstraction is an important approach for proving the correctness of computer programs. There are many implementations of this approach available, but unfortunately, the various implementations are difficult to reuse and combine, and the successful techniques have to be re-implemented in new tools again and again. We address this problem by contributing the tool cegar-pt, which views abstraction as program transformation and integrates different verification components off-the-shelf. The idea is to use existing components without having to change their implementation, while still adjusting the precision of the abstraction using the successful CEGAR approach. The approach of cegar-pt is largely general: It only restricts the abstraction to transform, given a precision that defines the level of abstraction, one program into another program. The abstraction by program transformation can over-approximate the data flow (e.g., havoc some variables, use more abstract types) or the control flow (e.g., loop abstraction, slicing). To illustrate our tool, we provide a demonstration video, accessible at https://youtu.be/ASZ6hoq8asE.","Software Verification,Program Analysis,Loop Abstraction,Precision Adjustment,CEGAR,CPAchecker,Program Transformations",cegar pt:一种通过程序转换进行抽象的工具,摘要是证明计算机程序正确性的一种重要方法。这种方法有很多实现，但不幸的是，各种实现很难重用和组合，成功的技术必须在新的工具中一次又一次地重新实现。我们通过提供工具cegar pt来解决这个问题，该工具将抽象视为程序转换，并集成现成的不同验证组件。其思想是使用现有组件而不必更改其实现，同时仍然使用成功的CEGAR方法调整抽象的精度。cegar pt的方法在很大程度上是通用的：它只限制抽象将一个程序转换为另一个程序，给定定义抽象级别的精度。通过程序转换的抽象可以过度近似数据流（例如，破坏一些变量，使用更抽象的类型）或控制流（例如循环抽象、切片）。为了说明我们的工具，我们提供了一个演示视频，可访问https://youtu.be/ASZ6hoq8asE.,软件验证，程序分析，循环抽象，精度调整，CEGAR，CPAchecker，程序转换,,,
J9YR9KFW,2023,https://doi.org/10.1109/ASE56229.2023.00066,ASE 2023,Improving Design Reviews at Google,"Design review is an important initial phase of the software development life-cycle where stakeholders gain and discuss early insights into the design's viability, discover potentially costly mistakes, and identify inconsistencies and inadequacies. For improved development velocity, it is important that design owners get their designs approved as Quickly as possible. In this paper, we discuss how engineering design reviews are typically conducted at Google, and propose a novel, structured, automated solution to improve design review velocity. Based on data collected on 141,652 approved documents authored by 41,030 users over four years, we show that our proposed solution decreases median time-to-approval by 25%, and provides further gains when used consistently. We also provide qualitative data to demonstrate our solution's success, discuss factors that impact design review latency, propose strategies to tackle them, and share lessons learned from the usage of our solution.","design,design review,review and evaluation,peer reviewing,architecture review,engineering design",改进谷歌的设计评论,设计评审是软件开发生命周期的一个重要初始阶段，利益相关者在这里获得并讨论对设计可行性的早期见解，发现潜在的代价高昂的错误，并识别不一致和不足之处。为了提高开发速度，设计所有者尽快获得设计批准是很重要的。在本文中，我们讨论了谷歌通常是如何进行工程设计审查的，并提出了一种新颖、结构化、自动化的解决方案来提高设计审查速度。基于对41030名用户在四年内撰写的141652份已批准文档收集的数据，我们发现，我们提出的解决方案将中位批准时间缩短了25%，并且在持续使用时提供了进一步的收益。我们还提供定性数据来证明我们的解决方案的成功，讨论影响设计审查延迟的因素，提出解决这些问题的策略，并分享从使用我们的解决方法中吸取的经验教训。,设计，设计评审，评审和评估，同行评审，架构评审，工程设计,,,
B63RFBNC,2023,https://doi.org/10.1109/ASE56229.2023.00209,ASE 2023,Compiler Auto-Tuning via Critical Flag Selection,"Widely used compilers like GCC usually have hundreds of optimizations controlled by optimization flags, which can be enabled or disabled during compilation to improve the runtime performance of a compiled program. Due to the large number of optimization flags and their combination, it is difficult for compiler users to tune compiler optimization flags manually. In the literature, many auto-tuning techniques have been proposed, which find a desired setting on all optimization flags (i.e., an optimization sequence) by designing different search strategies in the entire optimization space. Due to the huge search space, these techniques suffer from the widely-recognized efficiency problem. To reduce the search space, in this paper, we propose a critical-flag selection based approach CFSCA which first finds flags potentially relevant to the target program by analyzing program structure and compiler documentation, and then identifies critical flags through statistical analysis on the program's predicted runtime performance with various optimization sequences. With the reduced search space, CFSCA selects a desired optimization sequence. To evaluate the performance of the proposed approach CFSCA, we conduct an extensive experimental study on the latest version of the compiler GCC with a widely used benchmark cBench. The experimental results show that CFSCA significantly outperforms the four compared techniques, including the state-of-art technique BOCA.","Compiler,Compiler Auto-tuning,Critical Flag Selection,Search",通过关键标志选择实现编译器自动调整,广泛使用的编译器，如GCC，通常有数百个由优化标志控制的优化，可以在编译过程中启用或禁用优化标志，以提高编译程序的运行时性能。由于大量的优化标志及其组合，编译器用户很难手动调整编译器优化标志。在文献中，已经提出了许多自动调整技术，其通过在整个优化空间中设计不同的搜索策略来在所有优化标志（即优化序列）上找到期望的设置。由于搜索空间巨大，这些技术存在着公认的效率问题。为了减少搜索空间，本文提出了一种基于关键标志选择的CFSCA方法，该方法首先通过分析程序结构和编译器文档来找到与目标程序潜在相关的标志，然后通过对程序在各种优化序列下的预测运行时性能进行统计分析来识别关键标志。随着搜索空间的减少，CFSCA选择所需的优化序列。为了评估所提出的方法CFSCA的性能，我们使用广泛使用的基准cBench对最新版本的编译器GCC进行了广泛的实验研究。实验结果表明，CFSCA显著优于四种比较技术，包括最先进的技术BOCA。,编译器，编译器自动调整，关键标志选择，搜索,,,
XQF9SEJM,2023,https://doi.org/10.1109/ASE56229.2023.00137,ASE 2023,A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models,"Patch robustness certification ensures no patch within a given bound on a sample can manipulate a deep learning model to predict a different label. However, existing techniques cannot certify samples that cannot meet their strict bars at the classifier level or the patch region level. This paper proposes MajorCert. MajorCert firstly finds all possible label sets manipulatable by the same patch region on the same sample across the underlying classifiers, then enumerates their combinations element-wise, and finally checks whether the majority invariant of all these combinations is intact to certify samples.","Patch robustness,certification,invariant",一种用于深度学习模型的补丁鲁棒性认证的多数不变方法,补丁稳健性认证确保样本上给定边界内的任何补丁都不能操纵深度学习模型来预测不同的标签。然而，现有技术无法证明在分类器级别或补丁区域级别不能满足其严格标准的样本。本文提出了MajorCert。MajorCert首先在底层分类器中找到同一样本上同一补丁区域可操作的所有可能的标签集，然后逐元素枚举它们的组合，最后检查所有这些组合的多数不变量是否完整，以验证样本。,补丁健壮性，认证，不变,,,
DCMSS4CW,2023,https://doi.org/10.1109/ASE56229.2023.00188,ASE 2023,WADIFF: A Differential Testing Framework for WebAssembly Runtimes,"WebAssembly (Wasm) runtime provides a virtual machine that can execute the WebAssembly modules and is widely used in different areas (e.g., browsers, edge computing, blockchain). Thus, the precision and reliability of the WebAssembly runtime are important and deserve our attention. To ensure the correctness and detect potential bugs in WebAssembly runtimes, we propose WADIFF, a differential testing framework, which consists of a sufficient test case generator and a deterministic differential testing engine. To evaluate the effectiveness of WADIFF, we apply it to seven popular WebAssembly runtimes and found 417 inconsistent instructions due to bugs and different implementations in the runtimes. Furthermore, we identify 21 bugs from 7 WebAssembly runtimes, and 8 of them are confirmed by their developers.","WebAssembly,Differential Testing",WADIFF：一个用于WebAssembly运行时的差分测试框架,WebAssembly（Wasm）运行时提供了一个可以执行WebAssembly模块的虚拟机，并广泛应用于不同领域（例如浏览器、边缘计算、区块链）。因此，WebAssembly运行时的准确性和可靠性非常重要，值得我们关注。为了确保WebAssembly运行时的正确性并检测潜在的错误，我们提出了WADIFF，这是一个差分测试框架，它由一个足够的测试用例生成器和一个确定的差分测试引擎组成。为了评估WADIFF的有效性，我们将其应用于七个流行的WebAssembly运行时，发现417条指令不一致，原因是运行时存在错误和实现方式不同。此外，我们从7个WebAssembly运行时中发现了21个bug，其中8个已经得到了开发人员的确认。,WebAssembly，差分测试,,,
LEVZEHPG,2023,https://doi.org/10.1109/ASE56229.2023.00176,ASE 2023,An Automated and Flexible Multilingual Bug-Fix Dataset Construction System,"Developing effective data-driven automated bug-fixing approaches is heavily relying on large bug-fix datasets. However, the granularity of current repository-mined bug-fixing datasets is usually at the function level, without meta-information such as the fault type. In order to alleviate the open challenge of precisely mining code snippets with bugs, their fix, location, and types from open source repositories, in this paper, we propose a flexible, extensible, and automated multilingual bug-fix dataset construction system, that is, the Multilingual Bug-Fix Constructor (MBFC). Furthermore, we release a large-scale and fine-grained Multi-lingual Bug-Fix (M-BF) dataset automatically built using the proposed system, which includes a total of 921,825 Bug-Fix pairs that are from 442,164 different open-source software projects starting from January 2020 to September 2020 in the initial version. It is expected that our system and dataset can benefit the development of innovative and practical program repair methods, thereby improving the efficiency of program debugging and code review processes.","automated dataset constructing,bug-fix dataset,automated program repair,neural program repair",一个自动化灵活的多语言Bug修复数据集构建系统,开发有效的数据驱动的自动错误修复方法在很大程度上依赖于大型错误修复数据集。然而，当前存储库挖掘的bug修复数据集的粒度通常在功能级别，没有故障类型等元信息。为了缓解从开源存储库中精确挖掘包含错误、错误修复、位置和类型的代码片段的开放挑战，本文提出了一个灵活、可扩展和自动化的多语言错误修复数据集构建系统，即多语言错误纠正构造器（MBFC）。此外，我们发布了一个使用所提出的系统自动构建的大规模、细粒度的多语言Bug Fix（M-BF）数据集，该数据集包括921825个Bug修复对，它们来自442164个不同的开源软件项目，从2020年1月到2020年9月的初始版本。我们的系统和数据集有望有助于开发创新实用的程序修复方法，从而提高程序调试和代码审查过程的效率。,自动数据集构建，错误修复数据集，自动程序修复，神经程序修复,,,
2CHT56E5,2023,https://doi.org/10.1109/ASE56229.2023.00087,ASE 2023,Symbolic Verification of Fuzzy Logic Models,"Fuzzy logic is widely applied in various applications. However, verifying the correctness of fuzzy logic models can be difficult. This extended abstract presents our ongoing work on verifying fuzzy logic models. We treat a fuzzy logic model as a program and propose a verification method based on symbolic execution for fuzzy logic models. We have developed and implemented the environment models for the common functions and the inference rules in fuzzy logic models. Our preliminary evaluation shows the potential of our verification method.","fuzzy logic model,verification,symbolic execution,SMT",模糊逻辑模型的符号验证,模糊逻辑广泛应用于各种应用中。然而，验证模糊逻辑模型的正确性可能很困难。这个扩展摘要介绍了我们正在进行的模糊逻辑模型验证工作。我们将模糊逻辑模型视为一个程序，并提出了一种基于符号执行的模糊逻辑模型验证方法。我们开发并实现了模糊逻辑模型中常见函数的环境模型和推理规则。我们的初步评估显示了我们的核查方法的潜力。,模糊逻辑模型，验证，符号执行，SMT,,,
6AKE3VKY,2023,https://doi.org/10.1109/ASE56229.2023.00157,ASE 2023,The Devil is in the Tails: How Long-Tailed Code Distributions Impact Large Language Models,"Learning-based techniques, especially advanced Large Language Models (LLMs) for code, have gained considerable popularity in various software engineering (SE) tasks. However, most existing works focus on designing better learning-based models and pay less attention to the properties of datasets. Learning-based models, including popular LLMs for code, heavily rely on data, and the data's properties (e.g., data distribution) could significantly affect their behavior. We conducted an exploratory study on the distribution of SE data and found that such data usually follows a skewed distribution (i.e., long-tailed distribution) where a small number of classes have an extensive collection of samples, while a large number of classes have very few samples. We investigate three distinct SE tasks and analyze the impacts of long-tailed distribution on the performance of LLMs for code. Our experimental results reveal that the long-tailed distribution has a substantial impact on the effectiveness of LLMs for code. Specifically, LLMs for code perform between 30.0% and 254.0% worse on data samples associated with infrequent labels compared to data samples of frequent labels. Our study provides a better understanding of the effects of long-tailed distributions on popular LLMs for code and insights for the future development of SE automation.","Codes,Automation,Tail,Data models,Behavioral sciences,Task analysis,Software engineering",魔鬼在尾巴上：长尾代码分布如何影响大型语言模型,基于学习的技术，特别是用于代码的高级大型语言模型（LLM），在各种软件工程（SE）任务中已经相当流行。然而，现有的大多数工作都专注于设计更好的基于学习的模型，而很少关注数据集的特性。基于学习的模型，包括流行的代码LLM，在很大程度上依赖于数据，而数据的属性（例如数据分布）可能会显著影响它们的行为。我们对SE数据的分布进行了探索性研究，发现此类数据通常遵循偏斜分布（即长尾分布），其中少数类别具有大量样本，而大量类别的样本很少。我们研究了三个不同的SE任务，并分析了长尾分布对代码LLM性能的影响。我们的实验结果表明，长尾分布对代码LLM的有效性有很大影响。具体而言，与频繁标签的数据样本相比，代码的LLM在与不频繁标签相关的数据样本上的表现差30.0%至254.0%。我们的研究为代码提供了对长尾分布对流行LLM的影响的更好理解，并为SE自动化的未来发展提供了见解。,代码，自动化，Tail，数据模型，行为科学，任务分析，软件工程,,,
AKGJ9I2P,2023,https://doi.org/10.1109/ASE56229.2023.00081,ASE 2023,Expediting Neural Network Verification via Network Reduction,"A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including $\alpha,\beta$ -crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also show that network reduction can improve the availability of existing verification tools on many networks by reducing them into sequential neural networks.","Neural Network Verification,Network Reduction,Pre-processing",通过网络约简加速神经网络验证,已经提出了一系列验证方法来验证深度神经网络的安全特性，以确保网络在关键应用中正确运行。然而，许多知名的验证工具仍然难以应对复杂的网络架构和庞大的网络规模。在这项工作中，我们提出了一种网络约简技术作为验证前的预处理方法。所提出的方法通过消除稳定的ReLU神经元来减少神经网络，并将其转换为由ReLU和仿射层组成的序列神经网络，这可以由大多数验证工具处理。我们在最先进的完整和不完整验证工具上实例化了约简技术，包括$\alpha、\beta$-crown、VeriNet和PRIMA。我们在一组大型基准上的实验表明，所提出的技术可以显著减少神经网络并加快现有的验证工具。此外，实验结果还表明，网络约简可以通过将许多网络上的现有验证工具简化为顺序神经网络来提高其可用性。,神经网络验证，网络约简，预处理,,,
5FRIXURB,2023,https://doi.org/10.1109/ASE56229.2023.00126,ASE 2023,CertPri: Certifiable Prioritization for Deep Neural Networks via Movement Cost in Feature Space,"Deep neural networks (DNNs) have demonstrated their outperformance in various software systems, but also exhibit misbehavior and even result in irreversible disasters. Therefore, it is crucial to identify the misbehavior of DNN-based software and improve DNNs' quality. Test input prioritization is one of the most appealing ways to guarantee DNNs' quality, which prioritizes test inputs so that more bug-revealing inputs can be identified earlier with limited time and manual labeling efforts. However, the existing prioritization methods are still limited from three aspects: certifiability, effectiveness, and generalizability. To overcome the challenges, we propose CertPri, a test input prioritization technique designed based on a movement cost perspective of test inputs in DNNs' feature space. CertPri differs from previous works in three key aspects: (1) certifiable - it provides a formal robustness guarantee for the movement cost; (2) effective - it leverages formally guaranteed movement costs to identify malicious bug-revealing inputs; and (3) generic - it can be applied to various tasks, data, models, and scenarios. Extensive evaluations across 2 tasks (i.e., classification and regression), 6 data forms, 4 model structures, and 2 scenarios (i.e., white-box and black-box) demonstrate CertPri's superior performance. For instance, it significantly improves 53.97 % prioritization effectiveness on average compared with baselines. Its robustness and generalizability are 1.41~2.00 times and 1.33~3.39 times that of baselines on average, respectively. The code of CertPri is open-sourced at https://github.com/haibinzheng/CertPri.","Deep neural network,test input prioritization,deep learning testing,movement cost,certifiable prioritization",CertPri：通过特征空间中的移动成本实现深度神经网络的可认证优先级,深度神经网络（DNN）在各种软件系统中都表现出了优异的性能，但也表现出不当行为，甚至导致不可逆转的灾难。因此，识别基于DNN的软件的不当行为，提高DNN的质量至关重要。测试输入优先级是保证DNN质量的最有吸引力的方法之一，它对测试输入进行优先级排序，以便在有限的时间和手动标记工作的情况下更早地识别出更多揭示错误的输入。然而，现有的优先级排序方法仍然从可证明性、有效性和可推广性三个方面受到限制。为了克服这些挑战，我们提出了CertPri，这是一种基于DNN特征空间中测试输入的移动成本视角设计的测试输入优先级排序技术。CertPri在三个关键方面与以往的工作不同：（1）可证明性——它为移动成本提供了形式上的稳健性保证；（2） 有效-它利用正式保证的移动成本来识别恶意漏洞暴露输入；和（3）通用-它可以应用于各种任务、数据、模型和场景。对2个任务（即分类和回归）、6个数据表单、4个模型结构和2个场景（即白盒和黑盒）的广泛评估证明了CertPri的卓越性能。例如，与基线相比，它平均显著提高了53.97%的优先级有效性。其稳健性和可推广性分别是基线的1.41~2.00倍和1.33~3.39倍。CertPri的代码开源于https://github.com/haibinzheng/CertPri.,深度神经网络，测试输入优先级，深度学习测试，移动成本，可认证优先级,,,
LG5AS8UX,2023,https://doi.org/10.1109/ASE56229.2023.00061,ASE 2023,EndWatch: A Practical Method for Detecting Non-Termination in Real-World Software,"Detecting non-termination is crucial for ensuring program correctness and security, such as preventing denial-of-service attacks. While termination analysis has been studied for many years, existing methods have limited scalability and are only effective on small programs. To address this issue, we propose a practical termination checking technique, called EndWatch, for detecting non-termination caused by infinite loops through testing. Specifically, we introduce two methods to generate non-termination oracles based on checking state revisits, i.e., if the program returns to a previously visited state at the same program location, it does not terminate. The non-termination oracles can be incorporated into testing tools (e.g., AFL used in this paper) to detect non-termination in large programs. For linear loops, we perform symbolic execution on individual loops to infer State Revisit Conditions (SRCs) and instrument SRCs into target loops. For non-linear loops, we instrument target loops for checking concrete state revisits during execution. We evaluated EndWatch on standard benchmarks with small-sized programs and real-world projects with large-sized programs. The evaluation results show that EndWatch is more effective than the state-of-the-art tools on standard benchmarks (detecting 87% of non-terminating programs while the best baseline detects only 67%), and useful in detecting non-termination in real-world projects (detecting 90% of known non-termination CVEs and 4 unknown bugs).","Non-termination detection,Static analysis,Dynamic testing,Testing oracle generation",EndWatch：一种在现实软件中检测非终止的实用方法,检测未终止对于确保程序的正确性和安全性至关重要，例如防止拒绝服务攻击。虽然终止分析已经研究了很多年，但现有的方法可扩展性有限，仅对小程序有效。为了解决这个问题，我们提出了一种实用的终止检查技术，称为EndWatch，用于通过测试检测由无限循环引起的非终止。具体来说，我们介绍了两种基于检查状态重访生成非终止预言机的方法，即，如果程序在同一程序位置返回到以前访问过的状态，则不会终止。非终止预言机可以被纳入测试工具（例如本文中使用的AFL）中，以检测大型程序中的非终止。对于线性循环，我们在单个循环上执行符号执行，以推断状态重访条件（SRC），并将SRC仪器化为目标循环。对于非线性循环，我们对目标循环进行检测，以检查执行过程中的具体状态重新访问。我们在小型程序的标准基准和大型程序的真实项目上评估了EndWatch。评估结果表明，在标准基准测试中，EndWatch比最先进的工具更有效（检测到87%的非终止程序，而最佳基准测试仅检测到67%），在检测现实世界项目中的非终止方面也很有用（检测到90%的已知非终止CVE和4个未知错误）。,非终止检测，静态分析，动态测试，测试预言机生成,,,
8C8R7XU5,2023,https://doi.org/10.1109/ASE56229.2023.00059,ASE 2023,EALink: An Efficient and Accurate Pre-Trained Framework for Issue-Commit Link Recovery,"Issue-commit links, as a type of software traceability links, play a vital role in various software development and maintenance tasks. However, they are typically deficient, as developers often forget or fail to create tags when making commits. Existing studies have deployed deep learning techniques, including pretrained models, to improve automatic issue-commit link recovery. Despite their promising performance, we argue that previous approaches have four main problems, hindering them from recovering links in large software projects. To overcome these problems, we propose an efficient and accurate pre-trained framework called EALink for issue-commit link recovery. EALink requires much fewer model parameters than existing pre-trained methods, bringing efficient training and recovery. Moreover, we design various techniques to improve the recovery accuracy of EALink. We construct a large-scale dataset and conduct extensive experiments to demonstrate the power of EALink. Results show that EALink outperforms the state-of-the-art methods by a large margin (15.23%-408.65%) on various evaluation metrics. Meanwhile, its training and inference overhead is orders of magnitude lower than existing methods. We provide our implementation and data at https://github.com/KDEGroup/EALink.","issue-commit link recovery,software traceability",EALink：一个高效、准确的问题提交链接恢复预训练框架,问题提交链接作为软件可追溯性链接的一种，在各种软件开发和维护任务中发挥着至关重要的作用。然而，它们通常是有缺陷的，因为开发人员在提交时经常忘记或无法创建标记。现有研究已经部署了深度学习技术，包括预训练的模型，以改进自动问题提交链接恢复。尽管它们的性能很有希望，但我们认为以前的方法有四个主要问题，阻碍了它们在大型软件项目中恢复链接。为了克服这些问题，我们提出了一个高效、准确的预训练框架EALink，用于问题提交链接恢复。EALink比现有的预训练方法需要更少的模型参数，从而带来高效的训练和恢复。此外，我们还设计了各种技术来提高EALink的恢复精度。我们构建了一个大规模的数据集，并进行了大量的实验来证明EALink的威力。结果表明，EALink在各种评估指标上都大大优于最先进的方法（15.23%-408.65%）。同时，它的训练和推理开销比现有方法低几个数量级。我们在https://github.com/KDEGroup/EALink.,问题提交链接恢复，软件可追溯性,,,
EA38W4GM,2023,https://doi.org/10.1109/ASE56229.2023.00072,ASE 2023,OrdinalFix: Fixing Compilation Errors via Shortest-Path CFL Reachability,"The development of correct and efficient software can be hindered by compilation errors, which must be fixed to ensure the code's syntactic correctness and program language constraints. Neural network-based approaches have been used to tackle this problem, but they lack guarantees of output correctness and can require an unlimited number of modifications. Fixing compilation errors within a given number of modifications is a challenging task. We demonstrate that finding the minimum number of modifications to fix a compilation error is NP-hard. To address compilation error fixing problem, we propose OrdinalFix, a complete algorithm based on shortest-path CFL (context-free language) reachability with attribute checking that is guaranteed to output a program with the minimum number of modifications required. Specifically, OrdinalFix searches possible fixes from the smallest to the largest number of modifications. By incorporating merged attribute checking to enhance efficiency, the time complexity of OrdinalFix is acceptable for application. We evaluate OrdinalFix on two datasets and demonstrate its ability to fix compilation errors within reasonable time limit. Comparing with existing approaches, OrdinalFix achieves a success rate of 83.5 %, surpassing all existing approaches (71.7%).","Compilation Error,CFL Reachability",OrdinalFix:通过最短路径CFL可达性修复编译错误,编译错误可能会阻碍正确高效软件的开发，必须修复编译错误以确保代码的语法正确性和程序语言约束。基于神经网络的方法已经被用来解决这个问题，但它们缺乏输出正确性的保证，并且可能需要无限数量的修改。在给定数量的修改中修复编译错误是一项具有挑战性的任务。我们证明，找到修复编译错误的最小修改次数是NP困难的。为了解决编译错误修复问题，我们提出了OrdinalFix，这是一种基于最短路径CFL（上下文无关语言）可达性和属性检查的完整算法，可以保证以最少的修改次数输出程序。具体来说，OrdinalFix从修改的最小数量到最大数量搜索可能的修复。通过结合合并属性检查来提高效率，OrdinalFix的时间复杂性对于应用程序来说是可以接受的。我们在两个数据集上评估了OrdinalFix，并证明了它在合理的时间限制内修复编译错误的能力。与现有方法相比，OrdinalFix的成功率为83.5%，超过了所有现有方法（71.7%）。,编译错误，CFL可达性,,,
U7UUUJAN,2023,https://doi.org/10.1109/ASE56229.2023.00011,ASE 2023,DroneReqValidator: Facilitating High Fidelity Simulation Testing for Uncrewed Aerial Systems Developers,"Rigorous testing of small Uncrewed Aerial Systems (sUAS) is crucial to ensure their safe and reliable deployment in the real world. sUAS developers aim to validate the reliability and safety of their applications through simulation testing. However, the dynamic nature of the real-world environment, including factors such as challenging weather conditions and wireless interference, causes unique software faults that may only be revealed through field testing. Considering the high cost and impracticality of conducting field testing in thousands of environmental contexts and conditions, there exists a pressing need to develop automated techniques that can generate high-fidelity, realistic environments enabling sUAS developers to deploy their applications and conduct thorough simulation testing in close- to- reality environmental conditions. To address this need, DroneReqValidator (DRV) offers a comprehensive small Unmanned Aerial Vehicle (sUAV) simulation ecosystem that automatically generates realistic environments based on developer-specified constraints, monitors sUAV activities against predefined safety parameters, and generates detailed acceptance test reports for effective debugging and analysis of sUAV applications. Providing these capabilities, DRV offers a valuable solution for enhancing the testing and development process of sUAS. The comprehensive demo of DRV is available at https://www.youtube.com/watch?v=Fd9ft55gbO8","Simulation,Automated Analysis,Uncrewed Aerial Vehicles",DroneReqValidator：为未编码的航空系统开发人员提供高保真度模拟测试,对小型无人机系统（sUAS）进行严格测试对于确保其在现实世界中的安全可靠部署至关重要。sUAS开发人员的目标是通过模拟测试来验证其应用程序的可靠性和安全性。然而，现实世界环境的动态性质，包括具有挑战性的天气条件和无线干扰等因素，会导致独特的软件故障，这些故障只能通过现场测试来揭示。考虑到在数千种环境和条件下进行现场测试的高成本和不切实际性，迫切需要开发能够生成高保真、逼真环境的自动化技术，使无人机开发人员能够部署他们的应用程序，并在接近现实的环境条件下进行彻底的模拟测试。为了满足这一需求，DroneReqValidator（DRV）提供了一个全面的小型无人机（sUAV）模拟生态系统，该生态系统根据开发人员指定的约束条件自动生成逼真的环境，根据预定义的安全参数监测sUAV活动，并生成详细的验收测试报告，用于有效调试和分析sUAV应用程序。DRV提供了这些功能，为增强无人飞行器的测试和开发过程提供了一个有价值的解决方案。DRV的全面演示可在https://www.youtube.com/watch?v=Fd9ft55gbO8,模拟，自动分析，无人机,,,
ENZGDISP,2023,https://doi.org/10.1109/ASE56229.2023.00141,ASE 2023,How Android Apps Break the Data Minimization Principle: An Empirical Study,"The Data Minimization Principle is crucial for protecting individual privacy. However, existing Android runtime permissions do not guarantee this principle. Moreover, the lack of an automatic enforcement mechanism leads to uncertainty as to whether apps strictly comply with this principle. To bridge this gap, we conduct the first systematic empirical study on violations of the Data Minimization Principle and design a new enforcement tool called GUIMind to detect them. GUIMind first utilizes a reinforcement learning model to explore app activities and monitor access to sensitive APIs that require sensitive permissions, and then it leverages an existing tool to detect such violations. We evaluate the performance of GUIMind using 120 real-world Android apps. The results indicate that GUIMind can achieve a detection accuracy of 96.1%, effectively accelerating the empirical study. Our empirical research is mainly focused on the prevalence of violations, the responses of administrators to violations, and the potential factors and characteristics that lead to violations, such as typical violations, app categories, and personal data types. Our study reveals that 83.5% of apps contain at least one privacy violation, with health apps being the most severe. In addition, telephony information is the most commonly leaked personal data type, accounting for 71.1%. Finally, we randomly selected 60 non-compliant apps for reporting to the administrator, whose responses confirm the effectiveness of our approach.","Data minimization principle,Privacy violation detection,Reinforcement learning",安卓应用如何打破数据最小化原则：一项实证研究,数据最小化原则对于保护个人隐私至关重要。然而，现有的Android运行时权限并不能保证这一原则。此外，由于缺乏自动执行机制，应用程序是否严格遵守这一原则也存在不确定性。为了弥补这一差距，我们对违反数据最小化原则的行为进行了首次系统的实证研究，并设计了一种名为GUIMind的新执法工具来检测它们。GUIMind首先利用强化学习模型来探索应用程序活动，并监控对需要敏感权限的敏感API的访问，然后利用现有工具来检测此类违规行为。我们使用120个真实世界的Android应用程序来评估GUIMind的性能。结果表明，GUIMind可以实现96.1%的检测准确率，有效地加速了实证研究。我们的实证研究主要集中在违规行为的普遍性、管理员对违规行为的反应，以及导致违规行为的潜在因素和特征，如典型违规行为、应用类别和个人数据类型。我们的研究显示，83.5%的应用程序至少包含一次隐私侵犯，其中健康应用程序最为严重。此外，电话信息是最常见的泄露个人数据类型，占71.1%。最后，我们随机选择了60个不合规的应用程序向管理员报告，管理员的回复证实了我们方法的有效性。,数据最小化原则，隐私违规检测，强化学习,,,
HNPVS7DW,2023,https://doi.org/10.1109/ASE56229.2023.00071,ASE 2023,ICTDroid: Parameter-Aware Combinatorial Testing for Components of Android Apps,"Components are the fundamental building blocks of Android applications. Different functional modules represented by components often rely on inter-component communication mechanisms to achieve cross-module data transfer and method invocation. It is necessary to conduct robustness testing on components to prevent component launching crashes and privacy leaks caused by unexpected input parameters. However, as the complexity of the input parameter structure and the diversity of possible inputs, developers may overlook specific inputs that result in exceptions. At the same time, the vast input space also brings challenges to efficient component testing. In this paper, we designed an automated test generation and execution tool for Android application components named ICTDroid, which combines static parameter extraction and adaptive-strength combinatorial testing generation to detect bugs with a compact test suite. Experiments have shown that the tool triggers 205 unique exceptions in 30 open-source applications with 1,919 test cases in 83 minutes, where the developers have confirmed six defects in three issues we reported.","Android App,Inter-Component Communication,Combinatorial Testing,Automated Testing",ICTDroid：Android应用程序组件的参数感知组合测试,组件是Android应用程序的基本构建块。以组件为代表的不同功能模块往往依赖于组件间的通信机制来实现跨模块的数据传输和方法调用。有必要对组件进行稳健性测试，以防止意外输入参数导致的组件启动崩溃和隐私泄露。然而，由于输入参数结构的复杂性和可能输入的多样性，开发人员可能会忽略导致异常的特定输入。同时，庞大的输入空间也给高效的组件测试带来了挑战。在本文中，我们为Android应用程序组件设计了一个名为ICTDroid的自动测试生成和执行工具，该工具结合了静态参数提取和自适应强度组合测试生成，以通过紧凑的测试套件来检测错误。实验表明，该工具在83分钟内触发了30个开源应用程序中的205个独特异常，其中1919个测试用例，开发人员在我们报告的三个问题中确认了六个缺陷。,安卓应用程序，组件间通信，组合测试，自动化测试,,,
C4HUFTGF,2023,https://doi.org/10.1109/ASE56229.2023.00139,ASE 2023,Eiffel: Inferring Input Ranges of Significant Floating-point Errors via Polynomial Extrapolation,"Existing search heuristics used to find input values that result in significant floating-point (FP) errors or small ranges that cover them are accompanied by severe constraints, complicating their implementation and restricting their general applicability. This paper introduces an error analysis tool called Eiffel to infer error-inducing input ranges instead of searching them. Given an FP expression with its domain $\mathcal{D}$, Eiffel first constructs an error data set by sampling values across a smaller domain $\mathcal{R}$ and assembles these data into clusters. If more than two clusters are formed, Eiffel derives polynomial curves that best fit the bound coordinates of the error-inducing ranges in $\mathcal{R}$, extrapolating them to infer all target ranges of $\mathcal{D}$ and reporting the maximal error. Otherwise, Eiffel simply returns the largest error across $\mathcal{R}$. Experimental results show that Eiffel exhibits a broader applicability than Atomu and $\mathbf{S}^{3}$ FP by successfully detecting the errors of all 70 considered benchmarks while the two baselines only report errors for part of them. By taking as input the inferred ranges of Eiffel, Herbie obtains an average accuracy improvement of 3.35 bits and up to 53.3 bits.","Extrapolation,Error analysis,Benchmark testing,Software engineering",Eiffel：通过多项式外推推断有效浮点误差的输入范围,用于查找导致显著浮点（FP）错误或覆盖这些错误的小范围的输入值的现有搜索启发式方法伴随着严重的约束，使其实现复杂化，并限制了其普遍适用性。本文介绍了一种称为Eiffel的误差分析工具，用于推断引起误差的输入范围，而不是搜索它们。给定域为$\mathcal｛D｝$的FP表达式，Eiffel首先通过在较小的域$\mathical｛R｝$中采样值来构造错误数据集，并将这些数据组装成簇。如果形成两个以上的簇，则Eiffel导出最适合$\mathcal｛R｝$中的误差诱导范围的边界坐标的多项式曲线，对其进行外推以推断$\mathical｛D｝$的所有目标范围，并报告最大误差。否则，Eiffel只返回$\mathcal｛R｝$中的最大错误。实验结果表明，与Atomu和$\mathbf｛S｝^｛3｝$FP相比，Eiffel表现出更广泛的适用性，因为它成功地检测了所有70个考虑的基准的误差，而这两个基准只报告了其中一部分的误差。通过将Eiffel的推断范围作为输入，Herbie获得了3.35比特和高达53.3比特的平均精度提高。,外推，误差分析，基准测试，软件工程,,,
DAKYPZKM,2023,https://doi.org/10.1109/ASE56229.2023.00045,ASE 2023,Learning to Locate and Describe Vulnerabilities,"Automatically discovering software vulnerabilities is a long-standing pursuit for software developers and security analysts. Since detection tools usually provide limited information for vulnerability inspection, recent work turns the attention to identify fine-grained vulnerabilities, i.e., vulnerable statements. However, existing work for vulnerability localization struggles to capture long-range and integral dependency information due to the bottleneck of Graph Neural Networks (GNNs). Moreover, little research has been done to help developers understand detected vulnerabilities, leaving vulnerability diagnosis a challenging task. In this paper, we propose VulTeller, a deep learning-based approach that can automatically locate vulnerable statements in a function and more importantly, can describe the vulnerability. Our approach focuses on extracting precise control and data dependencies in the code, achieved through modeling control flow paths and employing taint analysis. We design a novel neural model that encodes the control flows and taint flows which reside in the control flow paths, and decodes them via node classification and an attentional decoder for the two tasks respectively. We conduct extensive experiments with real-world vulnerabilities to evaluate the proposed approach. The evaluation results, including quantitative measurement and human evaluation, demonstrate that our approach is highly effective and outperforms state-of-the-art approaches. Our work for the first time formulates the problem of vulnerability description generation, and makes one step further towards automated vulnerability diagnosis.","Vulnerability Diagnosis,Vulnerability Localization,Description Generation,Deep Learning",学习定位和描述漏洞,自动发现软件漏洞是软件开发人员和安全分析师长期以来的追求。由于检测工具通常为漏洞检查提供有限的信息，最近的工作将注意力转向识别细粒度的漏洞，即易受攻击的语句。然而，由于图神经网络（GNN）的瓶颈，现有的漏洞定位工作难以捕获长期和完整的依赖信息。此外，很少有研究帮助开发人员了解检测到的漏洞，这使得漏洞诊断成为一项具有挑战性的任务。在本文中，我们提出了VulTeller，这是一种基于深度学习的方法，可以自动定位函数中的漏洞语句，更重要的是，可以描述漏洞。我们的方法侧重于在代码中提取精确的控制和数据依赖关系，通过建模控制流路径和使用污点分析来实现。我们设计了一个新的神经模型，对控制流路径中的控制流和污染流进行编码，并分别通过节点分类和注意力解码器对其进行解码。我们对现实世界中的漏洞进行了广泛的实验，以评估所提出的方法。包括定量测量和人类评估在内的评估结果表明，我们的方法非常有效，优于最先进的方法。我们的工作首次阐述了漏洞描述生成的问题，并朝着自动化漏洞诊断迈出了一步。,漏洞诊断，漏洞定位，描述生成，深度学习,,,
UDJLACS6,2023,https://doi.org/10.1109/ASE56229.2023.00058,ASE 2023,Mitigating Persistence of Open-Source Vulnerabilities in Maven Ecosystem,"Vulnerabilities from third-party libraries (TPLs) have been unveiled to threaten the Maven ecosystem in the long term. Despite patches being released promptly after vulnerabilities are disclosed, the libraries and applications in the community still use the vulnerable versions, which makes the vulnerabilities persistent in the Maven ecosystem (e.g., the notorious Log4Shell still greatly influences the Maven ecosystem nowadays from 2021). Both academic and industrial researchers have proposed user-oriented standards and solutions to address vulnerabilities, while such solutions fail to tackle the ecosystem-wide persistent vulnerabilities because it requires a collective effort from the community to timely adopt patches without introducing breaking issues. To seek an ecosystem-wide solution, we first carried out an empirical study to examine the prevalence of persistent vulnerabilities in the Maven ecosystem. Then, we identified affected libraries for alerts by implementing an algorithm monitoring downstream dependents of vulnerabilities based on an up-to-date dependency graph. Based on them, we further quantitatively revealed that patches blocked by upstream libraries caused the persistence of vulnerabilities. After reviewing the drawbacks of existing countermeasures, to address them, we proposed a solution for range restoration (Ranger) to automatically restore the compatible and secure version ranges of dependencies for downstream dependents. The automatic restoration requires no manual effort from the community, and the code-centric compatibility assurance ensures smooth upgrades to patched versions. Moreover, Ranger along with the ecosystem monitoring can timely alert developers of blocking libraries and suggest flexible version ranges to rapidly unblock patch versions. By evaluation, Ranger could restore 75.64% of ranges which automatically remediated 90.32% of vulnerable downstream projects.","Open-source Software,Software Security,Java",缓解Maven生态系统中开源漏洞的持久性,来自第三方库（TPL）的漏洞已被揭露，将长期威胁Maven生态系统。尽管在漏洞被披露后立即发布了补丁，但社区中的库和应用程序仍然使用易受攻击的版本，这使得漏洞在Maven生态系统中持续存在（例如，从2021年开始，臭名昭著的Log4Shell仍然对Maven生态系统产生了巨大影响）。学术和工业研究人员都提出了以用户为导向的标准和解决方案来解决漏洞，而这些解决方案未能解决整个生态系统的持续漏洞，因为需要社区共同努力，在不引入破坏性问题的情况下及时采用补丁。为了寻求全生态系统的解决方案，我们首先进行了一项实证研究，以检查Maven生态系统中持续脆弱性的普遍性。然后，我们通过实现一种算法来识别受影响的库，该算法基于最新的依赖关系图来监控漏洞的下游依赖关系。在此基础上，我们进一步定量揭示了上游库阻塞的补丁导致漏洞的持久性。在回顾了现有对策的缺点后，为了解决这些缺点，我们提出了一种范围恢复解决方案（Ranger），以自动恢复下游依赖项的兼容和安全版本范围的依赖项。自动恢复不需要社区手动操作，以代码为中心的兼容性保证确保顺利升级到修补版本。此外，Ranger和生态系统监控可以及时提醒开发人员阻止库，并建议灵活的版本范围来快速解锁补丁版本。经过评估，Ranger可以恢复75.64%的范围，自动修复90.32%的脆弱下游项目。,开源软件，软件安全，Java,,,
WQZBSP6L,2023,https://doi.org/10.1109/ASE56229.2023.00111,ASE 2023,InfeRE: Step-by-Step Regex Generation via Chain of Inference,"Automatically generating regular expressions (abbrev. regexes) from natural language description (NL2RE) has been an emerging research area. Prior studies treat regex as a linear sequence of tokens and generate the final expressions autoregressively in a single pass. They did not take into account the step-by-step internal text-matching processes behind the final results. This significantly hinders the efficacy and interpretability of regex generation by neural language models. In this paper, we propose a new paradigm called InfeRE, which decomposes the generation of regexes into chains of step-bystep inference. To enhance the robustness, we introduce a self-consistency decoding mechanism that ensembles multiple outputs sampled from different models. We evaluate InfeRE on two publicly available datasets, NL-RX-Turk and KB13, and compare the results with state-of-the-art approaches and the popular tree-based generation approach TRANX. Experimental results show that InfeRE substantially outperforms previous baselines, yielding 16.3% and 14.7% improvement in DFA@5 accuracy on two datasets, respectively.","Regex Generation,Chain of Inference,Self-Consistency Decoding",InfeRE：通过推理链逐步生成Regex,从自然语言描述中自动生成正则表达式是一个新兴的研究领域。先前的研究将正则表达式视为一个线性的令牌序列，并在一次迭代中自回归生成最终表达式。他们没有考虑最终结果背后的逐步内部文本匹配过程。这严重阻碍了神经语言模型生成正则表达式的有效性和可解释性。在本文中，我们提出了一种新的范式，称为InfeRE，它将正则表达式的生成分解为逐步推理的链。为了增强鲁棒性，我们引入了一种自一致性解码机制，该机制将从不同模型采样的多个输出集合在一起。我们在两个公开可用的数据集NL RX Turk和KB13上评估了InfeRE，并将结果与最先进的方法和流行的基于树的生成方法TRANX进行了比较。实验结果表明，InfeRE显著优于以前的基线，在DFA@5分别在两个数据集上的准确性。,Regex生成，推理链，自洽译码,,,
EXU7FTG3,2023,https://doi.org/10.1109/ASE56229.2023.00063,ASE 2023,Gamma: Revisiting Template-Based Automated Program Repair Via Mask Prediction,"Automated program repair (APR) aims to fix software bugs without manual debugging efforts and plays a crucial role in software development and maintenance. Template-based APR has been widely investigated and shown promising results. However, it is challenging for template-based APR to select the appropriate donor code, which is an important repair ingredient for generating candidate patches. Inappropriate donor code may cause plausible but incorrect patch generation even with correct fix patterns, limiting the repair performance. In this paper, we aim to revisit template-based APR, and propose Gamma, to directly leverage large pre-trained language models for donor code generation. Our main insight is that instead of retrieving donor code in the local buggy file, we can directly predict the correct code tokens based on the context code snippets and repair patterns by a cloze task. Specifically, (1) Gamma revises a variety of fix templates from state-of-the-art template-based APR techniques (i.e., TBar) and transforms them into mask patterns. (2) Gamma adopts a pre-trained language model to predict the correct code for masked code as a fill-in-the-blank task. Although our idea is general and can be built on various existing pre-trained language models, we have implemented Gamma as a practical APR tool based on the recent UniXcoder model. The experimental results demonstrate that Gamma correctly repairs 82 bugs on Defects4J-v1.2, which achieves 20.59% (14 bugs) and 26.15% (17 bugs) improvement over the previous state-of-the-art template-based approach TBar and learning-based one Recoder. Furthermore, Gamma repairs 45 bugs and 22 bugs from the additional Defects4J-v2.0 and QuixBugs, indicating the generalizability of Gamma in addressing the dataset overfitting issue. We also prove that adopting other pre-trained language models can provide substantial advancement, e.g., CodeBERT-based and ChatGPT-based Gamma is able to fix 80 and 67 bugs on Defects4J-v1.2, indicating the scalability of Gamma. Overall, our study highlights the promising future of adopting pre-trained models to generate correct patches on top of fix patterns in practice.","Automated Program Repair,Fix Pattern,Pretrained Model,LLM4SE",Gamma：通过掩码预测修改基于模板的自动程序修复,自动程序修复（APR）旨在修复软件错误，而无需手动调试，在软件开发和维护中发挥着至关重要的作用。基于模板的APR已经得到了广泛的研究，并显示出有希望的结果。然而，基于模板的APR选择合适的供体代码是具有挑战性的，这是生成候选补丁的重要修复成分。不适当的供体代码可能会导致生成看似合理但不正确的修补程序，即使修复模式正确，也会限制修复性能。在本文中，我们的目标是重新审视基于模板的APR，并提出Gamma，以直接利用大型预训练语言模型来生成供体代码。我们的主要见解是，我们可以通过完形填空任务基于上下文代码片段和修复模式直接预测正确的代码标记，而不是在本地Bugy文件中检索供体代码。具体而言，（1）Gamma从最先进的基于模板的APR技术（即TBar）中修改了各种固定模板，并将其转换为掩模图案。（2） Gamma采用预先训练的语言模型来预测掩码代码的正确代码，作为填空任务。尽管我们的想法是通用的，并且可以建立在各种现有的预先训练的语言模型上，但我们已经在最近的UniXcoder模型的基础上实现了Gamma作为一种实用的APR工具。实验结果表明，Gamma正确修复了Defects4J-v1.2上的82个错误，与之前最先进的基于模板的方法TBar和基于学习的方法Recoder相比，分别提高了20.59%（14个错误）和26.15%（17个错误）。此外，Gamma修复了额外的Defects4J-v2.0和QuixBugs中的45个错误和22个错误，这表明Gamma在解决数据集过拟合问题方面的可推广性。我们还证明，采用其他预先训练的语言模型可以提供实质性的进步，例如，基于CodeBERT和基于ChatGPT的Gamma能够修复Defects4J-v1.2上的80个和67个错误，这表明了Gamma的可扩展性。总的来说，我们的研究强调了在实践中采用预先训练的模型在修复模式之上生成正确补丁的前景。,自动程序修复，修复模式，预训练模型，LLM4SE,,,
TFFSDIGJ,2023,https://doi.org/10.1109/ASE56229.2023.00178,ASE 2023,iASTMapper: An Iterative Similarity-Based Abstract Syntax Tree Mapping Algorithm,"Abstract syntax tree (AST) mapping algorithms are widely used to locate the code changes in a file revision by mapping the AST nodes of the source code before and after the code changes. A recent differential testing of three state-of- the-art AST mapping algorithms, i.e., GumTree, MTDiff, and IJM, reveals that the algorithms generate inaccurate mappings for a considerable number of file revisions. We find that the inaccurate mappings could be caused by the mutual influence: the mappings of lower-level AST nodes (e.g., tokens) have impacts on the mappings of higher-level AST nodes (e.g., statements) and vice versa. This mutual influence issue is rarely considered by existing algorithms. In this paper, we propose an algorithm, called iASTMapper, that iteratively map two ASTs based on the similarities between AST nodes. Given a file revision, we extract three types of AST nodes in different levels of program structures (i.e., tokens, statements, and inner-statements) from the ASTs of the two source code files. We first build mappings of the unchanged statements and inner-statements. Then, we use an iterative method to map the rest of the nodes without mapping. For each of the three types of nodes, we iteratively map the nodes based on their similarities measured using heuristic rules. We further use an iterative mechanism to connect the three iterative mapping processes by considering the mutual influence between the mappings of different types of nodes. Finally, a series of code edit actions are generated from the node mappings to help users understand and locate the code changes during revisions. We conduct experiments to compare iASTMapper with three baselines, i.e., GumTree, MTDiff, and IJM, by automatically evaluating 210,997 file revisions from ten Java projects. Furthermore, we manually evaluate the correctness of the code edit actions generated for 200 file revisions with 12 evaluators. The results demonstrate that iASTMapper outperforms the baselines. iASTMapper can generate shorter code edit actions by at least 1.29% than the baselines, with a high accuracy of 96.23%.","AST mapping,Code change analysis",iASTMapper：一种基于迭代相似性的抽象语法树映射算法,摘要语法树（AST）映射算法被广泛用于通过映射源代码在代码更改前后的AST节点来定位文件修订中的代码更改。最近对三种最先进的AST映射算法（即GumTree、MTDiff和IJM）进行的差分测试表明，这些算法为相当多的文件修订生成了不准确的映射。我们发现，不准确的映射可能是由相互影响引起的：较低级别AST节点（如令牌）的映射对较高级别AST节点的映射（如语句）有影响，反之亦然。现有算法很少考虑这种相互影响的问题。在本文中，我们提出了一种称为iASTMapper的算法，该算法基于AST节点之间的相似性迭代映射两个AST。给定文件修订，我们从两个源代码文件的AST中提取不同级别程序结构中的三种类型的AST节点（即令牌、语句和内部语句）。我们首先构建未更改语句和内部语句的映射。然后，我们使用迭代方法来映射其余的节点，而不进行映射。对于三种类型的节点中的每一种，我们都会根据使用启发式规则测量的相似性来迭代映射节点。通过考虑不同类型节点的映射之间的相互影响，我们进一步使用迭代机制来连接三个迭代映射过程。最后，从节点映射中生成一系列代码编辑操作，以帮助用户理解和定位修订期间的代码更改。我们通过自动评估来自十个Java项目的210997个文件修订，将iASTMapper与三个基线（即GumTree、MTDiff和IJM）进行了比较实验。此外，我们使用12个评估器手动评估为200个文件修订生成的代码编辑操作的正确性。结果表明iASTMapper的性能优于基线。iASTMapper可以生成比基线更短的代码编辑操作，至少1.29%，准确率高达96.23%。,AST映射，代码更改分析,,,
CFQ3NT5H,2023,https://doi.org/10.1109/ASE56229.2023.00022,ASE 2023,Evolve the Model Universe of a System Universe,"Uncertain, unpredictable, real-time, and lifelong evolution causes operational failures in intelligent software systems, leading to significant damages, safety and security hazards, and tragedies. To fully unleash such systems' potential and facilitate their wider adoption, ensuring the trustworthiness of their decision-making under uncertainty is the prime challenge. To overcome this challenge, an intelligent software system and its operating environment should be continuously monitored, tested, and refined during its lifetime operation. Existing technologies, such as digital twins, can enable continuous synchronisation with such systems to reflect their most up-to-date states. Such representations are often in the form of prior-knowledge-based and machine-learning models, together called ‘model universe’. In this paper, we present our vision of combining techniques from software engineering, evolutionary computation, and machine learning to support the model universe evolution.","Model Universe,System Universe,Coevolution,Epigenetics,Machine Learning",演化系统宇宙的模型宇宙,不确定性、不可预测性、实时性和终身性的进化会导致智能软件系统的操作故障，导致重大损失、安全隐患和悲剧。要充分释放此类系统的潜力并促进其更广泛的采用，确保其决策在不确定性下的可信度是首要挑战。为了克服这一挑战，智能软件系统及其操作环境应在其生命周期运行期间不断进行监控、测试和改进。现有技术，如数字孪生，可以与此类系统持续同步，以反映其最新状态。这种表示通常采用基于先验知识和机器学习模型的形式，统称为“模型宇宙”。在本文中，我们提出了将软件工程、进化计算和机器学习技术相结合来支持模型宇宙进化的愿景。,模型宇宙，系统宇宙，协同进化，表观遗传学，机器学习,,,
UJT4UUZ4,2023,https://doi.org/10.1109/ASE56229.2023.00163,ASE 2023,"PreciseBugCollector: Extensible, Executable and Precise Bug-Fix Collection: Solution for Challenge 8: Automating Precise Data Collection for Code Snippets with Bugs, Fixes, Locations, and Types","Bug datasets are vital for enabling deep learning techniques to address software maintenance tasks related to bugs. However, existing bug datasets suffer from precise and scale limitations: they are either small-scale but precise with manual validation or large-scale but imprecise with simple commit message processing. In this paper, we introduce Precise-BugCollector, a precise, multi -language bug collection approach that overcomes these two limitations. PreciseBugCollector is based on two novel components: a) A bug tracker to map the codebase repositories with external bug repositories to trace bug type information, and b) A bug injector to generate project-specific bugs by injecting noise into the correct codebases and then executing them against their test suites to obtain test failure messages. We implement PreciseBugCollector against three sources: 1) A bug tracker that links to the national vulnerability data set (NVD) to collect general-wise vulnerabilities, 2) A bug tracker that links to OSS-Fuzz to collect general-wise bugs, and 3) A bug injector based on 16 injection rules to generate project-wise bugs. To date, PreciseBugCollector comprises 1057818 bugs extracted from 2968 open-source projects. Of these, 12602 bugs are sourced from bug repositories (NVD and OSS-Fuzz), while the remaining 1045216 project-specific bugs are generated by the bug injector. Considering the challenge objectives, we argue that a bug injection approach is highly valuable for the industrial setting, since project-specific bugs align with domain knowledge, share the same codebase, and adhere to the coding style employed in industrial projects.","Bug datasets,Program repair,Software testing and debugging",PreciseBugCollector：可扩展、可执行和精确的Bug修复集合：挑战8的解决方案：自动收集包含Bug、修复、位置和类型的代码段的精确数据,Bug数据集对于使用深度学习技术解决与Bug相关的软件维护任务至关重要。然而，现有的bug数据集存在精确性和规模限制：它们要么是小规模但精确的手动验证，要么是大规模但不精确的简单提交消息处理。在本文中，我们介绍了Precise BugCollector，一种精确的、多语言的错误收集方法，它克服了这两个限制。PreciseBugCollector基于两个新颖的组件：a）错误跟踪器，用于将代码库与外部错误库映射，以跟踪错误类型信息；b）错误注入器，用于通过向正确的代码库中注入噪声来生成项目特定的错误，然后针对其测试套件执行这些错误，以获得测试失败消息。我们针对三个来源实现了PreciseBugCollector：1）一个链接到国家漏洞数据集（NVD）以收集一般漏洞的漏洞跟踪器，2）一个连接到OSS Fuzz以收集一般错误的漏洞跟踪器；3）一个基于16个注入规则的漏洞注入器以生成项目错误。迄今为止，PreciseBugCollector包含从2968个开源项目中提取的1057818个bug。其中，12602个bug来自bug存储库（NVD和OSS Fuzz），而其余1045216个特定于项目的bug则由bug注入器生成。考虑到挑战目标，我们认为错误注入方法对工业环境非常有价值，因为项目特定的错误与领域知识一致，共享相同的代码库，并遵循工业项目中使用的编码风格。,Bug数据集，程序修复，软件测试和调试,,,
F4KNHHND,2023,https://doi.org/10.1109/ASE56229.2023.00179,ASE 2023,Scene-Driven Exploration and GUI Modeling for Android Apps,"Due to the competitive environment, mobile apps are usually produced under pressure with lots of complicated functionality and UI pages. Therefore, it is challenging for various roles to design, understand, test, and maintain these apps. The extracted transition graphs for apps such as ATG, WTG, and STG have a low transition coverage and coarse-grained granularity, which limits the existing methods of graphical user interface (GUI) modeling by UI exploration. To solve these problems, in this paper, we propose SceneDroid, a scene-driven exploration approach to extracting the GUI scenes dynamically by integrating a series of novel techniques including smart exploration, state fuzzing, and indirect launching strategies. We present the GUI scenes as a scene transition graph (SceneTG) to model the GUI of apps with high transition coverage and fine-grained granularity. Compared with the existing GUI modeling tools, SceneDroid has improved by 168.74% in the coverage of transition pairs and 162.42% in scene extraction. Apart from the effectiveness evaluation of SceneDroid, we also illustrate the future potential of SceneDroid as a fundamental capability to support app development, reverse engineering, and GUI rearession testing.","Android app,Scene-driven exploration,GUI exploration,GUI modeling",Android应用程序的场景驱动探索和GUI建模,由于竞争环境，移动应用程序通常是在压力下生产的，具有许多复杂的功能和UI页面。因此，设计、理解、测试和维护这些应用程序对各个角色来说都是一项挑战。为ATG、WTG和STG等应用程序提取的过渡图具有较低的过渡覆盖率和粗粒度，这限制了通过UI探索进行图形用户界面建模的现有方法。为了解决这些问题，本文提出了SceneDroid，这是一种场景驱动的探索方法，通过集成一系列新技术，包括智能探索、状态模糊和间接启动策略，动态提取GUI场景。我们将GUI场景呈现为场景转换图（SceneTG），以对具有高转换覆盖率和细粒度的应用程序的GUI进行建模。与现有的GUI建模工具相比，SceneDroid在过渡对的覆盖率上提高了168.74%，在场景提取方面提高了162.42%。除了对SceneDroid的有效性进行评估外，我们还说明了SceneDrid作为支持应用程序开发、逆向工程和GUI重新部署测试的基本功能的未来潜力。,Android应用程序，场景驱动的探索，GUI探索，GUI建模,,,
8MNKI32B,2023,https://doi.org/10.1109/ASE56229.2023.00055,ASE 2023,ArduinoProg: Towards Automating Arduino Programming,"Writing code for Arduino poses unique challenges. A developer 1) needs hardware-specific knowledge about the interface configuration between the Arduino controller and the I/Ohardware, 2) identifies a suitable driver library for the I/O hardware, and 3) follows certain usage patterns of the driver library in order to use them properly. In this work, based on a study of real-world user queries posted in the Arduino forum, we propose ArduinoProg to address such challenges. ArduinoProg consists of three components, i.e., Library Retriever, Configuration Classifier, and Pattern Generator. Given a query, Library Retriever retrieves library names relevant to the I/O hardware identified from the query using vector-based similarity matching. Configuration Classifier predicts the interface configuration between the I/O hardware and the Arduino controller based on the method definitions of each library. Pattern Generator generates the usage pattern of a library using a sequence-to-sequence deep learning model. We have evaluated ArduinoProg using real-world queries, and our results show that the components of ArduinoProg can generate accurate and useful suggestions to guide developers in writing Arduino code. Demo video: bit.ly/3Y3aeBe Tool: https://huggingface.co/spaces/imamnurby/ArduinoProg Code and data: https://github.com/imamnurby/ArduProg","arduino programming,information retrieval,code generation,deep learning",ArduinoProg：实现Arduino编程自动化,为Arduino编写代码带来了独特的挑战。开发人员1）需要关于Arduino控制器和I/O硬件之间的接口配置的硬件特定知识，2）为I/O硬件识别合适的驱动程序库，以及3）遵循驱动程序库的某些使用模式以便正确使用它们。在这项工作中，基于对Arduino论坛上发布的真实世界用户查询的研究，我们提出了ArduinoProg来解决这些挑战。ArduinoProg由三个组件组成，即库检索器、配置分类器和模式生成器。给定查询，Library Retriever使用基于向量的相似性匹配来检索与从查询中识别的I/O硬件相关的库名称。配置分类器根据每个库的方法定义预测I/O硬件和Arduino控制器之间的接口配置。模式生成器使用序列到序列的深度学习模型生成库的使用模式。我们使用真实世界的查询对ArduinoProg进行了评估，结果表明ArduinoPro的组件可以生成准确有用的建议，以指导开发人员编写Arduino代码。演示视频：bit.ly/3aeBe工具：https://huggingface.co/spaces/imamnurby/ArduinoProg代码和数据：https://github.com/imamnurby/ArduProg,arduino编程，信息检索，代码生成，深度学习,,,
4U26TSK3,2023,https://doi.org/10.1109/ASE56229.2023.00196,ASE 2023,QuraTest: Integrating Quantum Specific Features in Quantum Program Testing,"The recent fast development of quantum computers breaks several computation limitations that are difficult for conventional computers. Up to the present, although many approaches and tools have been proposed to test quantum programs, the fundamental features of quantum programs, i.e., magnitude, phase, and entanglement, have been largely overlooked, leading to limited fault detection capability and reduced testing effectiveness. To address this problem, we propose an automated testing framework named QURATEST, equipped with three test case generators (including two newly proposed techniques, UCNOT and IQFT in this paper, as well as one based on Random techniques) to test quantum programs. Overall, the proposed generators enable the generation of diverse test inputs by considering the quantum features of quantum programs. In the experiments, we perform an in-depth evaluation of QURATEST from three aspects: generated test case diversity, output coverage of the program under test, and fault detection capability. The results demonstrate the potential of our newly proposed techniques in that IQFT can generate the most diverse test cases regarding magnitude, phase, and entanglement, with 66% cell coverage. Comparatively, the Random approach only has 10% cell coverage. Regarding the evaluations of the output coverage, IQFT can achieve the highest output coverage in 70.2% (33 out of 47) of all quantum programs. In terms of fault detection, UCNOT outperforms the other two techniques. Specifically, the test cases generated by UCNOT have the best mutation score in 88.4% (23 out of 26) quantum programs.","Quantum program,test case generation,magnitude,phase,entanglement",QuraTest：在量子程序测试中集成量子特性,量子计算机最近的快速发展打破了传统计算机难以解决的几个计算限制。到目前为止，尽管已经提出了许多方法和工具来测试量子程序，但量子程序的基本特征，即幅度、相位和纠缠，在很大程度上被忽视了，导致故障检测能力有限，测试效率降低。为了解决这个问题，我们提出了一个名为QURATEST的自动化测试框架，该框架配备了三个测试用例生成器（包括本文中新提出的两种技术，UCNOT和IQFT，以及一种基于随机技术的技术）来测试量子程序。总体而言，通过考虑量子程序的量子特性，所提出的生成器能够生成不同的测试输入。在实验中，我们从生成的测试用例多样性、被测程序的输出覆盖率和故障检测能力三个方面对QURATEST进行了深入的评估。结果证明了我们新提出的技术的潜力，因为IQFT可以生成关于幅度、相位和纠缠的最多样化的测试用例，具有66%的细胞覆盖率。相比之下，随机方法仅具有10%的小区覆盖率。关于输出覆盖率的评估，IQFT可以在70.2%（47个中的33个）的所有量子程序中实现最高的输出覆盖率。在故障检测方面，UCNOT优于其他两种技术。具体而言，UCNOT生成的测试用例在88.4%（26个中的23个）的量子程序中具有最佳的突变分数。,量子程序，测试用例生成，幅度，相位，纠缠,,,
3K4NSAYM,2023,https://doi.org/10.1109/ASE56229.2023.00069,ASE 2023,Demystifying Template-Based Invariant Generation for Bit-Vector Programs,"The template-based approach to invariant generation is a parametric and relatively complete methodology for inferring loop invariants. The relative completeness ensures the generated invariants' accuracy up to the template's form and the inductive condition. However, there has been limited in advancing the approach to bit-precise reasoning, which involves modeling integers using bit-vector arithmetic. This is unfortunate because bit-precise reasoning is crucial for faithfully and accurately modeling machine integer semantics and, thus, for ensuring sound and precise program verification. In this experience paper, we present an experimental study of bit-precise, template-based invariant generation on three fronts: the precision of different invariant templates, the performance of different constraint solvers for solving the constraints, and the effectiveness of the template-based approach compared to existing bit-precise verification techniques. Through an extensive experimental evaluation over a wide range of benchmarks, we find that (1) the choices of invariant templates and constraint solvers have varying degrees of impact on the precision and efficiency of invariant generation; (2) the template-based approach can handle benchmarks that other approaches for bit-vectors cannot handle. The results also reveal several guidelines for advancing future research on template-based invariant generation.","Invariant generation,constraint solving,comparison and analysis",基于模板的比特矢量程序不变量生成,基于模板的不变量生成方法是一种推断循环不变量的参数化且相对完整的方法。相对完整性保证了生成的不变量在模板形式和归纳条件下的准确性。然而，在推进比特精确推理方法方面存在局限性，该方法涉及使用比特向量算法对整数进行建模。这是不幸的，因为位精确推理对于忠实和准确地建模机器整数语义至关重要，从而确保可靠和精确的程序验证。在这篇经验论文中，我们从三个方面对基于模板的比特精确不变量生成进行了实验研究：不同不变量模板的精度，不同约束求解器求解约束的性能，以及与现有比特精确验证技术相比，基于模板的方法的有效性。通过对各种基准进行广泛的实验评估，我们发现：（1）不变量模板和约束求解器的选择对不变量生成的精度和效率有不同程度的影响；（2） 基于模板的方法可以处理比特向量的其他方法无法处理的基准。研究结果还为推进未来基于模板的不变量生成研究提供了一些指导。,不变量生成，约束求解，比较与分析,,,
RXI39XCR,2023,https://doi.org/10.1109/ASE56229.2023.00096,ASE 2023,A Closer Look at Different Difficulty Levels Code Generation Abilities of ChatGPT,"Code generation aims to generate source code implementing human requirements illustrated with natural language specifications. With the rapid development of intelligent software engineering, automated code generation has become a hot research topic in both artificial intelligence and software engineering, and researchers have made significant achievements on code generation. More recently, large language models (LLMs) have demonstrated outstanding performance on code generation tasks, such as ChatGPT released by OpenAI presents the fantastic potential on automated code generation. However, the existing studies are limited to exploring LLMs' ability for generating code snippets to solve simple programming problems, the task of competition-level code generation has never been investigated. The specifications of the programming competition are always complicated and require the specific input/output format as well as the high-level algorithmic reasoning ability. In this study, we conduct the first large empirical study to investigate the zero-shot learning ability of ChatGPT for solving competition programming problems. Specifically, we warm up the design of prompts by using the Human-Eval dataset. Then, we apply the well-designed prompt to the competition-level code generation dataset, namely APPS, to further explore the effectiveness of using ChatGPT for solving competition problems. We collect ChatGPT's outputs on 5,000 code competition problems, the evaluation results show that it can successfully pass 25.4% test cases. By further feeding extra information (e.g, test failed information) to ChatGPT, we observe that ChatGPT has the potential to fix partial pass into a fully pass program. Moreover, we investigate the solutions generated by LLMs and the existing solutions, we find that it prefers to directly copy the code instead of re-write when facing more difficult problems. Finally, we evaluate the code quality generated by ChatGPT in terms of “code cleanness”, we observe that the generated codes are with small functions and file sizes, which are in line with the standard of clean code.","code generation,program competition,Chat-GPT,large language model,clean code",ChatGPT不同难度级别的代码生成能力,代码生成旨在生成实现自然语言规范所示的人类需求的源代码。随着智能软件工程的快速发展，自动代码生成已成为人工智能和软件工程的研究热点，研究人员在代码生成方面取得了重大成果。最近，大型语言模型（LLM）在代码生成任务上表现出了出色的性能，例如OpenAI发布的ChatGPT在自动代码生成方面展现了巨大的潜力。然而，现有的研究仅限于探索LLM生成代码片段以解决简单编程问题的能力，竞争级别的代码生成任务从未被研究过。编程竞赛的规范总是很复杂，需要特定的输入/输出格式以及高水平的算法推理能力。在本研究中，我们进行了第一次大型实证研究，以调查ChatGPT解决竞争编程问题的零样本学习能力。具体来说，我们通过使用人类评估数据集来预热提示的设计。然后，我们将精心设计的提示应用于竞争级别的代码生成数据集，即APPS，以进一步探索使用ChatGPT解决竞争问题的有效性。我们收集了ChatGPT对5000个代码竞争问题的输出，评估结果表明，它可以成功通过25.4%的测试用例。通过进一步向ChatGPT提供额外信息（例如，测试失败信息），我们观察到ChatGPT有可能将部分通过修复为完全通过程序。此外，我们研究了LLM生成的解决方案和现有的解决方案，发现当面临更困难的问题时，它更喜欢直接复制代码，而不是重写。最后，我们从“代码清洁度”的角度评估了ChatGPT生成的代码质量，我们观察到生成的代码具有较小的函数和文件大小，符合干净代码的标准。,代码生成，程序竞争，聊天GPT，大型语言模型，干净代码,,,
8QN3ILS9,2023,https://doi.org/10.1109/ASE56229.2023.00121,ASE 2023,An Intentional Forgetting-Driven Self-Healing Method for Deep Reinforcement Learning Systems,"Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook. As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which often occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach for adapting the DRL agent in response to the environment's conditions shifts. However, successive shifts of considerable magnitude may cause the production environment to drift from its original state. Recent studies have shown that these environmental drifts tend to drive CL into long, or even unsuccessful, healing cycles, which arise from inefficiencies such as catastrophic forgetting, warm-starting failure, and slow convergence. In this paper, we propose Dr. DRL, an effective self-healing approach for DRL systems that integrates a novel mechanism of intentional forgetting into vanilla CL (i.e., standard CL) to overcome its main issues. Dr. DRL deliberately erases the DRL system's minor behaviors to systematically prioritize the adaptation of the key problem-solving skills. Using well-established DRL algorithms, Dr. DRL is compared with vanilla CL on various drifted environments. Dr. DRL is able to reduce, on average, the healing time and fine-tuning episodes by, respectively, 18.74% and 17.72%. Dr. DRL successfully helps agents to adapt to 19.63% of drifted environments left unsolved by vanilla CL while maintaining and even enhancing by up to 45% the obtained rewards for drifted environments that are resolved by both approaches.","Deep Reinforcement Learning,Software Healing,Intentional Forgetting,Continual Learning",一种用于深度强化学习系统的有意遗忘驱动的自愈方法,深度强化学习（DRL）越来越多地应用于Netflix和Facebook等大型制作中。与大多数数据驱动系统一样，DRL系统可能由于环境漂移而表现出不理想的行为，这种情况经常发生在不断变化的生产环境中。持续学习（CL）是一种固有的自我修复方法，用于调整DRL代理以响应环境条件的变化。然而，相当大幅度的连续变化可能会导致生产环境偏离其原始状态。最近的研究表明，这些环境漂移往往会使CL进入漫长甚至不成功的治疗周期，这是由灾难性遗忘、热启动失败和缓慢收敛等低效现象引起的。在本文中，我们提出了DRL博士，这是一种有效的DRL系统自修复方法，它将一种新的有意遗忘机制集成到普通CL（即标准CL）中，以克服其主要问题。DRL博士故意删除DRL系统的次要行为，以系统地优先考虑关键解决问题技能的适应。使用成熟的DRL算法，Dr.DRL与香草CL在各种漂移环境中进行了比较。DRL博士能够平均将愈合时间和微调次数分别减少18.74%和17.72%。DRL博士成功地帮助特工适应了香草CL解决的19.63%的漂移环境，同时保持甚至提高了这两种方法解决的漂移环境获得的高达45%的奖励。,深度强化学习，软件治疗，有意遗忘，持续学习,,,
68PKT4AA,2023,https://doi.org/10.1109/ASE56229.2023.00169,ASE 2023,ACWRecommender: A Tool for Validating Actionable Warnings with Weak Supervision,"Static analysis tools have gained popularity among developers for finding potential bugs, but their widespread adoption is hindered by the accomnpanying high false alarm rates (up to 90%). To address this challenge, previous studies proposed the concept of actionable warnings, and apply machine-learning methods to distinguish actionable warnings from false alarms. Despite these efforts, our preliminary study suggests that the current methods used to collect actionable warnings are rather shaky and unreliable, resulting in a large proportion of invalid actionable warnings. In this work, we mined 68,274 reversions from Top-500 Github C repositories to create a substantia actionable warning dataset and assigned weak labels to each warning's likelihood of being a real bug. To automatically identify actionable warnings and recommend those with a high probability of being real bugs (AWHB), we propose a two-stage framework called ACWRecommender. In the first stage, our tool use a pre-trained model, i.e., UniXcoder, to identify actionable warnings from a huge number of SA tool's reported warnings. In the second stage, we rerank valid actionable warnings to the top by using weakly supervised learning. Experimental results showed that our tool outperformed several baselines for actionable warning detection (in terms of F1-score) and performed better for AWHB recommendation (in terms of nDCG and MRR). Additionaly, we also performed an in-the-wild evaluation, we manually validated 24 warnings out of 2,197 reported warnings on 10 randomly selected projects, 22 of which were confirmed by developers as real bugs, demonstrating the practical usage of our tool.","Actionable warning recommendation,Static analysis,Weak supervision,Data mining",ACWRRecommender：一种在弱监督下验证可操作警告的工具,静态分析工具因发现潜在的错误而在开发人员中广受欢迎，但其广泛采用受到高误报率（高达90%）的阻碍。为了应对这一挑战，先前的研究提出了可操作警告的概念，并应用机器学习方法来区分可操作警告和虚假警报。尽管做出了这些努力，但我们的初步研究表明，目前用于收集可操作警告的方法相当不稳定和不可靠，导致很大一部分可操作警告无效。在这项工作中，我们从Top-500 Github C存储库中挖掘了68274个反转，以创建一个实质性的可操作警告数据集，并为每个警告成为真正错误的可能性分配了弱标签。为了自动识别可操作的警告并推荐那些有很高概率是真实错误的警告（AWHB），我们提出了一个称为ACWRecommender的两阶段框架。在第一阶段，我们的工具使用预先训练的模型，即UniXcoder，从大量SA工具报告的警告中识别可操作的警告。在第二阶段，我们使用弱监督学习将有效的可操作警告重新排列到顶部。实验结果表明，我们的工具在可操作的警告检测方面（就F1分数而言）优于几个基线，在AWHB推荐方面（就NCG和MRR而言）表现更好。此外，我们还进行了一次野外评估，在随机选择的10个项目上，我们手动验证了2197个报告的警告中的24个，其中22个被开发人员确认为真正的错误，展示了我们工具的实际使用。,可操作的警告建议，静态分析，弱监督，数据挖掘,,,
8QXTANHQ,2023,https://doi.org/10.1109/ASE56229.2023.00211,ASE 2023,Potential Solutions to Challenges in C Program Repair: A Practical Perspective,"Automated program repair is to reduce the manual work for bug fixing by human developers. In recent 15 years, the research community of program repair has created many novel techniques. However, these techniques share several assumptions that cannot always be satisfied in daily software development. This badly hurts the application of program repair in practice. For example, many repair techniques assume that test cases are well written before patch generation; many techniques assume that specific language features can be ignored (or already-processed). In this paper, we propose a framework of C program repair, which mainly addresses two challenges: test-independent repair and preprocessor directive processing. Our solution to test-independent repair is to automatically construct patch conditions for C programs via parsing the syntax structures; our solution to preprocessor directive processing is to generate code symbols to replace preprocessor directives. We plan to implement these potential solutions with program analysis techniques. The goal of this paper is to present practical solutions for developers to automate C program repair.","C program repair,test-independent repair,preprocessor directives,industrial solutions,program analysis",C程序修复挑战的潜在解决方案：一个实用的视角,自动程序修复是为了减少人类开发人员修复错误的手动工作。近15年来，程序修复研究界创造了许多新颖的技术。然而，这些技术有几个共同的假设，这些假设在日常软件开发中并不总是能够得到满足。这严重损害了程序修复在实践中的应用。例如，许多修复技术假设测试用例在生成补丁之前编写良好；许多技术假设特定的语言特征可以被忽略（或者已经被处理）。在本文中，我们提出了一个C程序修复框架，主要解决了两个挑战：测试独立修复和预处理器指令处理。我们测试独立修复的解决方案是通过解析语法结构自动构建C程序的补丁条件；我们对预处理器指令处理的解决方案是生成代码符号来替换预处理器指令。我们计划用程序分析技术来实现这些潜在的解决方案。本文的目标是为开发人员提供自动化C程序修复的实用解决方案。,C程序修复，测试独立修复，预处理器指令，工业解决方案，程序分析,,,
98BUA7F9,2023,https://doi.org/10.1109/ASE56229.2023.00023,ASE 2023,Are We Ready to Embrace Generative AI for Software Q&A?,"Stack Overflow, the world's largest software Q&A (SQA) website, is facing a significant traffic drop due to the emergence of generative AI techniques. ChatGPT is banned by Stack Overflow after only 6 days from its release. The main reason provided by the official Stack Overflow is that the answers generated by ChatGPT are of low quality. To verify this, we conduct a comparative evaluation of human-written and ChatGPT-generated answers. Our methodology employs both automatic comparison and a manual study. Our results suggest that human-written and ChatGPT-generated answers are semantically similar, however, human-written answers outperform ChatGPT-generated ones consistently across multiple aspects, specifically by 10% on the overall score. We release the data, analysis scripts, and detailed results at https://github.com/maxxbw54/GAI4SQA.","Generative AI,large language model,Software Q&A,Stack Overflow,ChatGPT",我们准备好接受软件问答的生成人工智能了吗？,由于生成人工智能技术的出现，世界上最大的软件问答网站Stack Overflow正面临着流量的大幅下降。ChatGPT在发布后仅6天就被Stack Overflow禁止。官方Stack Overflow提供的主要原因是ChatGPT生成的答案质量较低。为了验证这一点，我们对人工书写的答案和ChatGPT生成的答案进行了比较评估。我们的方法采用了自动比较和手动研究。我们的研究结果表明，人工书写的答案和ChatGPT生成的答案在语义上相似，然而，人工编写的答案在多个方面一致地优于ChatGPT产生的答案，特别是在总分上提高了10%。我们在上发布了数据、分析脚本和详细结果https://github.com/maxxbw54/GAI4SQA.,生成型人工智能，大型语言模型，软件问答，堆栈溢出，ChatGPT,,,
72TBH8WL,2023,https://doi.org/10.1109/ASE56229.2023.00175,ASE 2023,Understanding and Remediating Open-Source License Incompatibilities in the PyPI Ecosystem,"The reuse and distribution of open-source software must be in compliance with its accompanying open-source license. In modern packaging ecosystems, maintaining such compliance is challenging because a package may have a complex multi-layered dependency graph with many packages, any of which may have an incompatible license. Although prior research finds that license incompatibilities are prevalent, empirical evidence is still scarce in some modern packaging ecosystems (e.g., PyPI). It also remains unclear how developers remediate the license incompatibilities in the dependency graphs of their packages (including direct and transitive dependencies), let alone any automated approaches. To bridge this gap, we conduct a large-scale empirical study of license incompatibilities and their remediation practices in the PyPI ecosystem. We find that 7.27% of the PyPI package releases have license incompatibilities and 61.3 % of them are caused by transitive dependencies, causing challenges in their remediation; for remediation, developers can apply one of the five strategies: migration, removal, pinning versions, changing their own licenses, and negotiation. Inspired by our findings, we propose Silence, an SMT-solver-based approach to recommend license incompatibility remediations with minimal costs in package dependency graph. Our evaluation shows that the remediations proposed by Silencecan match 19 historical real-world cases (except for migrations not covered by an existing knowledge base) and have been accepted by five popular PyPI packages whose developers were previously unaware of their license incompatibilities.","open source license,license incompatibility and its remediation,dependency management,SMT solver",理解和修复PyPI生态系统中的开源许可证不兼容,开源软件的重用和分发必须符合其附带的开源许可证。在现代包装生态系统中，保持这种合规性是一项挑战，因为一个包装可能有一个复杂的多层依赖图，包含许多包装，其中任何一个都可能具有不兼容的许可证。尽管先前的研究发现许可证不兼容现象普遍存在，但在一些现代包装生态系统（例如PyPI）中，经验证据仍然很少。目前还不清楚开发人员如何补救其包的依赖关系图中的许可证不兼容性（包括直接和可传递依赖关系），更不用说任何自动化方法了。为了弥补这一差距，我们对PyPI生态系统中的许可证不兼容性及其补救措施进行了大规模的实证研究。我们发现，7.27%的PyPI包版本存在许可证不兼容，其中61.3%是由可传递依赖性引起的，这给它们的修复带来了挑战；对于补救，开发人员可以应用五种策略之一：迁移、删除、固定版本、更改自己的许可证以及协商。受我们研究结果的启发，我们提出了Silence，这是一种基于SMT求解器的方法，可以在包依赖关系图中以最小的成本推荐许可证不兼容修复。我们的评估表明，Silencecan提出的补救措施与19个历史真实世界案例相匹配（现有知识库未涵盖的迁移除外），并已被五个流行的PyPI包接受，这些包的开发人员以前不知道其许可证不兼容。,开源许可证，许可证不兼容性及其补救，依赖关系管理，SMT解决方案,,,
2UW4GKNL,2023,https://doi.org/10.1109/ASE56229.2023.00054,ASE 2023,PhyFu: Fuzzing Modern Physics Simulation Engines,"A physical simulation engine (PSE) is a software system that simulates physical environments and objects. Modern PSEs feature both forward and backward simulations, where the forward phase predicts the behavior of a simulated system, and the backward phase provides gradients (guidance) for learning-based control tasks, such as a robot arm learning to fetch items. This way, modern PSEs show promising support for learning-based control methods. To date, PSEs have been largely used in various high-profitable, commercial applications, such as games, movies, virtual reality (VR), and robotics. Despite the prosperous development and usage of PSEs by academia and industrial manufacturers such as Google and NVIDIA, PSEs may produce incorrect simulations, which may lead to negative results, from poor user experience in entertainment to accidents in robotics-involved manufacturing and surgical operations. This paper introduces PhyFu, a fuzzing framework designed specifically for PSEs to uncover errors in both forward and backward simulation phases. PHyFu mutates initial states and asserts if the PSE under test behaves consistently with respect to basic Physics Laws (PLs). We further use feedback-driven test input scheduling to guide and accelerate the search for errors. Our study of four PSEs covers mainstream industrial vendors (Google and NVIDIA) as well as academic products. We successfully uncover over 5K error-triggering inputs that generate incorrect simulation results spanning across the whole software stack of PSEs.","Software testing,Physics simulator,Robotics,Fuzzing",PhyFu：模糊化现代物理仿真引擎,物理模拟引擎（PSE）是一种模拟物理环境和对象的软件系统。现代PSE具有正向和反向模拟的特点，其中正向阶段预测模拟系统的行为，而反向阶段为基于学习的控制任务提供梯度（指导），例如学习获取物品的机械臂。通过这种方式，现代PSE显示出对基于学习的控制方法的有希望的支持。到目前为止，PSE已被广泛用于各种高利润的商业应用，如游戏、电影、虚拟现实（VR）和机器人。尽管学术界和工业制造商（如谷歌和NVIDIA）对PSE进行了蓬勃的开发和使用，但PSE可能会产生不正确的模拟，这可能会导致负面结果，从娱乐中的不良用户体验到机器人制造和外科手术中的事故。本文介绍了PhyFu，这是一个专门为PSE设计的模糊框架，用于发现前向和后向模拟阶段的错误。PHyFu变异初始状态，并断言被测PSE是否与基本物理定律（PL）一致。我们进一步使用反馈驱动的测试输入调度来指导和加速错误搜索。我们对四个PSE的研究涵盖了主流工业供应商（谷歌和NVIDIA）以及学术产品。我们成功地发现了超过5K的错误触发输入，这些输入在PSE的整个软件堆栈中产生了不正确的模拟结果。,软件测试，物理模拟器，机器人，引信,,,
VVMH2ZZJ,2023,https://doi.org/10.1109/ASE56229.2023.00127,ASE 2023,Compsuite: A Dataset of Java Library Upgrade Incompatibility Issues,"Modern software systems heavily rely on external libraries developed by third-parties to ensure efficient development. However, frequent library upgrades can lead to compatibility issues between the libraries and their client systems. In this paper, we introduce Compsuite, a dataset that includes 123 real-world Java client-library pairs where upgrading the library causes an incompatibility issue in the corresponding client. Each incompatibility issue in Compsuite is associated with a test case authored by the developers, which can be used to reproduce the issue. The dataset also provides a command-line interface that simplifies the execution and validation of each issue. With this infrastructure, users can perform an inspection of any incompatibility issue with the push of a button, or reproduce an issue step-by-step for a more detailed investigation. We make Compsuite publicly available to promote open science. We believe that various software analysis techniques, such as compatibility checking, debugging, and regression test selection, can benefit from Compsuite. The demonstration video of Compsuite is available at https://www.youtube.com/watch?v=7DQGsGs_65s.","Incompatibility issue,software libraries,dataset",Compsuite：Java库升级不兼容问题的数据集,现代软件系统严重依赖第三方开发的外部库来确保高效开发。但是，频繁的库升级可能会导致库与其客户端系统之间的兼容性问题。在本文中，我们介绍了Compsuite，这是一个包含123个真实世界Java客户端库对的数据集，其中升级库会导致相应客户端出现不兼容问题。Compsuite中的每个不兼容问题都与开发人员编写的测试用例相关联，该测试用例可用于重现问题。数据集还提供了一个命令行界面，简化了每个问题的执行和验证。有了这个基础设施，用户只需按下按钮就可以检查任何不兼容问题，或者逐步复制问题以进行更详细的调查。我们公开使用Compsuite来促进开放科学。我们相信，各种软件分析技术，如兼容性检查、调试和回归测试选择，都可以从Compuite中受益。Compsuite的演示视频可在https://www.youtube.com/watch?v=7DQGsGs_65s.,不兼容问题，软件库，数据集,,,
8CD8XM3R,2023,https://doi.org/10.1109/ASE56229.2023.00020,ASE 2023,EXPRESS 2.0: An Intelligent Service Management Framework for AIoT Systems in the Edge,"AIoT (Artificial Intelligence of Things) which integrates AI and IoT has received rapidly growing interest from the software engineering community in recent years. It is crucial to design scalable, efficient, and reliable software solutions for large-scale AIoT systems in edge computing environments. However, the lack of effective service management including the support for service collaboration, AI application, and data security in the edge, has seriously limited the development of AIoT systems. To seal this gap, we propose EXPRESS 2.0 which is an intelligent service management framework for AI oT in the edge. Specifically, on top of the existing EXPRESS platform, EXPRESS 2.0 includes the intelligent service collaboration management module, AI application management module, and data security management module. To demonstrate the effectiveness of the framework, we design and implement a last-mile delivery system using both UAVs (Unmanned Aerial Vehicles) and UGVs (Unmanned Ground Vehicles). The EXPRESS 2.0 is open-sourced at https://github.com/ISEC-AHU/EXPRESS2.0. A video demonstration of EXPRESS 2.0 is at https://youtu.be/GHKD_VvJD88.","AIoT System,Edge Computing,Service Collaboration,AI Application,Data Security",EXPRESS 2.0：边缘AIoT系统的智能服务管理框架,近年来，将人工智能和物联网相结合的AIoT（Artificial Intelligence of Things）受到了软件工程界日益增长的兴趣。为边缘计算环境中的大规模AIoT系统设计可扩展、高效和可靠的软件解决方案至关重要。然而，缺乏有效的服务管理，包括对服务协作、人工智能应用和边缘数据安全的支持，严重限制了AIoT系统的发展。为了填补这一空白，我们提出了EXPRESS 2.0，这是一个用于边缘AI oT的智能服务管理框架。具体来说，EXPRESS 2.0在现有EXPRESS平台之上，包括智能服务协同管理模块、AI应用管理模块和数据安全管理模块。为了证明该框架的有效性，我们设计并实现了一个使用无人机和无人地面车辆的最后一英里交付系统。EXPRESS 2.0开源于https://github.com/ISEC-AHU/EXPRESS2.0.EXPRESS 2.0的视频演示位于https://youtu.be/GHKD_VvJD88.,AIoT系统，边缘计算，服务协作，AI应用，数据安全,,,
VEJECK2W,2023,https://doi.org/10.1109/ASE56229.2023.00099,ASE 2023,HexT5: Unified Pre-Training for Stripped Binary Code Information Inference,"Decompilation is a widely used process for reverse engineers to significantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identifier names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a unified pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identifiers, and pseudo-code using novel pseudo-code-based pre-training objectives. We fine-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.","Reverse Engineering,Deep Learning,Binary Diffing,Information Inference,Programming Language Model",HexT5：条带二进制码信息推理的统一预训练,反编译是逆向工程师广泛使用的一个过程，通过将汇编代码提升到更高级别的类C语言伪代码来显著提高代码的可读性。然而，编译和剥离过程不可逆转地丢弃了对代码理解至关重要的高级语义信息，如注释、标识符名称和类型。现有的方法通常只恢复一种类型的信息，这使得它们对于语义推理来说是次优的。在本文中，我们将伪代码视为一种特殊的编程语言，然后提出了一个统一的预训练模型HexT5，该模型使用新的基于伪代码的预训练目标，在大量自然语言注释、源标识符和伪代码上进行训练。我们在各种下游任务上对HexT5进行了微调，包括代码摘要、变量名称恢复、函数名称恢复和相似性检测。综合实验表明，HexT5在四个下游任务上实现了最先进的性能，并证明了HexT5对二进制相关任务的鲁棒有效性和可推广性。,逆向工程，深度学习，二进制差分，信息推理，程序设计语言模型,,,
WWEWZIKS,2023,https://doi.org/10.1109/ASE56229.2023.00012,ASE 2023,ExpressAPR: Efficient Patch Validation for Java Automated Program Repair Systems,"Automated program repair (APR) approaches suffer from long patch validation time, which limits their practical application and receives relatively low attention. The patch validation process repeatedly executes tests to filter patches, and has been recognized as the dual of mutation analysis. We systematically investigate existing mutation testing techniques and recognize five families of acceleration techniques that are suitable for patch validation, two of which are never adapted to a general-purpose patch validator. We implement and demonstrate ExpressAPR, the first framework that combines five families of acceleration techniques for patch validation as the complete set. In our evaluation on 30 random Defects4J bugs and four APR systems, ExpressAPR accelerates patch validation for two order-of-magnitudes over plain validation or one order-of-magnitude over the state-of-the-art approach, benefiting APR researchers and users with a much shorter patch validation time. Demo video available at https://youtu.be/7AB-4VvBuuM Tool repo (source code + Docker image + evaluation dataset) available at https://github.com/ExpressAPR/ExpressAPR","Automated Program Repair,Patch Validation,Mutation Analysis,Test Execution",ExpressAPR:Java自动程序修复系统的高效补丁验证,自动程序修复（APR）方法的补丁验证时间很长，这限制了它们的实际应用，并且受到的关注相对较低。补丁验证过程重复执行测试以过滤补丁，并已被公认为突变分析的对偶。我们系统地研究了现有的突变测试技术，并识别了五个适用于补丁验证的加速技术家族，其中两个从未适用于通用补丁验证器。我们实现并演示了ExpressAPR，这是第一个将用于补丁验证的五个加速技术家族组合为一整套的框架。在我们对30个随机缺陷4J漏洞和四个APR系统的评估中，ExpressAPR比普通验证加速了两个数量级，比最先进的方法加速了一个数量级的补丁验证，使APR研究人员和用户受益，补丁验证时间更短。演示视频可在https://youtu.be/7AB-4VvBuuM工具回购（源代码+Docker图像+评估数据集）可在https://github.com/ExpressAPR/ExpressAPR,自动化程序修复，补丁验证，突变分析，测试执行,,,
5KFX7XZV,2023,https://doi.org/10.1109/ASE56229.2023.00052,ASE 2023,LEAP: Efficient and Automated Test Method for NLP Software,"The widespread adoption of DNNs in NLP software has highlighted the need for robustness. Researchers proposed various automatic testing techniques for adversarial test cases. However, existing methods suffer from two limitations: weak error-discovering capabilities, with success rates ranging from 0% to 24.6% for BERT-based NLP software, and time inefficiency, taking 177.8s to 205.28s per test case, making them challenging for time-constrained scenarios. To address these issues, this paper proposes LEAP, an automated test method that uses LEvy flight-based Adaptive Particle swarm optimization integrated with textual features to generate adversarial test cases. Specifically, we adopt Levy flight for population initialization to increase the diversity of generated test cases. We also design an inertial weight adaptive update operator to improve the efficiency of LEAP's global optimization of high-dimensional text examples and a mutation operator based on the greedy strategy to reduce the search time. We conducted a series of experiments to validate LEAP's ability to test NLP software and found that the average success rate of LEAP in generating adversarial test cases is 79.1%, which is 6.1% higher than the next best approach (PSOattack). While ensuring high success rates, LEAP significantly reduces time overhead by up to 147.6s compared to other heuristic-based methods. Additionally, the experimental results demonstrate that LEAP can generate more transferable test cases and significantly enhance the robustness of DNN-based systems.","NLP Software Testing,Particle Swarm Optimization",LEAP：一种高效、自动化的NLP软件测试方法,DNN在NLP软件中的广泛采用凸显了对健壮性的需求。研究人员提出了各种对抗性测试用例的自动测试技术。然而，现有的方法存在两个局限性：错误发现能力较弱，基于BERT的NLP软件的成功率在0%到24.6%之间；时间效率低下，每个测试用例需要177.8到205.28秒，这使得它们在时间受限的场景中具有挑战性。为了解决这些问题，本文提出了LEAP，这是一种自动测试方法，使用基于LEvy飞行的自适应粒子群优化与文本特征相结合来生成对抗性测试用例。具体来说，我们采用Levy飞行进行总体初始化，以增加生成的测试用例的多样性。我们还设计了一个惯性权重自适应更新算子来提高LEAP对高维文本示例的全局优化效率，以及一个基于贪婪策略的变异算子来减少搜索时间。我们进行了一系列实验来验证LEAP测试NLP软件的能力，发现LEAP在生成对抗性测试用例方面的平均成功率为79.1%，比次优方法（PSOattack）高6.1%。在确保高成功率的同时，与其他基于启发式的方法相比，LEAP显著减少了147.6s的时间开销。此外，实验结果表明，LEAP可以生成更多可转移的测试用例，并显著增强基于DNN的系统的鲁棒性。,NLP软件测试，粒子群优化,,,
H9GDX6SI,2023,https://doi.org/10.1109/ASE56229.2023.00047,ASE 2023,The Plastic Surgery Hypothesis in the Era of Large Language Models,"Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names. The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based approaches to exploit such existing code ingredients. However, as recent APR research starts focusing on LLM-based approaches, the plastic surgery hypothesis has been largely ignored. In this paper, we ask the following question: How useful is the plastic surgery hypothesis in the era of LLMs? Interestingly, LLM-based APR presents a unique opportunity to fully automate the plastic surgery hypothesis via fine-tuning (training on the buggy project) and prompting (directly providing valuable code ingredients as hints to the LLM). To this end, we propose FitRepair, which combines the direct usage of LLMs with two domain-specific fine-tuning strategies and one prompting strategy (via information retrieval and static analysis) for more powerful APR. While traditional APR techniques require intensive manual efforts in both generating patches based on the plastic surgery hypothesis and guaranteeing patch validity, our approach is fully automated and general. Moreover, while it is very challenging to manually design heuristics/patterns for effectively leveraging the hypothesis, due to the power of LLMs in code vectorization/understanding, even partial/imprecise project-specific information can still guide LLMs in generating correct patches! Our experiments on the widely studied Defects4j 1.2 and 2.0 datasets show that FitRepair fixes 89 and 44 bugs (substantially outperforming baseline techniques by 15 and 8), respectively, demonstrating a promising future of the plastic surgery hypothesis in the era of LLMs.","Training,Codes,Computer bugs,Surgery,Manuals,Static analysis,Maintenance engineering",大语言模型时代的整形外科假说,自动程序修复（APR）渴望为输入错误程序自动生成补丁。传统的APR工具通常通过使用模板、启发法和正式规范来关注特定的错误类型和修复。然而，这些技术在缺陷类型和补丁种类方面是有限的。因此，研究人员设计了各种基于学习的APR工具，最近的工作重点是直接使用大型语言模型（LLM）进行APR。虽然基于LLM的APR工具包能够在许多修复数据集上实现最先进的性能，但用于直接修复的LLM并不完全了解项目特定信息，如唯一变量或方法名称。整形手术假说是APR的一个众所周知的见解，它指出修复错误的代码成分通常已经存在于同一项目中。传统的APR工具通过设计手动或启发式的方法来利用这些现有的代码成分，在很大程度上利用了整形手术假设。然而，随着最近的APR研究开始关注基于LLM的方法，整形外科假说在很大程度上被忽视了。在这篇论文中，我们提出了以下问题：整形外科假说在LLMs时代有多有用？有趣的是，基于LLM的APR提供了一个独特的机会，可以通过微调（对bug项目进行培训）和提示（直接提供有价值的代码成分作为LLM的提示）来完全自动化整形手术假设。为此，我们提出了FitRepair，它将LLM的直接使用与两种特定领域的微调策略和一种提示策略（通过信息检索和静态分析）相结合，以获得更强大的APR。虽然传统的APR技术需要在基于整形手术假设生成补丁和保证补丁有效性方面进行密集的手动工作，我们的方法是完全自动化和通用的。此外，尽管手动设计启发式/模式以有效利用假设非常具有挑战性，但由于LLM在代码矢量化/理解方面的强大功能，即使是部分/不精确的项目特定信息也可以指导LLM生成正确的补丁！我们在广泛研究的Defects4j 1.2和2.0数据集上的实验表明，FitRepair分别修复了89个和44个错误（显著优于基线技术15个和8个），表明整形外科假说在LLM时代有着光明的未来。,培训，代码，计算机错误，外科手术，手册，静态分析，维护工程,,,
8PR2XMNK,2023,https://doi.org/10.1109/ASE56229.2023.00144,ASE 2023,When Less is Enough: Positive and Unlabeled Learning Model for Vulnerability Detection,"Automated code vulnerability detection has gained increasing attention in recent years. The deep learning (DL)-based methods, which implicitly learn vulnerable code patterns, have proven effective in vulnerability detection. The performance of DL-based methods usually relies on the quantity and quality of labeled data. However, the current labeled data are generally automatically collected, such as crawled from human-generated commits, making it hard to ensure the quality of the labels. Prior studies have demonstrated that the non-vulnerable code (i.e., negative labels) tends to be unreliable in commonly-used datasets, while vulnerable code (i.e., positive labels) is more determined. Considering the large numbers of unlabeled data in practice, it is necessary and worth exploring to leverage the positive data and large numbers of unlabeled data for more accurate vulnerability detection. In this paper, we focus on the Positive and Unlabeled (PU) learning problem for vulnerability detection and propose a novel model named PILOT, i.e., Positive and unlabeled Learning mOdel for vulnerability deTection. PILOT only learns from positive and unlabeled data for vulnerability detection. It mainly contains two modules: (1) A distance-aware label selection module, aiming at generating pseudo-labels for selected unlabeled data, which involves the inter-class distance prototype and progressive fine-tuning; (2) A mixed-supervision representation learning module to further alleviate the influence of noise and enhance the discrimination of representations. Extensive experiments in vulnerability detection are conducted to evaluate the effectiveness of PILOT based on real-world vulnerability datasets. The experimental results show that PILOT outperforms the popular weakly supervised methods by 2.78%-18.93% in the PU learning setting. Compared with the state-of-the-art methods, PILOT also improves the performance of 1.34%-12.46 % in F1 score metrics in the supervised setting. In addition, PILOT can identify 23 mislabeled from the FFMPeg+Qemu dataset in the PU learning setting based on manual checking.","Software vulnerability detection,positive and unlabeled learning,source code representation",当更少就足够了：漏洞检测的积极无标签学习模型,近年来，自动代码漏洞检测越来越受到关注。基于深度学习（DL）的方法隐式学习易受攻击的代码模式，已被证明在漏洞检测中是有效的。基于DL的方法的性能通常取决于标记数据的数量和质量。然而，当前标记的数据通常是自动收集的，例如从人工生成的提交中爬网，这使得很难确保标签的质量。先前的研究表明，在常用的数据集中，非易受攻击的代码（即负标签）往往是不可靠的，而易受攻击代码（即正标签）更为确定。考虑到实践中大量的未标记数据，利用阳性数据和大量未标记数据进行更准确的漏洞检测是必要的，也是值得探索的。在本文中，我们重点研究了漏洞检测的阳性和未标记（PU）学习问题，并提出了一个新的模型PILOT，即用于漏洞检测的正向和未标记学习model。PILOT只从阳性和未标记的数据中学习漏洞检测。它主要包括两个模块：（1）距离感知标签选择模块，旨在为选定的未标记数据生成伪标签，涉及类间距离原型和渐进微调；（2） 混合监督表征学习模块，进一步减轻噪声的影响，增强表征的辨别能力。基于真实世界的漏洞数据集，对漏洞检测进行了广泛的实验，以评估PILOT的有效性。实验结果表明，在PU学习环境中，PILOT的性能优于流行的弱监督方法2.78%-18.93%。与最先进的方法相比，PILOT在监督环境中的F1成绩指标也提高了1.34%-12.46%的性能。此外，在PU学习设置中，PILOT可以基于手动检查从FFMPeg+Qemu数据集中识别23个错误标记。,软件漏洞检测，正向和无标记学习，源代码表示,,,
XYJDRGWU,2023,https://doi.org/10.1109/ASE56229.2023.00168,ASE 2023,Fork Entropy: Assessing the Diversity of Open Source Software Projects' Forks,"On open source software (OSS) platforms such as GitHub, forking and accepting pull-requests is an important approach for OSS projects to receive contributions, especially from external contributors who cannot directly commit into the source repositories. Having a large number of forks is often considered as an indicator of a project being popular. While extensive studies have been conducted to understand the reasons of forking, communications between forks, features and impacts of forks, there are few quantitative measures that can provide a simple yet informative way to gain insights about an OSS project's forks besides their count. Inspired by studies on biodiversity and OSS team diversity, in this paper, we propose an approach to measure the diversity of an OSS project's forks (i.e., its fork population). We devise a novel fork entropy metric based on Rao's quadratic entropy to measure such diversity according to the forks' modifications to project files. With properties including symmetry, continuity, and monotonicity, the proposed fork entropy metric is effective in quantifying the diversity of a project's fork population. To further examine the usefulness of the proposed metric, we conduct empirical studies with data retrieved from fifty projects on GitHub. We observe significant correlations between a project's fork entropy and different outcome variables including the project's external productivity measured by the number of external contributors' commits, acceptance rate of external contributors' pull-requests, and the number of reported bugs. We also observe significant interactions between fork entropy and other factors such as the number of forks. The results suggest that fork entropy effectively enriches our understanding of OSS projects' forks beyond the simple number of forks, and can potentially support further research and applications.","Open Source Software,Diversity of Forks,Fork Entropy,Rao's Quadratic Entropy,Mining Software Repositories",叉熵：评估开源软件项目叉的多样性,在GitHub等开源软件平台上，分叉和接受pull请求是OSS项目接收贡献的重要方法，尤其是来自无法直接提交到源代码库的外部贡献者的贡献。拥有大量的分叉通常被认为是一个项目受欢迎的指标。虽然已经进行了广泛的研究来了解分叉的原因、分叉之间的通信、分叉的特征和影响，但除了分叉的数量之外，很少有定量测量可以提供一种简单而信息丰富的方式来深入了解OSS项目的分叉。受生物多样性和OSS团队多样性研究的启发，在本文中，我们提出了一种测量OSS项目分支（即分支种群）多样性的方法。我们设计了一种新的基于Rao二次熵的叉熵度量，以根据叉对项目文件的修改来测量这种多样性。所提出的叉熵度量具有对称性、连续性和单调性等特性，可以有效地量化项目叉种群的多样性。为了进一步检验所提出的指标的有用性，我们对GitHub上50个项目的数据进行了实证研究。我们观察到项目的分叉熵与不同的结果变量之间的显著相关性，包括通过外部贡献者的提交数量、外部贡献者拉取请求的接受率和报告的错误数量来衡量的项目的外部生产力。我们还观察到分叉熵与其他因素（如分叉数量）之间的显著相互作用。结果表明，fork熵有效地丰富了我们对OSS项目fork的理解，超越了简单的fork数量，并有可能支持进一步的研究和应用。,开源软件，分叉的多样性，分叉熵，Rao二次熵，挖掘软件库,,,
P89C3AXD,2023,https://doi.org/10.1109/ASE56229.2023.00043,ASE 2023,LogOnline: A Semi-Supervised Log-Based Anomaly Detector Aided with Online Learning Mechanism,"Logs are prevalent in modern cloud systems and serve as a valuable source of information for system maintenance. Over the years, a lot of research and industrial efforts have been devoted to the field of log-based anomaly detection. Through analyzing the limitations of existing approaches, we find that most of them still suffer from practical issues and are thus hard to be applied in real-world scenarios. For example, supervised approaches are dependent on a large amount of labeled log data for training, which can require much manual labeling effort. Besides, log instability, which is a pervasive issue in real-world systems, poses great challenge to existing methods, especially under the presence of many dissimilar new log events. To overcome these problems, we propose LogOnline, which is a semi supervised anomaly detector aided with online learning mechanism. The semi-supervised nature of LogOnline makes it able to get rid of the erroneous and time-consuming manual labeling of log data. Based on our proposed online learning mechanism, LogOnline can learn the normal sequence patterns continuously as new log sequences emerge, thus staying robust to unstable log data. Unlike previous works, the proposed online learning mechanism requires no labeled log data nor human intervention in the process. We have evaluated LogOnline on two widely used public datasets, and the experimental results demonstrate the effectiveness of LogOnline. In particular, LogOnline achieves a comparable result with the studied supervised approaches, outperforming all semi-supervised counterparts. When the log instability issue is more common, LogOnline exhibits the best performance over all compared approaches, further confirming its practicability.","Log Analysis,Anomaly Detection,Log Instability,Online Learning",LogOnline：一种辅助在线学习机制的半监督日志异常检测器,日志在现代云系统中很普遍，是系统维护的宝贵信息来源。多年来，基于日志的异常检测领域投入了大量的研究和工业努力。通过分析现有方法的局限性，我们发现大多数方法仍然存在实际问题，因此很难在现实世界中应用。例如，监督方法依赖于大量标记的日志数据进行训练，这可能需要大量的手动标记工作。此外，日志不稳定性是现实世界系统中普遍存在的问题，它对现有的方法提出了巨大的挑战，尤其是在存在许多不同的新日志事件的情况下。为了克服这些问题，我们提出了LogOnline，这是一种辅助在线学习机制的半监督异常检测器。LogOnline的半监督性质使其能够摆脱错误且耗时的日志数据手动标记。基于我们提出的在线学习机制，LogOnline可以在新的日志序列出现时连续学习正常的序列模式，从而对不稳定的日志数据保持鲁棒性。与以前的工作不同，所提出的在线学习机制不需要标记日志数据，也不需要人工干预。我们在两个广泛使用的公共数据集上对LogOnline进行了评估，实验结果证明了LogOnline的有效性。特别是，LogOnline获得了与所研究的监督方法相当的结果，优于所有半监督方法。当日志不稳定性问题更为常见时，LogOnline在所有比较方法中表现出最佳性能，进一步证实了其实用性。,日志分析，异常检测，日志不稳定性，在线学习,,,
FMFY45Q2,2023,https://doi.org/10.1109/ASE56229.2023.00130,ASE 2023,Generating Variable Explanations via Zero-shot Prompt Learning,"As basic elements in program, variables convey essential information that is critical for program comprehension and maintenance. However, understanding the meanings of variables in program is not always easy for developers, since poor-quality variable names are prevalent while such variable are less informative for program comprehension. Therefore, in this paper, we target at generating concise natural language explanations for variables to facilitate program comprehension. In particular, there are two challenges in variable explanation generation, including the lack of training data and the association with complex code contexts around the variable. To address these issues, we propose a novel approach ZeroVar,which leverages code pre-trained models and zero-shot prompt learning to generate explanations for the variable based on its code context. ZeroVarcontains two stages: (i) a pre-training stage that continually pre-trains a base model (i.e., CodeT5) to recover the randomly-masked parameter descriptions in method docstrings; and (ii) a zero-shot prompt learning stage that leverages the pre-trained model to generate explanations for a given variable via the prompt constructed with the variable and its belonging method context. We then extensively evaluate the quality and usefulness of the variable explanations generated by ZeroVar.We construct an evaluation dataset of 773 variables and their reference explanations. Our results show that ZeroVarcan generate higher-quality explanations than baselines, not only on automated metrics such as BLEU and ROUGE, but also on human metrics such as correctness, completeness, and conciseness. Moreover, we further assess the usefulness of ZeroVAR-generated explanations on two downstream tasks related to variable naming quality, i.e., abbreviation expansion and spelling correction. For abbreviation expansion, the generated variable explanations can help improve the present rate (+13.1%), precision (+3.6%), and recall (+10.0%) of the state-of-the-art abbreviation explanation approach. For spelling correction, by using the generated explanations we can achieve higher hit@1 (+162.9(%) and hit@3 (+49.6%) than the recent variable representation learning approach.","variable explanation,naming quality,code pretrained models,prompt learning",通过零样本提示学习生成变量解释,作为程序中的基本元素，变量传递对程序理解和维护至关重要的基本信息。然而，对于开发人员来说，理解程序中变量的含义并不总是容易的，因为质量较差的变量名称很普遍，而这些变量对程序理解的信息较少。因此，在本文中，我们的目标是为变量生成简洁的自然语言解释，以便于程序理解。特别是，在变量解释生成中存在两个挑战，包括缺乏训练数据和与变量周围复杂代码上下文的关联。为了解决这些问题，我们提出了一种新的方法ZeroVar，该方法利用代码预训练模型和零样本提示学习，根据变量的代码上下文生成变量的解释。ZeroVar包含两个阶段：（i）持续预训练基本模型（即CodeT5）以恢复方法文档字符串中随机屏蔽的参数描述的预训练阶段；以及（ii）零样本提示学习阶段，其利用预先训练的模型经由用变量及其所属方法上下文构建的提示来生成对给定变量的解释。然后，我们广泛评估ZeroVar生成的变量解释的质量和有用性。我们构建了一个由773个变量及其参考解释组成的评估数据集。我们的结果表明，ZeroVar可以生成比基线更高质量的解释，不仅在BLEU和ROUGE等自动化指标上，而且在正确性、完整性和简洁性等人工指标上。此外，我们进一步评估了ZeroVAR生成的解释对与变量命名质量相关的两个下游任务的有用性，即缩写扩展和拼写更正。对于缩写扩展，生成的变量解释可以帮助提高最先进的缩写解释方法的当前比率（+113.1%）、精度（+3.6%）和召回率（+10.0%）。对于拼写校正，通过使用生成的解释，我们可以获得更高的hit@1（+162.9（%）和hit@3（+49.6%）。,变量解释，命名质量，代码预训练模型，即时学习,,,
E53EWJ4W,2023,https://doi.org/10.1109/ASE56229.2023.00158,ASE 2023,Personalized First Issue Recommender for Newcomers in Open Source Projects,"Many open source projects provide good first issues (GFIs) to attract and retain newcomers. Although several automated GFI recommenders have been proposed, existing recommenders are limited to recommending generic GFIs without considering differences between individual newcomers. However, we observe mismatches between generic GFIs and the diverse background of newcomers, resulting in failed attempts, discouraged onboarding, and delayed issue resolution. To address this problem, we assume that personalized first issues (PFIs) for newcomers could help reduce the mismatches. To justify the assumption, we empirically analyze 37 newcomers and their first issues resolved across multiple projects. We find that the first issues resolved by the same newcomer share similarities in task type, programming language, and project domain. These findings underscore the need for a PFI recommender to improve over state-of-the-art approaches. For that purpose, we identify features that influence newcomers' personalized selection of first issues by analyzing the relationship between possible features of the newcomers and the characteristics of the newcomers' chosen first issues. We find that the expertise preference, OSS experience, activeness, and sentiment of newcomers drive their personalized choice of the first issues. Based on these findings, we propose a Personalized First Issue Recommender (PFIRec), which employs LamdaMART to rank candidate issues for a given newcomer by leveraging the identified influential features. We evaluate PFIRec using a dataset of 68,858 issues from 100 GitHub projects. The evaluation results show that PFIRec outperforms existing first issue recommenders, potentially doubling the probability that the top recommended issue is suitable for a specific newcomer and reducing one-third of a newcomer's unsuccessful attempts to identify suitable first issues, in the median. We provide a replication package at https://zenodo.org/record/7915841.","Computer languages,Trajectory,Task analysis,Best practices,Software development management,Software engineering,Guidelines",为开源项目中的新手提供个性化的第一期推荐程序,许多开源项目提供了良好的第一期（GFI）来吸引和留住新来者。尽管已经提出了几种自动GFI推荐器，但现有的推荐器仅限于推荐通用GFI，而不考虑单个新用户之间的差异。然而，我们观察到通用GFI与新来者的不同背景之间的不匹配，导致尝试失败、入职受阻和问题解决延迟。为了解决这个问题，我们假设新来者的个性化优先问题（PFI）可以帮助减少不匹配。为了证明这一假设的合理性，我们实证分析了37个新来者及其在多个项目中解决的第一个问题。我们发现，同一个新人解决的第一个问题在任务类型、编程语言和项目领域有相似之处。这些发现强调了PFI建议者需要改进最先进的方法。为此，我们通过分析新来者的可能特征与新来者选择的第一期问题的特征之间的关系，确定了影响新来者个性化选择第一期的特征。我们发现，新来者的专业偏好、OSS经验、积极性和情感推动了他们对第一个问题的个性化选择。基于这些发现，我们提出了一种个性化的首次问题推荐器（PFIRec），该推荐器使用LamdaMART通过利用已识别的有影响力的特征来对给定新人的候选问题进行排名。我们使用来自100个GitHub项目的68858期数据集来评估PFIRec。评估结果显示，PFIRec的表现优于现有的第一期推荐人，可能会使推荐的第一期适合特定新人的概率增加一倍，并在中位数中减少新人识别合适第一期的失败尝试的三分之一。我们在提供复制包https://zenodo.org/record/7915841.,计算机语言，轨迹，任务分析，最佳实践，软件开发管理，软件工程，指南,,,
YBCMBMJI,2023,https://doi.org/10.1109/ASE56229.2023.00091,ASE 2023,CoMSA: A Modeling-Driven Sampling Approach for Configuration Performance Testing,"Highly configurable systems enable customers to flexibly configure the systems in diverse deployment environments. The flexibility of configurations also poses challenges for performance testing. On one hand, there exist a massive number of possible configurations; while on the other hand, the time and resources are limited for performance testing, which is already a costly process during software development. Modeling the performance of configurations is one of the solutions to reduce the cost of configuration performance testing. Although prior research proposes various modeling and sampling techniques to build configuration performance models, the sampling approaches used in the model typically do not consider the accuracy of the performance models, leading to potential sub-optimal performance modeling results in practice. In this paper, we present a modeling-driven sampling approach (CoMSA) to improve the performance modeling of highly configurable systems. The intuition of CoMSA is to select samples based on their uncertainties to the performance models. In other words, the configurations that have the more uncertain performance prediction results by the performance models are more likely to be selected as further training samples to improve the model. CoMSA is designed by considering both scenarios where 1) the software projects do not have historical performance testing results (cold start) and 2) there exist historical performance testing results (warm start). We evaluate the performance of our approach in four subjects, namely LRZIP, LLVM, x264, and SQLite. Through the evaluation result, we can conclude that our sampling approaches could highly enhance the accuracy of the prediction models and the efficiency of configuration performance testing compared to other baseline sampling approaches.","Training,Costs,Codes,Uncertainty,Buildings,Predictive models,Software",CoMSA：一种用于配置性能测试的建模驱动采样方法,高度可配置的系统使客户能够在不同的部署环境中灵活配置系统。配置的灵活性也给性能测试带来了挑战。一方面，存在大量可能的配置；而另一方面，性能测试的时间和资源是有限的，这在软件开发过程中已经是一个昂贵的过程。对配置的性能进行建模是降低配置性能测试成本的解决方案之一。尽管先前的研究提出了各种建模和采样技术来构建配置性能模型，但模型中使用的采样方法通常不考虑性能模型的准确性，从而在实践中导致潜在的次优性能建模结果。在本文中，我们提出了一种建模驱动采样方法（CoMSA），以改进高度可配置系统的性能建模。CoMSA的直觉是根据性能模型的不确定性来选择样本。换句话说，性能模型的性能预测结果更不确定的配置更有可能被选择为进一步的训练样本以改进模型。CoMSA的设计考虑了以下两种情况：1）软件项目没有历史性能测试结果（冷启动），2）存在历史性能测试成果（热启动）。我们在四个主题中评估了我们的方法的性能，即LRZIP、LLVM、x264和SQLite。通过评估结果，我们可以得出结论，与其他基线采样方法相比，我们的采样方法可以大大提高预测模型的准确性和配置性能测试的效率。,培训，成本，代码，不确定性，建筑，预测模型，软件,,,
EA26F5HY,2023,https://doi.org/10.1109/ASE56229.2023.00050,ASE 2023,Delving into Commit-Issue Correlation to Enhance Commit Message Generation Models,"Commit message generation (CMG) is a challenging task in automated software engineering that aims to generate natural language descriptions of code changes for commits. Previous methods all start from the modified code snippets, outputting commit messages through template-based, retrieval-based, or learning-based models. While these methods can summarize what is modified from the perspective of code, they struggle to provide reasons for the commit. The correlation between commits and issues that could be a critical factor for generating rational commit messages is still unexplored. In this work, we delve into the correlation between commits and issues from the perspective of dataset and methodology. We construct the first dataset anchored on combining correlated commits and issues. The dataset consists of an unlabeled commit-issue parallel part and a labeled part in which each example is provided with human-annotated rational information in the issue. Furthermore, we propose ExGroFi (Extraction, Grounding, Ene-tuning), a novel paradigm that can introduce the correlation between commits and issues into the training phase of models. To evaluate whether it is effective, we perform comprehensive experiments with various state-of-the-art CMG models. The results show that compared with the original models, the performance of ExGroFi-enhanced models is significantly improved.","Commit Message Generation,Dataset Construction,Code Representation Learning",深入研究提交问题相关性以增强提交消息生成模型,提交消息生成（CMG）是自动化软件工程中一项具有挑战性的任务，旨在为提交生成代码更改的自然语言描述。以前的方法都是从修改后的代码片段开始的，通过基于模板、基于检索或基于学习的模型输出提交消息。虽然这些方法可以从代码的角度总结修改的内容，但它们很难提供提交的原因。提交和问题之间的相关性可能是生成理性提交消息的关键因素，但尚未得到探索。在这项工作中，我们从数据集和方法论的角度深入研究了提交和问题之间的相关性。我们构建了第一个以组合相关提交和问题为基础的数据集。数据集由一个未标记的提交问题并行部分和一个标记部分组成，其中每个示例都提供了问题中的人工注释理性信息。此外，我们提出了ExGroFi（提取、基础、Ene调优），这是一种新的范式，可以将提交和问题之间的相关性引入模型的训练阶段。为了评估它是否有效，我们用各种最先进的CMG模型进行了全面的实验。结果表明，与原始模型相比，ExGroFi增强模型的性能有了显著提高。,提交消息生成，数据集构建，代码表示学习,,,
EFAEU97H,2023,https://doi.org/10.1109/ASE56229.2023.00197,ASE 2023,VRGuide: Efficient Testing of Virtual Reality Scenes via Dynamic Cut Coverage,"Virtual Reality (VR) is an emerging technique that has been applied to more and more areas such as gaming, remote conference, and education. Since VR user interface has very different characteristics compared with traditional graphic user interface (GUI), VR applications also require new testing techniques for quality assurance. Recently, some frameworks (e.g., VRTest) have been proposed to automate VR user interface testing by automatically controlling the player camera. However, their testing strategies are not able to address VR-specific testing challenges such as object occlusion and movement. In this paper, we propose a novel testing technique called VRGuide to explore VR scenes more efficiently. In particular, VRGuide adapts a computer geometry technique called Cut Extension to optimize the camera routes for covering all interact-able objects. We compared the testing strategy with VRTest on eight top VR software projects with scenes. The results show that VRGuide is able to achieve higher test coverage upon testing timeout in two of the projects, and achieve saturation coverage with averagely 31% less testing time than VRTest on the remaining six projects. Furthermore, VRGuide detected and reported four unknown bugs confirmed by developers, only one of which is also detected by VRTest.","Software Testing,Virtual Reality,Scene Exploration",VRGuide：通过动态剪切覆盖有效测试虚拟现实场景,虚拟现实（VR）是一种新兴技术，已被应用于游戏、远程会议和教育等越来越多的领域。由于VR用户界面与传统的图形用户界面（GUI）有着非常不同的特性，VR应用程序也需要新的测试技术来保证质量。最近，已经提出了一些框架（例如，VRTest）来通过自动控制玩家相机来自动化VR用户界面测试。然而，他们的测试策略无法解决VR特定的测试挑战，如物体遮挡和移动。在本文中，我们提出了一种新的测试技术，称为VRGuide，以更有效地探索VR场景。特别是，VRGuide采用了一种名为“剪切扩展”的计算机几何技术来优化相机路线，以覆盖所有可交互对象。我们将测试策略与VRTest在八个具有场景的顶级VR软件项目上进行了比较。结果表明，在其中两个项目中，VRGuide能够在测试超时时实现更高的测试覆盖率，并且在其余六个项目中实现饱和覆盖率，平均比VRTest少31%的测试时间。此外，VRGuide检测并报告了开发人员确认的四个未知错误，其中只有一个错误也被VRTest检测到。,软件测试，虚拟现实，场景探索,,,
ZD75576P,2023,https://doi.org/10.1109/ASE56229.2023.00199,ASE 2023,REEF: A Framework for Collecting Real-World Vulnerabilities and Fixes,"Software plays a crucial role in our daily lives, and therefore the quality and security of software systems have become increasingly important. However, vulnerabilities in software still pose a significant threat, as they can have serious consequences. Recent advances in automated program repair have sought to automatically detect and fix bugs using data-driven techniques. Sophisticated deep learning methods have been applied to this area and have achieved promising results. However, existing benchmarks for training and evaluating these techniques remain limited, as they tend to focus on a single programming language and have relatively small datasets. Moreover, many benchmarks tend to be outdated and lack diversity, focusing on a specific codebase. Worse still, the quality of bug explanations in existing datasets is low, as they typically use imprecise and uninformative commit messages as explanations. To address these issues, we propose an automated collecting framework REEF to collect REal-world vulnErabilities and Fixes from open-source repositories. We focus on vulnerabilities since they are exploitable and have serious consequences. We develop a multi-language crawler to collect vulnerabilities and their fixes, and design metrics to filter for high-quality vulnerability-fix pairs. Furthermore, we propose a neural language model-based approach to generate high-quality vulnerability explanations, which is key to producing informative fix messages. Through extensive experiments, we demonstrate that our approach can collect high-quality vulnerability-fix pairs and generate strong explanations. The dataset we collect contains 4,466 CVEs with 30,987 patches (including 236 CWE) across 7 programming languages with detailed related information, which is superior to existing benchmarks in scale, coverage, and quality. Evaluations by human experts further confirm that our framework produces high-quality vulnerability explanations.","Vulnerability,Data collection,Bug fix",REEF：收集真实世界漏洞和修复的框架,软件在我们的日常生活中发挥着至关重要的作用，因此软件系统的质量和安全性变得越来越重要。然而，软件中的漏洞仍然构成重大威胁，因为它们可能会产生严重后果。自动程序修复的最新进展试图使用数据驱动技术自动检测和修复错误。复杂的深度学习方法已被应用于该领域，并取得了可喜的成果。然而，用于训练和评估这些技术的现有基准仍然有限，因为它们往往专注于单一的编程语言，并且数据集相对较小。此外，许多基准往往已经过时，缺乏多样性，只关注特定的代码库。更糟糕的是，现有数据集中错误解释的质量很低，因为它们通常使用不精确和无信息的提交消息作为解释。为了解决这些问题，我们提出了一个自动收集框架REEF，从开源存储库中收集REal世界漏洞和修复。我们关注漏洞，因为它们是可利用的，会产生严重后果。我们开发了一个多语言爬网程序来收集漏洞及其修复，并设计指标来筛选高质量的漏洞修复对。此外，我们提出了一种基于神经语言模型的方法来生成高质量的漏洞解释，这是生成信息修复消息的关键。通过广泛的实验，我们证明了我们的方法可以收集高质量的漏洞修复对，并生成强有力的解释。我们收集的数据集包含7种编程语言的4466个CVE和30987个补丁（包括236个CWE），并提供了详细的相关信息，在规模、覆盖范围和质量方面都优于现有的基准测试。人类专家的评估进一步证实，我们的框架提供了高质量的脆弱性解释。,漏洞，数据收集，Bug修复,,,
9ZN3A6HV,2023,https://doi.org/10.1109/ASE56229.2023.00189,ASE 2023,An Image is Worth a Thousand Toxic Words: A Metamorphic Testing Framework for Content Moderation Software,"The exponential growth of social media platforms has brought about a revolution in communication and content dissemination in human society. Nevertheless, these platforms are being increasingly misused to spread toxic content, including hate speech, malicious advertising, and pornography, leading to severe negative consequences such as harm to teenagers' mental health. Despite tremendous efforts in developing and deploying textual and image content moderation methods, malicious users can evade moderation by embedding texts into images, such as screenshots of the text, usually with some interference. We find that modern content moderation software's performance against such malicious inputs remains underexplored. In this work, we propose OASIS, a metamorphic testing framework for content moderation software. OASIS employs 21 transform rules summarized from our pilot study on 5,000 real-world toxic contents collected from 4 popular social media applications, including Twitter, Instagram, Sina Weibo, and Baidu Tieba. Given toxic textual contents, OASIS can generate image test cases, which preserve the toxicity yet are likely to bypass moderation. In the evaluation, we employ OASIS to test five commercial textual content moderation software from famous companies (i.e., Google Cloud, Microsoft Azure, Baidu Cloud, Alibaba Cloud and Tencent Cloud), as well as a state-of-the-art moderation research model. The results show that OASIS achieves up to 100% error finding rates. Moreover, through retraining the models with the test cases generated by OASIS, the robustness of the moderation model can be improved without performance degradation.","Software testing,metamorphic relations,content moderation software",一张图片抵得上千言万语：一个内容调节软件的变形测试框架,社交媒体平台的指数级增长带来了人类社会传播和内容传播的革命。尽管如此，这些平台正越来越多地被滥用来传播有毒内容，包括仇恨言论、恶意广告和色情内容，导致严重的负面后果，如损害青少年的心理健康。尽管在开发和部署文本和图像内容审核方法方面付出了巨大努力，但恶意用户可以通过在图像中嵌入文本（如文本截图）来逃避审核，通常会受到一些干扰。我们发现，现代内容审核软件对此类恶意输入的性能仍然没有得到充分的研究。在这项工作中，我们提出了OASIS，一个用于内容审核软件的变形测试框架。OASIS采用了21条转换规则，这些规则是我们对从Twitter、Instagram、新浪微博和百度贴吧等4个流行社交媒体应用程序中收集的5000条真实世界有毒内容进行试点研究总结而来的。考虑到有毒的文本内容，OASIS可以生成图像测试用例，这些用例保留了毒性，但可能绕过适度。在评估中，我们使用OASIS测试了五个著名公司的商业文本内容审核软件（即谷歌云、微软Azure、百度云、阿里云和腾讯云），以及一个最先进的审核研究模型。结果表明，OASIS实现了高达100%的错误查找率。此外，通过用OASIS生成的测试用例对模型进行重新训练，可以在不降低性能的情况下提高调节模型的鲁棒性。,软件测试，变形关系，内容审核软件,,,
HZPL76X4,2023,https://doi.org/10.1109/ASE56229.2023.00120,ASE 2023,MLIRSmith: Random Program Generation for Fuzzing MLIR Compiler Infrastructure,"MLIR (Multi-Level Intermediate Representation) compiler infrastructure has gained popularity in recent years to support the construction of many compilers. Instead of designing a new IR with a single abstraction for each domain, MLIR compiler infrastructure provides systematic passes to support a wide range of functionalities for benefiting multiple domains together and introduces dialects to support different levels of abstraction in MLIR. Due to its fundamental role in compiler community, ensuring its quality is very critical. In this work, we propose MLIRSmith, the first fuzzing technique for MLIR compiler infrastructure. MLIRSmith employs a two-phase strategy to generate valid and diverse MLIR programs, which first constructs diverse program templates guided by extended MLIR syntax rules and then generates valid MLIR programs through template instantiation guided by our designed context-sensitive grammar. After applying MLIRSmith to the latest revision of MLIR compiler infrastructure, we detected 53 previously unknown bugs, among which 49/38 have been confirmed/fixed by developers. We also transform the high-level programs generated by NNSmith (a high-level program generator for deep learning compilers) to MLIR programs for indirectly fuzzing MLIR compiler infrastructure. During the same testing time, MLIRSmith largely outperforms such an indirect technique by detecting 328.57% more bugs and covering 194.67%/225.87% more lines/branches in MLIR compiler infrastructure.","Test Program Generation,Compiler Fuzzing,MLIR Compiler Infrastructure",MLIRSmith：用于模糊化MLIR编译器基础结构的随机程序生成,MLIR（Multi-Level Intermediate Representation，多级中间表示）编译器基础设施近年来越来越受欢迎，以支持许多编译器的构建。MLIR编译器基础设施不是为每个域设计一个具有单一抽象的新IR，而是提供系统通行证来支持广泛的功能，使多个域一起受益，并引入方言来支持MLIR中的不同抽象级别。由于它在编译器社区中的基本作用，确保它的质量是非常关键的。在这项工作中，我们提出了MLIRSmith，这是MLIR编译器基础设施的第一个模糊技术。MLIRSmith采用两阶段策略生成有效和多样化的MLIR程序，该策略首先在扩展的MLIR语法规则的指导下构建多样化的程序模板，然后在我们设计的上下文敏感语法的指导下通过模板实例化生成有效的MLIR程序。在将MLIRSmith应用于MLIR编译器基础设施的最新版本后，我们检测到53个以前未知的错误，其中49/38个已由开发人员确认/修复。我们还将NNSmith（深度学习编译器的高级程序生成器）生成的高级程序转换为MLIR程序，用于间接模糊MLIR编译器基础设施。在相同的测试时间内，MLIRSmith在很大程度上优于这种间接技术，在MLIR编译器基础结构中检测到的错误增加了328.57%，覆盖的行/分支增加了194.67%/225.87%。,测试程序生成，编译器模糊化，MLIR编译器基础结构,,,
TEKWU76J,2023,https://doi.org/10.1109/ASE56229.2023.00070,ASE 2023,Detecting Smart Home Automation Application Interferences with Domain Knowledge,"Trigger-action programming (TAP) is a widely used development paradigm that simplifies the Internet of Things (loT) automation. However, the exceptional interactions between automation applications may result in interferences, such as conflicts and infinite loops, which cause undesirable consequences and even security and safety risks. While several techniques have been proposed to address this problem, they are often restricted in handling explicit and simple conflicts without considering contextual influences. In addition, they suffer from performance issues when applying to large-scale applications. To address these challenges, we design an effective and practical tool KnowDetector with comprehensive domain knowledge to detect application interferences. To detect application interferences, KnowDetector constructs an automation graph with 1) events, conditions, and actions from automation applications, 2) vertices representing physical environment channels, and 3) edges derived from potential semantic relations between the vertices. In order to make the graph extensively capture the interactions between automation applications, we propose a knowledge model named KnowloT that accurately characterizes loT devices with command-level loT services and the intricate relations between these services and the contextual environment. We abstract the interference detection into a graph pattern-matching problem and summarize ten application interference patterns of four types. Finally, KnowDetector can efficiently detect application interferences by searching for sub-graphs matching the patterns within the automation graph. We evaluated KnowDetector on three real-world datasets. The results demonstrated that it outperformed the other state-of-the-art tools with the highest precision, recall, and F-measure. In addition, KnowDetector is scalable to detect application interferences within a large number of applications with a minimal time overhead.","smart home platform,TAP,automation application interference,Internet of Things",利用领域知识检测智能家居自动化应用干扰,触发动作编程（TAP）是一种广泛使用的开发范式，它简化了物联网（loT）自动化。然而，自动化应用程序之间的异常交互可能会导致干扰，如冲突和无限循环，从而导致不良后果，甚至安全和安全风险。虽然已经提出了几种技术来解决这个问题，但它们在处理明确和简单的冲突时往往受到限制，而不考虑上下文的影响。此外，在应用于大型应用程序时，它们还会遇到性能问题。为了应对这些挑战，我们设计了一个有效实用的工具KnowDetector，该工具具有全面的领域知识，可以检测应用程序干扰。为了检测应用程序干扰，KnowDetector构建了一个自动化图，其中包括1）来自自动化应用程序的事件、条件和操作，2）表示物理环境通道的顶点，以及3）从顶点之间的潜在语义关系导出的边。为了使图广泛地捕捉自动化应用程序之间的交互，我们提出了一个名为KnowloT的知识模型，该模型准确地描述了具有命令级loT服务的loT设备，以及这些服务与上下文环境之间的复杂关系。我们将干扰检测抽象为一个图形模式匹配问题，并总结了四种类型的十种应用干扰模式。最后，KnowDetector可以通过搜索与自动化图中的模式匹配的子图来有效地检测应用程序干扰。我们在三个真实世界的数据集上评估了KnowDetector。结果表明，它在精度、召回率和F测量方面优于其他最先进的工具。此外，KnowDetector可扩展，以最小的时间开销检测大量应用程序中的应用程序干扰。,智能家居平台，TAP，自动化应用干扰，物联网,,,
HZC82XVX,2023,https://doi.org/10.1109/ASE56229.2023.00036,ASE 2023,Zero-Config Fuzzing for Microservices,"The microservice paradigm is a popular software development pattern that breaks down a large application into smaller, independent services. While this approach offers several advantages, such as scalability, agility, and flexibility, it also introduces new security challenges. This paper presents a novel approach to securing microservice architectures using fuzz testing. Fuzz testing is known to find programming errors in software by feeding it with unexpected or random inputs. In this paper, we propose a zero-config fuzz test generation technique for microservices that can maximize coverage of internal states by mutating both the incoming requests and the backend responses from dependent services. We successfully deployed our technique to over 95 % of C++ services built on Google's internal microservice platform. It reported and got fixed thousands of errors in real-world microservice applications.","fuzz testing,automation,microservice",微服务的零配置模糊,微服务模式是一种流行的软件开发模式，它将大型应用程序分解为较小的独立服务。虽然这种方法提供了一些优势，如可扩展性、灵活性和灵活性，但它也带来了新的安全挑战。本文提出了一种使用模糊测试来保护微服务架构的新方法。众所周知，模糊测试通过向软件提供意外或随机输入来发现软件中的编程错误。在本文中，我们提出了一种用于微服务的零配置模糊测试生成技术，该技术可以通过改变来自依赖服务的传入请求和后端响应来最大限度地覆盖内部状态。我们成功地将我们的技术部署到了95%以上建立在谷歌内部微服务平台上的C++服务中。它报告并修复了现实世界中微服务应用程序中的数千个错误。,模糊测试，自动化，微服务,,,
M6JFWAB7,2023,https://doi.org/10.1109/ASE56229.2023.00062,ASE 2023,QuCAT: A Combinatorial Testing Tool for Quantum Software,"With the increased developments in quantum computing, the availability of systematic and automatic testing approaches for quantum programs is becoming increasingly essential. To this end, we present the quantum software testing tool QuCAT for combinatorial testing of quantum programs. QuCAT provides two functionalities of use. With the first functionality, the tool generates a test suite of a given strength (e.g., pairwise). With the second functionality, it generates test suites with increasing strength until a failure is triggered or a maximum strength is reached. QuCAT uses two test oracles to check the correctness of test outputs. We assess the cost and effectiveness of QuCAT with 3 faulty versions of 5 quantum programs. Results show that combinatorial test suites with a low strength can find faults with limited cost, while a higher strength performs better to trigger some difficult faults with relatively higher cost.","quantum programs,software testing,combinatorial testing",量子软件的组合测试工具QuCAT,随着量子计算的不断发展，量子程序的系统和自动测试方法变得越来越重要。为此，我们提出了用于量子程序组合测试的量子软件测试工具QuCAT。QuCAT提供了两种使用功能。使用第一个功能，该工具生成给定强度的测试套件（例如，成对）。使用第二个功能，它生成强度不断增加的测试套件，直到触发故障或达到最大强度。QuCAT使用两个测试预言器来检查测试输出的正确性。我们用5个量子程序的3个错误版本评估了QuCAT的成本和有效性。结果表明，强度较低的组合测试套件可以在有限的成本下发现故障，而强度较高的测试套件可以更好地触发一些成本相对较高的困难故障。,量子程序，软件测试，组合测试,,,
6XAC4X2V,2023,https://doi.org/10.1109/ASE56229.2023.00046,ASE 2023,An Energy-Aware Approach to Design Self-Adaptive AI-based Applications on the Edge,"The advent of edge devices dedicated to machine learning tasks enabled the execution of AI-based applications that efficiently process and classify the data acquired by the resource-constrained devices populating the Internet of Things. The proliferation of such applications (e.g., critical monitoring in smart cities) demands new strategies to make these systems also sustainable from an energetic point of view. In this paper, we present an energy-aware approach for the design and deployment of self-adaptive AI-based applications that can balance application objectives (e.g., accuracy in object detection and frames processing rate) with energy consumption. We address the problem of determining the set of configurations that can be used to self-adapt the system with a meta-heuristic search procedure that only needs a small number of empirical samples. The final set of configurations are selected using weighted gray relational analysis, and mapped to the operation modes of the self-adaptive application. We validate our approach on an AI-based application for pedestrian detection. Results show that our self-adaptive application can outperform non-adaptive baseline configurations by saving up to 81% of energy while loosing only between 2% and 6 % in accuracy.","self-adaptive,energy-aware,AI-based,multi-objective,edge computing,internet-of-things",一种基于能量感知的边缘自适应人工智能应用程序设计方法,专用于机器学习任务的边缘设备的出现使基于人工智能的应用程序能够有效地处理和分类由物联网中资源受限的设备获取的数据。此类应用的激增（例如，智能城市中的关键监控）需要新的战略，从能源的角度来看，使这些系统也具有可持续性。在本文中，我们提出了一种用于设计和部署自适应人工智能应用程序的能量感知方法，该方法可以平衡应用程序目标（例如，对象检测的准确性和帧处理率）与能量消耗。我们通过只需要少量经验样本的元启发式搜索过程来解决确定可用于自适应系统的配置集的问题。使用加权灰色关系分析来选择最终的配置集，并将其映射到自适应应用程序的操作模式。我们在基于人工智能的行人检测应用程序上验证了我们的方法。结果表明，我们的自适应应用程序可以优于非自适应基线配置，节省高达81%的能量，而精度仅降低2%至6%。,自适应，能源感知，基于人工智能，多目标，边缘计算，物联网,,,
JN7FAMMJ,2023,https://doi.org/10.1109/ASE56229.2023.00067,ASE 2023,ConfTainter: Static Taint Analysis For Configuration Options,"The prevalence and severity of software configuration-induced issues have driven the design and development of a number of detection and diagnosis techniques. Many of these techniques need to perform static taint analysis on configuration-related variables to analyze the data flow, control flow, and execution paths given by configuration options. However, existing taint analysis or static slicer tools are not suitable for configuration analysis due to the complex effects of configuration on program behaviors. In this experience paper, we conducted an empirical study on the propagation policy of configuration options. We concluded four rules of how configurations affect program behaviors, among which implicit data-flow and control-flow propagation are often ignored by existing tools. We report our experience designing and implementing a taint analysis infrastructure for configurations, ConfTainter. It can support various kinds of configuration analysis, e.g., explicit or implicit analysis for data or control flow. Based on the infrastructure, researchers and developers can easily implement analysis techniques for different configuration-related targets, e.g., misconfiguration detection. We evaluated the effectiveness of ConfTainter on 5 popular open-source systems. The result shows that the accuracy rate of data- and control-flow analysis is 96.1% and 97.7%, and the recall rate is 94.2% and 95.5%, respectively. We also apply ConfTainter to two types of configuration-related tasks: misconfiguration detection and configuration-related bug detection. The result shows that ConfTainter is highly applicable for configuration-related tasks with a few lines of code.","static taint analysis,configuration,data flow,control flow",ConfTainter：配置选项的静态污染分析,软件配置引发的问题的普遍性和严重性推动了许多检测和诊断技术的设计和开发。这些技术中的许多都需要对与配置相关的变量执行静态污染分析，以分析配置选项提供的数据流、控制流和执行路径。然而，由于配置对程序行为的复杂影响，现有的污染分析或静态切片器工具不适合进行配置分析。在这篇经验论文中，我们对配置选项的传播策略进行了实证研究。我们总结了配置如何影响程序行为的四条规则，其中隐式数据流和控制流传播通常被现有工具忽视。我们报告了我们设计和实现配置污染分析基础设施ConfTaint的经验。它可以支持各种配置分析，例如，对数据或控制流的显式或隐式分析。基于该基础设施，研究人员和开发人员可以轻松实现针对不同配置相关目标的分析技术，例如错误配置检测。我们评估了ConfTainter在5个流行的开源系统上的有效性。结果表明，数据流和控制流分析的准确率分别为96.1%和97.7%，召回率分别为94.2%和95.5%。我们还将ConfTaint应用于两种类型的配置相关任务：错误配置检测和配置相关错误检测。结果表明，ConfTainter非常适用于只需几行代码的配置相关任务。,静态污染分析，配置，数据流，控制流,,,
7SBDVC7G,2023,https://doi.org/10.1109/ASE56229.2023.00112,ASE 2023,Using Deep Learning to Automatically Improve Code Readability,"Reading source code occupies most of developer's daily activities. Any maintenance and evolution task requires developers to read and understand the code they are going to modify. For this reason, previous research focused on the definition of techniques to automatically assess the readability of a given snippet. However, when many unreadable code sections are detected, developers might be required to manually modify them all to improve their readability. While existing approaches aim at solving specific readability-related issues, such as improving variable names or fixing styling issues, there is still no approach to automatically suggest which actions should be taken to improve code readability. In this paper, we define the first holistic readability-improving approach. As a first contribution, we introduce a methodology for automatically identifying readability-improving commits, and we use it to build a large dataset of 122k commits by mining the whole revision history of all the projects hosted on GitHub between 2015 and 2022. We show that such a methodology has ~86% accuracy. As a second contribution, we train and test the T5 model to emulate what developers did to improve readability. We show that our model achieves a perfect prediction accuracy between 21% and 28%. The results of a manual evaluation we performed on 500 predictions shows that when the model does not change the behavior of the input and it applies changes (34% of the cases), in the large majority of the cases (79.4%) it allows to improve code readability.","code readability,large language models,t5",使用深度学习自动提高代码可读性,阅读源代码占据了开发人员的大部分日常活动。任何维护和进化任务都需要开发人员阅读并理解他们要修改的代码。出于这个原因，以前的研究集中在自动评估给定片段可读性的技术的定义上。然而，当检测到许多不可读的代码段时，开发人员可能需要手动修改它们以提高可读性。虽然现有的方法旨在解决与可读性相关的特定问题，例如改进变量名或修复样式问题，但仍然没有自动建议应该采取哪些操作来提高代码可读性的方法。在本文中，我们定义了第一种整体可读性改进方法。作为第一个贡献，我们介绍了一种自动识别可读性改进提交的方法，并使用它通过挖掘2015年至2022年间GitHub上托管的所有项目的整个修订历史，构建了一个包含122k个提交的大型数据集。我们表明，这种方法的准确率约为86%。作为第二个贡献，我们对T5模型进行了培训和测试，以模拟开发人员为提高可读性所做的工作。我们表明，我们的模型实现了在21%和28%之间的完美预测精度。我们对500个预测进行的手动评估结果表明，当模型不改变输入的行为并应用更改（34%的情况）时，在绝大多数情况下（79.4%），它可以提高代码的可读性。,代码可读性，大型语言模型，t5,,,
ZZNX8MEW,2023,https://doi.org/10.1109/ASE56229.2023.00192,ASE 2023,Increasing the Responsiveness of Web Applications by Introducing Lazy Loading,"Front-end developers want their applications to contain no more code than is needed in order to minimize the amount of time that elapses between visiting a web page and the page becoming responsive. However, front-end code is typically written in JavaScript, the ubiquitous “language of the web”, and tends to rely heavily on third-party packages. While the reuse of packages improves developer productivity, it is notorious for resulting in very large “bloated” applications, resulting in a degraded end-user experience. One way to combat such bloat is to lazily load external packages on an as-needed basis, for which support was added to JavaScript in 2020 when asynchronous, dynamic imports were added to the language standard. Unfortunately, migrating existing projects to take advantage of this feature is nontrivial, as the code changes required to introduce asynchrony may involve complex, non-local transformations. In this work, we propose an approach for automatically introducing lazy loading of third-party packages in JavaScript applications. Our approach relies on static analysis to identify external packages that can be loaded lazily and generates the code transformations required to lazily load those packages. Since the static analysis is unsound, these transformations are presented as suggestions that programmers should review and test carefully. We implement this approach in a tool called Lazifier, and evaluate Lazifier on 10 open-source front-end JavaScript applications, showing that each application was successfully refactored, reducing initial application size and load times in all cases. On average, for these applications, Lazifier reduces initial application size by 36.2 %, initial load time by 29.7 %, and unsoundness did not arise in any of these applications.","JavaScript,client-side,refactoring,static analysis,lazy loading,dynamic loading",通过引入延迟加载提高Web应用程序的响应能力,前端开发人员希望他们的应用程序包含的代码不超过所需的数量，以最大限度地减少访问网页和页面响应之间的时间。然而，前端代码通常是用普遍存在的“网络语言”JavaScript编写的，并且往往严重依赖第三方软件包。虽然包的重用提高了开发人员的生产力，但它因导致非常大的“膨胀”应用程序而臭名昭著，从而导致最终用户体验下降。对抗这种膨胀的一种方法是根据需要延迟加载外部包，2020年，当异步、动态导入被添加到语言标准中时，JavaScript中添加了对外部包的支持。不幸的是，迁移现有项目以利用这一功能并不重要，因为引入异步所需的代码更改可能涉及复杂的非本地转换。在这项工作中，我们提出了一种在JavaScript应用程序中自动引入第三方包延迟加载的方法。我们的方法依赖于静态分析来识别可以延迟加载的外部包，并生成延迟加载这些包所需的代码转换。由于静态分析是不健全的，因此这些转换是程序员应该仔细审查和测试的建议。我们在一个名为Lazier的工具中实现了这种方法，并在10个开源前端JavaScript应用程序上评估了Lazier，表明每个应用程序都被成功重构，在所有情况下都减少了初始应用程序的大小和加载时间。平均而言，对于这些应用程序，Lazer将初始应用程序大小减少了36.2%，初始加载时间减少了29.7%，并且在这些应用程序中都没有出现不稳定的情况。,JavaScript，客户端，重构，静态分析，延迟加载，动态加载,,,
ACG23K4U,2023,https://doi.org/10.1109/ASE56229.2023.00014,ASE 2023,Open Source Software Tools for Data Management and Deep Model Training Automation,"Designing and optimizing deep models require managing large datasets and conducting carefully designed controlled experiments that depend on large sets of hyper-parameters and problem dependent software/data configurations. These experiments are executed by training the model under observation with varying configurations. Since executing a typical training run can take days even on proven acceleration fabrics such as Graphics Processing Units (GPU), properly managing training data, avoiding human error in configuration preparations and securing the repeatability of the experiments are of utmost importance. In this paper, we present two open source software tools that aim to achieve these goals, namely, a Dataset Manager (DatumAid) tool and a Training Automation Manager (OrchesTrain) tool. DatumAid is a software tool that integrates with Computer Vision Annotation Tool (CVAT) to facilitate the management of annotated datasets. By adding additional functionality, DatumAid allows users to filter labeled data, manipulate datasets, and export datasets for training purposes. The tool adopts a simple code structure while providing flexibility to users through configuration files. OrchesTrain aims to automate model training process by facilitating rapid preparation and training of models in the desired style for the intended tasks. Users can seamlessly integrate their models prepared in the PyTorch library into the system and leverage the full capabilities of OrchesTrain. It enables the simultaneous or separate usage of Wandb, MLflow, and TensorBoard loggers. To ensure reproducibility of the conducted experiments, all configurations and codes are saved to the selected logger in an appropriate structure within a YAML file along with the serialized model files. Both software tools are publicly available on GitHub.","dataset management,training automation,deep model,augmentation",用于数据管理和深度模型训练自动化的开源软件工具,设计和优化深度模型需要管理大型数据集，并进行精心设计的受控实验，这些实验依赖于大型超参数集和问题相关的软件/数据配置。这些实验是通过在不同配置的观察下训练模型来执行的。由于即使在经过验证的加速结构（如图形处理单元（GPU））上执行典型的训练运行也可能需要数天时间，因此正确管理训练数据、避免配置准备中的人为错误以及确保实验的可重复性至关重要。在本文中，我们提出了两个旨在实现这些目标的开源软件工具，即数据集管理器（DatumAid）工具和培训自动化管理器（OrchesTrain）工具。DatumAid是一种与计算机视觉注释工具（CVAT）集成的软件工具，用于促进注释数据集的管理。通过添加附加功能，DatumAid允许用户过滤标记数据、操作数据集和导出数据集以用于培训目的。该工具采用了简单的代码结构，同时通过配置文件为用户提供了灵活性。OrchesTrain旨在通过促进以预期任务所需风格快速准备和训练模型，实现模型训练过程的自动化。用户可以将PyTorch库中准备的模型无缝集成到系统中，并充分利用OrchesTrain的全部功能。它允许同时或单独使用Wandb、MLflow和TensorBoard记录器。为了确保所进行实验的再现性，所有配置和代码都以YAML文件中的适当结构与序列化模型文件一起保存到选定的记录器中。这两种软件工具都可以在GitHub上公开使用。,数据集管理，训练自动化，深度模型，扩充,,,
T8Y82FJ8,2023,https://doi.org/10.1109/ASE56229.2023.00170,ASE 2023,DCLINK: Bridging Data Constraint Changes and Implementations in FinTech Systems,"A FinTech system is a cluster of FinTech applications that intensively interact with databases containing a large quantity of user data. To ensure data consistency, it is a common practice to specify data constraints to validate data at runtime. However, data constraints often evolve according to changes in business requirements. Meanwhile, the developers can hardly keep up with the latest requirements during the development cycle. Such an information barrier increases the communication burden and prevents FinTech applications from being updated in time, impeding the development cycle significantly. In this paper, we present a comprehensive empirical study on data constraints in FinTech systems, investigating how they evolve and affect the development process. Our results show that developers find it hard to update their code timely because no mapping from data constraint changes to code is provided. Inspired by the findings from code updates respecting data constraint changes, we propose DCLINK, a traceability link analysis for linking each data constraint change to target methods demanding the code update in the FinTech application. We extensively evaluate DCLINK upon real-world change cases in Ant Group. The results show that DCLINK can effectively and efficiently localize the target methods.","static analysis,impact analysis,data constraint,FinTech application",DCLINK：桥接金融科技系统中的数据约束变化和实现,金融科技系统是指与包含大量用户数据的数据库进行密集交互的金融科技应用程序集群。为了确保数据的一致性，通常的做法是指定数据约束来在运行时验证数据。然而，数据约束通常会随着业务需求的变化而变化。同时，在开发周期中，开发人员很难跟上最新的需求。这种信息障碍增加了通信负担，阻碍了FinTech应用程序的及时更新，严重阻碍了开发周期。在本文中，我们对金融科技系统中的数据约束进行了全面的实证研究，研究了它们是如何演变和影响发展过程的。我们的结果表明，开发人员发现很难及时更新他们的代码，因为没有提供从数据约束更改到代码的映射。受有关数据约束变化的代码更新结果的启发，我们提出了DCLINK，这是一种可追溯性链接分析，用于将每个数据约束变化与FinTech应用程序中要求代码更新的目标方法联系起来。我们根据Ant Group中真实世界的变化案例对DCLINK进行了广泛的评估。结果表明，DCLINK能够有效定位目标方法。,静态分析，影响分析，数据约束，金融科技应用,,,
XRGDC58V,2023,https://doi.org/10.1109/ASE56229.2023.00037,ASE 2023,Neural SZZ Algorithm,"The SZZ algorithm has been widely used for identifying bug-inducing commits. However, it suffers from low precision, as not all deletion lines in the bug-fixing commit are related to the bug fix. Previous studies have attempted to address this issue by using static methods to filter out noise, e.g., comments and refactoring operations in the bug-fixing commit. However, these methods have two limitations. First, it is challenging to include all refactoring and non-essential change patterns in a tool, leading to the potential exclusion of relevant lines and the inclusion of irrelevant lines. Second, applying these tools might not always improve performance. In this paper, to address the aforementioned challenges, we propose NEURALSZZ, a deep learning approach for detecting the root cause deletion lines in a bug-fixing commit and using them as input for the SZZ algorithm. NEURALSZZ first constructs a heterogeneous graph attention network model that captures the semantic relationships between each deletion line and the other deletion and addition lines. To pinpoint the root cause of a bug, Neuralszz uses a learning-to-rank technique to rank all deletion lines in the commit. To evaluate the effectiveness of NEURALSZZ, we utilize three datasets containing high-quality bug-fixing and bug-inducing commits. The experiment results show that NEURALSZZ outperforms various baseline methods, e.g., traditional machine learning-based approaches and BI-LSTM in identifying the root cause of bugs. Moreover, by utilizing the top-ranked deletion lines and applying the SZZ algorithm, Neuralszz demonstrates better precision and F1-score compared to previous SZZ algorithms.","SZZ Algorithm,Deep Learning,Heterogeneous Graph Attention Network,Learning to Rank",神经SZZ算法,SZZ算法已被广泛用于识别引发错误的提交。然而，它的精度很低，因为并非bug修复提交中的所有删除行都与bug修复相关。以前的研究试图通过使用静态方法来过滤噪声来解决这个问题，例如错误修复提交中的注释和重构操作。然而，这些方法有两个局限性。首先，将所有重构和非必要的更改模式都包含在一个工具中是很有挑战性的，这可能会导致相关行被排除，而不相关行被包含。其次，应用这些工具可能并不总是能提高性能。在本文中，为了解决上述挑战，我们提出了NEURALSZZ，这是一种深度学习方法，用于检测错误修复提交中的根本原因删除线，并将其用作SZZ算法的输入。NEURALSZZ首先构建了一个异构图注意力网络模型，该模型捕捉了每条删除线与其他删除线和添加线之间的语义关系。为了找出错误的根本原因，Neuralszz使用学习排序技术对提交中的所有删除行进行排序。为了评估NEURALSZ的有效性，我们使用了三个数据集，其中包含高质量的错误修复和错误诱导提交。实验结果表明，NEURALSZZ在识别错误的根本原因方面优于各种基线方法，如传统的基于机器学习的方法和BI-LSTM。此外，通过利用排名靠前的删除线并应用SZZ算法，与以前的SZZ算法相比，Neuralszz表现出更好的精度和F1分数。,SZZ算法，深度学习，异构图注意力网络，学习排序,,,
RUHU222E,2023,https://doi.org/10.1109/ASE56229.2023.00090,ASE 2023,Revisiting and Improving Retrieval-Augmented Deep Assertion Generation,"Unit testing validates the correctness of the unit under test and has become an essential activity in software development process. A unit test consists of a test prefix that drives the unit under test into a particular state, and a test oracle (e.g., assertion), which specifies the behavior in that state. To reduce manual efforts in conducting unit testing, Yu et al. proposed an integrated approach (integration for short), combining information retrieval with a deep learning-based approach, to generate assertions for a unit test. Despite being promising, there is still a knowledge gap as to why or where integration works or does not work. In this paper, we describe an in-depth analysis of the effectiveness of integration. Our analysis shows that: ① The overall performance of integration is mainly due to its success in retrieving assertions. ② integration struggles to understand the semantic differences between the retrieved focal-test (focal-test includes a test prefix and a unit under test) and the input focal-test, resulting in many tokens being incorrectly modified; ③ integration is limited to specific types of edit operations (i.e., replacement) and cannot handle token addition or deletion. To improve the effectiveness of assertion generation, this paper proposes a novel retrieve-and-edit approach named EDITAS. Specifically, Editas first retrieves a similar focal-test from a pre-defined corpus and treats its assertion as a prototype. Then, Editas reuses the information in the prototype and edits the prototype automatically. Editas is more generalizable than integration because it can ❶ comprehensively understand the semantic differences between input and similar focal-tests; ❷ apply appropriate assertion edit patterns with greater flexibility; and ❸ generate more diverse edit actions than just replacement operations. We conduct experiments on two large-scale datasets and the experimental results demonstrate that Editas outperforms the state-of-the-art approaches, with an average improvement of 10.00%-87.48% and 3.30%-42.65% in accuracy and BLEU score, respectively.","Unit Testing,Assertion Generation,Test Assertion,Deep Learning",改进和改进检索增强的深度断言生成,单元测试验证被测单元的正确性，已成为软件开发过程中的一项重要活动。单元测试由一个将被测单元驱动到特定状态的测试前缀和一个指定该状态下行为的测试预言符（例如断言）组成。为了减少进行单元测试的人工工作量，Yu等人提出了一种集成方法（简称集成），将信息检索与基于深度学习的方法相结合，为单元测试生成断言。尽管很有希望，但对于集成为什么有效或在哪里无效，仍然存在知识差距。在本文中，我们对集成的有效性进行了深入的分析。我们的分析表明：①集成的整体性能主要归功于它在检索断言方面的成功。②集成难以理解检索到的焦点测试（焦点测试包括测试前缀和被测单元）和输入的焦点测试之间的语义差异，导致许多标记被错误地修改；③集成仅限于特定类型的编辑操作（即替换），并且不能处理令牌添加或删除。为了提高断言生成的有效性，本文提出了一种新的检索和编辑方法EDITAS。具体来说，Editas首先从预定义的语料库中检索类似的焦点测试，并将其断言视为原型。然后，Editas重用原型中的信息并自动编辑原型。Editas比集成更具通用性，因为它可以❶ 全面理解输入和相似焦点测试之间的语义差异；❷ 以更大的灵活性应用适当的断言编辑模式；和❸ 生成比替换操作更多样的编辑操作。我们在两个大型数据集上进行了实验，实验结果表明Editas优于最先进的方法，准确率和BLEU得分分别平均提高了10.00%-87.48%和3.30%-42.65%。,单元测试，断言生成，测试断言，深度学习,,,
7B4IZCTB,2023,https://doi.org/10.1109/ASE56229.2023.00016,ASE 2023,Live Programming for Finite Model Finders,"Finite model finders give users the ability to specify properties of a system in mathematical logic and then automatically find concrete examples, called solutions, that satisfy the properties. These solutions are often viewed as a key benefit of model finders, as they create an exploratory environment for developers to engage with their model. In practice, users find less benefit from these solutions than expected. For years, researchers believed that the problem was that too many solutions are produced. However, a recent user study found that users actually prefer enumerating a broad set of solutions. Inspired by a recent user study on Alloy, a modeling language backed by a finite model finder, we believe that the issue is that solutions are too removed from the logical constraints that generate them to help users build an understanding of the constraints themselves. In this paper, we outline a proof-of-concept for live programming of Alloy models in which writing the model and exploring solutions are intertwined. We highlight how this development environment enables more productive feedback loops between the developer, the model and the solutions.","Relational Model Finders,Live Programming",有限模型查找器的实时编程,有限模型查找器让用户能够在数学逻辑中指定系统的属性，然后自动找到满足这些属性的具体示例，称为解决方案。这些解决方案通常被视为模型查找器的一个关键优势，因为它们为开发人员创建了一个探索性的环境来参与他们的模型。在实践中，用户发现从这些解决方案中获得的好处比预期的要少。多年来，研究人员认为问题在于产生了太多的解决方案。然而，最近的一项用户研究发现，用户实际上更喜欢列举一系列广泛的解决方案。受用户最近对Alloy（一种由有限模型查找器支持的建模语言）的研究启发，我们认为问题在于，解决方案过于脱离生成它们的逻辑约束，无法帮助用户自己理解约束。在本文中，我们概述了Alloy模型的实时编程的概念验证，其中编写模型和探索解决方案是交织在一起的。我们强调了这种开发环境如何在开发人员、模型和解决方案之间实现更高效的反馈循环。,关系模型查找器，实时编程,,,
XP3RINRV,2023,https://doi.org/10.1109/ASE56229.2023.00166,ASE 2023,On-the-fly Improving Performance of Deep Code Models via Input Denoising,"Deep learning has been widely adopted to tackle various code-based tasks by building deep code models based on a large amount of code snippets. While these deep code models have achieved great success, even state-of-the-art models suffer from noise present in inputs leading to erroneous predictions. While it is possible to enhance models through retraining/fine-tuning, this is not a once-and-for-all approach and incurs significant overhead. In particular, these techniques cannot on-the-fly improve performance of (deployed) models. There are currently some techniques for input denoising in other domains (such as image processing), but since code input is discrete and must strictly abide by complex syntactic and semantic constraints, input denoising techniques in other fields are almost not applicable. In this work, we propose the first input denoising technique (i.e., CodeDenoise) for deep code models. Its key idea is to localize noisy identifiers in (likely) mispredicted inputs, and denoise such inputs by cleansing the located identifiers. It does not need to retrain or reconstruct the model, but only needs to cleanse inputs on-the-fly to improve performance. Our experiments on 18 deep code models (i.e., three pre-trained models with six code-based datasets) demonstrate the effectiveness and efficiency of CodeDenoise. For example, on average, CodeDenoise successfully denoises 21.91% of mispredicted inputs and improves the original models by 2.04% in terms of the model accuracy across all the subjects in an average of 0.48 second spent on each input, substantially outperforming the widely-used fine-tuning strategy.","Input Denoising,Code Model,Deep Learning",通过输入去噪动态提高深度代码模型的性能,通过基于大量代码片段构建深度代码模型，深度学习已被广泛用于处理各种基于代码的任务。虽然这些深度代码模型已经取得了巨大的成功，但即使是最先进的模型也会受到输入中存在的噪声的影响，从而导致错误的预测。虽然可以通过再培训/微调来增强模型，但这不是一种一劳永逸的方法，而且会产生大量开销。特别是，这些技术不能快速提高（部署的）模型的性能。目前在其他领域（如图像处理）也有一些输入去噪技术，但由于代码输入是离散的，必须严格遵守复杂的句法和语义约束，因此其他领域的输入去噪方法几乎不适用。在这项工作中，我们提出了用于深度代码模型的第一种输入去噪技术（即CodeDenoise）。它的关键思想是在（可能）预测错误的输入中定位有噪声的标识符，并通过清理定位的标识符来对这些输入进行去噪。它不需要重新训练或重建模型，只需要在运行中清除输入以提高性能。我们在18个深度代码模型上的实验（即，具有六个基于代码的数据集的三个预训练模型）证明了CodeDenoise的有效性和效率。例如，平均而言，CodeDenoise在每个输入上平均花费0.48秒，成功地去除了21.91%的预测错误输入，并在所有受试者的模型精度方面将原始模型提高了2.04%，大大优于广泛使用的微调策略。,输入去噪，代码模型，深度学习,,,
2KXP5B2P,2023,https://doi.org/10.1109/ASE56229.2023.00149,ASE 2023,Code Difference Guided Adversarial Example Generation for Deep Code Models,"Adversarial examples are important to test and enhance the robustness of deep code models. As source code is discrete and has to strictly stick to complex grammar and semantics constraints, the adversarial example generation techniques in other domains are hardly applicable. Moreover, the adversarial example generation techniques specific to deep code models still suffer from unsatisfactory effectiveness due to the enormous ingredient search space. In this work, we propose a novel adversarial example generation technique (i.e., CODA) for testing deep code models. Its key idea is to use code differences between the target input (i.e., a given code snippet as the model input) and reference inputs (i.e., the inputs that have small code differences but different prediction results with the target input) to guide the generation of adversarial examples. It considers both structure differences and identifier differences to preserve the original semantics. Hence, the ingredient search space can be largely reduced as the one constituted by the two kinds of code differences, and thus the testing process can be improved by designing and guiding corresponding equivalent structure transformations and identifier renaming transformations. Our experiments on 15 deep code models demonstrate the effective-ness and efficiency of CODA, the naturalness of its generated examples, and its capability of enhancing model robustness after adversarial fine-tuning. For example, CODA reveals 88.05 % and 72.51 % more faults in models than the state-of-the-art techniques (i.e., CARROT and ALERT) on average, respectively.","Adversarial Example,Code Model,Guided Testing,Code Transformation",深度代码模型的代码差分引导对抗性示例生成,对抗性示例对于测试和增强深度代码模型的健壮性非常重要。由于源代码是离散的，并且必须严格遵守复杂的语法和语义约束，因此对抗性示例生成技术在其他领域几乎不适用。此外，由于巨大的成分搜索空间，特定于深度代码模型的对抗性示例生成技术仍然存在不令人满意的效果。在这项工作中，我们提出了一种新的对抗性示例生成技术（即CODA），用于测试深度代码模型。其关键思想是使用目标输入（即，作为模型输入的给定代码片段）和参考输入（即具有较小代码差异但与目标输入的预测结果不同的输入）之间的代码差异来指导对抗性示例的生成。它同时考虑了结构差异和标识符差异，以保留原始语义。因此，成分搜索空间可以大大减少为由两种代码差异构成的空间，因此可以通过设计和指导相应的等效结构转换和标识符重命名转换来改进测试过程。我们在15个深度代码模型上的实验证明了CODA的有效性和效率，其生成示例的自然性，以及在对抗性微调后增强模型鲁棒性的能力。例如，CODA在模型中显示的故障平均分别比最先进的技术（即CARROT和ALERT）多88.05%和72.51%。,对抗性示例，代码模型，引导测试，代码转换,,,
CIJBGY4U,2023,https://doi.org/10.1109/ASE56229.2023.00076,ASE 2023,Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases,"Large Language Models (LLMs) have demonstrated remarkable performance in code completion. However, due to the lack of domain-specific knowledge, they may not be optimal in completing code that requires intensive domain knowledge for example completing the library names. Although there are several works that have confirmed the effectiveness of fine-tuning techniques to adapt language models for code completion in specific domains. They are limited by the need for constant fine-tuning of the model when the project is in constant iteration. To address this limitation, in this paper, we propose $k$ NM-LM, a retrieval-augmented language model (R-LM), that integrates domain knowledge into language models without fine-tuning. Different from previous techniques, our approach is able to automatically adapt to different language models and domains. Specifically, it utilizes the in-domain code to build the retrieval-based database decoupled from LM, and then combines it with LM through Bayesian inference to complete the code. The extensive experiments on the completion of intra-project and intra-scenario have confirmed that $k$ NM-LM brings about appreciable enhancements when compared to CodeGPT and UnixCoder. A deep analysis of our tool including the responding speed, storage usage, specific type code completion, and API invocation completion has confirmed that $k$ NM-LM provides satisfactory performance, which renders it highly appropriate for domain adaptive code completion. Furthermore, our approach operates without the requirement for direct access to the language model's parameters. As a result, it can seamlessly integrate with black-box code completion models, making it easy to integrate our approach as a plugin to further enhance the performance of these models.","domain adaptive code completion,retrieval-augment language model",基于语言模型和解耦域数据库的域自适应代码完成,大型语言模型（LLM）在代码完成方面表现出了非凡的性能。然而，由于缺乏特定领域的知识，它们在完成需要密集领域知识的代码（例如完成库名称）时可能不是最佳的。尽管有几项工作已经证实了微调技术的有效性，以适应特定领域中代码完成的语言模型。当项目处于不断迭代中时，它们受到模型不断微调需求的限制。为了解决这一限制，在本文中，我们提出了$k$NM-LM，一种检索增强语言模型（R-LM），它将领域知识集成到语言模型中，而无需进行微调。与以前的技术不同，我们的方法能够自动适应不同的语言模型和领域。具体来说，它利用域内代码来构建与LM解耦的基于检索的数据库，然后通过贝叶斯推理将其与LM相结合来完成代码。在项目内和场景内完成的大量实验已经证实，与CodeGPT和UnixCoder相比，$k$NM-LM带来了显著的增强。对我们的工具进行深入分析，包括响应速度、存储使用情况、特定类型代码完成情况和API调用完成情况，确认$k$NM-LM提供了令人满意的性能，这使其非常适合于域自适应代码完成。此外，我们的方法在不需要直接访问语言模型参数的情况下运行。因此，它可以与黑盒代码完成模型无缝集成，从而可以轻松地将我们的方法作为插件进行集成，以进一步提高这些模型的性能。,领域自适应代码补全检索扩充语言模型,,,
HWBJBNF3,2023,https://doi.org/10.1109/ASE56229.2023.00174,ASE 2023,Software Engineering Using Autonomous Agents: Are We There Yet?,"Autonomous agents equipped with Large Language Models (LLMs) are rapidly gaining prominence as a revolutionary technology within the realm of Software Engineering. These intelligent and autonomous systems demonstrate the capacity to perform tasks and make independent decisions, leveraging their intrinsic reasoning and decision-making abilities. This paper delves into the current state of autonomous agents, their capabilities, challenges, and opportunities in Software Engineering practices. By employing different prompts (with or without context), we conclude the advantages of contextrich prompts for autonomous agents. Prompts with context enhance user requirement understanding, avoiding irrelevant details that could hinder task comprehension and degrade model performance, particularly when dealing with complex frameworks such as Spring Boot, Django, Flask, etc. This exploration is conducted using Auto-GPT (v0.3.0), an open-source application powered by GPT-3.5 and GPT-4 which intelligently connects the “thoughts” of Large Language Models (LLMs) to independently accomplish the assigned goals or tasks.","Autonomous agents,Large Language Models (LLMs),SDLC",使用自主代理的软件工程：我们已经实现了吗？,配备了大型语言模型（LLM）的自治代理作为软件工程领域的一项革命性技术正在迅速获得重视。这些智能和自主系统展示了执行任务和做出独立决策的能力，利用了其内在的推理和决策能力。本文深入探讨了自主代理的现状、它们的能力、挑战和软件工程实践中的机遇。通过使用不同的提示（有上下文或无上下文），我们得出了上下文提示对自主主体的优势。上下文提示增强了用户对需求的理解，避免了可能阻碍任务理解和降低模型性能的无关细节，特别是在处理复杂框架（如Spring Boot、Django、Flask等）时。此探索是使用Auto GPT（v0.3.0）进行的，一个由GPT-3.5和GPT-4支持的开源应用程序，它智能地连接大型语言模型（LLM）的“思想”，以独立完成指定的目标或任务。,自治代理，大型语言模型（LLM），SDLC,,,
CBZ5IB6B,2023,https://doi.org/10.1109/ASE56229.2023.00180,ASE 2023,SMT Solver Validation Empowered by Large Pre-Trained Language Models,"SMT solvers are utilized to check the satisfiability of logic formulas and have been applied in various crucial domains, including software verification, test case generation, and program synthesis. However, bugs hidden in SMT solvers can lead to severe consequences, causing erroneous results in these domains. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Despite several testing approaches proposed for SMT solvers, generating effective test formulas to comprehensively test SMT solvers remains a challenge. To address this challenge, in this study, we propose to port large language models (LLMs) to generate SMT formulas for fuzzing solvers. Specifically, the study presents a novel retrain-finetune pipeline to unleash the potential of language models to generate effective SMT formulas and improve their generation performance through data augmentation. We implemented our approach as a practical fuzzing tool, named LasT,and then extensively tested the state-of-the-art SMT solvers, namely Z3, cvc5, and Bitwuzla. To date, Last has successfully uncovered 65 genuine bugs for the solvers, of which 45 have been fixed by the developers.","SMT solver,fuzzing,large language model,retrain-finetune,data augmentation",大型预训练语言模型支持SMT解算器验证,SMT求解器用于检查逻辑公式的可满足性，并已应用于各种关键领域，包括软件验证、测试用例生成和程序合成。然而，SMT求解器中隐藏的错误可能会导致严重的后果，从而在这些域中导致错误的结果。因此，确保SMT求解器的可靠性和稳健性至关重要。尽管为SMT求解器提出了几种测试方法，但生成有效的测试公式来全面测试SMT求解器仍然是一个挑战。为了应对这一挑战，在本研究中，我们建议移植大型语言模型（LLM）来生成模糊求解器的SMT公式。具体而言，该研究提出了一种新的再培训微调管道，以释放语言模型的潜力，生成有效的SMT公式，并通过数据扩充提高其生成性能。我们将我们的方法实现为一个名为LasT的实用模糊工具，然后广泛测试了最先进的SMT求解器，即Z3、cvc5和Bitwuzla。到目前为止，Last已经成功地为求解器发现了65个真正的错误，其中45个已经被开发人员修复。,SMT求解器，模糊化，大型语言模型，再培训微调，数据扩充,,,
WDII5JS4,2023,https://doi.org/10.1109/ASE56229.2023.00102,ASE 2023,Who is the Real Hero? Measuring Developer Contribution via Multi-Dimensional Data Integration,"Proper incentives are important for motivating developers in open-source communities, which is crucial for maintaining the development of open-source software healthy. To provide such incentives, an accurate and objective developer contribution measurement method is needed. However, existing methods rely heavily on manual peer review, lacking objectivity and transparency. The metrics of some automated works about effort estimation use only syntax-level or even text-level information, such as changed lines of code, which lack robustness. Furthermore, some works about identifying core developers provide only a qualitative understanding without a quantitative score or have some project-specific parameters, which makes them not practical in real-world projects. To this end, we propose CVALUE, a multidimensional information fusion-based approach to measure developer contributions. CVALUE extracts both syntax and semantic information from the source code changes in four dimensions: modification amount, understandability, inter-function and intra-function impact of modification. It fuses the information to produce the contribution score for each of the commits in the projects. Experimental results show that CVALUE outperforms other approaches by 19.59% on 10 real-world projects with manually labeled ground truth. We validated and proved that the performance of CVALUE, which takes 83.39 seconds per commit, is acceptable to be applied in real-world projects. Furthermore, we performed a large-scale experiment on 174 projects and detected 2,282 developers having inflated commits. Of these, 2,050 developers did not make any syntax contribution; and 103 were identified as bots.","Open-Source Incentive,Mining Software Repositories,Program Analysis",谁是真正的英雄？通过多维数据集成衡量开发人员的贡献,适当的激励措施对于激励开源社区的开发人员非常重要，这对于保持开源软件的健康开发至关重要。为了提供这种激励，需要一种准确客观的开发者贡献衡量方法。然而，现有方法严重依赖人工同行评审，缺乏客观性和透明度。一些关于工作量估计的自动化工作的度量只使用语法级别甚至文本级别的信息，例如更改的代码行，这些信息缺乏稳健性。此外，一些关于识别核心开发人员的工作只提供了定性的理解，而没有定量的分数，或者有一些特定于项目的参数，这使得它们在现实世界的项目中不实用。为此，我们提出了CVALUE，这是一种基于多维信息融合的方法，用于衡量开发人员的贡献。CVALUE从源代码的修改量、可理解性、修改的函数间和函数内影响四个维度提取语法和语义信息。它融合信息，为项目中的每个提交生成贡献分数。实验结果表明，CVALUE在10个具有手动标记基本事实的真实世界项目中比其他方法高出19.59%。我们验证并证明了CVALUE的性能，每次提交需要83.39秒，可以在现实世界的项目中应用。此外，我们在174个项目上进行了大规模实验，发现2282名开发人员的提交被夸大了。其中，2050名开发人员没有做出任何语法贡献；103人被确认为机器人。,开源激励，挖掘软件存储库，程序分析,,,
6DWEQ92P,2023,https://doi.org/10.1109/ASE56229.2023.00110,ASE 2023,DeFiWarder: Protecting DeFi Apps from Token Leaking Vulnerabilities,"Decentralized Finance (DeFi) apps have rapidly proliferated with the development of blockchain and smart contracts, whose maximum total value locked (TVL) has exceeded 100 billion dollars in the past few years. These apps allow users to interact and perform complicated financial activities. However, the vulnerabilities hiding in the smart contracts of DeFi apps have resulted in numerous security incidents, with most of them leading to funds (tokens) leaking and resulting in severe financial loss. In this paper, we summarize Token Leaking vulnerability of DeFi apps, which enable someone to abnormally withdraw funds that far exceed their deposits. Due to the massive amount of funds in DeFi apps, it is crucial to protect DeFi apps from Token Leaking vulnerabilities. Unfortunately, existing tools have limitations in addressing this vulnerability. To address this issue, we propose DeFiWarder, a tool that traces on-chain transactions and protects DeFi apps from Token Leaking vulnerabilities. Specifically, DeFiWarder first records the execution logs (traces) of smart contracts. It then accurately recovers token transfers within transactions to catch the funds flow between users and DeFi apps, as well as the relations between users based on role mining. Finally, DeFiWarder utilizes anomaly detection to reveal Token Leaking vulnerabilities and related attack behaviors. We conducted experiments to demonstrate the effectiveness and efficiency of DeFiWarder. Specifically, DeFi-Warder successfully revealed 25 Token Leaking vulnerabilities from 30 Defi apps. Moreover, its efficiency supports real-time detection of token leaking within on-chain transactions. In addition, we summarize five major reasons for Token Leaking vulnerability to assist DeFi apps in protecting their funds.","Smart Contract,DeFi,Token Leaking,Vulnerability",DeFiWarder：保护DeFi应用程序免受代币泄露漏洞,随着区块链和智能合约的发展，去中心化金融（DeFi）应用程序迅速激增，在过去几年中，其最大锁定总价值（TVL）已超过1000亿美元。这些应用程序允许用户进行交互并执行复杂的金融活动。然而，DeFi应用程序的智能合约中隐藏的漏洞导致了许多安全事件，其中大多数导致资金（代币）泄漏，并导致严重的财务损失。在本文中，我们总结了DeFi应用程序的代币泄露漏洞，该漏洞使人们能够异常提取远远超过存款的资金。由于DeFi应用程序中有大量资金，保护DeFi程序免受代币泄露漏洞的影响至关重要。不幸的是，现有工具在解决该漏洞方面存在局限性。为了解决这个问题，我们提出了DeFiWarder，这是一种跟踪链上交易并保护DeFi应用程序免受代币泄露漏洞攻击的工具。具体来说，DeFiWarder首先记录智能合约的执行日志（跟踪）。然后，它准确地恢复交易中的代币转账，以捕捉用户和DeFi应用程序之间的资金流，以及基于角色挖掘的用户之间的关系。最后，DeFiWarder利用异常检测来揭示代币泄漏漏洞和相关攻击行为。我们进行了实验来证明DeFiWarder的有效性和效率。具体而言，DeFi Warder成功揭露了30款DeFi应用程序中的25个代币泄露漏洞。此外，它的效率支持实时检测链上交易中的令牌泄漏。此外，我们总结了代币泄漏漏洞的五个主要原因，以帮助DeFi应用程序保护其资金。,智能合约，DeFi，代币泄露，漏洞,,,
PRNWA8MI,2023,https://doi.org/10.1109/ASE56229.2023.00118,ASE 2023,Two Birds with One Stone: Multi-Derivation for Fast Context-Free Language Reachability Analysis,"Context-free language (CFL) reachability is a fundamental framework for formulating program analyses. CFL-reachability analysis works on top of an edge-labeled graph by deriving reachability relations and adding them as labeled edges to the graph. Existing CFL-reachability algorithms typically adopt a single-reachability relation derivation (SRD) strategy, i.e., one reachability relation is derived at a time. Unfortunately, this strategy can lead to redundancy, hindering the efficiency of the analysis. To address this problem, this paper proposes Pearl, a multi-derivation approach that reduces derivation redundancy for transitive relations that frequently arise when solving reachability relations, significantly improving the efficiency of CFL-reachability analysis. Our key insight is that multiple edges involving transitivity can be simultaneously derived via batch propagation of reachability relations on the transitivity-aware subgraphs that are induced from the original edge-labeled graph. We evaluate the performance of Pearl on two clients, i.e., context-sensitive value-flow analysis and field-sensitive alias analysis for C/C++. By eliminating a large amount of redundancy, Pearl achieves average speedups of 82.73x for value-flow analysis and 155.26x for alias analysis over the standard CFL-reachability algorithm. The comparison with Pocr, a state-of-the-art CFL-reachability solver, shows that Pearl runs 10.1x (up to 29.2x) and 2.37x (up to 4.22x) faster on average respectively for value-flow analysis and alias analysis with less consumed memory.","CFL-Reachability,transitive relations",一石二鸟：快速上下文无关语言可达性分析的多元推导,上下文无关语言（CFL）可达性是制定程序分析的基本框架。CFL可达性分析通过导出可达性关系并将它们作为标记边添加到图中，在边标记图的顶部进行工作。现有的CFL可达性算法通常采用单个可达性关系推导（SRD）策略，即一次推导一个可达性关系。不幸的是，这种策略可能会导致冗余，阻碍分析的效率。为了解决这个问题，本文提出了Pearl，这是一种多导数方法，它减少了在求解可达性关系时经常出现的传递关系的导数冗余，显著提高了CFL可达性分析的效率。我们的关键见解是，通过在从原始边标记图导出的传递性感知子图上批量传播可达性关系，可以同时导出涉及传递性的多条边。我们评估了Pearl在两个客户端上的性能，即C/C++的上下文敏感的值流分析和字段敏感的别名分析。通过消除大量冗余，与标准CFL可达性算法相比，Pearl实现了82.73倍的值流分析平均加速和155.26倍的别名分析平均加速。与最先进的CFL可达性求解器Pocr的比较表明，Pearl在消耗较少内存的情况下，在价值流分析和别名分析中的平均运行速度分别快10.1x（高达29.2x）和2.37x（高达4.22x）。,CFL可达性，传递关系,,,
A7I62K39,2023,https://doi.org/10.1109/ASE56229.2023.00097,ASE 2023,Leakpair: Proactive Repairing of Memory Leaks in Single Page Web Applications,"Modern web applications often resort to application development frameworks such as React, Vue.js, and Angular. While the frameworks facilitate the development of web applications with several useful components, they are inevitably vulnerable to unmanaged memory consumption since the frameworks often produce Single Page Applications (SPAs). Web applications can be alive for hours and days with behavior loops, in such cases, even a single memory leak in a SPA app can cause performance degradation on the client side. However, recent debugging techniques for web applications still focus on memory leak detection, which requires manual tasks and produces imprecise results. We propose Leakpair,a technique to repair memory leaks in single page applications. Given the insight that memory leaks are mostly non-functional bugs and fixing them might not change the behavior of an application, the technique is designed to proactively generate patches to fix memory leaks, without leak detection, which is often heavy and tedious. To generate effective patches, Leakpairfollows the idea of pattern-based program repair since the automated repair strategy shows successful results in many recent studies. We evaluate the technique on more than 20 open-source projects without using explicit leak detection. The patches generated by our technique are also submitted to the projects as pull requests. The results show that Leakpaircan generate effective patches to reduce memory consumption that are acceptable to developers. In addition, we execute the test suites given by the projects after applying the patches, and it turns out that the patches do not cause any functionality breakage; this might imply that Leakpaircan generate non-intrusive patches for memory leaks.","memory leaks,program repair,non-intrusive fixes,single page applications",Leakpair：主动修复单页Web应用程序中的内存泄漏,现代web应用程序通常采用React、Vue.js和Angular等应用程序开发框架。虽然这些框架通过几个有用的组件促进了web应用程序的开发，但它们不可避免地容易受到非托管内存消耗的影响，因为这些框架通常会产生单页应用程序（SPA）。Web应用程序可以通过行为循环存活数小时或数天，在这种情况下，即使SPA应用程序中的单个内存泄漏也可能导致客户端性能下降。然而，最近针对web应用程序的调试技术仍然侧重于内存泄漏检测，这需要手动任务，并且会产生不精确的结果。我们提出了Leakpair，这是一种修复单页应用程序中内存泄漏的技术。鉴于内存泄漏大多是非功能性错误，修复它们可能不会改变应用程序的行为，该技术旨在主动生成修补程序来修复内存泄漏，而无需进行泄漏检测，这通常既繁重又乏味。为了生成有效的补丁，Leakpair遵循了基于模式的程序修复的思想，因为自动修复策略在最近的许多研究中都显示出了成功的结果。我们在20多个开源项目上评估了这项技术，但没有使用明确的泄漏检测。我们的技术生成的补丁也会作为拉取请求提交给项目。结果表明，Leakpair可以生成有效的补丁来减少开发人员可以接受的内存消耗。此外，我们在应用补丁后执行项目提供的测试套件，结果发现补丁不会导致任何功能破坏；这可能意味着Leakpair可以为内存泄漏生成非侵入性补丁。,内存泄漏，程序修复，非侵入性修复，单页应用程序,,,
GH7IGAVY,2023,https://doi.org/10.1109/ASE56229.2023.00064,ASE 2023,Contrastive Learning for API Aspect Analysis,"We present a novel approach - CLAA - for API aspect detection in API reviews that utilizes transformer models trained with a supervised contrastive loss objective function. We evaluate CLAA using performance and impact analysis. For performance analysis, we utilized a benchmark dataset on developer discussions collected from Stack Overflow and compare the results to those obtained using state-of-the-art transformer models. Our experiments show that contrastive learning can significantly improve the performance of transformer models in detecting aspects such as Performance, Security, Usability, and Documentation. For impact analysis, we performed empirical and developer study. On a randomly selected and manually labeled 200 online reviews, CLAA achieved 92% accuracy while the SOTA baseline achieved 81.5%. According to our developer study involving 10 participants, the use of Stack Overflow + CLAA resulted in increased accuracy and confidence during API selection. Replication package: https://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023.","API aspects,Contrastive learning,Transformers,API review,Aspect detection,LIME",API方面分析的对比学习,我们提出了一种新的方法——CLAA——用于API评论中的API方面检测，该方法利用用监督对比损失目标函数训练的变换器模型。我们使用性能和影响分析来评估CLAA。为了进行性能分析，我们使用了从Stack Overflow收集的关于开发人员讨论的基准数据集，并将结果与使用最先进的转换器模型获得的结果进行了比较。我们的实验表明，对比学习可以显著提高变压器模型在检测性能、安全性、可用性和文档等方面的性能。对于影响分析，我们进行了实证和开发者研究。在随机选择并手动标记的200条在线评论中，CLAA的准确率达到92%，而SOTA基线达到81.5%。根据我们涉及10名参与者的开发人员研究，在API选择过程中，使用Stack Overflow+CLAA提高了准确率和可信度。复制包：https://github.com/disa-lab/Contrastive-Learning-API-Aspect-ASE2023.,API方面，对比学习，Transformers，API评审，方面检测，LIME,,,
PPPQ8UWF,2023,https://doi.org/10.1109/ASE56229.2023.00026,ASE 2023,Bridging the Gap Between Academia and Industry in Machine Learning Software Defect Prediction: Thirteen Considerations,"This experience paper describes thirteen considerations for implementing machine learning software defect prediction (ML SDP) in vivo. Specifically, we provide the following report on the ground of the most important observations and lessons learned gathered during a large-scale research effort and introduction of ML SDP to the system-level testing quality assurance process of one of the leading telecommunication vendors in the world — Nokia. We adhere to a holistic and logical progression based on the principles of the business analysis body of knowledge: from identifying the need and setting requirements, through designing and implementing the solution, to profitability analysis, stakeholder management, and handover. Conversely, for many years, industry adoption has not kept up the pace of academic achievements in the field, despite promising potential to improve quality and decrease the cost of software products for many companies worldwide. Therefore, discussed considerations hopefully help researchers and practitioners bridge the gaps between academia and industry.","machine learning,software defect prediction,Nokia 5G,industry introduction,experience paper",在机器学习软件缺陷预测方面弥合学术界和工业界的差距：十三点考虑,这篇经验论文描述了在体内实现机器学习软件缺陷预测（ML SDP）的十三个注意事项。具体而言，我们提供以下报告，基于在大规模研究工作中收集的最重要的观察结果和经验教训，并将ML SDP引入世界领先电信供应商之一诺基亚的系统级测试质量保证过程。我们坚持基于业务分析知识体系原则的整体和逻辑发展：从确定需求和设定要求，到设计和实施解决方案，再到盈利能力分析、利益相关者管理和移交。相反，多年来，尽管全球许多公司有望提高软件产品的质量和降低成本，但行业采用率并没有跟上该领域学术成就的步伐。因此，讨论的考虑因素有望帮助研究人员和从业者弥合学术界和工业界之间的差距。,机器学习，软件缺陷预测，诺基亚5G，行业介绍，经验论文,,,
I7GK4HTR,2023,https://doi.org/10.1109/ASE56229.2023.00201,ASE 2023,BugMiner: Automating Precise Bug Dataset Construction by Code Evolution History Mining,"Bugs and their fixes in the code evolution histories are important assets for many software engineering tasks such as deriving new state-of-the-art automatic bug fixing techniques. Existing bug datasets are either manually built which is difficult to grow efficiently to a scale large enough for massive data analysis, or lack of precise information of how bugs are introduced and fixed which is critical for in-depth analysis such as buggy/fixing code identification. Moreover, the types of the bugs are typically missing in the existing bug datasets, limiting the possibility of developing high-precision type-specific approaches for enterprise-level purposes. In this work, we propose BugMiner, an approach to automatically collecting bugs from code repositories by isolating the critical changes of the bugs. We also propose a learning-based approach for automating bug type classification with relatively small manual labels of bug types. We evaluate our approach regarding the precision of bug information and the efficiency of the bug-mining process with 2,082 bugs automatically mined from 100 open-source projects. We demonstrate the improved effectiveness and efficiency in bug-fixing location identification, compared to the SOTA BugBuilder, and high recall and precision in bug-inducing location identification. We also compare our learning-based bug classification approach to traditional baseline method, indicating about 17 % improvement in classification effectiveness under macro-F1.","bug dataset,bug-fixing,bug-inducing,automatic bug mining",BugMiner：通过代码进化历史挖掘自动构建精确的Bug数据集,代码进化历史中的错误及其修复是许多软件工程任务的重要资产，例如衍生新的最先进的自动错误修复技术。现有的bug数据集要么是手动构建的，很难有效地扩展到足以进行大规模数据分析的规模，要么缺乏关于如何引入和修复bug的精确信息，这对于深入分析（如bug/修复代码识别）至关重要。此外，现有的bug数据集中通常缺少bug的类型，这限制了为企业级目的开发高精度类型特定方法的可能性。在这项工作中，我们提出了BugMiner，这是一种通过隔离bug的关键更改来自动从代码库中收集bug的方法。我们还提出了一种基于学习的方法，通过相对较小的错误类型手动标签来自动进行错误类型分类。我们从100个开源项目中自动挖掘了2082个bug，评估了我们的方法，包括bug信息的准确性和bug挖掘过程的效率。与SOTA BugBuilder相比，我们展示了修复错误位置识别的有效性和效率的提高，以及引发错误的位置识别的高召回率和精度。我们还将基于学习的错误分类方法与传统的基线方法进行了比较，表明在macro-F1下，分类有效性提高了约17%。,bug数据集，bug修复，bug诱导，自动bug挖掘,,,
XILT3XI7,2023,https://doi.org/10.1109/ASE56229.2023.00115,ASE 2023,Green AI Quotient: Assessing Greenness of AI-based software and the way forward,"As the world takes cognizance of AI's growing role in greenhouse gas(GHG) and carbon emissions, the focus of AI research & development is shifting towards inclusion of energy efficiency as another core metric. Sustainability, a core agenda for most organizations, is also being viewed as a core non-functional requirement in software engineering. A similar effort is being undertaken to extend sustainability principles to AI-based systems with focus on energy efficient training and inference techniques. But an important question arises, does there even exist any metrics or methods which can quantify adoption of “green” practices in the life cycle of AI-based systems? There is a huge gap which exists between the growing research corpus related to sustainable practices in AI research and its adoption at an industry scale. The goal of this work is to introduce a methodology and novel metric for assessing “greenness” of any AI-based system and its development process, based on energy efficient AI research and practices. The novel metric, termed as Green AI Quotient, would be a key step towards AI practitioner's Green AI journey. Empirical validation of our approach suggest that Green AI Quotient is able to encourage adoption and raise awareness regarding sustainable practices in AI lifecycle.","artificial intelligence,deep learning,sustainability,green AI,carbon emissions",绿色人工智能商：评估基于人工智能的软件的绿色性和前进的道路,随着世界认识到人工智能在温室气体和碳排放中日益重要的作用，人工智能研发的重点正在转向将能源效率作为另一个核心指标。可持续性是大多数组织的核心议程，也被视为软件工程中的核心非功能需求。正在进行类似的努力，将可持续性原则扩展到基于人工智能的系统，重点是节能训练和推理技术。但一个重要的问题出现了，是否存在任何指标或方法可以量化基于人工智能的系统生命周期中“绿色”实践的采用？人工智能研究中与可持续实践相关的研究语料库不断增长，与行业规模的采用之间存在巨大差距。这项工作的目标是在节能人工智能研究和实践的基础上，引入一种评估任何基于人工智能的系统及其开发过程的“绿色性”的方法和新指标。这个被称为“绿色人工智能商”的新指标将是人工智能从业者绿色人工智能之旅的关键一步。对我们方法的实证验证表明，绿色人工智能商能够鼓励采用并提高人们对人工智能生命周期中可持续实践的认识。,人工智能，深度学习，可持续发展，绿色人工智能，碳排放,,,
E83X9K5V,2023,https://doi.org/10.1109/ASE56229.2023.00094,ASE 2023,AutoConf: Automated Configuration of Unsupervised Learning Systems Using Metamorphic Testing and Bayesian Optimization,"Unsupervised learning systems using clustering have gained significant attention for numerous applications due to their unique ability to discover patterns and structures in large unlabeled datasets. However, their effectiveness highly depends on their configuration, which requires domain-specific expertise and often involves numerous manual trials. Specifically, selecting appropriate algorithms and hyperparameters adds to the complexity of the configuration process. In this paper, we propose, apply, and assess an automated approach (AutoConf) for configuring unsupervised learning systems using clustering, leveraging metamorphic testing and Bayesian optimization. Metamorphic testing is utilized to verify the configurations of unsupervised learning systems by applying a series of input transformations. We use Bayesian optimization guided by metamorphic-testing output to automatically identify the optimal configuration. The approach aims to streamline the configuration process and enhance the effectiveness of unsupervised learning systems. It has been evaluated through experiments on six datasets from three domains for anomaly detection. The evaluation results show that our approach can find configurations outperforming the baseline approaches as they achieved a recall of 0.89 and a precision of 0.84 (on average).","Measurement,Clustering algorithms,Manuals,Bayes methods,Complexity theory,Unsupervised learning,Optimization",AutoConf:使用变形测试和贝叶斯优化的无监督学习系统的自动配置,使用聚类的无监督学习系统由于其在大型未标记数据集中发现模式和结构的独特能力，在许多应用中获得了极大的关注。然而，它们的有效性在很大程度上取决于它们的配置，这需要特定领域的专业知识，并且通常需要大量的手动试验。具体而言，选择合适的算法和超参数增加了配置过程的复杂性。在本文中，我们提出、应用并评估了一种自动方法（AutoConf），用于使用聚类、利用变形测试和贝叶斯优化来配置无监督学习系统。变形测试用于通过应用一系列输入变换来验证无监督学习系统的配置。我们使用以变形测试输出为指导的贝叶斯优化来自动识别最优配置。该方法旨在简化配置过程，提高无监督学习系统的有效性。它已经通过在三个领域的六个数据集上的实验进行了评估，用于异常检测。评估结果表明，我们的方法可以找到优于基线方法的配置，因为它们实现了0.89的召回率和0.84的精度（平均值）。,测量，聚类算法，手册，贝叶斯方法，复杂性理论，无监督学习，优化,,,
B8HNHS97,2023,https://doi.org/10.1109/ASE56229.2023.00122,ASE 2023,Information Retrieval-Based Fault Localization for Concurrent Programs,"Information retrieval-based fault localization (IRFL) techniques have been proposed as a solution to identify the files that are likely to contain faults that are root causes of failures reported by users. These techniques have been extensively studied to accurately rank source files, however, none of the existing approaches have focused on the specific case of concurrent programs. This is a critical issue since concurrency bugs are notoriously difficult to identify. To address this problem, this paper presents a novel approach called BLCoiR, which aims to reformulate bug report queries to more accurately localize source files related to concurrency bugs. The key idea of BLCoiR is based on a novel knowledge graph (KG), which represents the domain entities extracted from the concurrency bug reports and their semantic relations. The KG is then transformed into the IR query to perform fault localization. BLCoiR leverages natural language processing (NLP) and concept modeling techniques to construct the knowledge graph. Specifically, NLP techniques are used to extract relevant entities from the bug reports, such as the word entities related to concurrency constructs. These entities are then linked together based on their semantic relationships, forming the KG. We have conducted an empirical study on 692 concurrency bug reports from 44 real-world applications. The results show that BLCoiR outperforms existing IRFL techniques in terms of accuracy and efficiency in localizing concurrency bugs. BLCoiR demonstrates effectiveness of using a knowledge graph to model the domain entities and their relationships, providing a promising direction for future research in this area.","Concurrent program,fault localization,information retrieval",基于信息检索的并发程序故障定位,基于信息检索的故障定位（IRFL）技术已被提出作为识别可能包含故障的文件的解决方案，这些故障是用户报告的故障的根本原因。这些技术已经被广泛研究，以准确地对源文件进行排名，然而，现有的方法都没有关注并发程序的具体情况。这是一个关键问题，因为众所周知，并发错误很难识别。为了解决这个问题，本文提出了一种称为BLCoiR的新方法，旨在重新制定错误报告查询，以更准确地定位与并发错误相关的源文件。BLCoiR的核心思想是基于一种新的知识图（KG），它表示从并发错误报告中提取的域实体及其语义关系。然后将KG转换为IR查询以执行故障定位。BLCoiR利用自然语言处理（NLP）和概念建模技术来构建知识图。具体来说，NLP技术用于从错误报告中提取相关实体，例如与并发结构相关的单词实体。然后，这些实体根据它们的语义关系连接在一起，形成KG。我们对来自44个真实世界应用程序的692个并发错误报告进行了实证研究。结果表明，BLCoiR在定位并发错误方面的准确性和效率优于现有的IRFL技术。BLCoiR证明了使用知识图对领域实体及其关系进行建模的有效性，为该领域的未来研究提供了一个有希望的方向。,并行程序，故障定位，信息检索,,,
FE8EALV4,2023,https://doi.org/10.1109/ASE56229.2023.00212,ASE 2023,Symbolic Fixpoint Algorithms for Logical LTL Games,"Two-player games are a fruitful way to represent and reason about several important synthesis tasks. These tasks include controller synthesis (where one asks for a controller for a given plant such that the controlled plant satisfies a given temporal specification), program repair (setting values of variables to avoid exceptions), and synchronization synthesis (adding lock/unlock statements in multi-threaded programs to satisfy safety assertions). In all these applications, a solution directly corresponds to a winning strategy for one of the players in the induced game. In turn, logically-specified games offer a powerful way to model these tasks for large or infinite-state systems. Much of the techniques proposed for solving such games typically rely on abstraction-refinement or template-based solutions. In this paper, we show how to apply classical fixpoint algorithms, that have hitherto been used in explicit, finite-state, settings, to a symbolic logical setting. We implement our techniques in a tool called GENSys-LTL and show that they are not only effective in synthesizing valid controllers for a variety of challenging benchmarks from the literature, but often compute maximal winning regions and maximally-permissive controllers. We achieve 46.38X speed-up over the state of the art and also scale well for non-trivial LTL specifications.","reactive synthesis,symbolic algorithms,program synthesis,program repair,two-player games",逻辑LTL对策的符号不动点算法,双人游戏是表示和推理几个重要综合任务的一种富有成效的方式。这些任务包括控制器合成（要求给定工厂的控制器，使受控工厂满足给定的时间规范）、程序修复（设置变量值以避免异常）和同步合成（在多线程程序中添加锁定/解锁语句以满足安全断言）。在所有这些应用程序中，解决方案直接对应于诱导游戏中玩家之一的获胜策略。反过来，逻辑指定的游戏为大型或无限状态系统提供了一种强大的建模方法。为解决此类游戏而提出的许多技术通常依赖于抽象细化或基于模板的解决方案。在本文中，我们展示了如何将迄今为止在显式、有限状态设置中使用的经典不动点算法应用于符号逻辑设置。我们在一个名为GENSys-LTL的工具中实现了我们的技术，并表明它们不仅在为文献中的各种具有挑战性的基准合成有效控制器方面是有效的，而且经常计算最大获胜区域和最大允许控制器。在现有技术的基础上，我们实现了46.38X的加速，并且可以很好地扩展到非琐碎的LTL规范。,反应合成，符号算法，程序合成，程序修复，双人游戏,,,
WM6BWTLH,2023,https://doi.org/10.1109/ASE56229.2023.00162,ASE 2023,Polyglot Code Smell Detection for Infrastructure as Code with GLITCH,"This paper presents GLITCH, a new technology-agnostic framework that enables automated polyglot code smell detection for Infrastructure as Code scripts. GLITCH uses an intermediate representation on which different code smell detectors can be defined. It currently supports the detection of nine security smells and nine design & implementation smells in scripts written in Ansible, Chef, Docker, Puppet, or Terraform. Studies conducted with GLITCH not only show that GLITCH can reduce the effort of writing code smell analyses for multiple IaC technologies, but also that it has higher precision and recall than current state-of-the-art tools. A video describing and demonstrating GLITCH is available at: https://youtu.be/E4RhCcZjWbk.","devops,infrastructure as code,code smells,security smells,design smells,implementation smells,Ansible,Chef,Docker,Puppet,Terraform,intermediate model,static analysis",使用GLITCH对基础设施进行Polyglot代码气味检测,本文介绍了GLITCH，这是一个新的技术无关框架，它可以实现基础设施作为代码脚本的自动多语言代码气味检测。GLITCH使用一个中间表示，在该中间表示上可以定义不同的代码气味检测器。目前，它支持在用Ansible、Chef、Docker、Puppet或Terraform编写的脚本中检测九种安全气味和九种设计与实现气味。使用GLITCH进行的研究不仅表明，GLITCH可以减少为多种IaC技术编写代码气味分析的工作量，而且它比当前最先进的工具具有更高的精度和召回率。描述和演示GLITCH的视频可在以下网址获取：https://youtu.be/E4RhCcZjWbk.,devops，作为代码的基础设施，代码气味，安全气味，设计气味，实现气味，Ansible，Chef，Docker，Puppet，Terraform，中间模型，静态分析,,,
VN482NI5,2023,https://doi.org/10.1109/ASE56229.2023.00143,ASE 2023,From Misuse to Mastery: Enhancing Code Generation with Knowledge-Driven AI Chaining,"Large Language Models (LLMs) have shown promising results in automatic code generation by improving coding efficiency to a certain extent. However, generating high-quality and reliable code remains a formidable task because of LLMs' lack of good programming practice, especially in exception handling. In this paper, we first conduct an empirical study and summarize three crucial challenges of LLMs in exception handling, i.e., incomplete exception handling, incorrect exception handling and abuse of try-catch. We then try prompts with different granularities to address such challenges, finding fine-grained knowledge-driven prompts works best. Based on our empirical study, we propose a novel Knowledge-driven Prompt Chaining-based code generation approach, name KPC, which decomposes code generation into an AI chain with iterative check-rewrite steps and chains fine-grained knowledge-driven prompts to assist LLMs in considering exception-handling specifications. We evaluate our KPC-based approach with 3,079 code generation tasks extracted from the Java official API documentation. Extensive experimental results demonstrate that the KPC-based approach has considerable potential to ameliorate the quality of code generated by LLMs. It achieves this through proficiently managing exceptions and obtaining remarkable enhancements of 109.86% and 578.57% with static evaluation methods, as well as a reduction of 18 runtime bugs in the sampled dataset with dynamic validation.","Large Language Model,Code Generation,Knowledge-driven Prompt,API Misuse",从误用到精通：用知识驱动的AI链增强代码生成,大型语言模型（LLM）在一定程度上提高了编码效率，在代码自动生成方面显示出了良好的效果。然而，生成高质量和可靠的代码仍然是一项艰巨的任务，因为LLM缺乏良好的编程实践，尤其是在异常处理方面。在本文中，我们首先进行了实证研究，总结了LLM在异常处理中面临的三个关键挑战，即不完整的异常处理、不正确的异常处理和滥用try-catch。然后，我们尝试使用不同粒度的提示来应对这些挑战，发现细粒度的知识驱动提示效果最好。基于我们的实证研究，我们提出了一种新的基于知识驱动的提示链的代码生成方法，称为KPC，该方法将代码生成分解为具有迭代检查重写步骤的AI链，并链接细粒度的知识驱动提示，以帮助LLM考虑异常处理规范。我们使用从Java官方API文档中提取的3079个代码生成任务来评估我们基于KPC的方法。大量的实验结果表明，基于KPC的方法在改善LLM生成的代码质量方面具有相当大的潜力。它通过熟练管理异常实现了这一点，并通过静态评估方法获得了109.86%和578.57%的显著增强，以及通过动态验证减少了采样数据集中的18个运行时错误。,大型语言模型，代码生成，知识驱动提示，API误用,,,
WQFQTMHK,2023,https://doi.org/10.1109/ASE56229.2023.00019,ASE 2023,Smart Prompt Advisor: Multi-Objective Prompt Framework for Consistency and Best Practices,"Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions. and basic control and data flow are met.","Prompt Engineering,Artificial Intelligence,Deep Learning,LLM,Ontology",智能提示顾问：一致性和最佳实践的多目标提示框架,最近，由数十亿个参数组成的大型语言模型（LLM）取得了突破性进展，能够揭示对各种自然语言处理（NLP）任务的非凡见解。这些模型的性能责任在于输入提示的复杂性和完整性。最小化即兴关键字提示的增强周期变得至关重要，因为它直接影响开发解决方案的上市时间和成本。然而，这个过程不可避免地会在用户的学习曲线/熟练程度和提示的完整性之间进行权衡，因为生成这样的解决方案是一个渐进的过程。在本文中，我们设计了一个新颖的解决方案，并以Visual Studio Code IDE插件的形式实现了它，它可以通过学习使用关键字增强的潜在提示意图来优化这种权衡。在开发安全代码时，这将倾向于与开发人员的语义集合保持一致，确保参数和局部变量名称、返回表达式、简单的前置和后置条件。并满足基本控制和数据流。,Prompt Engineering，人工智能，深度学习，LLM，本体论,,,
P2J8JX3T,2023,https://doi.org/10.1109/ASE56229.2023.00031,ASE 2023,Generative Type Inference for Python,"Python is a popular dynamic programming language, evidenced by its ranking as the second most commonly used language on GitHub. However, its dynamic type system can lead to potential type errors, leading researchers to explore automatic type inference approaches for Python programs. Existing type inference approaches can be generally grouped into three categories, i.e., rule-based, supervised, and cloze- style approaches. The rule-based type inference approaches can ensure the accuracy of predicted variable types, but they suffer from low coverage problems caused by dynamic features and external calls. Supervised type inference approaches, while feature-agnostic and able to mitigate the low coverage problem, require large, high- quality annotated datasets and are limited to pre-defined types. As zero-shot approaches, the cloze-style approaches reformulate the type inference problem into a fill-in-the-blank problem by leveraging the general knowledge in powerful pre-trained code models. However, their performance is limited since they ignore the domain knowledge from static typing rules which reflect the inference logic. What is more, their predictions are not interpretable, hindering developers' understanding and verification of the results. This paper introduces Typegen, a few-shot generative type inference approach that incorporates static domain knowledge from static analysis. Typegen creates chain-of-thought (COT) prompts by translating the type inference steps of static analysis into prompts based on the type dependency graphs (TDGs), enabling language models to learn from how static analysis infers types. By combining COT prompts with code slices and type hints, TypegEnconstructs example prompts from human annotations. Typeg Enonly requires very few annotated examples to teach language models to generate similar COT prompts via in-context learning. Moreover, Typeg Enenhances the interpretability of results through the use of the input- explanation-output strategy, which generates both explanations and type predictions in COT prompts. Experiments show that Typegen outperforms the best baseline Type4Py by 10.0% for argument type prediction and 22.5 % in return value type prediction in terms of top-l Exact Match by using only five examples. Furthermore, Typeg Enachieves substantial improvements of 27 % to 84 % compared to the zero-shot performance of large language models with parameter sizes ranging from 1.3B to 175B in terms of top-I Exact Match.","type inference,chain-of-thought,generative model",Python的生成类型推理,Python是一种流行的动态编程语言，在GitHub上排名第二。然而，它的动态类型系统可能会导致潜在的类型错误，导致研究人员探索Python程序的自动类型推理方法。现有的类型推理方法通常可以分为三类，即基于规则的、监督的和完形填空式的方法。基于规则的类型推理方法可以确保预测变量类型的准确性，但它们存在由动态特征和外部调用引起的低覆盖率问题。监督类型推理方法虽然与特征无关，并且能够缓解低覆盖率问题，但需要大的、高质量的注释数据集，并且仅限于预定义的类型。正如零样本方法一样，完形式方法通过利用强大的预先训练的代码模型中的一般知识，将类型推理问题重新表述为填充空白问题。然而，它们的性能是有限的，因为它们忽略了静态类型规则中反映推理逻辑的领域知识。更重要的是，他们的预测是不可解释的，阻碍了开发人员对结果的理解和验证。本文介绍了Typegen，这是一种结合了静态分析中的静态领域知识的少镜头生成类型推理方法。Typegen通过将静态分析的类型推理步骤转换为基于类型依赖图（TDG）的提示，创建思想链（COT）提示，使语言模型能够从静态分析如何推断类型中学习。通过将COT提示与代码片和类型提示相结合，TypegEnco从人工注释构建示例提示。Typeg Enonly只需要很少的注释示例来教授语言模型通过上下文学习生成类似的COT提示。此外，Typeg Ene通过使用输入-解释-输出策略来增强结果的可解释性，该策略在COT提示中生成解释和类型预测。实验表明，在top-l精确匹配方面，Typegen在参数类型预测方面优于最佳基线Type4Py 10.0%，在返回值类型预测方面仅使用五个示例就优于最佳基线Type4Py 22.5%。此外，与参数大小从1.3B到175B的大型语言模型的零样本性能相比，Typeg Ena在top-I精确匹配方面实现了27%到84%的显著改进。,类型推理，思维链，生成模型,,,
8ZC5D2G3,2023,https://doi.org/10.1109/ASE56229.2023.00202,ASE 2023,PURLTL: Mining LTL Specification from Imperfect Traces in Testing,"Formal specifications are widely used in software testing approaches, while writing such specifications is a time-consuming job. Recently, a number of methods have been proposed to mine specifications from execution traces, typically in the form of linear temporal logic (LTL). However, existing works have the following disadvantages: (1) ignoring the negative impact of imperfect traces, which come from partial profiling, missing context information, or buggy programs; (2) relying on templates, resulting in limited expressiveness; (3) requesting negative traces, which are usually unavailable in practice. In this paper, we propose PURLTL, which is able to mine arbitrary LTL specifications from imperfect traces. To alleviate the search space explosion and the wrong search bias, we propose a neural-based method to search LTL formulae, which, intuitively, simulates LTL path checking through differentiable parameter operations. To solve the problem of lacking negative traces, we transform the problem into learning from positive and unlabeled samples, by means of data augmentation and applying positive and unlabeled learning to the training process. Experiments show that our approach surpasses the previous start-of-the-art (SOTA) approach by a large margin. Besides, the results suggest that our approach is not only robust with imperfect traces, but also does not rely on formula templates.","specification mining,machine learning",PURLTL：从测试中的不完美痕迹中挖掘LTL规范,形式化规范在软件测试方法中被广泛使用，而编写这样的规范是一项耗时的工作。最近，已经提出了许多方法来从执行跟踪中挖掘规范，通常以线性时间逻辑（LTL）的形式。然而，现有的工作有以下缺点：（1）忽略了不完美痕迹的负面影响，这些痕迹来自于部分评测、上下文信息缺失或程序错误；（2） 依赖模板，导致表达能力有限；（3） 请求负跟踪，这在实践中通常是不可用的。在本文中，我们提出了PURLTL，它能够从不完美的轨迹中挖掘任意的LTL规范。为了缓解搜索空间爆炸和错误搜索偏差，我们提出了一种基于神经网络的LTL公式搜索方法，该方法通过可微参数运算直观地模拟了LTL路径检查。为了解决缺乏负迹的问题，我们通过数据扩充和将正样本和未标记样本的学习应用于训练过程，将该问题转化为从正样本和无标记样本中学习。实验表明，我们的方法在很大程度上超过了以前的技术起点（SOTA）方法。此外，结果表明，我们的方法不仅具有不完美轨迹的鲁棒性，而且不依赖于公式模板。,规范挖掘，机器学习,,,
DXB672NH,2023,https://doi.org/10.1109/ASE56229.2023.00160,ASE 2023,Coding and Debugging by Separating Secret Code Toward Secure Remote Development,"It is a higher priority for organizations to keep their source code secured. When a certain specific code includes a secret such as intellectual property, they need to pay special attention to prevent the secret code from leaking outside. On the other hand, sometimes code leaks comes from acts by inside programmers. This industrial paper proposes a MORDEn (Micro Organized Remote Development Environment) toward preventing code leaks. MORDEn enables programmers capable of coding and debugging by physically separating secret code from their client. We also introduce a showcase that demonstrates the feasibility of MORDEn from a case study project using it.","Remote development environment,secret code,Language Server Protocol (LSP),Debug Adapter Protocol (DAP)",实现安全远程开发的密码分离编码与调试,对于组织来说，保护源代码的安全是一个更高的优先级。当某个特定代码包含知识产权等机密时，他们需要特别注意防止该机密代码泄漏到外部。另一方面，有时代码泄漏来自内部程序员的行为。这篇工业论文提出了一种防止代码泄漏的MORDEn（微组织远程开发环境）。MORDEn使程序员能够通过将秘密代码与客户端物理分离来进行编码和调试。我们还介绍了一个展示，从使用MORDEn的案例研究项目中证明了它的可行性。,远程开发环境，密码，语言服务器协议（LSP），调试适配器协议（DAP）,,,
XAN7REUM,2023,https://doi.org/10.1109/ASE56229.2023.00104,ASE 2023,How to Train Your Neural Bug Detector: Artificial vs Real Bugs,"Real bug fixes found in open source repositories seem to be the perfect source for learning to localize and repair real bugs. Yet, the scale of existing bug fix collections is typically too small for training data-intensive neural approaches. Neural bug detectors are hence almost exclusively trained on artificial bugs, produced by mutating existing source code and thus easily obtainable at large scales. However, neural bug detectors trained on artificial bugs usually underperform when faced with real bugs. To address this shortcoming, we set out to explore the impact of training on real bug fixes at scale. Our systematic study compares neural bug detectors trained on real bug fixes, artificial bugs and mixtures of real and artificial bugs at various dataset scales and with varying training techniques. Based on our insights gained from training on a novel dataset of 33k real bug fixes, we were able to identify a training setting capable of significantly improving the performance of existing neural bug detectors by up to 170% on simple bugs in Python. In addition, our evaluation shows that further gains can be expected by increasing the size of the real bug fix dataset or the code dataset used for generating artificial bugs. To facilitate future research on neural bug detection, we release our real bug fix dataset, trained models and code.","program repair,bug detection,bug fixes,learning to debug",如何训练你的神经错误检测器：人工错误与真实错误,在开源存储库中找到的真正的bug修复似乎是学习本地化和修复真正bug的完美来源。然而，现有错误修复集合的规模通常太小，无法训练数据密集型神经方法。因此，神经错误检测器几乎完全针对人工错误进行训练，人工错误是通过变异现有源代码产生的，因此很容易在大规模范围内获得。然而，在人工错误上训练的神经错误检测器在面对真实错误时通常表现不佳。为了解决这个缺点，我们开始探索大规模训练对真正的错误修复的影响。我们的系统研究比较了在不同数据集规模和不同训练技术下，在真实错误修复、人工错误以及真实错误和人工错误的混合物上训练的神经错误检测器。基于我们在一个包含33k个真实错误修复的新数据集上的训练所获得的见解，我们能够确定一个训练设置，该设置能够在Python中的简单错误上将现有神经错误检测器的性能显著提高170%。此外，我们的评估表明，通过增加真实错误修复数据集或用于生成人工错误的代码数据集的大小，可以预期进一步的收益。为了促进未来对神经错误检测的研究，我们发布了真正的错误修复数据集、训练的模型和代码。,程序修复，错误检测，错误修复，学习调试,,,
XEEKVDKG,2023,https://doi.org/10.1109/ASE56229.2023.00193,ASE 2023,CAT-LM Training Language Models on Aligned Code And Tests,"Testing is an integral but often neglected part of the software development process. Classical test generation tools such as EvoSuite generate behavioral test suites by optimizing for coverage, but tend to produce tests that are hard to understand. Language models trained on code can generate code that is highly similar to that written by humans, but current models are trained to generate each file separately, as is standard practice in natural language processing, and thus fail to consider the code-under-test context when producing a test file. In this work, we propose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style language model with 2.7 Billion parameters, trained on a corpus of Python and Java projects. We utilize a novel pretraining signal that explicitly considers the mapping between code and test files when available. We also drastically increase the maximum sequence length of inputs to 8,192 tokens, 4x more than typical code generation models, to ensure that the code context is available to the model when generating test code. We analyze its usefulness for realistic applications, showing that sampling with filtering (e.g., by compilability, coverage) allows it to efficiently produce tests that achieve coverage similar to ones written by developers while resembling their writing style. By utilizing the code context, CAT-LM generates more valid tests than even much larger language models trained with more data (CodeGen 16B and StarCoder) and substantially outperforms a recent test-specific model (TeCo) at test completion. Overall, our work highlights the importance of incorporating software-specific insights when training language models for code and paves the way to more powerful automated test generation.","test generation,test completion,large language models,code-test alignment",基于对齐代码和测试的CAT-LM训练语言模型,测试是软件开发过程中不可或缺但经常被忽视的一部分。EvoSuite等经典测试生成工具通过优化覆盖范围来生成行为测试套件，但往往会生成难以理解的测试。在代码上训练的语言模型可以生成与人类编写的代码高度相似的代码，但当前的模型被训练为单独生成每个文件，这是自然语言处理中的标准做法，因此在生成测试文件时无法考虑测试上下文下的代码。在这项工作中，我们提出了对齐代码和测试语言模型（CAT-LM），这是一个GPT风格的语言模型，具有27亿个参数，在Python和Java项目的语料库上训练。我们使用了一种新的预训练信号，当可用时，该信号明确考虑代码和测试文件之间的映射。我们还将输入的最大序列长度大幅增加到8192个令牌，比典型的代码生成模型多4倍，以确保在生成测试代码时，代码上下文可用于模型。我们分析了它在实际应用中的有用性，表明带过滤的采样（例如，通过可编译性、覆盖率）使它能够有效地生成测试，实现与开发人员编写的测试相似的覆盖率，同时与他们的写作风格相似。通过利用代码上下文，CAT-LM生成的测试比用更多数据训练的更大的语言模型（CodeGen 16B和StarCoder）更有效，并且在测试完成时显著优于最近的测试专用模型（TeCo）。总的来说，我们的工作强调了在为代码训练语言模型时结合软件特定见解的重要性，并为更强大的自动化测试生成铺平了道路。,测试生成，测试完成，大型语言模型，代码测试对齐,,,
3LQQ838S,2023,https://doi.org/10.1109/ASE56229.2023.00117,ASE 2023,MELT: Mining Effective Lightweight Transformations from Pull Requests,"Software developers often struggle to update APIs, leading to manual, time-consuming, and error-prone processes. We introduce Melt, a new approach that generates lightweight API migration rules directly from pull requests in popular library repositories. Our key insight is that pull requests merged into open-source libraries are a rich source of information sufficient to mine API migration rules. By leveraging code examples mined from the library source and automatically generated code examples based on the pull requests, we infer transformation rules in Comby, a language for structural code search and replace. Since inferred rules from single code examples may be too specific, we propose a generalization procedure to make the rules more applicable to client projects. Melt rules are syntax-driven, interpretable, and easily adaptable. Moreover, unlike previous work, our approach enables rule inference to seamlessly integrate into the library workflow, removing the need to wait for client code migrations. We evaluated Melt on pull requests from four popular libraries, successfully mining 461 migration rules from code examples in pull requests and 114 rules from auto-generated code examples. Our generalization procedure increases the number of matches for mined rules by 9×. We applied these rules to client projects and ran their tests, which led to an overall decrease in the number of warnings and fixing some test cases demonstrating MELT's effectiveness in real-world scenarios.","software refactoring,api migration",MELT：从拉取请求中挖掘有效的轻量级转换,软件开发人员经常难以更新API，导致手动、耗时且容易出错的过程。我们介绍了Melt，这是一种新方法，可以直接从流行库存储库中的拉取请求生成轻量级API迁移规则。我们的关键见解是，合并到开源库中的pull请求是一个丰富的信息来源，足以挖掘API迁移规则。通过利用从库源中挖掘的代码示例和基于拉取请求自动生成的代码示例，我们推断出Comby中的转换规则，Comby是一种用于结构代码搜索和替换的语言。由于从单个代码示例中推断出的规则可能过于具体，我们提出了一个泛化过程，使规则更适用于客户端项目。Melt规则是语法驱动的、可解释的，并且易于调整。此外，与以前的工作不同，我们的方法使规则推理能够无缝集成到库工作流中，从而无需等待客户端代码迁移。我们对来自四个流行库的拉取请求进行了Melt评估，成功地从拉取请求中的代码示例中挖掘了461条迁移规则，从自动生成的代码实例中挖掘了114条规则。我们的泛化过程将挖掘规则的匹配数量增加了9倍。我们将这些规则应用于客户项目并运行了他们的测试，这导致了警告数量的总体减少，并修复了一些测试案例，证明了MELT在现实场景中的有效性。,软件重构，api迁移,,,
X37TYV2Z,2023,https://doi.org/10.1109/ASE56229.2023.00032,ASE 2023,Vicious Cycles in Distributed Software Systems,"A major threat to distributed software systems' reliability is vicious cycles, which are observed when an event in the distributed software system's execution causes a system degradation, and the degradation, in turn, causes more of such events. Vicious cycles often result in large-scale cloud outages that are hard to recover from due to their self-reinforcing nature. This paper formally defines Vicious Cycle, and conducts the first in-depth study of 33 real-world vicious cycles in 13 widely-used open-source distributed software systems, shedding light on the root causes, triggering conditions, and fixing strategies of vicious cycles, with over a dozen concrete implications to combat them. Our findings show that the majority of the vicious cycles are caused by incorrect error handlers, where the handlers do not obtain enough information to distinguish between 1) an error induced by incoming requests and 2) an error induced by an unexpected interference from another error handler. This paper further performs a feasibility study by 1) building a monitoring tool that prevents one type of vicious cycle by collecting information to make a more informed decision in error handling, and 2) investigating the effectiveness of one commonly suggested practice-injecting exponential backoff-to prevent vicious cycles induced by unconstrained retry.","distributed software systems,vicious cycles",分布式软件系统中的恶性循环,分布式软件系统可靠性的一个主要威胁是恶性循环，当分布式软件系统执行中的事件导致系统降级，而降级反过来又导致更多此类事件时，就会观察到恶性循环。恶性周期通常会导致大规模的云中断，由于其自我强化的性质，这些中断很难恢复。本文正式定义了恶性循环，并首次深入研究了13个广泛使用的开源分布式软件系统中的33个真实世界的恶性循环，揭示了恶性循环的根源、触发条件和修复策略，并提出了十几个具体的对策。我们的研究结果表明，大多数恶性循环是由错误的错误处理程序引起的，其中处理程序没有获得足够的信息来区分1）由传入请求引起的错误和2）由来自另一个错误处理程序的意外干扰引起的错误。本文通过以下方式进一步进行了可行性研究：1）构建一种监测工具，通过收集信息来在错误处理中做出更明智的决定，从而防止一种类型的恶性循环；2）研究一种通常建议的注入指数退避的做法的有效性，以防止无约束重试引发的恶性循环。,分布式软件系统，恶性循环,,,
6IH2ITWA,2023,https://doi.org/10.1109/ASE56229.2023.00204,ASE 2023,Towards a Knowledge Base of Common Sustainability Weaknesses in Green Software Development,"With the climate crisis looming, engineering sustainable software systems become crucial to optimize resource utilization, minimize environmental impact, and foster a greener, more resilient digital ecosystem. For developers, getting access to automated tools that analyze code and suggest sustainability-related optimizations becomes extremely important from a learning and implementation perspective. However, there is currently a dearth of such tools due to the lack of standardized knowledge, which serves as the foundation of these tools. In this paper, we motivate the need for the development of a standard knowledge base of commonly occurring sustainability weaknesses in code, and propose an initial way of doing that. Furthermore, through preliminary experiments, we demonstrate why existing knowledge regarding software weaknesses cannot be re-tagged “as is” to sustainability without significant due diligence, thereby urging further explorations in this ecologically significant domain.","Codes,Knowledge based systems,Green products,Software systems,Resource management,Sustainable development,Standards",绿色软件开发中常见的可持续性弱点的知识库,随着气候危机的临近，设计可持续的软件系统对于优化资源利用、最大限度地减少环境影响和培育更环保、更有弹性的数字生态系统至关重要。对于开发人员来说，从学习和实现的角度来看，获得分析代码和建议可持续性相关优化的自动化工具变得极其重要。然而，由于缺乏作为这些工具基础的标准化知识，目前缺乏此类工具。在本文中，我们提出了开发代码中常见的可持续性弱点的标准知识库的需求，并提出了一种初步的方法。此外，通过初步实验，我们证明了为什么在没有重大尽职调查的情况下，关于软件弱点的现有知识不能“原样”重新标记为可持续性，从而敦促在这一具有生态意义的领域进行进一步探索。,代码，基于知识的系统，绿色产品，软件系统，资源管理，可持续发展，标准,,,
UI7HBIHV,2023,https://doi.org/10.1109/ASE56229.2023.00083,ASE 2023,"Unifying Defect Prediction, Categorization, and Repair by Multi-Task Deep Learning","Just-In- Time defect prediction models can identify defect-inducing commits at check-in time and many approaches are proposed with remarkable performance. However, these approaches still have a few limitations which affect their effectiveness and practical usage: (1) partially using semantic information or structure information of code, (2) coarsely providing results to a commit (buggy or clean), and (3) independently investigating the defect prediction model and defect repair model. In this study, to handle the aforementioned limitations, we propose a unified defect prediction and repair framework named COMPDEFECT,which can identify whether a changed function inside a commit is defect-prone, categorize the type of defect, and repair such a defect automatically if it falls into several scenarios, e.g., defects with single statement fixes, or those that match a small set of defect templates. Technically, the first two tasks in COMPDEFECT are treated as a multiclass classification task, while the last task is treated as a sequence generation task. To verify the effectiveness of COMPDEFECT, we first build a large-scale function-level dataset (i.e., 21,047) named Function-SStuBs4J and then compare COMPDEFECT with tens of state-of-the-art (SOTA) approaches by considering five performance measures. The experimental results indicate that COMPDEFECT outperforms all SOTAs with a substantial improvement in three tasks separately. Moreover, the pipeline experimental results also indicate the feasibility of COMPDEFECT to unify three tasks in a model.","Just-in-time Defect Prediction,Defect Categorization,Defect Repair",通过多任务深度学习统一缺陷预测、分类和修复,实时缺陷预测模型可以在签入时识别导致缺陷的提交，并且提出了许多具有显著性能的方法。然而，这些方法仍然有一些限制，影响了它们的有效性和实际使用：（1）部分使用代码的语义信息或结构信息，（2）粗略地向提交提供结果（bug或clean），以及（3）独立地研究缺陷预测模型和缺陷修复模型。在这项研究中，为了应对上述限制，我们提出了一个名为COMPDEFECT的统一缺陷预测和修复框架，该框架可以识别提交中更改的函数是否容易出现缺陷，对缺陷类型进行分类，并在缺陷属于几种情况时自动修复，例如，单语句修复的缺陷，或者那些匹配一小组缺陷模板的模板。从技术上讲，COMPDEFECT中的前两个任务被视为多类分类任务，而最后一个任务则被视为序列生成任务。为了验证COMPDEFECT的有效性，我们首先构建了一个名为function-SStuBs4J的大规模功能级数据集（即21047），然后通过考虑五个性能指标，将COMPDEFECT与数十种最先进的（SOTA）方法进行比较。实验结果表明，COMPDEFECT的性能优于所有SOTA，在三项任务中分别有了显著的改进。此外，流水线实验结果也表明了COMPDEFECT将三个任务统一在一个模型中的可行性。,实时缺陷预测，缺陷分类，缺陷修复,,,
LL4CNZJJ,2023,https://doi.org/10.1109/ASE56229.2023.00084,ASE 2023,Function-Level Vulnerability Detection Through Fusing Multi-Modal Knowledge,"Software vulnerabilities damage the functionality of software systems. Recently, many deep learning-based approaches have been proposed to detect vulnerabilities at the function level by using one or a few different modalities (e.g., text representation, graph-based representation) of the function and have achieved promising performance. However, some of these existing studies have not completely leveraged these diverse modalities, particularly the underutilized image modality, and the others using images to represent functions for vulnerability detection have not made adequate use of the significant graph structure underlying the images. In this paper, we propose MVulD, a multi-modal-based function-level vulnerability detection approach, which utilizes multi-modal features of the function (i.e., text representation, graph representation, and image representation) to detect vulnerabilities. Specifically, MVulD utilizes a pre-trained model (i.e., UniXcoder) to learn the semantic information of the textual source code, employs the graph neural network to distill graph-based representation, and makes use of computer vision techniques to obtain the image representation while retaining the graph structure of the function. We conducted a large-scale experiment on 25,816 functions. The experimental results show that MVulD improves four state-of-the-art baselines by 30.8%-81.3%, 12.8%-27.4%, 48.8%-115%, and 22.9%-141% in terms of F1-score, Accuracy, Precision, and PR-AUC respectively.","Vulnerability Detection,Computer Vision,Deep Learning,Multi-Modal Code Representations",融合多模态知识的功能级漏洞检测,软件漏洞会破坏软件系统的功能。最近，已经提出了许多基于深度学习的方法，通过使用函数的一种或几种不同模式（例如，文本表示、基于图的表示）来检测函数级别的漏洞，并取得了良好的性能。然而，这些现有研究中的一些并没有完全利用这些不同的模式，特别是未充分利用的图像模式，而其他使用图像来表示漏洞检测功能的研究则没有充分利用图像背后的重要图结构。在本文中，我们提出了MVulD，这是一种基于多模态的函数级漏洞检测方法，它利用函数的多模态特征（即文本表示、图表示和图像表示）来检测漏洞。具体而言，MVulD利用预先训练的模型（即UniXcoder）来学习文本源代码的语义信息，使用图神经网络来提取基于图的表示，并利用计算机视觉技术来获得图像表示，同时保留函数的图结构。我们对25816个函数进行了大规模实验。实验结果表明，MVulD在F1评分、准确度、精密度和PR-AUC方面分别提高了30.8%-81.3%、12.8%-27.4%、48.8%-115%和22.9%-141%。,漏洞检测，计算机视觉，深度学习，多模式代码表示,,,
5PEN22JC,2023,https://doi.org/10.1109/ASE56229.2023.00203,ASE 2023,Software Entity Recognition with Noise-Robust Learning,"Recognizing software entities such as library names from free-form text is essential to enable many software engineering (SE) technologies, such as traceability link recovery, automated documentation, and API recommendation. While many approaches have been proposed to address this problem, they suffer from small entity vocabularies or noisy training data, hindering their ability to recognize software entities mentioned in sophisticated narratives. To address this challenge, we leverage the Wikipedia taxonomy to develop a comprehensive entity lexicon with 79K unique software entities in 12 fine-grained types, as well as a large labeled dataset of over 1.7M sentences. Then, we propose self-regularization, a noise-robust learning approach, to the training of our software entity recognition (SER) model by accounting for many dropouts. Results show that models trained with self-regularization outperform both their vanilla counterparts and state-of-the-art approaches on our Wikipedia benchmark and two Stack Overflow benchmarks. We release our models11https://huggingface.co/taidng/wikiser-bert-base; https.//huggingface.co/taidng/wikiser-bert-large., data, and code for future research.22https://github.com/taidnguyen/software_entity_recognition","Software Entity Recognition,Datasets,Noise- Robust Learning",基于噪声鲁棒学习的软件实体识别,从自由格式文本中识别库名称等软件实体对于启用许多软件工程（SE）技术至关重要，如可追溯性链接恢复、自动化文档和API建议。虽然已经提出了许多方法来解决这个问题，但它们存在小实体词汇表或嘈杂的训练数据，阻碍了它们识别复杂叙述中提到的软件实体的能力。为了应对这一挑战，我们利用维基百科分类法开发了一个全面的实体词典，其中包含12种细粒度类型的79K个独特软件实体，以及一个超过170万句的大型标记数据集。然后，我们提出了自正则化，这是一种噪声鲁棒学习方法，通过考虑许多退出，来训练我们的软件实体识别（SER）模型。结果表明，在我们的维基百科基准测试和两个Stack Overflow基准测试中，用自正则化训练的模型优于普通模型和最先进的方法。我们发布了我们的型号11https://huggingface.co/taidng/wikiser-bert-base；https//huggingface.co/taidng/wikiser-bert-large.，未来研究的数据和代码。22https://github.com/taidnguyen/software_entity_recognition,软件实体识别，数据集，噪声鲁棒学习,,,
USUIRVMD,2023,https://doi.org/10.1109/ASE56229.2023.00185,ASE 2023,Precise Data-Driven Approximation for Program Analysis via Fuzzing,"Program analysis techniques such as abstract interpretation and symbolic execution suffer from imprecision due to over- and underapproximation, which results in false alarms and missed violations. To alleviate this imprecision, we propose a novel data structure, program state probability (PSP), that leverages execution samples to probabilistically approximate reachable program states. The core intuition of this approximation is that the probability of reaching a given state varies greatly, and thus we can considerably increase analysis precision at the cost of a small probability of unsoundness or incompleteness, which is acceptable when analysis targets bug-finding. Specifically, PSP enhances existing analyses by disregarding low-probability states deemed feasible by overapproximation and recognising high-probability states deemed infeasible by underapproximation. We apply PSP in three domains. First, we show that PSP enhances the precision of the Clam abstract interpreter in terms of MCC from 0.09 to 0.27 and F1 score from 0.22 to 0.34. Second, we demonstrate that a symbolic execution search strategy based on PSP that prioritises program states with a higher probability increases the number of found bugs and reduces the number of solver calls compared to state-of-the-art techniques. Third, a program repair patch prioritisation strategy based on PSP reduces the average patch rank by 26%.","Program Analysis,Fuzzing,Symbolic Execution,Program Repair,Abstract Interpretation",模糊程序分析的精确数据驱动逼近,程序分析技术，如抽象解释和符号执行，由于过度和不充分的逼近而导致不精确，这会导致误报和遗漏违规。为了减轻这种不精确性，我们提出了一种新的数据结构，程序状态概率（PSP），它利用执行样本来概率地近似可到达的程序状态。这种近似的核心直觉是，达到给定状态的概率变化很大，因此我们可以以不完整或不完整的小概率为代价来显著提高分析精度，当分析以发现错误为目标时，这是可以接受的。具体而言，PSP通过忽略过近似认为可行的低概率状态和识别过近似认为不可行的高概率状态来增强现有分析。我们在三个领域应用PSP。首先，我们表明PSP将Clam抽象解释器的MCC精度从0.09提高到0.27，F1得分从0.22提高到0.34。其次，我们证明，与最先进的技术相比，基于PSP的符号执行搜索策略以更高的概率对程序状态进行优先级排序，可以增加发现错误的数量，并减少求解器调用的数量。第三，基于PSP的程序修复补丁优先级策略将平均补丁等级降低了26%。,程序分析，模糊化，符号执行，程序修复，抽象解释,,,
VYM8BQ4J,2023,https://doi.org/10.1109/ASE56229.2023.00191,ASE 2023,"To Share, or Not to Share: Exploring Test-Case Reusability in Fork Ecosystems","Code is often reused to facilitate collaborative development, to create software variants, to experiment with new ideas, or to develop new features in isolation. Social-coding platforms, such as GitHub, enable enhanced code reuse with forking, pull requests, and cross-project traceability. With these concepts, forking has become a common strategy to reuse code by creating clones (i.e., forks) of projects. Thereby, forking establishes fork ecosystems of co-existing projects that are similar, but developed in parallel, often with rather sporadic code propagation and synchronization. Consequently, forked projects vary in quality and often involve redundant development efforts. Unfortunately, as we will show, many projects do not benefit from test cases created in other forks, even though those test cases could actually be reused to enhance the quality of other projects. We believe that reusing test cases—in addition to the implementation code—can improve software quality, software maintainability, and coding efficiency in fork ecosystems. While researchers have worked on test-case-reuse techniques, their potential to improve the quality of real fork ecosystems is unknown. To shed light on test-case reusability, we study to what extent test cases can be reused across forked projects. We mined a dataset of test cases from 305 fork ecosystems on GitHub—totaling 1,089 projects—and assessed the potential for reusing these test cases among the forked projects. By performing a manual inspection of the test cases' applicability, by transplanting the test cases, and by analyzing the causes of non-applicability, we contribute an understanding of the benefits (e.g., uncovering bugs) and of the challenges (e.g., automated code transplantation, deciding about applicability) of reusing test cases in fork ecosystems.","test cases,reuse,test propagation,code transplantation,forking,ecosystems",共享还是不共享：探索Fork生态系统中的测试用例可重用性,代码经常被重用，以促进协作开发、创建软件变体、试验新想法或单独开发新功能。GitHub等社交编码平台通过分叉、拉取请求和跨项目可追溯性增强了代码重用。有了这些概念，分叉已经成为一种通过创建项目的克隆（即分叉）来重用代码的常见策略。因此，forking建立了共存项目的fork生态系统，这些项目是相似的，但是并行开发的，通常具有相当零星的代码传播和同步。因此，分叉项目的质量各不相同，并且经常涉及冗余的开发工作。不幸的是，正如我们将要展示的那样，许多项目并没有从其他fork中创建的测试用例中受益，尽管这些测试用例实际上可以重用以提高其他项目的质量。我们相信，除了实现代码之外，重用测试用例可以提高fork生态系统中的软件质量、软件可维护性和编码效率。虽然研究人员已经研究了测试用例重用技术，但它们提高真实分叉生态系统质量的潜力尚不可知。为了阐明测试用例的可重用性，我们研究了在多大程度上可以在分叉项目中重用测试用例。我们从GitHub上的305个分叉生态系统中挖掘了一个测试用例数据集，共1089个项目，并评估了在分叉项目中重用这些测试用例的潜力。通过对测试用例的适用性进行手动检查，通过移植测试用例，并通过分析不适用性的原因，我们有助于理解在fork生态系统中重用测试用例的好处（例如，发现错误）和挑战（例如，自动代码移植，决定适用性）。,测试用例，重用，测试传播，代码移植，分叉，生态系统,,,
5AXWXFIY,2023,https://doi.org/10.1109/ASE56229.2023.00119,ASE 2023,Where to Go Now? Finding Alternatives for Declining Packages in the npm Ecosystem,"Software ecosystems (e.g., npm, PyPI) are the backbone of modern software developments. Developers add new packages to ecosystems every day to solve new problems or provide alternative solutions, causing obsolete packages to decline in their importance to the community. Packages in decline are reused less over time and may become less frequently maintained. Thus, developers usually migrate their dependencies to better alternatives. Replacing packages in decline with better alternatives requires time and effort by developers to identify packages that need to be replaced, find the alternatives, asset migration benefits, and finally, perform the migration. This paper proposes an approach that automatically identifies packages that need to be replaced and finds their alternatives supported with real-world examples of open source projects performing the suggested migrations. At its core, our approach relies on the dependency migration patterns performed in the ecosystem to suggest migrations to other developers. We evaluated our approach on the npm ecosystem and found that 96% of the suggested alternatives are accurate. Furthermore, by surveying expert JavaScript developers, 67% of them indicate that they will use our suggested alternative packages in their future projects.","Dependency Suggestions,Dependency Quality,Package in decline,Dependency,npm,JavaScript",现在去哪里？在npm生态系统中寻找减少包的替代方案,软件生态系统（例如，npm、PyPI）是现代软件开发的支柱。开发人员每天都在生态系统中添加新的软件包，以解决新问题或提供替代解决方案，导致过时的软件包对社区的重要性下降。随着时间的推移，数量减少的程序包重复使用的次数会减少，维护频率也会降低。因此，开发人员通常会将他们的依赖关系迁移到更好的替代方案中。用更好的替代方案替换衰落的包需要开发人员花费时间和精力来确定需要替换的包，找到替代方案、资产迁移的好处，最后执行迁移。本文提出了一种方法，该方法可以自动识别需要替换的包，并通过执行建议迁移的开源项目的真实例子找到支持它们的替代方案。在其核心，我们的方法依赖于在生态系统中执行的依赖性迁移模式来向其他开发人员建议迁移。我们对npm生态系统的方法进行了评估，发现96%的建议替代方案是准确的。此外，通过调查JavaScript专家开发人员，67%的开发人员表示，他们将在未来的项目中使用我们建议的替代包。,依赖性建议，依赖性质量，下降包，依赖性，npm，JavaScript,,,
CWH9WPV3,2023,https://doi.org/10.1109/ASE56229.2023.00018,ASE 2023,Automating Bias Testing of LLMs,"Large Language Models (LLMs) are being quickly integrated in a myriad of software applications. This may introduce a number of biases, such as gender, age or ethnicity, in the behavior of such applications. To face this challenge, we explore the automatic generation of tests suites to assess the potential biases of an LLM. Each test is defined as a prompt used as input to the LLM and a test oracle that analyses the LLM output to detect the presence of biases.","testing,ethics,bias,fairness,large language models",LLM的自动偏差测试,大型语言模型（LLM）正在被快速集成到无数的软件应用程序中。这可能会在此类应用程序的行为中引入一些偏见，如性别、年龄或种族。为了应对这一挑战，我们探索了测试套件的自动生成，以评估LLM的潜在偏差。每个测试都被定义为一个提示，用作LLM的输入，以及一个分析LLM输出以检测偏差存在的测试预言机。,测试，道德，偏见，公平，大型语言模型,,,
QR39DUWC,2023,https://doi.org/10.1109/ASE56229.2023.00200,ASE 2023,Cell2Doc: ML Pipeline for Generating Documentation in Computational Notebooks,"Computational notebooks have become the go-to way for solving data-science problems. While they are designed to combine code and documentation, prior work shows that documentation is largely ignored by the developers because of the manual effort. Automated documentation generation can help, but existing techniques fail to capture algorithmic details and developers often end up editing the generated text to provide more explanation and sub-steps. This paper proposes a novel machine-learning pipeline, Cell2Doc, for code cell documentation in Python data science notebooks. Our approach works by identifying different logical contexts within a code cell, generating documentation for them separately, and finally combining them to arrive at the documentation for the entire code cell. Cell2Doc takes advantage of the capabilities of existing pre-trained language models and improves their efficiency for code cell documentation. We also provide a new benchmark dataset for this task, along with a data-preprocessing pipeline that can be used to create new datasets. We also investigate an appropriate input representation for this task. Our automated evaluation suggests that our best input representation improves the pre-trained model's performance by 2.5x on average. Further, Cell2Doc achieves 1.33x improvement during human evaluation in terms of correctness, informativeness, and readability against the corresponding standalone pretrained model.","Notebooks documentation,developer productivity,software maintenance,code documentation,AI4SW",Cell2Doc:ML管道，用于在计算笔记本中生成文档,计算笔记本已经成为解决数据科学问题的首选方式。虽然它们是为了将代码和文档结合在一起而设计的，但之前的工作表明，由于手动操作，文档在很大程度上被开发人员忽略了。自动生成文档可能会有所帮助，但现有技术无法捕捉算法细节，开发人员最终往往会编辑生成的文本，以提供更多的解释和子步骤。本文提出了一种新的机器学习管道Cell2Doc，用于Python数据科学笔记本中的代码单元文档。我们的方法是识别代码单元中的不同逻辑上下文，分别为它们生成文档，最后将它们组合在一起，以获得整个代码单元的文档。Cell2Doc利用了现有预先训练的语言模型的功能，并提高了它们编写代码单元文档的效率。我们还为此任务提供了一个新的基准数据集，以及一个可用于创建新数据集的数据预处理管道。我们还研究了该任务的适当输入表示。我们的自动化评估表明，我们的最佳输入表示将预训练模型的性能平均提高了2.5倍。此外，与相应的独立预训练模型相比，Cell2Doc在人类评估的正确性、信息性和可读性方面实现了1.33倍的改进。,笔记本文档，开发人员生产力，软件维护，代码文档，AI4SW,,,
3QDTFXSL,2023,https://doi.org/10.1109/ASE56229.2023.00024,ASE 2023,SpecFuzzer: A Tool for Inferring Class Specifications via Grammar-Based Fuzzing,"In object-oriented design, class specifications are primarily used to express properties describing the intended behavior of the class methods and constraints on class' objects. Although the presence of these specifications is important for various software engineering tasks such as test generation, bug finding and automated debugging, developers rarely write them. In this tool demo we present the details of SPEcFuzZER, a tool that aims at alleviating the problem of writing class specifications by using a combination of grammar-based fuzzing, dynamic invariant detection and mutation analysis to auto-maticallyautomatically infer specifications for Java classes. Given a class under analysis, SPEcFuzZER uses (i) a generator of candidate assertions derived from a grammar automatically extracted from the class; (ii) a dynamic invariant detector -Daikon- in order to discard the assertions invalidated by a test suite; and (iii) a mutation-based mechanism to cluster and rank assertions, so that similar constraints are grouped together and the stronger assertions are prioritized. The tool is available on GitHub at https://github.com/facumolina/specfuzzer, and the demo video can be found on YouTube: https://youtu.be/IfakNCbzOUg.","Oracle Problem,Specification Inference,Grammar-based Fuzzing",SpecFuzer：一种通过基于语法的模糊化推断类规范的工具,在面向对象设计中，类规范主要用于表示描述类方法的预期行为和类对象约束的属性。尽管这些规范的存在对于各种软件工程任务（如测试生成、错误发现和自动调试）很重要，但开发人员很少编写它们。在这个工具演示中，我们介绍了SPEcFuzZER的详细信息，该工具旨在通过使用基于语法的模糊、动态不变检测和突变分析的组合来自动推断Java类的规范，从而缓解编写类规范的问题。给定一个正在分析的类，SPEcFuzZER使用（i）从该类自动提取的语法派生的候选断言的生成器；（ii）动态不变检测器Daikon，以便丢弃由测试套件无效的断言；以及（iii）一种基于突变的机制，用于对断言进行聚类和排序，从而将类似的约束组合在一起，并对更强的断言进行优先级排序。该工具可在GitHub上获得，网址为https://github.com/facumolina/specfuzzer，演示视频可以在YouTube上找到：https://youtu.be/IfakNCbzOUg.,Oracle问题，规范推理，基于语法的模糊化,,,
E4RVEQTQ,2023,https://doi.org/10.1109/ICSE48619.2023.00029,ICSE 2023,How Do We Read Formal Claims? Eye-Tracking and the Cognition of Proofs about Algorithms,"Formal methods are used successfully in high-assurance software, but they require rigorous mathematical and logical training that practitioners often lack. As such, integrating formal methods into software has been associated with numerous challenges. While educators have placed emphasis on formalisms in undergraduate theory courses, such courses often struggle with poor student outcomes and satisfaction. In this paper, we present a controlled eye-tracking human study (n = 34) investigating the problem-solving strategies employed by students with different levels of incoming preparation (as assessed by theory coursework taken and pre-screening performance on a proof comprehension task), and how educators can better prepare low-outcome students for the rigorous logical reasoning that is a core part of formal methods in software engineering. Surprisingly, we find that incoming preparation is not a good predictor of student outcomes for formalism comprehension tasks, and that student self-reports are not accurate at identifying factors associated with high outcomes for such tasks. Instead, and importantly, we find that differences in outcomes can be attributed to performance for proofs by induction and recursive algorithms, and that better-performing students exhibit significantly more attention switching behaviors, a result that has several implications for pedagogy in terms of the design of teaching materials. Our results suggest the need for a substantial pedagogical intervention in core theory courses to better align student outcomes with the objectives of mastery and retaining the material, and thus bettering preparing students for high-assurance software engineering.","formalism comprehension,student cognition,eye-tracking,facial behavior analysis,human study",我们如何解读正式索赔？眼动追踪与算法证明的认知,形式化方法在高保证软件中得到了成功的应用，但它们需要严格的数学和逻辑训练，而从业者往往缺乏这些训练。因此，将形式化方法集成到软件中已经带来了许多挑战。虽然教育工作者在本科理论课程中强调形式主义，但这类课程往往难以获得较差的学生成绩和满意度。在本文中，我们提出了一项受控的眼动追踪人类研究（n=34），调查了具有不同入学准备水平的学生所采用的解决问题策略（通过所学理论课程和在证明理解任务中的预筛选表现进行评估），以及教育工作者如何更好地为低成绩学生做好严格逻辑推理的准备，这是软件工程形式化方法的核心部分。令人惊讶的是，我们发现，即将到来的准备并不能很好地预测学生在形式主义理解任务中的结果，而且学生的自我报告在识别与此类任务的高结果相关的因素方面也不准确。相反，更重要的是，我们发现结果的差异可以归因于归纳和递归算法的证明表现，表现更好的学生表现出更多的注意力转换行为，这一结果对教材设计方面的教育学有几点启示。我们的研究结果表明，需要对核心理论课程进行实质性的教学干预，以更好地使学生的成绩与掌握和保留材料的目标相一致，从而更好地为学生学习高保证软件工程做好准备。,形式主义理解，学生认知，眼动追踪，面部行为分析，人类研究,,,
PHDSR3VE,2023,https://doi.org/10.1109/ICSE48619.2023.00149,ICSE 2023,Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models,"Incident management for cloud services is a complex process involving several steps and has a huge impact on both service health and developer productivity. On-call engineers require significant amount of domain knowledge and manual effort for root causing and mitigation of production incidents. Recent advances in artificial intelligence has resulted in state-of-the-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization. In this work, we do the first large-scale study to evaluate the effectiveness of these models for helping engineers root cause and mitigate production incidents. We do a rigorous study at Microsoft, on more than 40,000 incidents and compare several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics. Lastly, our human evaluation with actual incident owners show the efficacy and future potential of using artificial intelligence for resolving cloud incidents.","Incident Management,Service Quality,GPT-3.x,Large Language Models",使用大型语言模型推荐云事件的根本原因和缓解步骤,云服务的事件管理是一个涉及多个步骤的复杂过程，对服务运行状况和开发人员生产力都有巨大影响。随叫随到的工程师需要大量的领域知识和手动工作来解决生产事故的根本原因和缓解措施。人工智能的最新进展产生了最先进的大型语言模型，如GPT-3.x（GPT-3.0和GPT-3.5），这些模型已被用于解决从问答到文本摘要的各种问题。在这项工作中，我们进行了第一次大规模研究，以评估这些模型在帮助工程师从根源上解决和减轻生产事故方面的有效性。我们在微软对40000多起事件进行了严格的研究，并使用语义和词汇指标对零样本、微调和多任务设置中的几种大型语言模型进行了比较。最后，我们对实际事件所有者的人工评估显示了使用人工智能解决云事件的有效性和未来潜力。,事件管理，服务质量，GPT-3.x，大型语言模型,,,
W73PTQLT,2023,https://doi.org/10.1109/ICSE48619.2023.00070,ICSE 2023,Triggers for Reactive Synthesis Specifications,"Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. Two of the main challenges in bringing reactive synthesis to practice are its very high worst-case complexity and the difficulty of writing declarative specifications using basic LTL operators. To address the first challenge, researchers have suggested the GR(1) fragment of LTL, which has an efficient poly-nomial time symbolic synthesis algorithm. To address the second challenge, specification languages include higher-level constructs that aim at allowing engineers to write succinct and readable specifications. One such construct is the triggers operator, as supported, e.g., in the Property Specification Language (PSL). In this work we introduce triggers into specifications for reactive synthesis. The effectiveness of our contribution relies on a novel encoding of regular expressions using symbolic finite automata (SFA) and on a novel semantics for triggers that, in contrast to PSL triggers, admits an efficient translation into GR(1). We show that our triggers are expressive and succinct, and prove that our encoding is optimal. We have implemented our ideas on top of the Spectra language and synthesizer. We demonstrate the usefulness and effectiveness of using triggers in specifications for synthesis, as well as the challenges involved in using them, via a study of more than 300 triggers written by undergraduate students who participated in a project class on writing specifications for synthesis. To the best of our knowledge, our work is the first to introduce triggers into specifications for reactive synthesis.","Reactive synthesis,Formal specifications",反应合成规范的触发器,反应性综合是一种自动过程，通过构建反应性系统的时间逻辑规范来获得正确的反应性系统。将反应式合成付诸实践的两个主要挑战是其在最坏情况下非常高的复杂性和使用基本LTL运算符编写声明性规范的困难。为了应对第一个挑战，研究人员提出了LTL的GR（1）片段，它具有高效的多项式时间符号合成算法。为了解决第二个挑战，规范语言包括更高级别的构造，旨在让工程师编写简洁可读的规范。一个这样的构造是触发器运算符，例如在属性规范语言（PSL）中得到支持。在这项工作中，我们将触发器引入反应合成的规范中。我们的贡献的有效性取决于使用符号有限自动机（SFA）对正则表达式的新编码，以及触发器的新语义，与PSL触发器相比，该语义允许有效地转换为GR（1）。我们展示了我们的触发器是表达和简洁的，并证明了我们的编码是最优的。我们已经在Spectra语言和合成器之上实现了我们的想法。我们通过对300多个触发器的研究，证明了在合成规范中使用触发器的有用性和有效性，以及使用触发器所涉及的挑战，这些触发器是由参加合成规范编写项目班的本科生编写的。据我们所知，我们的工作是第一次将触发器引入反应合成的规范中。,反应合成，正式规范,,,
FWIRSEE8,2023,https://doi.org/10.1109/ICSE48619.2023.00059,ICSE 2023,Fonte: Finding Bug Inducing Commits from Failures,"A Bug Inducing Commit (BIC) is a commit that introduces a software bug into the codebase. Knowing the relevant BIC for a given bug can provide valuable information for debugging as well as bug triaging. However, existing BIC identification techniques are either too expensive (because they require the failing tests to be executed against previous versions for bisection) or inapplicable at the debugging time (because they require post hoc artefacts such as bug reports or bug fixes). We propose Fonte, an efficient and accurate BIC identification technique that only requires test coverage. Fonte combines Fault Localisation (FL) with BIC identification and ranks commits based on the suspiciousness of the code elements that they modified. Fonte reduces the search space of BICs using failure coverage as well as a filter that detects commits that are merely style changes. Our empirical evaluation using 130 real-world BICs shows that Fonte significantly outperforms state-of-the-art BIC identification techniques based on Information Retrieval as well as neural code embedding models, achieving at least 39% higher MRR. We also report that the ranking scores produced by Fonte can be used to perform weighted bisection, further reducing the cost of BIC identification. Finally, we apply Fonte to a large-scale industry project with over 10M lines of code, and show that it can rank the actual BIC within the top five commits for 87% of the studied real batch-testing failures, and save the BIC inspection cost by 32% on average.","Bug Inducing Commit,Fault Localisation,Git,Weighted Bisection,Batch Testing",Fonte：从失败中寻找导致提交的错误,错误诱导提交（BIC）是一种将软件错误引入代码库的提交。了解给定错误的相关BIC可以为调试和错误测试提供有价值的信息。然而，现有的BIC识别技术要么过于昂贵（因为它们要求针对以前的版本执行失败的测试以进行平分），要么在调试时不适用（因为它们需要事后人工制品，如错误报告或错误修复）。我们提出了Fonte，这是一种高效准确的BIC识别技术，只需要测试覆盖范围。Fonte将故障定位（FL）与BIC识别相结合，并根据修改的代码元素的可疑性对提交进行排名。Fonte使用故障覆盖率以及检测仅为样式更改的提交的过滤器来减少BIC的搜索空间。我们使用130个真实世界的BIC进行的实证评估表明，Fonte显著优于基于信息检索和神经代码嵌入模型的最先进的BIC识别技术，实现了至少39%的高MRR。我们还报告了Fonte产生的排名分数可以用于进行加权平分，进一步降低了BIC识别的成本。最后，我们将Fonte应用于一个拥有超过1000万行代码的大型行业项目，并表明它可以将实际的BIC排在所研究的真实批量测试失败的前五名之内，并且平均节省32%的BIC检查成本。,Bug诱导提交，故障定位，Git，加权二分法，批量测试,,,
RYLMWGTW,2023,https://doi.org/10.1109/ICSE48619.2023.00174,ICSE 2023,Testing Database Engines via Query Plan Guidance,"Database systems are widely used to store and query data. Test oracles have been proposed to find logic bugs in such systems, that is, bugs that cause the database system to compute an incorrect result. To realize a fully automated testing approach, such test oracles are paired with a test case generation technique; a test case refers to a database state and a query on which the test oracle can be applied. In this work, we propose the concept of Query Plan Guidance (QPG) for guiding automated testing towards “interesting” test cases. SQL and other query languages are declarative. Thus, to execute a query, the database system translates every operator in the source language to one of the potentially many so-called physical operators that can be executed; the tree of physical operators is referred to as the query plan. Our intuition is that by steering testing towards exploring a variety of unique query plans, we also explore more interesting behaviors-some of which are potentially incorrect. To this end, we propose a mutation technique that gradually applies promising mutations to the database state, causing the DBMS to create potentially unseen query plans for subsequent queries. We applied our method to three mature, widely-used, and extensively-tested database systems-SQLite, TiDB, and CockroachDB-and found 53 unique, previously unknown bugs. Our method exercises $4.85-408.48\times$ more unique query plans than a naive random generation method and $7.46\times$ more than a code coverage guidance method. Since most database systems-including commercial ones-expose query plans to the user, we consider QPG a generally applicable, black-box approach and believe that the core idea could also be applied in other contexts (e.g., to measure the quality of a test suite).","automated testing,test case generation",通过查询计划指南测试数据库引擎,数据库系统被广泛用于存储和查询数据。已经提出了测试预言机来查找此类系统中的逻辑错误，即导致数据库系统计算错误结果的错误。为了实现完全自动化的测试方法，这种测试预言机与测试用例生成技术相结合；测试用例指的是一个数据库状态和一个可以应用测试oracle的查询。在这项工作中，我们提出了查询计划指导（QPG）的概念，用于指导自动化测试走向“有趣”的测试用例。SQL和其他查询语言是声明性的。因此，为了执行查询，数据库系统将源语言中的每个运算符翻译成可能被执行的许多所谓物理运算符中的一个；物理运算符的树被称为查询计划。我们的直觉是，通过引导测试探索各种独特的查询计划，我们还探索了更有趣的行为，其中一些行为可能是不正确的。为此，我们提出了一种突变技术，该技术将有希望的突变逐渐应用于数据库状态，使DBMS为后续查询创建潜在的看不见的查询计划。我们将我们的方法应用于三个成熟的、广泛使用的、经过广泛测试的数据库系统SQLite、TiDB和CockratchDB，发现了53个以前未知的独特错误。我们的方法比简单的随机生成方法多执行4.85-408.48\times$的唯一查询计划，比代码覆盖率指导方法多执行7.46\times$。由于包括商业数据库系统在内的大多数数据库系统都向用户公开查询计划，我们认为QPG是一种普遍适用的黑盒方法，并认为核心思想也可以应用于其他环境（例如，测量测试套件的质量）。,自动化测试，测试用例生成,,,
2QK3BZTI,2023,https://doi.org/10.1109/ICSE48619.2023.00057,ICSE 2023,Responsibility in Context: On Applicability of Slicing in Semantic Regression Analysis,"Numerous program slicing approaches aim to help developers troubleshoot regression failures - one of the most time-consuming development tasks. The main idea behind these approaches is to identify a subset of interdependent program statements relevant to the failure, minimizing the amount of code developers need to inspect. Accuracy and reduction rate achieved by slicing are the key considerations toward their applicability in practice: inspecting only the statements in a slice should be faster and more efficient than inspecting the code in full. In this paper, we report on our experiment applying one of the most recent and accurate slicing approaches, dual slicing, to the task of troubleshooting regression failures. As subjects, we use projects from the popular Defects4J benchmark and a systematically-collected set of eight large, open-source client-library project pairs with at least one library upgrade failure, which we refer to as LibRench. The results of our experiments show that the produced slices, while effective in reducing the scope of manual inspection, are still very large to be comfortably analyzed by a human. When inspecting these slices, we observe that most statements in a slice deal with the propagation of information between changed code blocks; these statements are essential for obtaining the necessary context for the changes but are not responsible for the failure directly. Motivated by this insight, we propose a novel approach, implemented in a tool named INPRESS, for further reducing the size of a slice by accurately identifying and summarizing the propagation-related code blocks. Our evaluation of INPRESS shows that it is able to produce slices that are 76% shorter than the original ones (207 vs. 2,007 execution statements, on average), thus, reducing the amount of information developers need to inspect without losing the necessary contextual information.","Program slicing,slice minimization,regression failures,case study",语境中的责任：语义回归分析中切片的适用性,许多程序切片方法旨在帮助开发人员解决回归故障，这是最耗时的开发任务之一。这些方法背后的主要思想是识别与故障相关的相互依赖的程序语句的子集，从而最大限度地减少开发人员需要检查的代码量。切片实现的准确性和减少率是它们在实践中适用性的关键考虑因素：只检查切片中的语句应该比完整检查代码更快、更高效。在本文中，我们报告了我们的实验，将最新的精确切片方法之一，双切片应用于故障排除。作为主题，我们使用了流行的Defects4J基准测试中的项目，以及一组系统收集的八个大型开源客户端库项目对，其中至少有一个库升级失败，我们称之为LibRench。我们的实验结果表明，所产生的切片虽然有效地减少了手动检查的范围，但仍然非常大，无法由人类舒适地进行分析。在检查这些切片时，我们观察到切片中的大多数语句都处理更改后的代码块之间的信息传播；这些语句对于获得必要的更改上下文至关重要，但不直接对失败负责。受此启发，我们提出了一种新的方法，该方法在名为INPRESS的工具中实现，通过准确识别和总结与传播相关的代码块来进一步减小切片的大小。我们对INPRESS的评估表明，它能够生成比原始切片短76%的切片（平均207条与2007条执行语句相比），从而减少了开发人员需要检查的信息量，而不会丢失必要的上下文信息。,程序切片，切片最小化，回归失败，案例研究,,,
NGQZVF8Z,2023,https://doi.org/10.1109/ICSE48619.2023.00162,ICSE 2023,Evidence Profiles for Validity Threats in Program Comprehension Experiments,"Searching for clues, gathering evidence, and reviewing case files are all techniques used by criminal investigators to draw sound conclusions and avoid wrongful convictions. Medicine, too, has a long tradition of evidence-based practice, in which administering a treatment without evidence of its efficacy is considered malpractice. Similarly, in software engineering (SE) research, we can develop sound methodologies and mitigate threats to validity by basing study design decisions on evidence. Echoing a recent call for the empirical evaluation of design decisions in program comprehension experiments, we conducted a 2-phases study consisting of systematic literature searches, snowballing, and thematic synthesis. We found out (1) which validity threat categories are most often discussed in primary studies of code comprehension, and we collected evidence to build (2) the evidence profiles for the three most commonly reported threats to validity. We discovered that few mentions of validity threats in primary studies (31 of 409) included a reference to supporting evidence. For the three most commonly mentioned threats, namely the influence of programming experience, program length, and the selected comprehension measures, almost all cited studies (17 of 18) did not meet our criteria for evidence. We show that for many threats to validity that are currently assumed to be influential across all studies, their actual impact may depend on the design and context of each specific study. Researchers should discuss threats to validity within the context of their particular study and support their discussions with evidence. The present paper can be one resource for evidence, and we call for more meta-studies of this type to be conducted, which will then inform design decisions in primary studies. Further, although we have applied our methodology in the context of program comprehension, our approach can also be used in other SE research areas to enable evidence-based experiment design decisions and meaningful discussions of threats to validity.","program comprehension,threats to validity,empirical software engineering",程序理解实验中有效性威胁的证据分析,寻找线索、收集证据和审查案件档案都是刑事调查人员用来得出合理结论和避免错误定罪的技巧。医学也有着悠久的循证实践传统，在没有疗效证据的情况下进行治疗被视为渎职。同样，在软件工程（SE）研究中，我们可以通过基于证据的研究设计决策来开发合理的方法并减轻对有效性的威胁。为了回应最近在程序理解实验中对设计决策进行实证评估的呼吁，我们进行了一项分两个阶段的研究，包括系统的文献搜索、滚雪球和主题综合。我们发现（1）在代码理解的初步研究中，哪些有效性威胁类别最常被讨论，我们收集了证据来建立（2）三种最常见的有效性威胁的证据档案。我们发现，在初步研究中，很少提到有效性威胁（409项研究中有31项）包括对支持证据的提及。对于三个最常提到的威胁，即编程经验的影响、程序长度和选定的理解措施，几乎所有引用的研究（18项中的17项）都不符合我们的证据标准。我们表明，对于目前被认为对所有研究都有影响的许多有效性威胁，其实际影响可能取决于每项具体研究的设计和背景。研究人员应在特定研究的背景下讨论对有效性的威胁，并用证据支持他们的讨论。本文可以作为证据来源，我们呼吁进行更多此类元研究，从而为初步研究的设计决策提供信息。此外，尽管我们已经将我们的方法应用于程序理解，但我们的方法也可以用于其他SE研究领域，以实现基于证据的实验设计决策和对有效性威胁的有意义的讨论。,程序理解，有效性威胁，经验软件工程,,,
ISQJDL8P,2023,https://doi.org/10.1109/ICSE48619.2023.00141,ICSE 2023,What Challenges Do Developers Face About Checked-in Secrets in Software Artifacts?,"Throughout 2021, GitGuardian's monitoring of public GitHub repositories revealed a two-fold increase in the number of secrets (database credentials, API keys, and other credentials) exposed compared to 2020, accumulating more than six million secrets. To our knowledge, the challenges developers face to avoid checked-in secrets are not yet characterized. The goal of our paper is to aid researchers and tool developers in understanding and prioritizing opportunities for future research and tool automation for mitigating checked-in secrets through an empirical investigation of challenges and solutions related to checked-in secrets. We extract 779 questions related to checked-in secrets on Stack Exchange and apply qualitative analysis to determine the challenges and the solutions posed by others for each of the challenges. We identify 27 challenges and 13 solutions. The four most common challenges, in ranked order, are: (i) store/version of secrets during deployment; (ii) store/version of secrets in source code; (iii) ignore/hide of secrets in source code; and (iv) sanitize VCS history. The three most common solutions, in ranked order, are: (i) move secrets out of source code/version control and use template config file; (ii) secret management in deployment; and (iii) use local environment variables. Our findings indicate that the same solution has been mentioned to mitigate multiple challenges. However, our findings also identify an increasing trend in questions lacking accepted solutions substantiating the need for future research and tool automation on managing secrets.","secrets,credentials,developers,software secret management,challenges,empirical study,stack exchange",关于软件工件中的已签入秘密，开发人员面临哪些挑战？,2021年全年，GitGuardian对GitHub公共存储库的监测显示，与2020年相比，暴露的秘密（数据库凭据、API密钥和其他凭据）数量增加了两倍，累积了600多万个秘密。据我们所知，开发人员在避免签入机密方面面临的挑战尚未确定。我们论文的目标是帮助研究人员和工具开发人员了解并优先考虑未来研究和工具自动化的机会，通过对与检入机密相关的挑战和解决方案的实证调查来减少检入机密。我们在Stack Exchange上提取了779个与签入机密相关的问题，并应用定性分析来确定挑战以及其他人对每个挑战提出的解决方案。我们确定了27个挑战和13个解决方案。按排序，四个最常见的挑战是：（i）在部署期间存储/版本机密；（ii）在源代码中存储/版本机密；（iii）忽视/隐藏源代码中的秘密；以及（iv）净化VCS历史。三种最常见的解决方案，按排名顺序是：（i）将机密从源代码/版本控制中移出，并使用模板配置文件；二部署中的秘密管理；以及（iii）使用局部环境变量。我们的研究结果表明，已经提到了相同的解决方案来缓解多重挑战。然而，我们的研究结果也发现，缺乏公认解决方案的问题呈增加趋势，这证明了未来在管理机密方面进行研究和工具自动化的必要性。,秘密，证书，开发人员，软件秘密管理，挑战，实证研究，堆栈交换,,,
N6CRXR3S,2023,https://doi.org/10.1109/ICSE48619.2023.00096,ICSE 2023,SecBench.js: An Executable Security Benchmark Suite for Server-Side JavaScript,"NPM is the largest software ecosystem in the world, offering millions of free, reusable packages. In recent years, various security threats to packages published on npm have been reported, including vulnerabilities that affect millions of users. To continuously improve techniques for detecting vulnerabilities and mitigating attacks that exploit them, a reusable benchmark of vulnerabilities would be highly desirable. Ideally, such a benchmark should be realistic, come with executable exploits, and include fixes of vulnerabilities. Unfortunately, there currently is no such benchmark, forcing researchers to repeatedly develop their own evaluation datasets and making it difficult to compare techniques with each other. This paper presents SecBench.js,, the first comprehensive benchmark suite of vulnerabilities and executable exploits for npm. The benchmark comprises 600 vulnerabilities, which cover the five most common vulnerability classes for server-side JavaScript. Each vulnerability comes with a payload that exploits the vulnerability and an oracle that validates successful exploitation. SecBench.js, enables various applications, of which we explore three in this paper: (i) cross-checking SecBench.js, against public security advisories reveals 168 vulnerable versions in 19 packages that are mislabeled in the advisories; (ii) applying simple code transformations to the exploits in our suite helps identify flawed fixes of vulnerabilities; (iii) dynamically analyzing calls to common sink APIs, e.g., exec(), yields a ground truth of code locations for evaluating vulnerability detectors. Beyond providing a reusable benchmark to the community, our work identified 20 zero-day vulnerabilities, most of which are already acknowledged by practitioners.","Fault diagnosis,Codes,Benchmark testing,Software,Safety,Security,Public policy",SecBench.js：用于服务器端JavaScript的可执行安全基准套件,NPM是世界上最大的软件生态系统，提供数百万个免费、可重复使用的软件包。近年来，据报道，npm上发布的软件包面临各种安全威胁，包括影响数百万用户的漏洞。为了不断改进检测漏洞和减轻利用漏洞的攻击的技术，非常需要一个可重复使用的漏洞基准。理想情况下，这样的基准应该是现实的，带有可执行的漏洞利用，并包括漏洞修复。不幸的是，目前还没有这样的基准，这迫使研究人员反复开发自己的评估数据集，并使技术之间的比较变得困难。本文介绍了SecBench.js，这是第一个针对npm的漏洞和可执行漏洞利用的综合基准套件。该基准测试包括600个漏洞，涵盖了服务器端JavaScript最常见的五个漏洞类别。每个漏洞都带有一个利用漏洞的有效负载和一个验证成功利用漏洞的预言机。SecBench.js支持各种应用程序，我们在本文中探讨了其中的三个：（i）根据公共安全咨询对SecBench.js进行交叉检查，发现咨询中错误标记的19个包中有168个易受攻击的版本；（ii）将简单的代码转换应用于我们套件中的漏洞利用，有助于识别有缺陷的漏洞修复；（iii）动态分析对通用接收器API的调用，例如exec（），可以得出用于评估漏洞检测器的代码位置的基本事实。除了为社区提供可重复使用的基准之外，我们的工作还确定了20个零日漏洞，其中大多数已经得到了从业者的认可。,故障诊断，代码，基准测试，软件，安全，安保，公共政策,,,
4PMYSAZX,2023,https://doi.org/10.1109/ICSE48619.2023.00134,ICSE 2023,Fairify: Fairness Verification of Neural Networks,"Fairness of machine learning (ML) software has become a major concern in the recent past. Although recent research on testing and improving fairness have demonstrated impact on real-world software, providing fairness guarantee in practice is still lacking. Certification of ML models is challenging because of the complex decision-making process of the models. In this paper, we proposed Fairify, an SMT-based approach to verify individual fairness property in neural network (NN) models. Individual fairness ensures that any two similar individuals get similar treatment irrespective of their protected attributes e.g., race, sex, age. Verifying this fairness property is hard because of the global checking and non-linear computation nodes in NN. We proposed sound approach to make individual fairness verification tractable for the developers. The key idea is that many neurons in the NN always remain inactive when a smaller part of the input domain is considered. So, Fairify leverages white-box access to the models in production and then apply formal analysis based pruning. Our approach adopts input partitioning and then prunes the NN for each partition to provide fairness certification or counterexample. We leveraged interval arithmetic and activation heuristic of the neurons to perform the pruning as necessary. We evaluated Fairify on 25 real-world neural networks collected from four different sources, and demonstrated the effectiveness, scalability and performance over baseline and closely related work. Fairify is also configurable based on the domain and size of the NN. Our novel formulation of the problem can answer targeted verification queries with relaxations and counterexamples, which have practical implications.","fairness,verification,machine learning",Fairify：神经网络的公平性验证,最近，机器学习（ML）软件的公平性已经成为一个主要问题。尽管最近关于测试和提高公平性的研究已经证明了对现实世界软件的影响，但在实践中提供公平性保障仍然缺乏。ML模型的认证具有挑战性，因为模型的决策过程很复杂。在本文中，我们提出了Fairify，这是一种基于SMT的方法来验证神经网络（NN）模型中的个体公平性。个人公平确保任何两个相似的人都能得到相似的待遇，而不考虑他们的受保护属性，例如种族、性别、年龄。由于神经网络中的全局检查和非线性计算节点，很难验证这种公平性。我们提出了一种合理的方法，使开发人员能够处理个人公平性验证。关键思想是，当考虑输入域的较小部分时，神经网络中的许多神经元总是保持不活动状态。因此，Fairify利用白盒访问生产中的模型，然后应用基于形式分析的修剪。我们的方法采用输入分区，然后为每个分区修剪NN，以提供公平性证明或反例。我们利用区间算法和神经元的激活启发式来执行必要的修剪。我们在从四个不同来源收集的25个真实世界的神经网络上评估了Fairify，并展示了其在基线和密切相关工作中的有效性、可扩展性和性能。Fairify还可以根据NN的域和大小进行配置。我们对该问题的新表述可以通过放松和反例来回答有针对性的验证问题，这具有实际意义。,公平，验证，机器学习,,,
2UGEXMIN,2023,https://doi.org/10.1109/ICSE48619.2023.00121,ICSE 2023,Columbus: Android App Testing Through Systematic Callback Exploration,"With the continuous rise in the popularity of Android mobile devices, automated testing of apps has become more important than ever. Android apps are event-driven programs. Unfortunately, generating all possible types of events by interacting with an app's interface is challenging for an automated testing approach. Callback-driven testing eliminates the need for event generation by directly invoking app callbacks. However, existing callback-driven testing techniques assume prior knowledge of Android callbacks, and they rely on a human expert, who is familiar with the Android API, to write stub code that prepares callback arguments before invocation. Since the Android API is very large and keeps evolving, prior techniques could only support a small fraction of callbacks present in the Android framework. In this work, we introduce Columbus, a callback-driven testing technique that employs two strategies to eliminate the need for human involvement: (i) it automatically identifies callbacks by simultaneously analyzing both the Android framework and the app under test; (ii) it uses a combination of under-constrained symbolic execution (primitive arguments), and type-guided dynamic heap introspection (object arguments) to generate valid and effective inputs. Lastly, Columbus integrates two novel feedback mechanisms-data dependency and crash-guidance- during testing to increase the likelihood of triggering crashes and maximizing coverage. In our evaluation, Columbus outperforms state-of-the-art model-driven, checkpoint-based, and callback-driven testing tools both in terms of crashes and coverage.","Android,app testing,callback",Columbus：通过系统回调探索进行Android应用程序测试,随着安卓移动设备的不断普及，应用程序的自动化测试变得比以往任何时候都更加重要。安卓应用程序是事件驱动程序。不幸的是，通过与应用程序的界面交互来生成所有可能类型的事件对于自动化测试方法来说是一项挑战。回调驱动的测试通过直接调用应用程序回调消除了生成事件的需要。然而，现有的回调驱动测试技术假设事先了解Android回调，并且它们依赖于熟悉Android API的人类专家来编写存根代码，该代码在调用之前准备回调参数。由于Android API非常庞大并且不断发展，因此现有技术只能支持Android框架中的一小部分回调。在这项工作中，我们介绍了Columbus，这是一种回调驱动的测试技术，它采用了两种策略来消除对人工参与的需求：（i）它通过同时分析Android框架和测试中的应用程序来自动识别回调；（ii）它使用约束不足的符号执行（基元参数）和类型引导的动态堆内省（对象参数）的组合来生成有效和有效的输入。最后，Columbus集成了两种新的反馈机制——数据依赖性和碰撞指导——在测试期间，以增加触发碰撞的可能性并最大限度地扩大覆盖范围。在我们的评估中，Columbus在崩溃和覆盖率方面都优于最先进的模型驱动、基于检查点和回调驱动的测试工具。,Android，应用程序测试，回调,,,
IE9PDREN,2023,https://doi.org/10.1109/ICSE48619.2023.00163,ICSE 2023,Developers' Visuo-spatial Mental Model and Program Comprehension,"Previous works from research and industry have proposed a spatial representation of code in a canvas, arguing that a navigational code space confers developers the freedom to organise elements according to their understanding. By allowing developers to translate logical relatedness into spatial proximity, this code representation could aid in code navigation and comprehension. However, the association between developers' code comprehension and their visuo-spatial mental model of the code is not yet well understood. This mental model is affected on the one hand by the spatial code representation and on the other by the visuo-spatial working memory of developers. We address this knowledge gap by conducting an online experiment with 20 developers following a between-subject design. The control group used a conventional tab-based code visualization, while the experimental group used a code canvas to complete three code comprehension tasks. Furthermore, we measure the participants' visuo-spatial working memory using a Corsi Block test at the end of the tasks. Our results suggest that, overall, neither the spatial representation of code nor the visuo-spatial working memory of developers has a significant impact on comprehension performance. However, we identified significant differences in the time dedicated to different comprehension activities such as navigation, annotation, and UI interactions.","Code comprehension,code navigation,developer productivity,IDE design,code visualization,cognitive studies",开发者Visuo空间心理模型与程序理解,研究和工业界先前的工作提出了在画布中对代码进行空间表示，认为导航代码空间赋予开发人员根据他们的理解组织元素的自由。通过允许开发人员将逻辑相关性转换为空间接近性，这种代码表示可以帮助代码导航和理解。然而，开发人员的代码理解和他们的代码视觉空间心理模型之间的联系还没有得到很好的理解。这种心理模型一方面受到空间代码表示的影响，另一方面受到开发人员视觉空间工作记忆的影响。我们通过对20名开发人员进行主题间设计的在线实验来解决这一知识差距。对照组使用传统的基于选项卡的代码可视化，而实验组使用代码画布来完成三项代码理解任务。此外，我们在任务结束时使用Corsi块测试来测量参与者的视觉空间工作记忆。我们的研究结果表明，总体而言，无论是代码的空间表示还是开发人员的视觉空间工作记忆，都不会对理解性能产生显著影响。然而，我们发现用于不同理解活动（如导航、注释和UI交互）的时间存在显著差异。,代码理解，代码导航，开发人员生产力，IDE设计，代码可视化，认知研究,,,
RIQUV8PT,2023,https://doi.org/10.1109/ICSE48619.2023.00081,ICSE 2023,When to Say What: Learning to Find Condition-Message Inconsistencies,"Programs often emit natural language messages, e.g., in logging statements or exceptions raised on unexpected paths. To be meaningful to users and developers, the message, i.e., what to say, must be consistent with the condition under which it gets triggered, i.e., when to say it. However, checking for inconsistencies between conditions and messages is challenging because the conditions are expressed in the logic of the programming language, while messages are informally expressed in natural language. This paper presents CMI-Finder, an approach for detecting condition-message inconsistencies. CMI-Finder is based on a neural model that takes a condition and a message as its input and then predicts whether the two are consistent. To address the problem of obtaining realistic, diverse, and large-scale training data, we present six techniques to generate large numbers of inconsistent examples to learn from automatically. Moreover, we describe and compare three neural models, which are based on binary classification, triplet loss, and fine-tuning, respectively. Our evaluation applies the approach to 300K condition-message statements extracted from 42 million lines of Python code. The best model achieves a precision of 78% at a recall of 72% on a dataset of past bug fixes. Applying the approach to the newest versions of popular open-source projects reveals 50 previously unknown bugs, 19 of which have been confirmed by the developers so far.","Codes,Computer bugs,Natural languages,Training data,Predictive models,Data models,Python",何时说什么：学会发现条件消息不一致,程序通常会发出自然语言消息，例如，在日志记录语句或意外路径上引发的异常中。为了对用户和开发人员有意义，消息（即说什么）必须与触发消息的条件（即何时说）一致。然而，检查条件和消息之间的不一致性很有挑战性，因为条件是用编程语言的逻辑表达的，而消息是用自然语言非正式表达的。本文介绍了CMIFinder，一种检测条件消息不一致的方法。CMI Finder基于一个神经模型，该模型以条件和消息为输入，然后预测两者是否一致。为了解决获得真实、多样和大规模训练数据的问题，我们提出了六种技术来生成大量不一致的示例以自动学习。此外，我们描述并比较了三种神经模型，它们分别基于二元分类、三元组丢失和微调。我们的评估将该方法应用于从4200万行Python代码中提取的300K条件消息语句。在过去的错误修复数据集上，最佳模型的精确度达到78%，召回率为72%。将这种方法应用于流行的开源项目的最新版本，会发现50个以前未知的错误，其中19个已被开发人员确认。,代码，计算机错误，自然语言，训练数据，预测模型，数据模型，Python,,,
3IIC8YMD,2023,https://doi.org/10.1109/ICSE48619.2023.00103,ICSE 2023,"""STILL AROUND"": Experiences and Survival Strategies of Veteran Women Software Developers","The intersection of ageism and sexism can create a hostile environment for veteran software developers belonging to marginalized genders. In this study, we conducted 14 interviews to examine the experiences of people at this intersection, primar-ily women, in order to discover the strategies they employed in order to successfully remain in the field. We identified 283 codes, which fell into three main categories: Strategies, Experiences, and Perception. Several strategies we identified, such as (Deliberately) Not Trying to Look Younger, were not previously described in the software engineering literature. We found that, in some compa-nies, older women developers are recognized as having particular value, further strengthening the known benefits of diversity in the workforce. Based on the experiences and strategies, we suggest organizations employing software developers to consider the benefits of hiring veteran women software developers. For example, companies can draw upon the life experiences of older women developers in order to better understand the needs of customers from a similar demographic. While we recognize that many of the strategies employed by our study participants are a response to systemic issues, we still consider that, in the short-term, there is benefit in describing these strategies for developers who are experiencing such issues today.","age,gender,intersectionality,software development,interview study,qualitative research",“依然存在”：资深女性软件开发人员的经验与生存策略,年龄歧视和性别歧视的交叉可能会为属于边缘化性别的资深软件开发人员创造一个充满敌意的环境。在这项研究中，我们进行了14次采访，以调查处于这一交叉点的人（主要是女性）的经历，以发现他们为成功留在该领域所采用的策略。我们确定了283个代码，它们分为三大类：策略、经验和感知。我们确定的几种策略，如（故意）不想看起来更年轻，以前在软件工程文献中没有描述过。我们发现，在一些公司，年长的女性开发人员被认为具有特殊的价值，这进一步加强了劳动力多样性的已知好处。根据经验和策略，我们建议雇佣软件开发人员的组织考虑雇佣资深女性软件开发人员带来的好处。例如，公司可以借鉴年长女性开发人员的生活经验，以便更好地了解来自类似人群的客户的需求。虽然我们认识到，我们的研究参与者所采用的许多策略都是对系统性问题的回应，但我们仍然认为，在短期内，描述这些策略对今天遇到此类问题的开发人员来说是有益的。,年龄，性别，交叉性，软件开发，访谈研究，定性研究,,,
8D49KLG2,2023,https://doi.org/10.1109/ICSE48619.2023.00044,ICSE 2023,Improving Java Deserialization Gadget Chain Mining via Overriding-Guided Object Generation,"Java (de)serialization is prone to causing security-critical vulnerabilities that attackers can invoke existing methods (gadgets) on the application's classpath to construct a gadget chain to perform malicious behaviors. Several techniques have been proposed to statically identify suspicious gadget chains and dynamically generate injection objects for fuzzing. However, due to their incomplete support for dynamic program features (e.g., Java runtime polymorphism) and ineffective injection object generation for fuzzing, the existing techniques are still far from satisfactory. In this paper, we first performed an empirical study to investigate the characteristics of Java deserialization vulnerabilities based on our manually collected 86 publicly known gadget chains. The empirical results show that 1) Java deserialization gadgets are usually exploited by abusing runtime polymorphism, which enables attackers to reuse serializable overridden methods; and 2) attackers usually invoke exploitable overridden methods (gadgets) via dynamic binding to generate injection objects for gadget chain construction. Based on our empirical findings, we propose a novel gadget chain mining approach, GCMiner, which captures both explicit and implicit method calls to identify more gadget chains, and adopts an overriding-guided object generation approach to generate valid injection objects for fuzzing. The evaluation results show that GCMiner significantly outperforms the state-of-the-art techniques, and discovers 56 unique gadget chains that cannot be identified by the baseline approaches.","Java deserialization vulnerability,gadget chain,method overriding,exploit generation",通过重写引导对象生成改进Java反序列化小工具链挖掘,Java（反）序列化容易导致安全关键漏洞，攻击者可以调用应用程序类路径上的现有方法（小工具）来构建小工具链以执行恶意行为。已经提出了几种技术来静态识别可疑的小工具链并动态生成用于模糊化的注入对象。然而，由于它们对动态程序特性（例如Java运行时多态性）的不完全支持以及模糊化的无效注入对象生成，现有技术仍然远远不能令人满意。在本文中，我们首先基于手动收集的86个已知小工具链进行了一项实证研究，以调查Java反序列化漏洞的特征。实证结果表明：1）Java反序列化小工具通常被滥用运行时多态性所利用，这使得攻击者能够重用可序列化的重写方法；以及2）攻击者通常通过动态绑定调用可利用的重写方法（gadget）来生成用于构建gadget链的注入对象。基于我们的经验发现，我们提出了一种新的小工具链挖掘方法GCMiner，它捕获显式和隐式方法调用来识别更多的小工具链，并采用覆盖引导对象生成方法来生成用于模糊化的有效注入对象。评估结果表明，GCMiner显著优于最先进的技术，发现了56个无法通过基线方法识别的独特小工具链。,Java反序列化漏洞，小工具链，方法重写，漏洞生成,,,
U9NGKP2Y,2023,https://doi.org/10.1109/ICSE48619.2023.00138,ICSE 2023,Cross-Domain Requirements Linking via Adversarial-based Domain Adaptation,"Requirements linking is the core of software system maintenance and evolution, and it is critical to assuring software quality. In practice, however, the requirements links are frequently absent or incorrectly labeled, and reconstructing such ties is time-consuming and error-prone. Numerous learning-based approaches have been put forth to address the problem. However, these approaches will lose effectiveness for the Cold-Start projects with few labeled samples. To this end, we propose RADIATION, an adversarial-based domain adaptation approach for cross-domain requirements linking. Generally, RADIATION firstly adopts an IDF-based Masking strategy to filter the domain-specific features. Then it pre-trains a linking model in the source domain with sufficient labeled samples and adapts the model to target domains using a distance-enhanced adversarial technique without using any labeled target samples. Evaluation on five public datasets shows that RADIATION could achieve 66.4% precision, 89.2% recall, and significantly outperform state-of-the-art baselines by 13.4% -42.9% F1. In addition, the designed components, i.e., IDF-based Masking and Distance-enhanced Loss, could significantly improve performance.","Cross-Domain Requirements Linking,Domain Adaptation,Adversarial Learning",通过基于对抗的领域自适应实现跨领域需求链接,需求链接是软件系统维护和演化的核心，是保证软件质量的关键。然而，在实践中，需求链接经常不存在或标记不正确，重建这种联系既耗时又容易出错。已经提出了许多基于学习的方法来解决这个问题。然而，这些方法对于标签样本较少的冷启动项目将失去有效性。为此，我们提出了RADIATION，这是一种用于跨领域需求链接的基于对抗性的领域自适应方法。通常，辐射首先采用基于IDF的掩蔽策略来过滤领域特定特征。然后，它在源域中用足够的标记样本预训练链接模型，并在不使用任何标记目标样本的情况下使用距离增强对抗性技术将模型调整到目标域。对五个公共数据集的评估表明，RADIATION可以实现66.4%的准确率、89.2%的召回率，并显著优于最先进的基线13.4%-42.9%F1。此外，所设计的组件，即基于IDF的掩蔽和距离增强损耗，可以显著提高性能。,跨领域需求链接，领域适应，对抗性学习,,,
WWJIIQ64,2023,https://doi.org/10.1109/ICSE48619.2023.00198,ICSE 2023,Duetcs: Code Style Transfer through Generation and Retrieval,"Coding style has direct impact on code comprehension. Automatically transferring code style to user's preference or consistency can facilitate project cooperation and maintenance, as well as maximize the value of open-source code. Existing work on automating code stylization is either limited to code formatting or requires human supervision in pre-defining style checking and transformation rules. In this paper, we present unsupervised methods to assist automatic code style transfer for arbitrary code styles. The main idea is to leverage Big Code database to learn style and content embedding separately to generate or retrieve a piece of code with the same functionality and the desired target style. We carefully encode style and content features, so that a style embedding can be learned from arbitrary code. We explored the capabilities of novel attention-based style generation models and meta-learning and implemented our ideas in DUETCS. We complement the learning-based approach with a retrieval mode, which uses the same embeddings to directly search for the desired piece of code in Big Code. Our experiments show that DUETCS captures more style aspects than existing baselines.","Symbiosis,Metalearning,Codes,Databases,Instruments,Maintenance engineering,Encoding",Duetcs：通过生成和检索实现代码风格转换,编码风格直接影响代码的理解。自动将代码风格转换为用户的偏好或一致性，可以促进项目合作和维护，并最大限度地提高开源代码的价值。现有的代码风格化自动化工作要么仅限于代码格式，要么需要在预先定义样式检查和转换规则时进行人工监督。在本文中，我们提出了无监督的方法来帮助任意代码样式的自动代码样式转移。其主要思想是利用大代码数据库分别学习样式和内容嵌入，以生成或检索具有相同功能和所需目标样式的代码。我们仔细地对样式和内容特征进行编码，以便可以从任意代码中学习样式嵌入。我们探索了新的基于注意力的风格生成模型和元学习的能力，并在DUETCS中实现了我们的想法。我们用检索模式来补充基于学习的方法，该模式使用相同的嵌入来直接在大代码中搜索所需的代码段。我们的实验表明，DUETCS比现有的基线捕获了更多的风格方面。,共生，元学习，代码，数据库，仪器，维护工程，编码,,,
6GHSZSUE,2023,https://doi.org/10.1109/ICSE48619.2023.00041,ICSE 2023,The Smelly Eight: An Empirical Study on the Prevalence of Code Smells in Quantum Computing,"Quantum Computing (QC) is a fast-growing field that has enhanced the emergence of new programming languages and frameworks. Furthermore, the increased availability of computational resources has also contributed to an influx in the development of quantum programs. Given that classical and QC are significantly different due to the intrinsic nature of quantum programs, several aspects of QC (e.g., performance, bugs) have been investigated, and novel approaches have been proposed. However, from a purely quantum perspective, maintenance, one of the major steps in a software development life-cycle, has not been considered by researchers yet. In this paper, we fill this gap and investigate the prevalence of code smells in quantum programs as an indicator of maintenance issues. We defined eight quantum-specific smells and validated them through a survey with 35 quantum developers. Since no tool specifically aims to detect quantum smells, we developed a tool called QSmell that supports the proposed quantum-specific smells. Finally, we conducted an empirical investigation to analyze the prevalence of quantum-specific smells in 15 open-source quantum programs. Our results showed that 11 programs (73.33%) contain at least one smell and, on average, a program has three smells. Furthermore, the long circuit is the most prevalent smell present in 53.33% of the programs.","Quantum computing,Quantum software engineering,Empirical study,Quantum-specific code smell",气味八：量子计算中代码气味盛行的实证研究,量子计算（QC）是一个快速发展的领域，它促进了新编程语言和框架的出现。此外，计算资源可用性的增加也促进了量子程序开发的涌入。由于量子程序的本质，经典程序和QC程序有很大的不同，因此对QC的几个方面（例如性能、漏洞）进行了研究，并提出了新的方法。然而，从纯粹的量子角度来看，维护是软件开发生命周期中的主要步骤之一，研究人员尚未考虑。在本文中，我们填补了这一空白，并研究了量子程序中代码气味的普遍性，将其作为维护问题的指标。我们定义了八种特定于量子的气味，并通过对35名量子开发者的调查进行了验证。由于没有专门用于检测量子气味的工具，我们开发了一种名为QSmell的工具，支持所提出的量子特定气味。最后，我们进行了一项实证调查，分析了15个开源量子程序中量子特定气味的流行情况。我们的结果显示，11个程序（73.33%）至少包含一种气味，平均而言，一个程序有三种气味。此外，长电路是53.33%的节目中最常见的气味。,量子计算，量子软件工程，实证研究，量子特定代码气味,,,
2N5LK6H5,2023,https://doi.org/10.1109/ICSE48619.2023.00172,ICSE 2023,Compiler Test-Program Generation via Memoized Configuration Search,"To ensure compilers' quality, compiler testing has received more and more attention, and test-program generation is the core task. In recent years, some approaches have been proposed to explore test configurations for generating more effective test programs, but they either are restricted by historical bugs or suffer from the cost-effectiveness issue. Here, we propose a novel test-program generation approach (called MCS) to further improving the performance of compiler testing. MCS conducts memoized search via multi-agent reinforcement learning (RL) for guiding the construction of effective test configurations based on the memoization for the explored test configurations during the on-the-fly compiler-testing process. During the process, the elaborate coordination among configuration options can be also well learned by multi-agent RL, which is required for generating bug-triggering test programs. Specifically, MCS considers the diversity among test configurations to efficiently explore the input space and the testing results under each explored configuration to learn which portions of space are more bug-triggering. Our extensive experiments on GCC and LLVM demonstrate the performance of MCS, significantly outperforming the state-of-the-art test-program generation approaches in bug detection. Also, MCS detects 16 new bugs on the latest trunk revisions of GCC and LLVM, and all of them have been confirmed or fixed by developers. MCS has been deployed by a global IT company (i.e., Huawei) for testing their in-house compiler, and detects 10 new bugs (covering all the 5 bugs detected by the compared approaches), all of which have been confirmed.","Compiler Testing,Test Program Generation,Reinforcement Learning,Configuration",通过Memoized配置搜索生成编译器测试程序,为了保证编译器的质量，编译器测试越来越受到重视，而测试程序的生成是其核心任务。近年来，人们提出了一些方法来探索测试配置，以生成更有效的测试程序，但它们要么受到历史错误的限制，要么受到成本效益问题的影响。在这里，我们提出了一种新的测试程序生成方法（称为MCS），以进一步提高编译器测试的性能。MCS通过多智能体强化学习（RL）进行记忆搜索，以指导在动态编译器测试过程中，基于对所探索的测试配置的记忆来构建有效的测试配置。在这个过程中，多智能体RL也可以很好地学习配置选项之间的精细协调，这是生成bug触发测试程序所必需的。具体而言，MCS考虑测试配置之间的多样性，以有效地探索输入空间，并考虑每个探索配置下的测试结果，以了解空间的哪些部分更容易引发错误。我们在GCC和LLVM上的大量实验证明了MCS的性能，在错误检测方面显著优于最先进的测试程序生成方法。此外，MCS在GCC和LLVM的最新主干修订版上检测到16个新错误，所有这些错误都已被开发人员确认或修复。MCS已由一家全球IT公司（即华为）部署，用于测试其内部编译器，并检测到10个新错误（涵盖了通过比较方法检测到的所有5个错误），所有这些错误都已得到确认。,编译器测试，测试程序生成，强化学习，配置,,,
DFHPAA77,2023,https://doi.org/10.1109/ICSE48619.2023.00120,ICSE 2023,Detecting Dialog-Related Keyboard Navigation Failures in Web Applications,"The ability to navigate the Web via the keyboard interface is critical to people with various types of disabilities. However, modern websites often violate web accessibility guidelines for keyboard navigability with respect to web dialogs. In this paper, we present a novel approach for automatically detecting web accessibility bugs that prevent or hinder keyboard users' ability to navigate dialogs in web pages. An extensive evaluation of our technique on real-world subjects showed that our technique is effective in detecting these dialog-related keyboard navigation failures.","Web Accessibility,WCAG,Software Testing,Keyboard Navigation,Dialog,Keyboard Accessibility,Web Dialog,Accessible Dialog,Dialog Accessibility",检测Web应用程序中与对话框相关的键盘导航故障,通过键盘界面浏览网络的能力对各种类型的残疾人来说至关重要。然而，现代网站经常违反关于网络对话框的键盘导航性的网络可访问性准则。在本文中，我们提出了一种新的方法来自动检测网络可访问性错误，这些错误会阻止或阻碍键盘用户在网页中导航对话框的能力。在现实世界中对我们的技术进行了广泛的评估，结果表明我们的技术在检测这些与对话框相关的键盘导航故障方面是有效的。,Web辅助功能，WCAG，软件测试，键盘导航，对话框，键盘辅助功能，Web对话框，可访问对话框，对话框辅助功能,,,
97GGZZ6Z,2023,https://doi.org/10.1109/ICSE48619.2023.00217,ICSE 2023,Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems,"Autonomous systems rely on a perception component to interpret their surroundings, and when misinterpretations occur, they can and have led to serious and fatal system-level failures. Yet, existing methods for testing perception software remain limited in both their capacity to efficiently generate test data that translates to real-world performance and in their diversity to capture the long tail of rare but safety-critical scenarios. These limitations are particularly evident for perception systems based on LiDAR sensors, which have emerged as a crucial component in modern autonomous systems due to their ability to provide a 3D scan of the world and operate in all lighting conditions. To address these limitations, we introduce a novel approach for testing LiDAR-based perception systems by leveraging existing real-world data as a basis to generate realistic and diverse test cases through mutations that preserve realism invariants while generating inputs rarely found in existing data sets, and automatically crafting oracles that identify potentially safety-critical issues in perception performance. We implemented our approach to assess its ability to identify perception failures, generating over 50,000 test inputs for five state-of-the-art LiDAR-based perception systems. We found that it efficiently generated test cases that yield errors in perception that could result in real consequences if these systems were deployed and does so at a low rate of false positives.","Software Testing and Validation,Machine Learning",为基于激光雷达的感知系统生成真实和多样化的测试,自主系统依赖感知组件来解释其周围环境，当误解发生时，它们可能并已经导致严重和致命的系统级故障。然而，现有的感知软件测试方法在有效生成转化为真实世界性能的测试数据的能力和捕捉罕见但安全关键场景长尾的多样性方面仍然有限。这些限制对于基于激光雷达传感器的感知系统来说尤其明显，由于其能够提供世界的3D扫描并在所有照明条件下运行，激光雷达传感器已成为现代自主系统中的关键组件。为了解决这些局限性，我们引入了一种测试基于激光雷达的感知系统的新方法，通过利用现有的真实世界数据作为基础，通过保持真实性不变量的突变生成真实多样的测试用例，同时生成在现有数据集中罕见的输入，以及自动制作神谕，识别感知性能中潜在的安全关键问题。我们实施了我们的方法来评估其识别感知失败的能力，为五个最先进的基于激光雷达的感知系统生成了50000多个测试输入。我们发现，它有效地生成了测试用例，这些测试用例会产生感知错误，如果部署这些系统，并且误报率很低，则可能会导致实际后果。,软件测试与验证，机器学习,,,
GF2NPYU8,2023,https://doi.org/10.1109/ICSE48619.2023.00182,ICSE 2023,Source Code Recommender Systems: The Practitioners' Perspective,"The automatic generation of source code is one of the long-lasting dreams in software engineering research. Several techniques have been proposed to speed up the writing of new code. For example, code completion techniques can recommend to developers the next few tokens they are likely to type, while retrieval-based approaches can suggest code snippets relevant for the task at hand. Also, deep learning has been used to automatically generate code statements starting from a natural language description. While research in this field is very active, there is no study investigating what the users of code recommender systems (i.e., software practitioners) actually need from these tools. We present a study involving 80 software developers to investigate the characteristics of code recommender systems they consider important. The output of our study is a taxonomy of 70 “requirements” that should be considered when designing code recommender systems. For example, developers would like the recommended code to use the same coding style of the code under development. Also, code recommenders being “aware” of the developers' knowledge (e.g., what are the framework/libraries they already used in the past) and able to customize the recommendations based on this knowledge would be appreciated by practitioners. The taxonomy output of our study points to a wide set of future research directions for code recommenders.","Code Recommender Systems,Empirical Study,Practitioners' Survey",源代码推荐系统：从业者的视角,源代码的自动生成是软件工程研究中长期以来的梦想之一。已经提出了几种技术来加快新代码的编写。例如，代码完成技术可以向开发人员推荐他们可能键入的下几个令牌，而基于检索的方法可以建议与手头任务相关的代码片段。此外，深度学习已被用于从自然语言描述开始自动生成代码语句。虽然这一领域的研究非常活跃，但没有研究调查代码推荐系统的用户（即软件从业者）实际需要从这些工具中得到什么。我们提出了一项涉及80名软件开发人员的研究，以调查他们认为重要的代码推荐系统的特征。我们研究的结果是70个“需求”的分类，在设计代码推荐系统时应该考虑这些需求。例如，开发人员希望推荐的代码使用与正在开发的代码相同的编码风格。此外，代码推荐人“了解”开发人员的知识（例如，他们过去已经使用的框架/库是什么），并能够根据这些知识定制推荐，这将受到从业者的赞赏。我们研究的分类输出为代码推荐器指明了一系列未来的研究方向。,代码推荐系统，实证研究，从业者调查,,,
MAY3G2DP,2023,https://doi.org/10.1109/ICSE48619.2023.00213,ICSE 2023,Automated Black-Box Testing of Mass Assignment Vulnerabilities in RESTful APIs,"Mass assignment is one of the most prominent vulnerabilities in RESTful APIs that originates from a misconfiguration in common web frameworks. This allows attackers to exploit naming convention and automatic binding to craft malicious requests that (massively) override data supposed to be read-only. In this paper, we adopt a black-box testing perspective to automatically detect mass assignment vulnerabilities in RESTful APIs. Indeed, execution scenarios are generated purely based on the OpenAPI specification, that lists the available operations and their message format. Clustering is used to group similar operations and reveal read-only fields, the latter are candidates for mass assignment. Then, test interaction sequences are automatically generated by instantiating abstract testing templates, with the aim of trying to use the found read-only fields to carry out a mass assignment attack. Test interactions are run, and their execution is assessed by a specific oracle, in order to reveal whether the vulnerability could be successfully exploited. The proposed novel approach has been implemented and evaluated on a set of case studies written in different programming languages. The evaluation highlights that the approach is quite effective in detecting seeded vulnerabilities, with a remarkably high accuracy.","REST API,Security testing,Black-box testing,Automated software testing,Mass assignment",RESTful API中大规模分配漏洞的自动黑匣子测试,批量分配是RESTful API中最突出的漏洞之一，源于常见web框架中的错误配置。这使得攻击者能够利用命名约定和自动绑定来制造恶意请求，从而（大规模）覆盖本应为只读的数据。在本文中，我们采用黑箱测试的视角来自动检测RESTful API中的大量分配漏洞。实际上，执行场景完全基于OpenAPI规范生成，该规范列出了可用的操作及其消息格式。聚类用于对类似的操作进行分组，并显示只读字段，后者是批量分配的候选字段。然后，通过实例化抽象测试模板来自动生成测试交互序列，目的是试图使用找到的只读字段来进行大规模分配攻击。运行测试交互，并由特定的oracle评估其执行情况，以揭示是否可以成功利用该漏洞。所提出的新方法已经在一组用不同编程语言编写的案例研究中得到了实现和评估。评估强调，该方法在检测种子漏洞方面非常有效，准确率非常高。,REST API，安全测试，Black-box测试，自动化软件测试，批量分配,,,
YH5RSQPM,2023,https://doi.org/10.1109/ICSE48619.2023.00022,ICSE 2023,Data Quality for Software Vulnerability Datasets,"The use of learning-based techniques to achieve automated software vulnerability detection has been of longstanding interest within the software security domain. These data-driven solutions are enabled by large software vulnerability datasets used for training and benchmarking. However, we observe that the quality of the data powering these solutions is currently ill-considered, hindering the reliability and value of produced outcomes. Whilst awareness of software vulnerability data preparation challenges is growing, there has been little investigation into the potential negative impacts of software vulnerability data quality. For instance, we lack confirmation that vulnerability labels are correct or consistent. Our study seeks to address such shortcomings by inspecting five inherent data quality attributes for four state-of-the-art software vulnerability datasets and the subsequent impacts that issues can have on software vulnerability prediction models. Surprisingly, we found that all the analyzed datasets exhibit some data quality problems. In particular, we found 20–71% of vulnerability labels to be inaccurate in real-world datasets, and 17-99% of data points were duplicated. We observed that these issues could cause significant impacts on downstream models, either preventing effective model training or inflating benchmark performance. We advocate for the need to overcome such challenges. Our findings will enable better consideration and assessment of software vulnerability data quality in the future.","software vulnerability,data quality,machine learning",软件漏洞数据集的数据质量,使用基于学习的技术来实现软件漏洞的自动检测一直是软件安全领域的关注焦点。这些数据驱动的解决方案由用于培训和基准测试的大型软件漏洞数据集实现。然而，我们观察到，目前为这些解决方案提供动力的数据质量考虑不周，阻碍了产生结果的可靠性和价值。虽然人们越来越意识到软件漏洞数据准备的挑战，但对软件漏洞数据质量的潜在负面影响的调查却很少。例如，我们没有确认漏洞标签是否正确或一致。我们的研究试图通过检查四个最先进的软件漏洞数据集的五个固有数据质量属性以及这些问题可能对软件漏洞预测模型产生的后续影响来解决这些缺陷。令人惊讶的是，我们发现所有分析的数据集都存在一些数据质量问题。特别是，我们发现在真实世界的数据集中，20-71%的漏洞标签是不准确的，17-99%的数据点是重复的。我们观察到，这些问题可能会对下游模型造成重大影响，要么阻碍有效的模型训练，要么夸大基准性能。我们主张必须克服这些挑战。我们的研究结果将有助于在未来更好地考虑和评估软件漏洞数据质量。,软件漏洞，数据质量，机器学习,,,
8TDWNWDX,2023,https://doi.org/10.1109/ICSE48619.2023.00102,ICSE 2023,SmallRace: Static Race Detection for Dynamic Languages - A Case on Smalltalk,"Smalltalk, one of the first object-oriented programming languages, has had a tremendous influence on the evolution of computer technology. Due to the simplicity and productivity provided by the language, Smalltalk is still in active use today by many companies with large legacy codebases and with new code written every day. A crucial problem in Smalltalk programming is the race condition. Like in any other parallel language, debugging race conditions is inherently challenging, but in Smalltalk, it is even more challenging due to its dynamic nature. Being a purely dynamically-typed language, Smalltalk allows assigning any object to any variable without type restrictions, and allows forking new threads to execute arbitrary anonymous code blocks passed as objects. In Smalltalk, race conditions can be introduced easily, but are difficult to prevent at runtime. We present SmallRace, a novel static race detection framework designed for multithreaded dynamic languages, with a focus on Smalltalk. A key component of SmallRace is SmallIR, a subset of LLVM IR, in which all variables are declared with the same type-a generic pointer 18✶. This allows SmallRace to design an effective interprocedural thread-sensitive pointer analysis to infer the concrete types of dynamic variables. SmallRace automatically translates Smalltalk source code into SmallIR, supports most of the modern Smalltalk syntax in Visual Works, and generates actionable race reports with detailed debugging information. Importantly, SmallRace has been used to analyze a production codebase in a large company with over a million lines of code, and it has found tens of complex race conditions in the production code.","Productivity,Visualization,Parallel languages,Codes,Instruction sets,Source coding,Debugging",SmallRace：动态语言的静态种族检测——以Smalltalk为例,Smalltalk是最早的面向对象编程语言之一，对计算机技术的发展产生了巨大的影响。由于该语言提供的简单性和生产力，Smalltalk至今仍被许多拥有大型遗留代码库和每天编写新代码的公司积极使用。Smalltalk编程中的一个关键问题是竞赛条件。与任何其他并行语言一样，调试竞争条件本身就具有挑战性，但在Smalltalk中，由于其动态特性，它更具挑战性。作为一种纯粹的动态类型语言，Smalltalk允许在没有类型限制的情况下将任何对象分配给任何变量，并允许派生新线程来执行作为对象传递的任意匿名代码块。在Smalltalk中，可以很容易地引入竞争条件，但在运行时很难防止。我们介绍了SmallRace，一个为多线程动态语言设计的新的静态竞争检测框架，重点介绍了Smallalk。SmallRace的一个关键组件是SmallIR，它是LLVM IR的一个子集，其中所有变量都用相同类型的通用指针18声明✶. 这允许SmallRace设计一个有效的过程间线程敏感指针分析，以推断动态变量的具体类型。SmallRace自动将Smalltalk源代码转换为SmallIR，支持Visual Works中的大多数现代Smalltalk语法，并生成具有详细调试信息的可操作的竞赛报告。重要的是，SmallRace已被用于分析一家拥有超过一百万行代码的大公司的生产代码库，并在生产代码中发现了数十种复杂的竞争条件。,工作效率，可视化，并行语言，代码，指令集，源代码编码，调试,,,
PC5G982N,2023,https://doi.org/10.1109/ICSE48619.2023.00077,ICSE 2023,PILAR: Studying and Mitigating the Influence of Configurations on Log Parsing,"The significance of logs has been widely acknowledged with the adoption of various log analysis techniques that assist in software engineering tasks. Many log analysis techniques require structured logs as input while raw logs are typically unstructured. Automated log parsing is proposed to convert unstructured raw logs into structured log templates. Some log parsers achieve promising accuracy, yet they rely on significant efforts from the users to tune the parameters to achieve optimal results. In this paper, we first conduct an empirical study to understand the influence of the configurable parameters of six state-of-the-art log parsers on their parsing results on three aspects: 1) varying the parameters while using the same dataset, 2) keeping the same parameters while using different datasets, and 3) using different samples from the same dataset. Our results show that all these parsers are sensitive to the parameters, posing challenges to their adoption in practice. To mitigate such challenges, we propose PILAR (Parameter Insensitive Log Parser), an entropy-based log parsing approach. We compare PILAR with the existing log parsers on the same three aspects and find that PILAR is the most parameter-insensitive one. In addition, PILAR achieves the second highest parsing accuracy and efficiency among all the state-of-the-art log parsers. This paper paves the road for easing the adoption of log analysis in software engineer practices.","Roads,Software,Task analysis,Software engineering",PILAR:研究和减轻配置对日志解析的影响,随着各种日志分析技术的采用，日志的重要性已被广泛认可，这些技术有助于软件工程任务。许多日志分析技术需要结构化日志作为输入，而原始日志通常是非结构化的。提出了将非结构化原始日志转换为结构化日志模板的自动化日志解析方法。一些日志解析器实现了有希望的准确性，但它们依赖于用户的大量努力来调整参数以获得最佳结果。在本文中，我们首先进行了一项实证研究，从三个方面了解了六个最先进的日志解析器的可配置参数对其解析结果的影响：1）在使用相同数据集时改变参数，2）在使用不同数据集时保持相同参数，3）使用来自同一数据集的不同样本。我们的结果表明，所有这些解析器都对参数敏感，这对它们在实践中的采用提出了挑战。为了缓解这些挑战，我们提出了一种基于熵的日志解析方法PILAR（参数不敏感日志解析器）。在这三个方面，我们将PILAR与现有的日志解析器进行了比较，发现PILAR是对参数最不敏感的一个。此外，PILAR在所有最先进的日志解析器中实现了第二高的解析精度和效率。本文为在软件工程师实践中简化日志分析铺平了道路。,道路，软件，任务分析，软件工程,,,
B4EYQDLW,2023,https://doi.org/10.1109/ICSE48619.2023.00031,ICSE 2023,UPCY: Safely Updating Outdated Dependencies,"Recent research has shown that developers hesitate to update dependencies and mistrust automated approaches such as Dependabot, since they are afraid of introducing incompatibilities that break their project. In fact, such approaches only suggest naïve updates for a single outdated library but do not ensure compatibility with other dependent libraries in the project. To alleviate this situation and support developers in finding updates with minimal incompatibilities, we present UPCY. UPCY applies the min-(s,t)-cut algorithm and leverages a graph database of Maven Central to identify a list of valid update steps to update a dependency to a target version while minimizing incompatibilities with other libraries. By executing 29,698 updates in 380 projects, we compare the effectiveness of UPCY with the naïve updates applied by state-of-the-art tools. We find that in 41.1% of the cases where the naïve approach fails UPCY generates updates with fewer incompatibilities, and even 70.1% of the generated updates have zero incompatibilities.","Semantic versioning,Library updates,Package management,Dependency management,Software maintenance",UPCY：安全更新过时的依赖项,最近的研究表明，开发人员在更新依赖关系时犹豫不决，并且不信任可靠等自动化方法，因为他们害怕引入不兼容性，从而破坏他们的项目。事实上，这种方法只建议对单个过时的库进行幼稚的更新，但不能确保与项目中其他依赖库的兼容性。为了缓解这种情况并支持开发人员找到具有最小不兼容性的更新，我们提出了UPCY。UPCY应用min-（s，t）-cut算法，并利用Maven Central的图形数据库来识别有效的更新步骤列表，以将依赖项更新到目标版本，同时最大限度地减少与其他库的不兼容。通过在380个项目中执行29698次更新，我们将UPCY的有效性与最先进工具应用的天真更新进行了比较。我们发现，在41.1%的天真方法失败的情况下，UPCY会生成不兼容较少的更新，甚至70.1%的生成更新没有不兼容。,语义版本控制，库更新，包管理，依赖关系管理，软件维护,,,
2SC5KCW8,2023,https://doi.org/10.1109/ICSE48619.2023.00091,ICSE 2023,PYEVOLVE: Automating Frequent Code Changes in Python ML Systems,"Because of the naturalness of software and the rapid evolution of Machine Learning (ML) techniques, frequently repeated code change patterns (CPATs) occur often. They range from simple API migrations to changes involving several complex control structures such as for loops. While manually performing CPATs is tedious, the current state-of-the-art techniques for inferring transformation rules are not advanced enough to handle unseen variants of complex CPATs, resulting in a low recall rate. In this paper we present a novel, automated workflow that mines CPATs, infers the transformation rules, and then transplants them automatically to new target sites. We designed, implemented, evaluated and released this in a tool, PYEVOLVE. At its core is a novel data-flow, control-flow aware transformation rule inference engine. Our technique allows us to advance the state-of-the-art for transformation-by-example tools; without it, 70% of the code changes that PYEVOLVE transforms would not be possible to automate. Our thorough empirical evaluation of over 40,000 transformations shows 97% precision and 94% recall. By accepting 90% of CPATs generated by PYEVOLVE in famous open-source projects, developers confirmed its changes are useful.","Python,Machine Learning,Repetitive code changes,Transformation by Example,Program synthesis,Programming by example,Program transformation",PYEVOLVE：Python ML系统中频繁代码更改的自动化,由于软件的自然性和机器学习（ML）技术的快速发展，经常出现频繁重复的代码更改模式（CPAT）。它们的范围从简单的API迁移到涉及多个复杂控制结构（如for循环）的更改。虽然手动执行CPAT是乏味的，但目前最先进的推断转换规则的技术还不足以处理复杂CPAT的看不见的变体，导致召回率较低。在本文中，我们提出了一种新的、自动化的工作流，该工作流挖掘CPAT，推断转换规则，然后将它们自动移植到新的目标站点。我们在一个名为PYEVOLVE的工具中设计、实现、评估并发布了这一点。其核心是一个新颖的数据流、控制流感知的转换规则推理引擎。我们的技术使我们能够通过示例工具推进最先进的转换；如果没有它，PYEVOLVE转换的70%的代码更改将不可能自动化。我们对40000多个转换进行了彻底的实证评估，结果显示97%的准确率和94%的召回率。通过接受PYEVOLVE在著名开源项目中生成的90%的CPAT，开发人员确认了它的更改是有用的。,Python，机器学习，重复代码更改，示例转换，程序合成，示例编程，程序转换,,,
XMW8LWV7,2023,https://doi.org/10.1109/ICSE48619.2023.00079,ICSE 2023,On the Temporal Relations between Logging and Code,"Prior work shows that misleading logging texts (i.e., the textual descriptions in logging statements) can be counterproductive for developers during their use of logs. One of the most important types of information provided by logs is the temporal information of the recorded system behavior. For example, a logging text may use a perfective aspect to describe a fact that an important system event has finished. Although prior work has performed extensive studies on automated logging suggestions, few of these studies investigate the temporal relations between logging and code. In this work, we make the first attempt to comprehensively study the temporal relations between logging and its corresponding source code. In particular, we focus on two types of temporal relations: (1) logical temporal relations, which can be inferred from the execution order between the logging statement and the corresponding source code; and (2) semantic temporal relations, which can be inferred based on the semantic meaning of the logging text. We first perform qualitative analyses to study these two types of logging-code temporal relations and the inconsistency between them. As a result, we derive rules to detect these two types of temporal relations and their inconsistencies. Based on these rules, we propose a tool named TempoLo to automatically detect the issues of temporal inconsistencies between logging and code. Through an evaluation of four projects, we find that TempoLo can effectively detect temporal inconsistencies with a small number of false positives. To gather developers' feedback on whether such inconsistencies are worth fixing, we report 15 detected instances from these projects to developers. 13 instances from three projects are confirmed and fixed, while two instances of the remaining project are pending at the time of this writing. Our work lays the foundation for describing temporal relations between logging and code and demonstrates the potential for a deeper understanding of the relationship between logging and code.","software logging,logging text,temporal relations",日志与代码的时间关系,先前的工作表明，误导性的日志文本（即日志语句中的文本描述）可能会在开发人员使用日志时适得其反。日志提供的最重要的信息类型之一是记录的系统行为的时间信息。例如，日志文本可以使用完美方面来描述一个重要系统事件已经完成的事实。尽管先前的工作已经对自动日志记录建议进行了广泛的研究，但这些研究很少研究日志记录和代码之间的时间关系。在这项工作中，我们首次尝试全面研究日志记录及其相应源代码之间的时间关系。特别地，我们关注两种类型的时间关系：（1）逻辑时间关系，它可以从日志记录语句和相应源代码之间的执行顺序推断出来；以及（2）语义-时间关系，其可以基于日志文本的语义推断。我们首先进行了定性分析，以研究这两种类型的日志代码时间关系以及它们之间的不一致性。因此，我们导出了检测这两种类型的时间关系及其不一致性的规则。基于这些规则，我们提出了一个名为TempoLo的工具来自动检测日志记录和代码之间的时间不一致问题。通过对四个项目的评估，我们发现TempoLo可以有效地检测时间上的不一致，并且有少量的误报。为了收集开发人员对此类不一致是否值得修复的反馈，我们向开发人员报告了从这些项目中检测到的15个实例。三个项目的13个实例已得到确认和修复，而在撰写本报告时，其余项目的两个实例尚待处理。我们的工作为描述日志记录和代码之间的时间关系奠定了基础，并展示了深入理解日志记录与代码之间关系的潜力。,软件日志，日志文本，时间关系,,,
3L6D7IAV,2023,https://doi.org/10.1109/ICSE48619.2023.00075,ICSE 2023,Revisiting Learning-based Commit Message Generation,"Commit messages summarize code changes and help developers understand the intention. To alleviate human efforts in writing commit messages, researchers have proposed various automated commit message generation techniques, among which learning-based techniques have achieved great success in recent years. However, existing evaluation on learning-based commit message generation relies on the automatic metrics (e.g., BLEU) widely used in natural language processing (NLP) tasks, which are aggregated scores calculated based on the similarity between generated commit messages and the ground truth. Therefore, it remains unclear what generated commit messages look like and what kind of commit messages could be precisely generated by existing learning-based techniques. To fill this knowledge gap, this work performs the first study to systematically investigate the detailed commit messages generated by learning-based techniques. In particular, we first investigate the frequent patterns of the commit messages generated by state-of-the-art learning-based techniques. Surprisingly, we find the majority (~90%) of their generated commit messages belong to simple patterns (i.e., addition/removal/fix/avoidance patterns). To further explore the reasons, we then study the impact of datasets, input representations, and model components. We surprisingly find that existing learning-based techniques have competitive performance even when the inputs are only represented by change marks (i.e., “+”/“-”/“ ”), It indicates that existing learning-based techniques poorly utilize syntax and semantics in the code while mostly focusing on change marks, which could be the major reason for generating so many pattern-matching commit messages. We also find that the pattern ratio in the training set might also positively affect the pattern ratio of generated commit messages; and model components might have different impact on the pattern ratio.","Commit Message Generation,Deep Learning,Pattern-based",重新审视基于学习的提交消息生成,提交消息总结代码更改并帮助开发人员理解其意图。为了减轻人类在编写提交消息方面的努力，研究人员提出了各种自动提交消息生成技术，其中基于学习的技术近年来取得了巨大成功。然而，对基于学习的提交消息生成的现有评估依赖于在自然语言处理（NLP）任务中广泛使用的自动度量（例如，BLEU），这些度量是基于生成的提交消息和基本事实之间的相似性计算的聚合分数。因此，目前尚不清楚生成的提交消息是什么样子的，以及现有的基于学习的技术可以精确生成什么类型的提交消息。为了填补这一知识空白，这项工作进行了第一项研究，系统地研究了基于学习的技术生成的详细提交消息。特别是，我们首先研究了最先进的基于学习的技术生成的提交消息的频繁模式。令人惊讶的是，我们发现他们生成的提交消息中的大多数（~90%）属于简单模式（即添加/删除/修复/避免模式）。为了进一步探究原因，我们研究了数据集、输入表示和模型组件的影响。我们惊讶地发现，即使输入仅由变化标记（即“+”/“-”/“）表示，现有的基于学习的技术也具有竞争力。这表明，现有的学习技术在代码中很少利用语法和语义，而主要关注变化标记，这可能是生成如此多模式匹配提交消息的主要原因。我们还发现，训练集中的模式比率也可能对生成的提交消息的模式比率产生积极影响；并且模型组件可能对图案比率具有不同的影响。,提交消息生成，深度学习，基于模式,,,
KU5757NQ,2023,https://doi.org/10.1109/ICSE48619.2023.00101,ICSE 2023,Detecting Isolation Bugs via Transaction Oracle Construction,"Transactions are used to maintain the data integrity of databases, and have become an indispensable feature in modern Database Management Systems (DBMSs). Despite extensive efforts in testing DBMSs and verifying transaction processing mechanisms, isolation bugs still exist in widely-used DBMSs when these DBMSs violate their claimed transaction isolation levels. Isolation bugs can cause severe consequences, e.g., incorrect query results and database states. In this paper, we propose a novel transaction testing approach, Transaction oracle construction (Troc), to automatically detect isolation bugs in DBMSs. The core idea of Troc is to decouple a transaction into independent statements, and execute them on their own database views, which are constructed under the guidance of the claimed transaction isolation level. Any divergence between the actual transaction execution and the independent statement execution indicates an isolation bug. We implement and evaluate Troc on three widely-used DBMSs, i.e., MySQL, MariaDB, and TiDB. We have detected 5 previously-unknown isolation bugs in the latest versions of these DBMSs.","Database system,transaction,isolation,oracle",通过事务Oracle构造检测隔离Bug,事务用于维护数据库的数据完整性，已成为现代数据库管理系统（DBMS）中不可或缺的功能。尽管在测试数据库管理系统和验证事务处理机制方面付出了大量努力，但当这些数据库管理系统违反其声称的事务隔离级别时，在广泛使用的数据库管理系统中仍然存在隔离漏洞。隔离错误可能会造成严重后果，例如查询结果和数据库状态不正确。在本文中，我们提出了一种新的事务测试方法，事务oracle构建（Troc），以自动检测DBMS中的隔离错误。Troc的核心思想是将事务解耦为独立的语句，并在它们自己的数据库视图上执行它们，这些视图是在声称的事务隔离级别的指导下构建的。实际事务执行和独立语句执行之间的任何差异都表明存在隔离错误。我们在三个广泛使用的数据库管理系统上实现并评估了Troc，即MySQL、MariaDB和TiDB。我们在这些DBMS的最新版本中检测到了5个以前未知的隔离错误。,数据库系统，事务，隔离，oracle,,,
JENDK6RI,2023,https://doi.org/10.1109/ICSE48619.2023.00113,ICSE 2023,AI-based Question Answering Assistance for Analyzing Natural-language Requirements,"By virtue of being prevalently written in natural language (NL), requirements are prone to various defects, e.g., inconsistency and incompleteness. As such, requirements are frequently subject to quality assurance processes. These processes, when carried out entirely manually, are tedious and may further overlook important quality issues due to time and budget pressures. In this paper, we propose QAssist - a question-answering (QA) approach that provides automated assistance to stakeholders, including requirements engineers, during the analysis of NL requirements. Posing a question and getting an instant answer is beneficial in various quality-assurance scenarios, e.g., incompleteness detection. Answering requirements-related questions automatically is challenging since the scope of the search for answers can go beyond the given requirements specification. To that end, QAssist provides support for mining external domain-knowledge resources. Our work is one of the first initiatives to bring together QA and external domain knowledge for addressing requirements engineering challenges. We evaluate QAssist on a dataset covering three application domains and containing a total of 387 question-answer pairs. We experiment with state-of-the-art QA methods, based primarily on recent large-scale language models. In our empirical study, QAssist localizes the answer to a question to three passages within the requirements specification and within the external domain-knowledge resource with an average recall of 90.1% and 96.5%, respectively. QAssist extracts the actual answer to the posed question with an average accuracy of 84.2%.","Natural-language Requirements,Question Answering (QA),Language Models,Natural Language Processing (NLP),Natural Language Generation (NLG),BERT,T5",基于人工智能的自然语言需求分析问答辅助,由于普遍使用自然语言（NL）编写，需求容易出现各种缺陷，例如不一致和不完整。因此，需求经常受到质量保证过程的约束。这些过程在完全手动执行时是乏味的，并且由于时间和预算压力，可能会进一步忽略重要的质量问题。在本文中，我们提出了QAssist——一种问答（QA）方法，在分析NL需求的过程中为利益相关者（包括需求工程师）提供自动化帮助。提出问题并获得即时答案在各种质量保证场景中都是有益的，例如，不完整性检测。自动回答与需求相关的问题具有挑战性，因为搜索答案的范围可能超出给定的需求规范。为此，QAssist为挖掘外部领域知识资源提供了支持。我们的工作是第一批将QA和外部领域知识结合起来解决需求工程挑战的举措之一。我们在一个涵盖三个应用领域的数据集上评估了QAssist，该数据集共包含387个问答对。我们试验了最先进的QA方法，主要基于最近的大规模语言模型。在我们的实证研究中，QAssist将问题的答案定位在需求规范和外部领域知识资源中的三个段落中，平均召回率分别为90.1%和96.5%。QAssist提取所提出问题的实际答案，平均准确率为84.2%。,自然语言要求，问答（QA），语言模型，自然语言处理（NLP），自然语言生成（NLG），BERT，T5,,,
X5WX3BFZ,2023,https://doi.org/10.1109/ICSE48619.2023.00128,ICSE 2023,Automated Repair of Programs from Large Language Models,"Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.","Large Language Model,Program Repair",大型语言模型中程序的自动修复,像Codex这样的大型语言模型已经显示出为许多编程任务生成代码的能力。然而，现有模型的成功率很低，尤其是对于复杂的编程任务。其中一个原因是语言模型缺乏对程序语义的认识，导致程序不正确，甚至程序无法编译。在本文中，我们系统地研究了自动程序修复（APR）技术是否可以修复LeetCode竞赛中语言模型产生的错误解决方案。目标是研究APR技术是否可以提高大型语言模型生成的代码的可靠性。我们的研究表明：（1）自动生成的代码与人工制作的解决方案有着共同的编程错误，这表明APR技术可能具有修复自动生成代码的潜力；（2） 考虑到统计故障定位方法提供的错误位置信息，新发布的Codex编辑模式支持编辑代码，在修复错误解决方案方面与现有的Java修复工具TBar和Recoder相似或更好。通过分析这些工具产生的实验结果，我们提出了几点建议：（1）增强APR工具以超越补丁空间的限制（例如，引入更灵活的故障定位）是可取的；（2） 由于大型语言模型可以通过对更多数据进行训练来导出更多的修复模式，未来的APR工具可以将重点从添加更多的修复图案转移到基于合成/语义的方法，（3）将语言模型与APR相结合来策划补丁成分，这是值得研究的。,大型语言模型，程序修复,,,
8REKSCKJ,2023,https://doi.org/10.1109/ICSE48619.2023.00060,ICSE 2023,RepresentThemAll: A Universal Learning Representation of Bug Reports,"Deep learning techniques have shown promising performance in automated software maintenance tasks associated with bug reports. Currently, all existing studies learn the customized representation of bug reports for a specific downstream task. Despite early success, training multiple models for multiple downstream tasks faces three issues: complexity, cost, and compatibility, due to the customization, disparity, and uniqueness of these automated approaches. To resolve the above challenges, we propose RepresentThemAll, a pre-trained approach that can learn the universal representation of bug reports and handle multiple downstream tasks. Specifically, RepresentThemAll is a universal bug report framework that is pre-trained with two carefully designed learning objectives: one is the dynamic masked language model and another one is a contrastive learning objective, “find yourself”. We evaluate the performance of RepresentThemAll on four downstream tasks, including duplicate bug report detection, bug report summarization, bug priority prediction, and bug severity prediction. Our experimental results show that RepresentThemAll outperforms all baseline approaches on all considered downstream tasks after well-designed fine-tuning.","Training,Deep learning,Software maintenance,Costs,Computer bugs,Stacking,Transformers",RepresentThemAll：Bug报告的通用学习表示,深度学习技术在与错误报告相关的自动化软件维护任务中表现出了良好的性能。目前，所有现有的研究都学习了针对特定下游任务的错误报告的自定义表示。尽管早期取得了成功，但由于这些自动化方法的定制性、差异性和独特性，为多个下游任务训练多个模型面临三个问题：复杂性、成本和兼容性。为了解决上述挑战，我们提出了RepresentThemAll，这是一种预先训练的方法，可以学习错误报告的通用表示并处理多个下游任务。具体来说，RepresentThemAll是一个通用的错误报告框架，它由两个精心设计的学习目标预先训练而成：一个是动态掩蔽语言模型，另一个是对比学习目标“找到自己”。我们评估了RepresentThemAll在四个下游任务上的性能，包括重复错误报告检测、错误报告摘要、错误优先级预测和错误严重性预测。我们的实验结果表明，经过精心设计的微调，RepresentThemAll在所有考虑的下游任务上都优于所有基线方法。,培训，深度学习，软件维护，成本，计算机错误，堆叠，变形金刚,,,
B3NXUAB4,2023,https://doi.org/10.1109/ICSE48619.2023.00084,ICSE 2023,Efficiency Matters: Speeding Up Automated Testing with GUI Rendering Inference,"Due to the importance of Android app quality assurance, many automated GUI testing tools have been developed. Although the test algorithms have been improved, the impact of GUI rendering has been overlooked. On the one hand, setting a long waiting time to execute events on fully rendered GUIs slows down the testing process. On the other hand, setting a short waiting time will cause the events to execute on partially rendered GUIs, which negatively affects the testing effectiveness. An optimal waiting time should strike a balance between effectiveness and efficiency. We propose AdaT, a lightweight image-based approach to dynamically adjust the inter-event time based on GUI rendering state. Given the real-time streaming on the GUI, AdaT presents a deep learning model to infer the rendering state, and synchronizes with the testing tool to schedule the next event when the GUI is fully rendered. The evaluations demonstrate the accuracy, efficiency, and effectiveness of our approach. We also integrate our approach with the existing automated testing tool to demonstrate the usefulness of AdaT in covering more activities and executing more events on fully rendered GUIs.","Efficient android GUI testing,GUI rendering,Machine Learning",效率至关重要：使用GUI渲染推理加速自动测试,由于Android应用程序质量保证的重要性，已经开发了许多自动化GUI测试工具。尽管测试算法得到了改进，但GUI渲染的影响却被忽视了。一方面，在完全渲染的GUI上设置长的等待时间来执行事件会减慢测试过程。另一方面，设置短的等待时间会导致事件在部分渲染的GUI上执行，这会对测试的有效性产生负面影响。最佳等待时间应在有效性和效率之间取得平衡。我们提出了AdaT，这是一种基于图像的轻量级方法，可以根据GUI渲染状态动态调整事件间时间。给定GUI上的实时流，AdaT提供了一个深度学习模型来推断渲染状态，并与测试工具同步，以在GUI完全渲染时安排下一个事件。评估证明了我们方法的准确性、效率和有效性。我们还将我们的方法与现有的自动化测试工具集成，以证明AdaT在覆盖更多活动和在完全渲染的GUI上执行更多事件方面的有用性。,高效的android GUI测试，GUI渲染，机器学习,,,
3HND752C,2023,https://doi.org/10.1109/ICSE48619.2023.00197,ICSE 2023,"Read It, Don't Watch It: Captioning Bug Recordings Automatically","Screen recordings of mobile applications are easy to capture and include a wealth of information, making them a popular mechanism for users to inform developers of the problems encountered in the bug reports. However, watching the bug recordings and efficiently understanding the semantics of user actions can be time-consuming and tedious for developers. Inspired by the conception of the video subtitle in movie industry, we present a lightweight approach CAPdroid to caption bug recordings automatically. CAPdroid is a purely image-based and non-intrusive approach by using image processing and convolutional deep learning models to segment bug recordings, infer user action attributes, and generate subtitle descriptions. The automated experiments demonstrate the good performance of CAPdroid in inferring user actions from the recordings, and a user study confirms the usefulness of our generated step descriptions in assisting developers with bug replay.","bug recording,video captioning,android app",读它，不要看它：自动为Bug记录添加字幕,移动应用程序的屏幕记录很容易捕捉，并且包含丰富的信息，这使得它们成为用户向开发人员通知错误报告中遇到的问题的流行机制。然而，对于开发人员来说，查看错误记录并有效地理解用户操作的语义可能是耗时且乏味的。受电影行业视频字幕概念的启发，我们提出了一种轻量级的方法CAPdroid来自动记录错误字幕。CAPdroid是一种纯粹基于图像的非侵入性方法，它使用图像处理和卷积深度学习模型来分割错误记录、推断用户动作属性和生成字幕描述。自动化实验证明了CAPdroid在从录音中推断用户动作方面的良好性能，一项用户研究证实了我们生成的步骤描述在帮助开发人员回放错误方面的有用性。,错误记录，视频字幕，安卓应用程序,,,
7LSDVDTT,2023,https://doi.org/10.1109/ICSE48619.2023.00114,ICSE 2023,"Strategies, Benefits and Challenges of App Store-inspired Requirements Elicitation","App store-inspired elicitation is the practice of exploring competitors' apps, to get inspiration for requirements. This activity is common among developers, but little insight is available on its practical use, advantages and possible issues. This paper aims to empirically analyse this technique in a realistic scenario, in which it is used to extend the requirements of a product that were initially captured by means of more traditional requirements elicitation interviews. Considering this scenario, we conduct an experimental simulation with 58 analysts and collect qualitative data. We perform thematic analysis of the data to identify strategies, benefits, and challenges of app store-inspired elicitation, as well as differences with respect to interviews in the considered elicitation setting. Our results show that: (1) specific guidelines and procedures are required to better conduct app store-inspired elicitation; (2) current search features made available by app stores are not suitable for this practice, and more tool support is required to help analysts in the retrieval and evaluation of competing products; (3) while interviews focus on the why dimension of requirements engineering (i.e., goals), app store-inspired elicitation focuses on how (i.e., solutions), offering indications for implementation and improved usability. Our study provides a framework for researchers to address existing challenges and suggests possible benefits to fostering app store-inspired elicitation among practitioners.","app store inspired elicitation,app store analysis,requirements elicitation,interviews,qualitative study,experimental simulation",应用商店启发需求挖掘的策略、优势和挑战,应用商店启发启发是探索竞争对手应用程序的实践，以获得需求的灵感。这种活动在开发人员中很常见，但对其实际用途、优点和可能存在的问题知之甚少。本文旨在在一个现实的场景中实证分析这一技术，在这个场景中，它被用来扩展最初通过更传统的需求启发访谈获得的产品的需求。考虑到这种情况，我们对58名分析师进行了实验模拟，并收集了定性数据。我们对数据进行主题分析，以确定应用商店启发启发启发的策略、好处和挑战，以及在所考虑的启发环境中与访谈的差异。我们的研究结果表明：（1）需要具体的指导方针和程序来更好地进行应用商店启发启发；（2） 目前应用商店提供的搜索功能不适合这种做法，需要更多的工具支持来帮助分析师检索和评估竞争产品；（3） 访谈的重点是需求工程的“为什么”维度（即目标），而应用商店启发的启发则集中在如何（即解决方案）、提供实现指示和提高可用性。我们的研究为研究人员解决现有挑战提供了一个框架，并提出了在从业者中培养应用商店启发启发的可能好处。,应用商店启发启发，应用商店分析，需求启发，访谈，定性研究，实验模拟,,,
5LN3HRNL,2023,https://doi.org/10.1109/ICSE48619.2023.00132,ICSE 2023,Usability-Oriented Design of Liquid Types for Java,"Developers want to detect bugs as early in the development lifecycle as possible, as the effort and cost to fix them increases with the incremental development of features. Ultimately, bugs that are only found in production can have catastrophic consequences. Type systems are effective at detecting many classes of bugs during development, often providing immediate feedback both at compile-time and while typing due to editor integration. Unfortunately, more powerful static and dynamic analysis tools do not have the same success due to providing false positives, not being immediate, or not being integrated into the language. Liquid Types extend the language type system with predicates, augmenting the classes of bugs that the compiler or IDE can catch compared to the simpler type systems available in mainstream programming languages. However, previous implementations of Liquid Types have not used human-centered methods for designing or evaluating their extensions. Therefore, this paper investigates how Liquid Types can be integrated into a mainstream programming language, Java, by proposing a new design that aims to lower the barriers to entry and adapts to problems that Java developers commonly encounter at runtime. Following a participatory design methodology, we conducted a developer survey to design the syntax of LiquidJava, our prototype. To evaluate if the added effort to writing Liquid Types in Java would convince users to adopt them, we conducted a user study with 30 Java developers. The results show that LiquidJava helped users detect and fix more bugs and that Liquid Types are easy to interpret and learn with few resources. At the end of the study, all users reported interest in adopting LiquidJava for their projects.","Usability,Java,Refinement Types,Liquid Types",面向可用性的Java液体类型设计,开发人员希望在开发生命周期中尽早发现错误，因为修复这些错误的工作量和成本随着功能开发的增加而增加。最终，只在生产中发现的错误可能会带来灾难性的后果。类型系统在开发过程中可以有效地检测许多类的错误，由于编辑器集成，通常在编译时和键入时都能提供即时反馈。不幸的是，更强大的静态和动态分析工具并没有取得同样的成功，因为它们提供了误报、不即时或没有集成到语言中。Liquid Types用谓词扩展了语言类型系统，与主流编程语言中可用的更简单的类型系统相比，增加了编译器或IDE可以捕获的bug类。然而，Liquid Types的先前实现没有使用以人为中心的方法来设计或评估其扩展。因此，本文研究了如何将Liquid Types集成到主流编程语言Java中，提出了一种新的设计，旨在降低进入门槛，并适应Java开发人员在运行时常见的问题。遵循参与式设计方法，我们进行了一次开发人员调查，以设计我们的原型LiquidJava的语法。为了评估用Java编写Liquid Types的额外努力是否会说服用户采用它们，我们对30名Java开发人员进行了一项用户研究。结果表明，LiquidJava帮助用户检测和修复了更多的错误，并且Liquid Types易于解释和学习，只需很少的资源。研究结束时，所有用户都表示有兴趣在他们的项目中采用LiquidJava。,可用性，Java，优化类型，液体类型,,,
3F55FWKU,2023,https://doi.org/10.1109/ICSE48619.2023.00186,ICSE 2023,Coverage Guided Fault Injection for Cloud Systems,"To support high reliability and availability, modern cloud systems are designed to be resilient to node crashes and reboots. That is, a cloud system should gracefully recover from node crashes/reboots and continue to function. However, node crashes/reboots that occur under special timing can trigger crash recovery bugs that lie in incorrect crash recovery protocols and their implementations. To ensure that a cloud system is free from crash recovery bugs, some fault injection approaches have been proposed to test whether a cloud system can correctly recover from various crash scenarios. These approaches are not effective in exploring the huge crash scenario space without developers' knowledge. In this paper, we propose Crash Fuzz, a fault injection testing approach that can effectively test crash recovery behaviors and reveal crash recovery bugs in cloud systems. CrashFuzz mutates the combinations of possible node crashes and reboots according to runtime feedbacks, and prioritizes the combinations that are prone to increase code coverage and trigger crash recovery bugs for smart exploration. We have implemented CrashFuzz and evaluated it on three popular open-source cloud systems, i.e., ZooKeeper, HDFS and HBase. CrashFuzz has detected 4 unknown bugs and 1 known bug. Compared with other fault injection approaches, CrashFuzz can detect more crash recovery bugs and achieve higher code coverage.","cloud system,crash recovery bug,fault injection,bug detection,fuzzing",云系统的覆盖引导故障注入,为了支持高可靠性和可用性，现代云系统被设计为对节点崩溃和重新启动具有弹性。也就是说，云系统应该从节点崩溃/重新启动中正常恢复并继续运行。然而，在特殊时间发生的节点崩溃/重新启动可能会触发错误的崩溃恢复协议及其实现中的崩溃恢复错误。为了确保云系统没有崩溃恢复错误，已经提出了一些故障注入方法来测试云系统是否能够从各种崩溃场景中正确恢复。在开发人员不知情的情况下，这些方法在探索巨大的崩溃场景空间时是无效的。在本文中，我们提出了Crash Fuzz，这是一种故障注入测试方法，可以有效地测试崩溃恢复行为，并揭示云系统中的崩溃恢复错误。CrashFuzz根据运行时反馈对可能的节点崩溃和重新启动的组合进行变异，并优先考虑容易增加代码覆盖率和触发崩溃恢复错误的组合，以进行智能探索。我们已经实现了CrashFuzz，并在三个流行的开源云系统上进行了评估，即ZooKeeper、HDFS和HBase。CrashFuzz检测到4个未知错误和1个已知错误。与其他故障注入方法相比，CrashFuzz可以检测到更多的故障恢复错误，并实现更高的代码覆盖率。,云系统，崩溃恢复错误，故障注入，错误检测，模糊,,,
VWSS6NTA,2023,https://doi.org/10.1109/ICSE48619.2023.00164,ICSE 2023,Two Sides of the Same Coin: Exploiting the Impact of Identifiers in Neural Code Comprehension,"Previous studies have demonstrated that neural code comprehension models are vulnerable to identifier naming. By renaming as few as one identifier in the source code, the models would output completely irrelevant results, indicating that identifiers can be misleading for model prediction. However, identifiers are not completely detrimental to code comprehension, since the semantics of identifier names can be related to the program semantics. Well exploiting the two opposite impacts of identifiers is essential for enhancing the robustness and accuracy of neural code comprehension, and still remains under-explored. In this work, we propose to model the impact of identifiers from a novel causal perspective, and propose a counterfactual reasoning-based framework named CREAM. CREAM explicitly captures the misleading information of identifiers through multi-task learning in the training stage, and reduces the misleading impact by counterfactual inference in the inference stage. We evaluate CREAM on three popular neural code comprehension tasks, including function naming, defect detection and code classification. Experiment results show that CREAM not only significantly outperforms baselines in terms of robustness (e.g., +37.9% on the function naming task at F1 score), but also achieve improved results on the original datasets (e.g., +0.5% on the function naming task at F1 score).","Training,Codes,Source coding,Semantics,Predictive models,Multitasking,Robustness",硬币的两面：利用标识符在神经代码理解中的影响,先前的研究表明，神经代码理解模型容易受到标识符命名的影响。通过在源代码中只重命名一个标识符，模型将输出完全不相关的结果，这表明标识符可能会误导模型预测。然而，标识符并不完全有害于代码理解，因为标识符名称的语义可以与程序语义相关。充分利用标识符的两种相反影响对于提高神经代码理解的稳健性和准确性至关重要，但这一点仍有待探索。在这项工作中，我们提出从一个新的因果角度对标识符的影响进行建模，并提出了一个基于反事实推理的框架CREAM。CREAM在训练阶段通过多任务学习明确捕捉识别者的误导信息，并在推理阶段通过反事实推理减少误导影响。我们在三个流行的神经代码理解任务上评估CREAM，包括函数命名、缺陷检测和代码分类。实验结果表明，CREAM不仅在稳健性方面显著优于基线（例如，在F1得分时，函数命名任务的稳健性提高了37.9%），而且在原始数据集上也取得了改进的结果（例如，F1得分时函数命名任务提高了0.5%）。,训练，代码，源代码编码，语义，预测模型，多任务处理，稳健性,,,
YCTUH2WJ,2023,https://doi.org/10.1109/ICSE48619.2023.00015,ICSE 2023,Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models,"Previous research on code intelligence usually trains a deep learning model on a fixed dataset in an offline manner. However, in real-world scenarios, new code repositories emerge incessantly, and the carried new knowledge is beneficial for providing up-to-date code intelligence services to developers. In this paper, we aim at the following problem: How to enable code intelligence models to continually learn from ever-increasing data? One major challenge here is catastrophic forgetting, meaning that the model can easily forget knowledge learned from previous datasets when learning from the new dataset. To tackle this challenge, we propose REPEAT, a novel method for continual learning of code intelligence models. Specifically, REPEAT addresses the catastrophic forgetting problem with representative exemplars replay and adaptive parameter regularization. The representative exemplars replay component selects informative and diverse exemplars in each dataset and uses them to re-train model periodically. The adaptive parameter regularization component recognizes important parameters in the model and adaptively penalizes their changes to preserve the knowledge learned before. We evaluate the proposed approach on three code intelligence tasks including code summarization, software vulnerability detection, and code clone detection. Extensive experiments demonstrate that REPEAT consistently outperforms baseline methods on all tasks. For example, REPEAT improves the conventional fine-tuning method by 1.22, 5.61, and 1.72 on code summarization, vulnerability detection and clone detection, respectively.","Deep learning,Adaptation models,Codes,Cloning,Data models,Software,Task analysis",跟上不断增长的数据：实现代码智能模型的持续学习,先前对代码智能的研究通常以离线的方式在固定的数据集上训练深度学习模型。然而，在现实世界中，新的代码库不断出现，所携带的新知识有利于为开发人员提供最新的代码智能服务。在本文中，我们针对以下问题：如何使代码智能模型能够从不断增长的数据中不断学习？这里的一个主要挑战是灾难性遗忘，这意味着模型在从新数据集学习时很容易忘记从以前数据集学习的知识。为了应对这一挑战，我们提出了REPEAT，这是一种持续学习代码智能模型的新方法。具体来说，REPEAT通过具有代表性的样本重放和自适应参数正则化来解决灾难性遗忘问题。代表性样本回放组件在每个数据集中选择信息丰富且多样化的样本，并使用它们定期重新训练模型。自适应参数正则化组件识别模型中的重要参数，并自适应地惩罚它们的变化，以保留之前学习的知识。我们在三个代码智能任务上评估了所提出的方法，包括代码摘要、软件漏洞检测和代码克隆检测。大量实验表明，REPEAT在所有任务上都始终优于基线方法。例如，REPEAT在代码摘要、漏洞检测和克隆检测方面分别将传统的微调方法改进了1.22、5.61和1.72。,深度学习，适应模型，代码，克隆，数据模型，软件，任务分析,,,
GII7VWN5,2023,https://doi.org/10.1109/ICSE48619.2023.00135,ICSE 2023,Leveraging Feature Bias for Scalable Misprediction Explanation of Machine Learning Models,"Interpreting and debugging machine learning models is necessary to ensure the robustness of the machine learning models. Explaining mispredictions can help significantly in doing so. While recent works on misprediction explanation have proven promising in generating interpretable explanations for mispredictions, the state-of-the-art techniques “blindly” deduce misprediction explanation rules from all data features, which may not be scalable depending on the number of features. To alleviate this problem, we propose an efficient misprediction explanation technique named Bias Guided Misprediction Diagnoser (BGMD), which leverages two prior knowledge about data: a) data often exhibit highly-skewed feature distributions and b) trained models in many cases perform poorly on subdataset with under-represented features. Next, we propose a technique named MAPS (Mispredicted Area UPweight Sampling). MAPS increases the weights of subdataset during model retraining that belong to the group that is prone to be mispredicted because of containing under-represented features. Thus, MAPS make retrained model pay more attention to the under-represented features. Our empirical study shows that our proposed BGMD outperformed the state-of-the-art misprediction diagnoser and reduces diagnosis time by 92%. Furthermore, MAPS outperformed two state-of-the-art techniques on fixing the machine learning model's performance on mispredicted data without compromising performance on all data. All the research artifacts (i.e., tools, scripts, and data) of this study are available in the accompanying website [1].","machine learning,data imbalance,rule induction,misprediction explanation",利用特征偏差进行机器学习模型的可扩展预测失误解释,解释和调试机器学习模型对于确保机器学习模型的稳健性是必要的。解释预测失误可以在很大程度上帮助做到这一点。虽然最近关于预测失误解释的工作被证明在为预测失误生成可解释解释的解释方面很有前景，但最先进的技术“盲目”地从所有数据特征中推导出预测失误解释规则，这可能无法根据特征的数量进行扩展。为了缓解这个问题，我们提出了一种有效的预测失误解释技术，称为偏差引导的预测失误诊断器（BGMD），它利用了关于数据的两个先验知识：a）数据通常表现出高度偏斜的特征分布；b）在许多情况下，训练的模型在特征表示不足的子数据集上表现不佳。接下来，我们提出了一种称为MAPS（Mispredicted Area UPweight Sampling）的技术。MAPS在模型再训练期间增加了子数据集的权重，这些子数据集属于由于包含未充分表示的特征而容易被预测错误的组。因此，MAPS使得重新训练的模型更加关注表示不足的特征。我们的实证研究表明，我们提出的BGMD优于最先进的预测失误诊断器，并将诊断时间缩短了92%。此外，在不影响所有数据性能的情况下，MAPS在修复机器学习模型对预测错误数据的性能方面优于两种最先进的技术。本研究的所有研究成果（即工具、脚本和数据）可在随附的网站[1]中获得。,机器学习，数据不平衡，规则归纳，预测失误解释,,,
EP8KCF3L,2023,https://doi.org/10.1109/ICSE48619.2023.00087,ICSE 2023,AChecker: Statically Detecting Smart Contract Access Control Vulnerabilities,"As most smart contracts have a financial nature and handle valuable assets, smart contract developers use access control to protect assets managed by smart contracts from being misused by malicious or unauthorized people. Unfortunately, programming languages used for writing smart contracts, such as Solidity, were not designed with a permission-based security model in mind. Therefore, smart contract developers implement access control checks based on their judgment and in an adhoc manner, which results in several vulnerabilities in smart contracts, called access control vulnerabilities. Further, the in-consistency in implementing access control makes it difficult to reason about whether a contract meets access control needs and is free of access control vulnerabilities. In this work, we propose AChecker - an approach for detecting access control vulnerabilities. Unlike prior work, AChecker does not rely on pre-defined patterns or contract transactions history. Instead, it infers access control implemented in smart contracts via static data-flow analysis. Moreover, the approach performs further symbolic-based analysis to distinguish cases when unauthorized people can obtain control of the contract as intended functionality. We evaluated AChecker on three public datasets of real-world smart contracts, including one which consists of contracts with assigned access control CVEs, and compared its effectiveness with eight analysis tools. The evaluation results showed that AChecker outperforms these tools in terms of both precision and recall. In addition, AChecker flagged vulnerabilities in 21 frequently-used contracts on Ethereum blockchain with 90% precision.","Smart contract,security,access control,data-flow analysis",AChecker：静态检测智能合约访问控制漏洞,由于大多数智能合约都具有金融性质，并处理有价值的资产，因此智能合约开发商使用访问控制来保护智能合约管理的资产不被恶意或未经授权的人滥用。不幸的是，用于编写智能合约的编程语言，如Solidity，在设计时没有考虑到基于权限的安全模型。因此，智能合约开发人员根据自己的判断，以特定的方式实施访问控制检查，这会导致智能合约中出现几个漏洞，称为访问控制漏洞。此外，实现访问控制的一致性使得很难判断合同是否满足访问控制需求并且没有访问控制漏洞。在这项工作中，我们提出了AChecker——一种检测访问控制漏洞的方法。与之前的工作不同，AChecker不依赖于预定义的模式或合同交易历史。相反，它通过静态数据流分析推断出智能合约中实现的访问控制。此外，该方法还执行了进一步的基于符号的分析，以区分未经授权的人可以作为预期功能获得合同控制权的情况。我们在真实世界智能合约的三个公共数据集上评估了AChecker，其中一个数据集由具有指定访问控制CVE的合约组成，并将其有效性与八个分析工具进行了比较。评估结果表明，AChecker在准确性和召回率方面都优于这些工具。此外，AChecker以90%的准确率标记了以太坊区块链上21个常用合同中的漏洞。,智能合约，安全，访问控制，数据流分析,,,
WF7IEXZM,2023,https://doi.org/10.1109/ICSE48619.2023.00123,ICSE 2023,Autonomy Is An Acquired Taste: Exploring Developer Preferences for GitHub Bots,"Software bots fulfill an important role in collective software development, and their adoption by developers promises increased productivity. Past research has identified that bots that communicate too often can irritate developers, which affects the utility of the bot. However, it is not clear what other properties of human-bot collaboration affect developers' preferences, or what impact these properties might have. The main idea of this paper is to explore characteristics affecting developer preferences for interactions between humans and bots, in the context of GitHub pull requests. We carried out an exploratory sequential study with interviews and a subsequent vignette-based survey. We find developers generally prefer bots that are personable but show little autonomy, however, more experienced developers tend to prefer more autonomous bots. Based on this empirical evidence, we recommend bot developers increase configuration options for bots so that individual developers and projects can configure bots to best align with their own preferences and project cultures.","Software Bot,Pull Request,Human Aspects",自主性是一种后天的品味：探索开发者对GitHub机器人的偏好,软件机器人在集体软件开发中发挥着重要作用，开发人员采用它们有望提高生产力。过去的研究表明，经常通信的机器人会激怒开发者，从而影响机器人的效用。然而，尚不清楚人类机器人协作的其他哪些特性会影响开发者的偏好，也不清楚这些特性可能会产生什么影响。本文的主要思想是在GitHub拉取请求的背景下，探索影响开发人员对人与机器人之间交互偏好的特征。我们进行了一项探索性的连续研究，包括访谈和随后的基于小插曲的调查。我们发现，开发人员通常更喜欢有个性但几乎没有自主性的机器人，然而，更有经验的开发人员往往更喜欢更自主的机器人。基于这一经验证据，我们建议机器人程序开发人员增加机器人程序的配置选项，以便各个开发人员和项目能够将机器人程序配置为最符合他们自己的偏好和项目文化。,软件机器人，拉取请求，人的方面,,,
HHI5R5KV,2023,https://doi.org/10.1109/ICSE48619.2023.00053,ICSE 2023,FedDebug: Systematic Debugging for Federated Learning Applications,"In Federated Learning (FL), clients independently train local models and share them with a central aggregator to build a global model. Impermissibility to access clients' data and collaborative training make FL appealing for applications with data-privacy concerns, such as medical imaging. However, these FL characteristics pose unprecedented challenges for debugging. When a global model's performance deteriorates, identifying the responsible rounds and clients is a major pain point. Developers resort to trial-and-error debugging with subsets of clients, hoping to increase the global model's accuracy or let future FL rounds retune the model, which are time-consuming and costly. We design a systematic fault localization framework, Fedde-bug,that advances the FL debugging on two novel fronts. First, Feddebug enables interactive debugging of realtime collaborative training in FL by leveraging record and replay techniques to construct a simulation that mirrors live FL. Feddebug'sbreakpoint can help inspect an FL state (round, client, and global model) and move between rounds and clients' models seam-lessly, enabling a fine-grained step-by-step inspection. Second, Feddebug automatically identifies the client(s) responsible for lowering the global model's performance without any testing data and labels-both are essential for existing debugging techniques. Feddebug's strengths come from adapting differential testing in conjunction with neuron activations to determine the client(s) deviating from normal behavior. Feddebug achieves 100% accuracy in finding a single faulty client and 90.3% accuracy in finding multiple faulty clients. Feddebug's interactive de-bugging incurs 1.2% overhead during training, while it localizes a faulty client in only 2.1% of a round's training time. With FedDebug,we bring effective debugging practices to federated learning, improving the quality and productivity of FL application developers.","software debugging,federated learning,testing,client,fault localization,neural networks,CNN",FedDebug：联邦学习应用程序的系统调试,在联合学习（FL）中，客户端独立训练本地模型，并与中央聚合器共享，以构建全局模型。访问客户数据的不可访问性和协作培训使FL对医疗成像等涉及数据隐私问题的应用程序具有吸引力。然而，这些FL特性对调试提出了前所未有的挑战。当全球模型的性能恶化时，确定负责的轮次和客户是一个主要的痛点。开发人员使用客户端子集进行试错调试，希望提高全局模型的准确性，或者让未来的FL轮次重新调整模型，这既耗时又昂贵。我们设计了一个系统的故障定位框架Fedde bug，它在两个新颖的方面推进了FL调试。首先，Feddebug通过利用记录和回放技术构建反映实时FL的模拟，实现了FL中实时协作训练的交互式调试。Feddebug的断点可以帮助检查FL状态（轮次、客户端和全局模型），并在轮次和客户端模型之间无缝移动，从而实现细粒度的逐步检查。其次，Feddebug在没有任何测试数据和标签的情况下自动识别负责降低全局模型性能的客户端，这两者对于现有的调试技术都是必不可少的。Feddebug的优势来自于将差分测试与神经元激活相结合，以确定客户端偏离正常行为。Feddebug在查找单个故障客户端方面实现了100%的准确率，在查找多个故障客户端方面达到了90.3%的准确率。Feddebug的交互式去bug在训练期间会产生1.2%的开销，而它只需一轮训练时间的2.1%就可以定位有故障的客户端。通过FedDebug，我们为联邦学习带来了有效的调试实践，提高了FL应用程序开发人员的质量和生产力。,软件调试，联合学习，测试，客户端，故障定位，神经网络，CNN,,,
MAS57RCS,2023,https://doi.org/10.1109/ICSE48619.2023.00133,ICSE 2023,Towards Understanding Fairness and its Composition in Ensemble Machine Learning,"Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensembles. Today, we do not understand these fully for different ensemble algorithms. In this paper, we comprehensively study popular real-world ensembles: Bagging, Boosting, Stacking, and Voting. We have developed a benchmark of 168 ensemble models collected from Kaggle on four popular fairness datasets. We use existing fairness metrics to understand the composition of fairness. Our results show that ensembles can be designed to be fairer without using mitigation techniques. We also identify the interplay between fairness composition and data characteristics to guide fair ensemble design. Finally, our benchmark can be leveraged for further research on fair ensembles. To the best of our knowledge, this is one of the first and largest studies on fairness composition in ensembles yet presented in the literature.","fairness,ensemble,machine learning,models",在集成机器学习中理解公平及其构成,机器学习（ML）软件在现代社会中被广泛采用，据报道，基于种族、性别、年龄等，机器学习对少数群体的公平性有影响。最近的许多工作都提出了测量和减轻ML模型中算法偏见的方法。现有的方法侧重于基于单个分类器的ML模型。然而，真实世界的ML模型通常由集成中的多个独立或依赖学习者组成（例如，随机森林），其中公平性以一种非平凡的方式组成。合奏中的公平是如何构成的？学习者的公平性对合奏的最终公平性有什么影响？公平的学习者会导致不公平的合奏吗？此外，研究表明，超参数会影响ML模型的公平性。合奏超参数更为复杂，因为它们影响学习者在不同类别的合奏中的组合方式。了解集合超参数对公平性的影响将有助于程序员设计公平的集合。今天，对于不同的集成算法，我们还没有完全理解这些。在本文中，我们全面研究了现实世界中流行的合奏：Bagging、Boosting、Stacking和Voting。我们在四个流行的公平数据集上开发了一个由Kaggle收集的168个集成模型组成的基准。我们使用现有的公平指标来理解公平的构成。我们的研究结果表明，在不使用缓解技术的情况下，可以设计出更公平的组合。我们还确定了公平组成和数据特征之间的相互作用，以指导公平的集成设计。最后，我们的基准可以用于对公平组合的进一步研究。据我们所知，这是迄今为止文献中首次也是最大规模的关于合奏中公平构图的研究之一。,公平，集成，机器学习，模型,,,
8VMLUWS6,2023,https://doi.org/10.1109/ICSE48619.2023.00199,ICSE 2023,On the Applicability of Language Models to Block-Based Programs,"Block-based programming languages like Scratch are increasingly popular for programming education and end-user programming. Recent program analyses build on the insight that source code can be modelled using techniques from natural language processing. Many of the regularities of source code that support this approach are due to the syntactic overhead imposed by textual programming languages. This syntactic overhead, however, is precisely what block-based languages remove in order to simplify programming. Consequently, it is unclear how well this modelling approach performs on block-based programming languages. In this paper, we investigate the applicability of language models for the popular block-based programming language Scratch. We model Scratch programs using n-gram models, the most essential type of language model, and transformers, a popular deep learning model. Evaluation on the example tasks of code completion and bug finding confirm that blocks inhibit predictability, but the use of language models is nevertheless feasible. Our findings serve as foundation for improving tooling and analyses for block-based languages.","Block-Based Programs,Scratch,Natural Language Model,Code Completion,Bugram",论语言模型对基于块程序的适用性,Scratch等基于块的编程语言在编程教育和最终用户编程中越来越受欢迎。最近的程序分析建立在源代码可以使用自然语言处理技术建模的基础上。支持这种方法的源代码的许多规则都是由于文本编程语言带来的语法开销。然而，这种语法开销正是基于块的语言为了简化编程而去除的。因此，尚不清楚这种建模方法在基于块的编程语言上的表现如何。在本文中，我们研究了流行的基于块的编程语言Scratch的语言模型的适用性。我们使用最基本的语言模型n-gram模型和流行的深度学习模型transformer为Scratch程序建模。对代码完成和错误发现的示例任务的评估证实了块抑制了可预测性，但使用语言模型仍然是可行的。我们的发现为改进基于块的语言的工具和分析奠定了基础。,基于块的程序，Scratch，自然语言模型，代码完成，Bugram,,,
F54L6HS4,2023,https://doi.org/10.1109/ICSE48619.2023.00024,ICSE 2023,A Comprehensive Study of Real-World Bugs in Machine Learning Model Optimization,"Due to the great advance in machine learning (ML) techniques, numerous ML models are expanding their application domains in recent years. To adapt for resource-constrained platforms such as mobile and Internet of Things (IoT) devices, pre-trained models are often processed to enhance their efficiency and compactness, using optimization techniques such as pruning and quantization. Similar to the optimization process in other complex systems, e.g., program compilers and databases, optimizations for ML models can contain bugs, leading to severe consequences such as system crashes and financial loss. While bugs in training, compiling and deployment stages have been extensively studied, there is still a lack of systematic understanding and characterization of model optimization bugs (MOBs). In this work, we conduct the first empirical study to identify and characterize MOBs. We collect a comprehensive dataset containing 371 MOBs from TensorFlow and PyTorch, the most extensively used open-source ML frameworks, covering the entire development time span of their optimizers (May 2019 to August 2022). We then investigate the collected bugs from various perspectives, including their symptoms, root causes, life cycles, detection and fixes. Our work unveils the status quo of MOBs in the wild, and reveals their features on which future detection techniques can be based. Our findings also serve as a warning to the developers and the users of ML frameworks, and an appeal to our research community to enact dedicated countermeasures.","Machine Learning,Model Optimization,Bugs",机器学习模型优化中真实世界Bug的综合研究,由于机器学习（ML）技术的巨大进步，近年来许多ML模型正在扩展其应用领域。为了适应移动和物联网（IoT）设备等资源受限的平台，通常会使用修剪和量化等优化技术来处理预先训练的模型，以提高其效率和紧凑性。与其他复杂系统（如程序编译器和数据库）中的优化过程类似，ML模型的优化可能包含错误，导致系统崩溃和财务损失等严重后果。虽然训练、编译和部署阶段的错误已经得到了广泛的研究，但仍然缺乏对模型优化错误（MOB）的系统理解和表征。在这项工作中，我们进行了第一次实证研究，以确定和表征MOB。我们从使用最广泛的开源ML框架TensorFlow和PyTorch收集了一个包含371个MOB的综合数据集，涵盖了它们的优化器的整个开发时间跨度（2019年5月至2022年8月）。然后，我们从各个角度调查收集到的bug，包括它们的症状、根本原因、生命周期、检测和修复。我们的工作揭示了野生MOB的现状，并揭示了它们的特征，这些特征是未来检测技术的基础。我们的发现也对ML框架的开发人员和用户发出了警告，并呼吁我们的研究社区制定专门的对策。,机器学习，模型优化，Bug,,,
R5IHLUYR,2023,https://doi.org/10.1109/ICSE48619.2023.00218,ICSE 2023,Rules of Engagement: Why and How Companies Participate in OSS,"Company engagement in open source (OSS) is now the new norm. From large technology companies to startups, companies are participating in the OSS ecosystem by open-sourcing their technology, sponsoring projects through funding or paid developer time. However, our understanding of the OSS ecosystem is rooted in the “old world” model where individual contributors sustain OSS projects. In this work, we create a more comprehensive understanding of the hybrid OSS landscape by investigating what motivates companies to contribute and how they contribute to OSS. We conducted interviews with 20 participants who have different roles (e.g., CEO, OSPO Lead, Ecosystem Strategist) at 17 different companies of different sizes from large companies (e.g. Microsoft, RedHat, Google, Spotify) to startups. Data from semi-structured interviews reveal that company motivations can be categorized into four levels (Founders' Vision, Reputation, Business Advantage, and Reciprocity) and companies participate through different mechanisms (e.g., Developers' Time, Mentoring Time, Advocacy & Promotion Time), each of which tie to the different types of motivations. We hope our findings nudge more companies to participate in the OSS ecosystem, helping make it robust, diverse, and sustainable.","Open Source,OSS,companies in open source,motivations,diversity",参与规则：公司为什么以及如何参与OSS,公司参与开源（OSS）现在是一种新的规范。从大型科技公司到初创公司，公司都在通过开源技术、通过资助或付费开发时间赞助项目来参与OSS生态系统。然而，我们对OSS生态系统的理解植根于“旧世界”模式，即个人贡献者维持OSS项目。在这项工作中，我们通过调查公司贡献的动机以及他们如何贡献OSS，对混合OSS环境有了更全面的了解。我们采访了20名参与者，他们在17家不同规模的公司担任不同的角色（如首席执行官、OSPO负责人、生态系统策略师），从大公司（如微软、RedHat、谷歌、Spotify）到初创公司。来自半结构化访谈的数据显示，公司动机可分为四个层次（创始人的愿景、声誉、商业优势和互惠），公司通过不同的机制参与（例如，开发人员的时间、指导时间、倡导和推广时间），每种机制都与不同类型的动机有关。我们希望我们的发现能推动更多的公司参与OSS生态系统，帮助其变得强大、多样和可持续。,开源，OSS，公司在开源，动机，多样性,,,
G6WKITGB,2023,https://doi.org/10.1109/ICSE48619.2023.00020,ICSE 2023,Operand-Variation-Oriented Differential Analysis for Fuzzing Binding Calls in PDF Readers,"Binding calls of embedded scripting engines introduce a serious attack surface in PDF readers. To effectively test binding calls, the knowledge of parameter types is necessary. Unfortunately, due to the absence or incompleteness of documentation and the lack of sufficient samples, automatic type reasoning for binding call parameters is a big challenge. In this paper, we propose a novel operand-variation-oriented differential analysis approach, which automatically extracts features from execution traces as oracles for inferring parameter types. In particular, the parameter types of a binding call are inferred by executing the binding call with different values of different types and investigating which types cause an expected effect on the instruction operands. The inferred type information is used to guide the test generation in fuzzing. Through the evaluation on two popular PDF readers (Adobe Reader and Foxit Reader), we demonstrated the accuracy of our type reasoning method and the effectiveness of the inferred type information for improving fuzzing in both code coverage and vulnerability discovery. We found 38 previously unknown security vulnerabilities, 26 of which were certified with CVE numbers.","binding call,PDF reader,type reasoning,fuzzing",面向操作数变异的差分分析在PDF阅读器中模糊绑定调用,嵌入式脚本引擎的绑定调用在PDF阅读器中引入了严重的攻击面。为了有效地测试绑定调用，参数类型的知识是必要的。不幸的是，由于文档的缺乏或不完整以及缺乏足够的样本，绑定调用参数的自动类型推理是一个巨大的挑战。在本文中，我们提出了一种新的面向操作数变化的差分分析方法，该方法可以自动从执行轨迹中提取特征，作为推断参数类型的神谕。特别地，绑定调用的参数类型是通过使用不同类型的不同值执行绑定调用并调查哪些类型会对指令操作数产生预期影响来推断的。推断出的类型信息用于指导模糊化中的测试生成。通过对两种流行的PDF阅读器（Adobe Reader和Foxit Reader）的评估，我们证明了我们的类型推理方法的准确性和推断的类型信息在提高代码覆盖率和漏洞发现方面的模糊性方面的有效性。我们发现了38个以前未知的安全漏洞，其中26个通过了CVE编号认证。,绑定调用，PDF阅读器，类型推理，模糊处理,,,
VRLRHJA4,2023,https://doi.org/10.1109/ICSE48619.2023.00115,ICSE 2023,Data-driven Recurrent Set Learning For Non-termination Analysis,"Termination is a fundamental liveness property for program verification. In this paper, we revisit the problem of non-termination analysis and propose the first data-driven learning algorithm for synthesizing recurrent sets, where the non-terminating samples are effectively speculated by a novel method. To ensure convergence of learning, we develop a learning algorithm which is guaranteed to converge to a valid recurrent set if one exists, and thus establish its relative completeness. The methods are implemented in a prototype tool, and experimental results on public benchmarks show its efficacy in proving non-termination as it outperforms state-of-the-art tools, both in terms of cases solved and performance. Evaluation on non-linear programs also demonstrates its ability to handle complex programs.","program termination,recurrent set,data-driven approach,black-box learning",用于非终止分析的数据驱动递归集学习,终止是程序验证的一个基本活性属性。在本文中，我们重新审视了非终止分析的问题，并提出了第一个用于合成递归集的数据驱动学习算法，其中通过一种新的方法有效地推测了非终止样本。为了保证学习的收敛性，我们开发了一种学习算法，该算法保证收敛到一个有效的递归集（如果存在），从而建立其相对完整性。这些方法在原型工具中实现，在公共基准上的实验结果表明，它在证明非终结性方面的有效性，因为它在解决的案例和性能方面都优于最先进的工具。对非线性程序的评估也证明了它处理复杂程序的能力。,程序终止，循环集，数据驱动方法，黑匣子学习,,,
9HTHCER9,2023,https://doi.org/10.1109/ICSE48619.2023.00155,ICSE 2023,Many-Objective Reinforcement Learning for Online Testing of DNN-Enabled Systems,"Deep Neural Networks (DNNs) have been widely used to perform real-world tasks in cyber-physical systems such as Autonomous Driving Systems (ADS). Ensuring the correct behavior of such DNN-Enabled Systems (DES) is a crucial topic. Online testing is one of the promising modes for testing such systems with their application environments (simulated or real) in a closed loop, taking into account the continuous interaction between the systems and their environments. However, the environmental variables (e.g., lighting conditions) that might change during the systems' operation in the real world, causing the DES to violate requirements (safety, functional), are often kept constant during the execution of an online test scenario due to the two major challenges: (1) the space of all possible scenarios to explore would become even larger if they changed and (2) there are typically many requirements to test simultaneously. In this paper, we present MORLOT (Many-Objective Rein-forcement Learning for Online Testing), a novel online testing approach to address these challenges by combining Reinforcement Learning (RL) and many-objective search. MORLOT leverages RL to incrementally generate sequences of environmental changes while relying on many-objective search to determine the changes so that they are more likely to achieve any of the uncovered objectives. We empirically evaluate MORLOT using CARLA, a high-fidelity simulator widely used for autonomous driving research, integrated with Transfuser, a DNN-enabled ADS for end-to-end driving. The evaluation results show that MORLOT is significantly more effective and efficient than alternatives with a large effect size. In other words, MORLOT is a good option to test DES with dynamically changing environments while accounting for multiple safety requirements.","DNN Testing,Reinforcement learning,Many objective search,Self-driving cars,Online testing",DNN系统在线测试的多目标强化学习,深度神经网络（DNN）已被广泛用于在诸如自动驾驶系统（ADS）的网络物理系统中执行真实世界的任务。确保这种DNN启用系统（DES）的正确行为是一个至关重要的主题。在线测试是一种很有前途的模式，可以在闭环中测试这些系统的应用环境（模拟或真实），同时考虑到系统与其环境之间的连续交互。然而，在现实世界中系统运行期间可能发生变化的环境变量（例如，照明条件），导致DES违反要求（安全、功能），由于以下两个主要挑战，在在线测试场景的执行过程中通常保持不变：（1）如果所有可能的场景发生变化，则需要探索的空间将变得更大；（2）通常有许多需求需要同时测试。在本文中，我们提出了MORLOT（用于在线测试的多目标强化学习），这是一种新的在线测试方法，通过将强化学习（RL）和多目标搜索相结合来应对这些挑战。MORLOT利用RL逐步生成环境变化序列，同时依靠多个目标搜索来确定变化，以便它们更有可能实现任何未发现的目标。我们使用CARLA对MORLOT进行了实证评估，CARLA是一种广泛用于自动驾驶研究的高保真度模拟器，与Transuser集成，Transuser是一种用于端到端驾驶的DNN ADS。评估结果表明，MORLOT比效果大小较大的替代方案更有效。换句话说，MORLOT是一个很好的选择，可以在动态变化的环境中测试DES，同时考虑多种安全要求。,DNN测试，强化学习，多目标搜索，自动驾驶汽车，在线测试,,,
GGLL9FIP,2023,https://doi.org/10.1109/ICSE48619.2023.00069,ICSE 2023,Concrat: An Automatic C-to-Rust Lock API Translator for Concurrent Programs,"Concurrent programs suffer from data races. To prevent data races, programmers use locks. However, programs can eliminate data races only when they acquire and release correct locks at correct timing. The lock API of C, in which people have developed a large portion of legacy system programs, does not validate the correct use of locks. On the other hand, Rust, a recently developed system programming language, provides a lock API that guarantees the correct use of locks via type checking. This makes rewriting legacy system programs in Rust a promising way to retrofit safety into them. Unfortunately, manual C-to-Rust translation is extremely laborious due to the discrepancies between their lock APIs. Even the state-of-the-art automatic C-to-Rust translator retains the C lock API, expecting developers to replace them with the Rust lock API. In this work, we propose an automatic tool to replace the C lock API with the Rust lock API. It facilitates C-to-Rust translation of concurrent programs with less human effort than the current practice. Our tool consists of a Rust code transformer that takes a lock summary as an input and a static analyzer that efficiently generates precise lock summaries. We show that the transformer is scalable and widely applicable while preserving the semantics; it transforms 66 KLOC in 2.6 seconds and successfully handles 74% of real-world programs. We also show that the analyzer is scalable and precise; it analyzes 66 KLOC in 4.3 seconds.","Concurrent computing,Semantics,Transforms,Manuals,Programming,Aging,Transformers",Concrat:一种用于并发程序的自动C到Rust锁API转换器,并发程序受到数据竞争的影响。为了防止数据竞争，程序员使用锁。然而，只有当程序在正确的时间获取并释放正确的锁时，才能消除数据竞争。C的锁API，人们已经在其中开发了很大一部分遗留系统程序，并不能验证锁的正确使用。另一方面，最近开发的系统编程语言Rust提供了一个锁API，通过类型检查来保证锁的正确使用。这使得在Rust中重写遗留系统程序成为一种很有前途的安全改造方式。不幸的是，由于它们的锁API之间存在差异，手动C-to-Rust翻译非常费力。即使是最先进的自动C-to-Rust翻译器也保留了C锁API，希望开发人员用Rust锁API取代它们。在这项工作中，我们提出了一个自动工具，用Rust锁API代替C锁API。与目前的做法相比，它简化了并行程序的C到Rust翻译，所需的人力更少。我们的工具由一个Rust代码转换器和一个静态分析器组成，前者将锁摘要作为输入，后者有效地生成精确的锁摘要。我们证明了transformer是可扩展的，并且在保留语义的同时具有广泛的适用性；它在2.6秒内转换了66个KLOC，并成功处理了74%的真实世界程序。我们还表明，该分析仪具有可扩展性和精确性；它在4.3秒内分析了66个KLOC。,并发计算，语义，转换，手册，编程，老化，转换器,,,
I882Y6Z5,2023,https://doi.org/10.1109/ICSE48619.2023.00147,ICSE 2023,Measuring and Mitigating Gaps in Structural Testing,"Structural code coverage is a popular test adequacy metric that measures the percentage of program structure (e.g., statement, branch, decision) executed by a test suite. While structural coverage has several benefits, previous studies suggested that code coverage is not a good indicator of a test suite's fault-detection effectiveness as coverage computation does not consider test oracle quality. In this research, we formally define the coverage gap in structural testing as the percentage of program structure that is executed but not observed by any test oracles. Our large-scale empirical study of 13 Java applications, 16K test cases and 51.6K test assertions shows that even for mature test suites, the gap can be as high as 51 percentage points (pp) and 34pp on average. Our study reveals that the coverage gap strongly and negatively correlates with a test suite's fault-detection effectiveness. To mitigate gaps, we propose a lightweight static analysis of program dependencies to produce a ranked recommendation of test focus methods that can reduce the gap and improve test suite quality. When considering 34.8K assertions in the test suite as ground truth, the recommender suggests two-thirds of the focus methods written by developers within the top five recommendations.","code coverage,checked coverage,test oracles,mutation testing,fault-detection effectiveness",测量和缓解结构测试中的差距,结构代码覆盖率是一种流行的测试充分性度量，用于衡量测试套件执行的程序结构（如语句、分支、决策）的百分比。虽然结构覆盖有几个好处，但之前的研究表明，代码覆盖率并不是测试套件故障检测有效性的良好指标，因为覆盖率计算不考虑测试oracle质量。在这项研究中，我们将结构测试中的覆盖率差距正式定义为任何测试预言机执行但未观察到的程序结构的百分比。我们对13个Java应用程序、16K测试用例和51.6K测试断言的大规模实证研究表明，即使对于成熟的测试套件，差距也可能高达51个百分点（pp），平均为34pp。我们的研究表明，覆盖率差距与测试套件的故障检测有效性强且负相关。为了减少差距，我们提出了一种程序依赖性的轻量级静态分析，以产生测试焦点方法的排名推荐，从而减少差距并提高测试套件的质量。当将测试套件中的34.8K断言视为基本事实时，推荐人建议开发人员在前五个推荐中编写三分之二的焦点方法。,代码覆盖率，检查覆盖率，测试oracles，突变测试，故障检测有效性,,,
RQFEI28U,2023,https://doi.org/10.1109/ICSE48619.2023.00152,ICSE 2023,Aries: Efficient Testing of Deep Neural Networks via Labeling-Free Accuracy Estimation,"Deep learning (DL) plays a more and more important role in our daily life due to its competitive performance in industrial application domains. As the core of DL-enabled systems, deep neural networks (DNNs) need to be carefully evaluated to ensure the produced models match the expected requirements. In practice, the de facto standard to assess the quality of DNNs in the industry is to check their performance (accuracy) on a collected set of labeled test data. However, preparing such labeled data is often not easy partly because of the huge labeling effort, i.e., data labeling is labor-intensive, especially with the massive new incoming unlabeled data every day. Recent studies show that test selection for DNN is a promising direction that tackles this issue by selecting minimal representative data to label and using these data to assess the model. However, it still requires human effort and cannot be automatic. In this paper, we propose a novel technique, named Aries, that can estimate the performance of DNNs on new unlabeled data using only the information obtained from the original test data. The key insight behind our technique is that the model should have similar prediction accuracy on the data which have similar distances to the decision boundary. We performed a large-scale evaluation of our technique on two famous datasets, CIFAR-10 and Tiny-ImageNet, four widely studied DNN models including ResNetl0l and DenseNetl21, and 13 types of data transformation methods. Results show that the estimated accuracy by Aries is only 0.03% - 2.60% off the true accuracy. Besides, Aries also outperforms the state-of-the-art labeling-free methods in 50 out of 52 cases and selection-labeling-based methods in 96 out of 128 cases.","deep learning testing,performance estimation,distribution shift",Aries:通过无标记精度估计对深度神经网络进行有效测试,深度学习由于其在工业应用领域的竞争力，在我们的日常生活中发挥着越来越重要的作用。作为DL使能系统的核心，深度神经网络（DNN）需要仔细评估，以确保生成的模型符合预期要求。在实践中，行业中评估DNN质量的事实标准是在收集的一组标记测试数据上检查其性能（准确性）。然而，准备这样的标记数据通常并不容易，部分原因是巨大的标记工作，即数据标记是劳动密集型的，尤其是每天都有大量新的未标记数据。最近的研究表明，DNN的测试选择是一个很有前途的方向，可以通过选择最小的代表性数据来标记并使用这些数据来评估模型来解决这个问题。然而，它仍然需要人类的努力，而且不可能是自动的。在本文中，我们提出了一种名为Aries的新技术，该技术可以仅使用从原始测试数据中获得的信息来估计DNN在新的未标记数据上的性能。我们的技术背后的关键见解是，该模型应该对距离决策边界相似的数据具有相似的预测精度。我们在两个著名的数据集CIFAR-10和Tiny ImageNet、四个广泛研究的DNN模型（包括ResNetl0l和DenseNetl21）以及13种类型的数据转换方法上对我们的技术进行了大规模评估。结果表明，Aries的估计精度仅比真实精度低0.03%-2.60%。此外，Aries在52例中有50例优于最先进的无标记方法，在128例中有96例优于基于选择标记的方法。,深度学习测试，性能评估，分布转移,,,
J9G7C7Z4,2023,https://doi.org/10.1109/ICSE48619.2023.00216,ICSE 2023,Doppelgänger Test Generation for Revealing Bugs in Autonomous Driving Software,"Vehicles controlled by autonomous driving software (ADS) are expected to bring many social and economic benefits, but at the current stage not being broadly used due to concerns with regard to their safety. Virtual tests, where autonomous vehicles are tested in software simulation, are common practices because they are more efficient and safer compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, system-atically producing bug-revealing tests for ADS remains a major challenge. To address this challenge, we introduce DoppelTest, a test generation approach for ADSes that utilizes a genetic algorithm to discover bug-revealing violations by generating scenarios with multiple autonomous vehicles that account for traffic control (e.g., traffic signals and stop signs). Our extensive evaluation shows that DoppelTest can efficiently discover 123 bug-revealing violations for a production-grade ADS (Baidu Apollo) which we then classify into 8 unique bug categories.","cyber-physical systems,autonomous driving systems,search-based software testing",Doppelgänger测试生成，用于揭示自动驾驶软件中的错误,自动驾驶软件（ADS）控制的车辆有望带来许多社会和经济效益，但由于对其安全性的担忧，目前尚未得到广泛使用。虚拟测试是自动驾驶汽车在软件模拟中进行测试的常见做法，因为与现场操作测试相比，虚拟测试更高效、更安全。具体来说，基于搜索的方法用于查找特别关键的情况。这些方法提供了一个自动生成测试的机会；然而，系统地为ADS生成漏洞揭示测试仍然是一个主要挑战。为了应对这一挑战，我们引入了DoppelTest，这是一种ADS的测试生成方法，它利用遗传算法，通过生成多辆自动驾驶汽车的场景来发现漏洞暴露违规行为，这些场景涉及交通控制（例如，交通信号灯和停车标志）。我们的广泛评估表明，DoppelTest可以有效地发现生产级ADS（百度Apollo）的123个漏洞暴露违规行为，然后我们将其分为8个独特的漏洞类别。,赛博物理系统，自动驾驶系统，基于搜索的软件测试,,,
IGIUYA2M,2023,https://doi.org/10.1109/ICSE48619.2023.00027,ICSE 2023,PExReport: Automatic Creation of Pruned Executable Cross-Project Failure Reports,"Modern software development extensively depends on existing libraries written by other developer teams from the same or a different organization. When a developer executes the software, the execution trace may go across the boundaries of multiple software products and create cross-project failures (CPFs). Existing studies show that a stand-alone executable failure report may enable the most effective communication, but creating such a report is often challenging due to the complicated files and dependencies interactions in the software ecosystems. In this paper, to solve the CPF report trilemma, we developed PExReport, which automatically creates stand-alone executable CPF reports. PExReport leverages build tools to prune source code and dependencies, and further analyzes the build process to create a pruned build environment for reproducing the CPF. We performed an evaluation on 74 software project issues with 198 CPFs, and the evaluation results show that PExReport can create executable CPF reports for 184 out of 198 test failures in our dataset, with an average reduction of 72.97% on source classes and the classes in internal JARs.","cross-project failure,executable failure report,failure reproduction,build tool,build environment,debloating",PExReport：自动创建修剪的可执行跨项目故障报告,现代软件开发在很大程度上依赖于来自同一或不同组织的其他开发团队编写的现有库。当开发人员执行软件时，执行跟踪可能会跨越多个软件产品的边界，并产生跨项目故障（CPF）。现有研究表明，独立的可执行故障报告可以实现最有效的通信，但由于软件生态系统中复杂的文件和依赖关系交互，创建这样的报告通常具有挑战性。在本文中，为了解决CPF报告的三重困境，我们开发了PExReport，它可以自动创建独立的可执行CPF报告。PExReport利用构建工具来修剪源代码和依赖项，并进一步分析构建过程，以创建一个修剪后的构建环境来复制CPF。我们用198个CPF对74个软件项目问题进行了评估，评估结果表明，PExReport可以为我们数据集中198个测试失败中的184个创建可执行的CPF报告，源类和内部JAR中的类的平均减少72.97%。,跨项目故障，可执行故障报告，故障再现，构建工具，构建环境，去浮动,,,
39724W78,2023,https://doi.org/10.1109/ICSE48619.2023.00196,ICSE 2023,Context-aware Bug Reproduction for Mobile Apps,"Bug reports are vital for software maintenance that allow the developers being informed of the problems encountered in the software. Before bug fixing, developers need to reproduce the bugs which is an extremely time-consuming and tedious task, and it is highly expected to automate this process. However, it is challenging to do so considering the imprecise or incomplete natural language described in reproducing steps, and the missing or ambiguous single source of information in GUI components. In this paper, we propose a context-aware bug reproduction approach ScopeDroid which automatically reproduces crashes from textual bug reports for mobile apps. It first constructs a state transition graph (STG) and extracts the contextual information of components. We then design a multi-modal neural matching network to derive the fuzzy matching matrix between all candidate GUI events and reproducing steps. With the STG and matching information, it plans the exploration path for reproducing the bug, and enriches the initial STG iteratively. We evaluate the approach on 102 bug reports from 69 popular Android apps, and it successfully reproduces 63.7% of the crashes, outper-forming the state-of-the-art baselines by 32.6% and 38.3%. We also evaluate the usefulness and robustness of ScopeDroid with promising results. Furthermore, to train the neural matching network, we develop a heuristic-based automated training data generation method, which can potentially motivate and facilitate other activities as user interface operations.","Software maintenance,Computer bugs,Neural networks,Training data,Robustness,Mobile applications,Data mining",移动应用程序的上下文感知错误再现,Bug报告对于软件维护至关重要，它可以让开发人员了解软件中遇到的问题。在修复错误之前，开发人员需要重现错误，这是一项极其耗时和乏味的任务，并且高度期望自动化这一过程。然而，考虑到复制步骤中描述的不精确或不完整的自然语言，以及GUI组件中缺少或不明确的单一信息源，这样做是具有挑战性的。在本文中，我们提出了一种上下文感知的错误再现方法ScopeDroid，该方法可以从移动应用程序的文本错误报告中自动再现崩溃。它首先构造了一个状态转换图（STG），并提取了组件的上下文信息。然后，我们设计了一个多模式神经匹配网络，以导出所有候选GUI事件和再现步骤之间的模糊匹配矩阵。利用STG和匹配信息，它规划了再现bug的探索路径，并迭代地丰富了初始STG。我们在69个流行的Android应用程序的102个错误报告中评估了该方法，它成功地再现了63.7%的崩溃，分别比最先进的基线高出32.6%和38.3%。我们还评估了ScopeDroid的有用性和稳健性，并取得了有希望的结果。此外，为了训练神经匹配网络，我们开发了一种基于启发式的自动训练数据生成方法，该方法可以潜在地激励和促进作为用户界面操作的其他活动。,软件维护，计算机错误，神经网络，训练数据，健壮性，移动应用程序，数据挖掘,,,
RS2L4NGA,2023,https://doi.org/10.1109/ICSE48619.2023.00082,ICSE 2023,SemParser: A Semantic Parser for Log Analytics,"Logs, being run-time information automatically generated by software, record system events and activities with their timestamps. Before obtaining more insights into the run-time status of the software, a fundamental step of log analysis, called log parsing, is employed to extract structured templates and parameters from the semi-structured raw log messages. However, current log parsers are all syntax-based and regard each message as a character string, ignoring the semantic information included in parameters and templates. Thus, we propose the first semantic-based parser SemParser to unlock the critical bottleneck of mining semantics from log messages. It contains two steps, an end-to-end semantics miner and a joint parser. Specifically, the first step aims to identify explicit semantics inside a single log, and the second step is responsible for jointly inferring implicit semantics and computing structural outputs according to the contextual knowledge base of the logs. To analyze the effectiveness of our semantic parser, we first demonstrate that it can derive rich semantics from log messages collected from six widely-applied systems with an average F1 score of 0.985. Then, we conduct two representative downstream tasks, showing that current downstream models improve their performance with appropriately extracted semantics by 1.2%-11.7% and 8.65% on two anomaly detection datasets and a failure identification dataset, respectively. We believe these findings provide insights into semantically understanding log messages for the log analysis community.","log parsing,semantic parser,log analytics",SemParser：一个用于日志分析的语义分析器,日志是由软件自动生成的运行时信息，记录系统事件和活动及其时间戳。在获得对软件运行时状态的更多见解之前，日志分析的一个基本步骤，称为日志解析，用于从半结构化的原始日志消息中提取结构化模板和参数。然而，当前的日志解析器都是基于语法的，并将每条消息视为字符串，忽略了参数和模板中包含的语义信息。因此，我们提出了第一个基于语义的解析器SemParser，以解开从日志消息中挖掘语义的关键瓶颈。它包含两个步骤，一个端到端语义挖掘器和一个联合解析器。具体来说，第一步旨在识别单个日志内的显式语义，第二步负责根据日志的上下文知识库联合推断隐式语义并计算结构输出。为了分析我们的语义解析器的有效性，我们首先证明了它可以从六个广泛应用的系统收集的日志消息中获得丰富的语义，平均F1得分为0.985。然后，我们进行了两个具有代表性的下游任务，表明当前的下游模型在两个异常检测数据集和一个故障识别数据集上通过适当提取语义分别提高了1.2%-11.7%和8.65%的性能。我们相信，这些发现为日志分析社区从语义上理解日志消息提供了见解。,日志解析，语义解析器，日志分析,,,
MFBL8KCW,2023,https://doi.org/10.1109/ICSE48619.2023.00093,ICSE 2023,Decomposing a Recurrent Neural Network into Modules for Enabling Reusability and Replacement,"Can we take a recurrent neural network (RNN) trained to translate between languages and augment it to support a new natural language without retraining the model from scratch? Can we fix the faulty behavior of the RNN by replacing portions associated with the faulty behavior? Recent works on decomposing a fully connected neural network (FCNN) and convolutional neural network (CNN) into modules have shown the value of engineering deep models in this manner, which is standard in traditional SE but foreign for deep learning models. However, prior works focus on the image-based multi-class classification problems and cannot be applied to RNN due to (a) different layer structures, (b) loop structures, (c) different types of input-output architectures, and (d) usage of both non-linear and logistic activation functions. In this work, we propose the first approach to decompose an RNN into modules. We study different types of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN modules can be reused and replaced in various scenarios. We evaluate our approach against 5 canonical datasets (i.e., Math QA, Brown Corpus, Wiki-toxicity, Cline OOS, and Tatoeba) and 4 model variants for each dataset. We found that decomposing a trained model has a small cost (Accuracy: -0.6%, BLEU score: +0.10%). Also, the decomposed modules can be reused and replaced without needing to retrain.","recurrent neural networks,decomposing,modules,modularity",将递归神经网络分解为模块以实现可重用性和可替换性,我们能在不从头开始重新训练模型的情况下，将经过训练的递归神经网络（RNN）在语言之间进行翻译，并对其进行扩充以支持新的自然语言吗？我们可以通过替换与错误行为相关的部分来修复RNN的错误行为吗？最近关于将全连接神经网络（FCNN）和卷积神经网络（CNN）分解为模块的工作表明了以这种方式设计深度模型的价值，这在传统SE中是标准的，但在深度学习模型中却是外来的。然而，由于（a）不同的层结构，（b）循环结构，（c）不同类型的输入输出结构，以及（d）非线性和逻辑激活函数的使用，现有的工作集中于基于图像的多类分类问题，并且不能应用于RNN。在这项工作中，我们提出了第一种将RNN分解为模块的方法。我们研究了不同类型的RNN，即香草、LSTM和GRU。此外，我们还展示了如何在各种场景中重用和替换此类RNN模块。我们针对5个规范数据集（即Math QA、Brown语料库、Wiki毒性、Cline OOS和Tatoeba）和每个数据集的4个模型变体来评估我们的方法。我们发现，分解训练好的模型成本很小（准确率：-0.6%，BLEU得分：+0.10%）。此外，分解后的模块可以重复使用和更换，而无需重新培训。,递归神经网络，分解，模块，模块,,,
SZ8DGE7H,2023,https://doi.org/10.1109/ICSE48619.2023.00153,ICSE 2023,CC: Causality-Aware Coverage Criterion for Deep Neural Networks,"Deep neural network (DNN) testing approaches have grown fast in recent years to test the correctness and robustness of DNNs. In particular, DNN coverage criteria are frequently used to evaluate the quality of a test suite, and a number of coverage criteria based on neuron-wise, layer-wise, and path-/trace-wise coverage patterns have been published to date. However, we see that existing criteria are insufficient to represent how one neuron would influence subsequent neurons; hence, we lack a concept of how neurons, when functioning as causes and effects, might jointly make a DNN prediction. Given recent advances in interpreting DNN internals using causal inference, we present the first causality-aware DNN coverage criterion, which evaluates a test suite by quantifying the extent to which the suite provides new causal relations for testing DNNs. Performing standard causal inference on DNNs presents both theoretical and practical hurdles. We introduce CC (causal coverage), a practical and efficient coverage criterion that integrates a set of optimizations using DNN domain-specific knowledge. We illustrate the efficacy of CC using diverse, real-world inputs and adversarial inputs, such as adversarial examples (AEs) and backdoor inputs. We demonstrate that CC outperforms previous DNN criteria under various settings with moderate cost.","machine learning testing,Causality Analysis,Software Engineering",CC：深度神经网络因果关系感知覆盖准则,近年来，深度神经网络（DNN）测试方法发展迅速，用于测试DNN的正确性和鲁棒性。特别是，DNN覆盖标准经常用于评估测试套件的质量，迄今为止，已经发布了许多基于神经元、层和路径/跟踪覆盖模式的覆盖标准。然而，我们看到现有的标准不足以代表一个神经元将如何影响随后的神经元；因此，我们缺乏神经元在作为因果发挥作用时如何共同做出DNN预测的概念。鉴于使用因果推理解释DNN内部的最新进展，我们提出了第一个因果关系感知的DNN覆盖标准，该标准通过量化测试套件为测试DNN提供新因果关系的程度来评估测试套件。在DNN上执行标准因果推理存在理论和实践障碍。我们介绍了CC（因果覆盖），这是一种实用高效的覆盖标准，它集成了一组使用DNN领域特定知识的优化。我们使用不同的真实世界输入和对抗性输入（如对抗性示例（AE）和后门输入）来说明CC的功效。我们证明了CC在各种成本适中的设置下都优于以前的DNN标准。,机器学习测试，因果关系分析，软件工程,,,
ZFJJQ796,2023,https://doi.org/10.1109/ICSE48619.2023.00016,ICSE 2023,Detecting JVM JIT Compiler Bugs via Exploring Two-Dimensional Input Spaces,"Java Virtual Machine (JVM) is the fundamental software system that supports the interpretation and execution of Java bytecode. To support the surging performance demands for the increasingly complex and large-scale Java programs, Just-In-Time (JIT) compiler was proposed to perform sophisticated runtime optimization. However, this inevitably induces various bugs, which are becoming more pervasive over the decades and can often cause significant consequences. To facilitate the design of effective and efficient testing techniques to detect JIT compiler bugs. This study first performs a preliminary study aiming to understand the characteristics of JIT compiler bugs and the corresponding triggering test cases. Inspired by the empirical findings, we propose JOpFuzzer, a new JVM testing approach with a specific focus on JIT compiler bugs. The main novelty of JOpFuzzer is embodied in three aspects. First, besides generating new seeds, JOpFuzzer also searches for diverse configurations along the new dimension of optimization options. Second, JOpFuzzer learns the correlations between various code features and different optimization options to guide the process of seed mutation and option exploration. Third, it leverages the profile data, which can reveal the program execution information, to guide the fuzzing process. Such nov-elties enable JOpFuzzer to effectively and efficiently explore the two-dimensional input spaces. Extensive evaluation shows that JOpFuzzer outperforms the state-of-the-art approaches in terms of the achieved code coverages. More importantly, it has detected 41 bugs in OpenJDK, and 25 of them have already been confirmed or fixed by the corresponding developers.","JVM,JIT Compiler,JVM Testing",通过探索二维输入空间检测JVM JIT编译器错误,Java虚拟机（JVM）是支持Java字节码解释和执行的基本软件系统。为了支持日益复杂和大规模的Java程序的性能需求，提出了实时（JIT）编译器来执行复杂的运行时优化。然而，这不可避免地会引发各种各样的错误，这些错误在几十年里变得越来越普遍，并且往往会造成重大后果。为了便于设计有效和高效的测试技术来检测JIT编译器错误。本研究首先进行了初步研究，旨在了解JIT编译器错误的特征以及相应的触发测试用例。受经验发现的启发，我们提出了JOpFuzzer，这是一种新的JVM测试方法，专门关注JIT编译器的错误。JOpFuzzer的主要新颖性体现在三个方面。首先，除了生成新的种子，JOpFuzzer还沿着优化选项的新维度搜索不同的配置。其次，JOpFuzzer学习各种代码特征和不同优化选项之间的相关性，以指导种子变异和选项探索的过程。第三，它利用可以揭示程序执行信息的概要文件数据来指导模糊化过程。这种新技术使JOpFuzzer能够有效地探索二维输入空间。广泛的评估表明，JOpFuzzer在实现的代码覆盖率方面优于最先进的方法。更重要的是，它在OpenJDK中检测到了41个错误，其中25个已经被相应的开发人员确认或修复。,JVM，JIT编译器，JVM测试,,,
HAYZMWEJ,2023,https://doi.org/10.1109/ICSE48619.2023.00125,ICSE 2023,Impact of Code Language Models on Automated Program Repair,"Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task. Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31%-1,267% improvement to CLMs and enables them to fix 46%-164 % more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs. This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem.","Automated Program Repair,Code Language Model,Fine-Tuning,Deep Learning",代码语言模型对程序自动修复的影响,自动程序修复（APR）旨在通过为有缺陷的程序生成补丁来帮助开发人员提高软件可靠性。尽管许多代码语言模型（CLM）已经开发出来，并在许多软件任务（如代码完成）中有效，但很少有全面、深入的工作来评估CLM的修复能力，并为APR任务微调CLM。首先，这项工作是第一次在四个APR基准上评估十个CLM，这表明令人惊讶的是，最好的CLM比最先进的基于深度学习（DL）的APR技术多修复了72%的错误。其次，我们在本文中创建了四个APR基准之一，以避免数据泄露，从而进行公平评估。第三，这是第一项利用APR训练数据对CLM进行微调的工作，这表明微调为CLM带来了31%-1267%的改进，并使它们能够比现有的基于DL的APR技术多修复46%-164%的错误。第四，这项工作研究了bug线的影响，表明CLM不能很好地利用bug线来修复bug，但微调后的CLM可能会过度依赖bug线。最后，本文分析了不同CLM的大小、时间和存储效率。这项工作为APR领域展示了有希望的方向，例如用APR特定的设计微调CLM，还提高了对CLM公平和全面评估的认识，并呼吁对预训练数据中使用的开源存储库进行更透明的报告，以解决数据泄露问题。,自动程序修复，代码语言模型，微调，深度学习,,,
GLP374HH,2023,https://doi.org/10.1109/ICSE48619.2023.00111,ICSE 2023,KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair,"Automated Program Repair (APR) improves soft-ware reliability by generating patches for a buggy program automatically. Recent APR techniques leverage deep learning (DL) to build models to learn to generate patches from existing patches and code corpora. While promising, DL-based APR techniques suffer from the abundant syntactically or semantically incorrect patches in the patch space. These patches often disobey the syntactic and semantic domain knowledge of source code and thus cannot be the correct patches to fix a bug. We propose a DL-based APR approach KNOD, which in-corporates domain knowledge to guide patch generation in a direct and comprehensive way. KNOD has two major novelties, including (1) a novel three-stage tree decoder, which directly generates Abstract Syntax Trees of patched code according to the inherent tree structure, and (2) a novel domain-rule distillation, which leverages syntactic and semantic rules and teacher-student distributions to explicitly inject the domain knowledge into the decoding procedure during both the training and inference phases. We evaluate KNOD on three widely-used benchmarks. KNOD fixes 72 bugs on the Defects4J v1.2, 25 bugs on the QuixBugs, and 50 bugs on the additional Defects4J v2.0 benchmarks, outperforming all existing APR tools.","Automated Program Repair,Abstract Syntax Tree,Deep Learning",KNOD：用于自动程序修复的领域知识提取树解码器,自动程序修复（APR）通过自动为有缺陷的程序生成补丁来提高软件的可靠性。最近的APR技术利用深度学习（DL）来构建模型，以学习从现有补丁和代码库中生成补丁。尽管有前景，但基于DL的APR技术在补丁空间中存在大量语法或语义不正确的补丁。这些补丁通常不符合源代码的语法和语义领域知识，因此不能成为修复错误的正确补丁。我们提出了一种基于DL的APR方法KNOD，该方法在企业领域知识中以直接和全面的方式指导补丁生成。KNOD有两个主要的新颖性，包括（1）一种新颖的三级树解码器，它根据固有的树结构直接生成修补代码的抽象语法树；以及（2）一种新的域规则提取，其利用句法和语义规则以及师生分布来在训练和推理阶段期间将领域知识显式地注入到解码过程中。我们在三个广泛使用的基准上评估KNOD。KNOD修复了Defects4J v1.2上的72个错误，QuixBugs上的25个错误，以及其他Defects4Jv2.0基准测试上的50个错误，性能优于所有现有的APR工具。,自动程序修复，抽象语法树，深度学习,,,
PZ7KLSI9,2023,https://doi.org/10.1109/ICSE48619.2023.00206,ICSE 2023,An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry,"Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems. In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries.","Software reuse,Empirical software engineering,Machine learning,Deep learning,Software supply chain,Engineering decision making,Cybersecurity,Trust",拥抱人脸深度学习模型注册中预训练模型重用的实证研究,深度神经网络（DNN）正被用作软件系统中的组件。随着最先进的体系结构变得越来越复杂，从头开始创建和专门化DNN变得越来越困难。沿着传统软件工程的道路，机器学习工程师已经开始重用大规模的预训练模型（PTM），并为下游任务微调这些模型。先前的工作已经研究了传统软件包的重用实践，以指导软件工程师更好地进行包维护和依赖性管理。我们缺乏类似的知识基础来指导预先训练的模型生态系统中的行为。在这项工作中，我们首次对PTM重用进行了实证研究。我们采访了来自最受欢迎的PTM生态系统“拥抱脸”的12名从业者，以了解PTM重用的实践和挑战。根据这些数据，我们对PTM重用的决策过程进行了建模。基于已确定的实践，我们描述了模型重用的有用属性，包括来源、再现性和可移植性。PTM重用面临的三个挑战是属性缺失、声称的性能与实际性能之间的差异以及模型风险。我们通过拥抱脸生态系统中的系统测量来证实这些已确定的挑战。我们的工作为通过自动测量有用属性和潜在攻击来优化深度学习生态系统的未来方向提供了信息，并展望了未来对模型注册的基础设施和标准化的研究。,软件复用，经验软件工程，机器学习，深度学习，软件供应链，工程决策，网络安全，信任,,,
CGJ6TWVR,2023,https://doi.org/10.1109/ICSE48619.2023.00045,ICSE 2023,Evaluating and Improving Hybrid Fuzzing,"To date, various hybrid fuzzers have been proposed for maximal program vulnerability exposure by integrating the power of fuzzing strategies and concolic executors. While the existing hybrid fuzzers have shown their superiority over conventional coverage-guided fuzzers, they seldom follow equivalent evaluation setups, e.g., benchmarks and seed corpora. Thus, there is a pressing need for a comprehensive study on the existing hybrid fuzzers to provide implications and guidance for future research in this area. To this end, in this paper, we conduct the first extensive study on state-of-the-art hybrid fuzzers. Surprisingly, our study shows that the performance of existing hybrid fuzzers may not well generalize to other experimental settings. Meanwhile, their performance advantages over conventional coverage-guided fuzzers are overall limited. In addition, instead of simply updating the fuzzing strategies or concolic executors, updating their coordination modes potentially poses crucial performance impact of hybrid fuzzers. Accordingly, we propose CoFuzz to improve the effectiveness of hybrid fuzzers by upgrading their coordination modes. Specifically, based on the baseline hybrid fuzzer QSYM, CoFuzz adopts edge-oriented scheduling to schedule edges for applying concolic execution via an online linear regression model with Stochastic Gradient Descent. It also adopts sampling-augmenting synchronization to derive seeds for applying fuzzing strategies via the interval path abstraction and John walk as well as incrementally updating the model. Our evaluation results indicate that CoFuzz can significantly increase the edge coverage (e.g., 16.31% higher than the best existing hybrid fuzzer in our study) and expose around 2X more unique crashes than all studied hybrid fuzzers. Moreover, CoFuzz successfully detects 37 previously unknown bugs where 30 are confirmed with 8 new CVEs and 20 are fixed.","Schedules,Image edge detection,Computer bugs,Linear regression,Stochastic processes,Pressing,Fuzzing",混合引信的评价与改进,到目前为止，已经提出了各种混合模糊器，通过集成模糊策略和一致执行器的能力来最大限度地暴露程序漏洞。虽然现有的混合模糊器已经显示出其优于传统的覆盖引导模糊器的优势，但它们很少遵循等效的评估设置，例如基准和种子语料库。因此，迫切需要对现有的混合模糊器进行全面的研究，为该领域的未来研究提供启示和指导。为此，在本文中，我们首次对最先进的混合模糊器进行了广泛的研究。令人惊讶的是，我们的研究表明，现有混合模糊器的性能可能无法很好地推广到其他实验环境中。同时，与传统的覆盖引导模糊器相比，它们的性能优势总体上是有限的。此外，更新它们的协调模式可能会对混合模糊器的性能产生关键影响，而不是简单地更新模糊策略或一致执行器。因此，我们提出了CoFuzz，通过升级混合模糊器的协调模式来提高其有效性。具体而言，基于基线混合模糊器QSYM，CoFuzz采用面向边缘的调度，通过具有随机梯度下降的在线线性回归模型来调度边缘，以应用协同执行。它还采用采样增强同步来导出种子，用于通过区间路径抽象和John walk应用模糊策略，以及增量更新模型。我们的评估结果表明，CoFuzz可以显著增加边缘覆盖率（例如，比我们研究中现有的最佳混合模糊器高16.31%），并比所有研究的混合模糊器暴露出大约2倍的独特崩溃。此外，CoFuzz成功检测到37个以前未知的错误，其中30个被8个新的CVE确认，20个被修复。,时间表，图像边缘检测，计算机错误，线性回归，随机过程，按下，模糊,,,
KCHSUCL4,2023,https://doi.org/10.1109/ICSE48619.2023.00144,ICSE 2023,Dependency Facade: The Coupling and Conflicts between Android Framework and Its Customization,"Mobile device vendors develop their customized Android OS (termed downstream) based on Google Android (termed upstream) to support new features. During daily independent development, the downstream also periodically merges changes of a new release from the upstream into its development branches, keeping in sync with the upstream. Due to a large number of commits to be merged, heavy code conflicts would be reported if auto-merge operations failed. Prior work has studied conflicts in this scenario. However, it is still unclear about the coupling between the downstream and the upstream (We term this coupling as the dependency facade), as well as how merge conflicts are related to this coupling. To address this issue, we first propose the DepFCD to reveal the dependency facade from three aspects, including interface-level dependencies that indicate a clear design boundary, intrusion-level dependencies which blur the boundary, and dependency constraints imposed by the upstream non-SDK restrictions. We then empirically investigate these three aspects (RQ1, RQ2, RQ3) and merge conflicts (RQ4) on the dependency facade. To support the study, we collect four open-source downstream projects and one industrial project, with 15 downstream and 15 corresponding upstream versions. Our study reveals interesting observations and suggests earlier mitigation of merge conflicts through a well-managed dependency facade. Our study will benefit the research about the coupling between upstream and downstream as well as the downstream maintenance practice.","Android,downstream,dependencies,merge conflict",依赖外观：安卓框架与定制的耦合与冲突,移动设备供应商基于谷歌安卓（称为上游）开发定制的安卓操作系统（称为下游），以支持新功能。在日常独立开发过程中，下游也会定期将来自上游的新版本的变化合并到其开发分支中，与上游保持同步。由于要合并大量提交，如果自动合并操作失败，则会报告严重的代码冲突。先前的工作已经研究了这种情况下的冲突。然而，目前还不清楚下游和上游之间的耦合（我们将这种耦合称为依赖门面），以及合并冲突如何与这种耦合相关。为了解决这个问题，我们首先提出了DepFCD，从三个方面揭示依赖关系的外观，包括指示清晰设计边界的接口级依赖关系、模糊边界的入侵级依赖关系以及上游非SDK限制施加的依赖关系约束。然后，我们实证研究了这三个方面（RQ1、RQ2、RQ3）和依赖门面上的合并冲突（RQ4）。为了支持这项研究，我们收集了四个开源下游项目和一个工业项目，其中有15个下游版本和15个相应的上游版本。我们的研究揭示了有趣的观察结果，并建议通过管理良好的依赖关系来更早地缓解合并冲突。我们的研究将有利于上下游耦合的研究以及下游维护实践。,Android，下游，依赖关系，合并冲突,,,
X9JSWTCJ,2023,https://doi.org/10.1109/ICSE48619.2023.00194,ICSE 2023,Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction,"Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose Libro, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of Libro shows that, on the widely studied Defects4J benchmark, Libro can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate Libro against 31 bug reports submitted after the collection of the LLM training data terminated: Libro produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show Libro has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.","test generation,natural language processing,software engineering",大型语言模型是少数测试者：探索基于LLM的通用错误复制,已经开发了许多自动测试生成技术来帮助开发人员编写测试。为了促进完全自动化，大多数现有技术的目的要么是增加覆盖范围，要么是生成探索性输入。然而，现有的测试生成技术在很大程度上无法实现更多的语义目标，例如生成测试来重现给定的错误报告。尽管如此，复制bug仍然很重要，因为我们的实证研究表明，由于问题而在开源存储库中添加的测试数量约为相应项目测试套件大小的28%。同时，由于难以将错误报告中的预期程序语义转换为测试预言机，现有的故障再现技术倾向于专门处理程序崩溃，这是所有错误报告的一小部分。为了从一般的错误报告中自动生成测试，我们提出了Libro，这是一个使用大型语言模型（LLM）的框架，已经证明它能够执行与代码相关的任务。由于LLM本身无法执行目标错误代码，我们将重点放在后处理步骤上，这些步骤有助于我们辨别LLM何时有效，并根据其有效性对生成的测试进行排名。我们对Libro的评估表明，在广泛研究的Defects4J基准上，Libro可以为33%的研究案例（750个案例中的251个）生成故障再现测试案例，同时建议首先对149个bug进行bug再现测试。为了减少数据污染（即LLM只需部分或全部记住测试代码的可能性），我们还根据LLM训练数据收集终止后提交的31份错误报告对Libro进行了评估：Libro对32%的研究错误报告进行了错误再现测试。总的来说，我们的结果表明Libro有潜力通过自动从错误报告中生成测试来显著提高开发人员的效率。,测试生成，自然语言处理，软件工程,,,
M3P3WNDJ,2023,https://doi.org/10.1109/ICSE48619.2023.00035,ICSE 2023,Smartmark: Software Watermarking Scheme for Smart Contracts,"A smart contract is a self-executing program on a blockchain to ensure an immutable and transparent agreement without the involvement of intermediaries. Despite its growing popularity for many blockchain platforms like Ethereum, no technical means is available even when a smart contract requires to be protected from being copied. One promising direction to claim a software ownership is software watermarking. However, applying existing software watermarking techniques is challenging because of the unique properties of a smart contract, such as a code size constraint, non-free execution cost, and no support for dynamic allocation under a virtual machine environment. This paper introduces a novel software watermarking scheme, dubbed Smartmark, aiming to protect the ownership of a smart contract against a pirate activity. Smartmark builds the control flow graph of a target contract runtime bytecode, and locates a collection of bytes that are randomly elected for representing a watermark. We implement a full-fledged prototype for Ethereum, applying Smartmark to 27,824 unique smart contract bytecodes. Our empirical results demonstrate that Smartmark can effectively embed a watermark into a smart contract and verify its presence, meeting the requirements of credibility and imperceptibility while incurring an acceptable performance degradation. Besides, our security analysis shows that Smartmark is resilient against viable watermarking corruption attacks; e.g., a large number of dummy opcodes are needed to disable a watermark effectively, resulting in producing an illegitimate smart contract clone that is not economical.","Smart contract,Software watermarking,Blockchain,Software copyrights",Smartmark：用于智能合约的软件水印方案,智能合约是区块链上的一种自动执行程序，用于确保在没有中介参与的情况下达成不可变和透明的协议。尽管它在以太坊等许多区块链平台上越来越受欢迎，但即使智能合约需要保护以免被复制，也没有可用的技术手段。声称软件所有权的一个有希望的方向是软件水印。然而，由于智能合约的独特特性，如代码大小约束、非免费执行成本以及不支持虚拟机环境下的动态分配，应用现有的软件水印技术是具有挑战性的。本文介绍了一种新的软件水印方案，称为Smartmark，旨在保护智能合约的所有权不受盗版活动的影响。Smartmark构建目标合约运行时字节码的控制流图，并定位随机选择的用于表示水印的字节集合。我们实现了一个完整的以太坊原型，将Smartmark应用于27824个独特的智能合约字节码。我们的实证结果表明，Smartmark可以有效地将水印嵌入智能合约中并验证其存在，满足可信度和不可见性的要求，同时导致可接受的性能下降。此外，我们的安全分析表明，Smartmark对可行的水印破坏攻击具有弹性；例如，需要大量的伪操作码来有效地禁用水印，从而产生不经济的非法智能合约克隆。,智能合约，软件水印，区块链，软件版权,,,
7IPPTLLP,2023,https://doi.org/10.1109/ICSE48619.2023.00187,ICSE 2023,Diver: Oracle-Guided SMT Solver Testing with Unrestricted Random Mutations,"We present Diver, a novel technique for effectively finding critical bugs in SMT solvers. Ensuring the correctness of SMT solvers is becoming increasingly important as many applications use solvers as a foundational basis. In response, several approaches for testing SMT solvers, which are classified into differential testing and oracle-guided approaches, have been proposed until recently. However, they are still unsatisfactory in that (1) differential testing approaches cannot validate unique yet important features of solvers, and (2) oracle-guided approaches cannot generate diverse tests due to their reliance on limited mutation rules. Diver aims to complement these shortcomings, particularly focusing on finding bugs that are missed by existing approaches. To this end, we present a new testing technique that performs oracle-guided yet unrestricted random mutations. We have used Diver to validate the most recent versions of three popular SMT solvers: CVC5, Z3 and dReal. In total, Diver found 25 new bugs, of which 21 are critical and directly affect the reliability of the solvers. We also empirically prove DIVER's own strength by showing that existing tools are unlikely to find the bugs discovered by Diver.","software testing,fuzzing,SMT solver",Diver:Oracle引导的无限制随机变异SMT解算器测试,我们介绍了Diver，这是一种在SMT求解器中有效发现关键错误的新技术。随着许多应用程序将解算器作为基础，确保SMT解算器的正确性变得越来越重要。作为回应，直到最近，还提出了几种测试SMT求解器的方法，分为差分测试和oracle引导方法。然而，它们仍然不令人满意，因为（1）差异测试方法无法验证求解器的独特但重要的特征，以及（2）预言机引导的方法由于依赖有限的突变规则而无法生成不同的测试。Diver旨在弥补这些不足，特别是专注于发现现有方法遗漏的错误。为此，我们提出了一种新的测试技术，可以执行预言机引导但不受限制的随机突变。我们使用Diver验证了三种流行SMT解算器的最新版本：CVC5、Z3和dReal。Diver总共发现了25个新错误，其中21个是关键的，直接影响求解器的可靠性。我们还通过证明现有工具不太可能发现DIVER发现的漏洞，实证证明了DIVER自身的实力。,软件测试，模糊化，SMT求解器,,,
NGV63QFW,2023,https://doi.org/10.1109/ICSE48619.2023.00048,ICSE 2023,BFTDETECTOR: Automatic Detection of Business Flow Tampering for Digital Content Service,"Digital content services provide users with a wide range of content, such as news, articles, or movies, while monetizing their content through various business models and promotional methods. Unfortunately, poorly designed or unpro-tected business logic can be circumvented by malicious users, which is known as business flow tampering. Such flaws can severely harm the businesses of digital content service providers. In this paper, we propose an automated approach that discov-ers business flow tampering flaws. Our technique automatically runs a web service to cover different business flows (e.g., a news website with vs. without a subscription paywall) to collect execution traces. We perform differential analysis on the execution traces to identify divergence points that determine how the business flow begins to differ, and then we test to see if the divergence points can be tampered with. We assess our approach against 352 real-world digital content service providers and discover 315 flaws from 204 websites, including TIME, Fortune, and Forbes. Our evaluation result shows that our technique successfully identifies these flaws with low false-positive and false-negative rates of 0.49% and 1.44%, respectively.","JavaScript,business flow tampering,dynamic analysis,vulnerability detection",BFTDETECTOR:数字内容服务业务流篡改的自动检测,数字内容服务为用户提供广泛的内容，如新闻、文章或电影，同时通过各种商业模式和促销方法将其内容货币化。不幸的是，恶意用户可以绕过设计不良或未受保护的业务逻辑，这就是所谓的业务流篡改。这些缺陷可能会严重损害数字内容服务提供商的业务。在本文中，我们提出了一种自动方法来发现业务流篡改缺陷。我们的技术自动运行web服务来覆盖不同的业务流（例如，有订阅付费墙和没有订阅付费墙的新闻网站），以收集执行跟踪。我们对执行跟踪执行差分分析，以确定决定业务流如何开始不同的分歧点，然后我们进行测试，看看这些分歧点是否可以被篡改。我们针对352家真实世界的数字内容服务提供商评估了我们的方法，并从204个网站中发现了315个缺陷，包括《时代》、《财富》和《福布斯》。我们的评估结果表明，我们的技术成功地识别了这些缺陷，假阳性率和假阴性率分别为0.49%和1.44%。,JavaScript，业务流篡改，动态分析，漏洞检测,,,
Y7YM9SKF,2023,https://doi.org/10.1109/ICSE48619.2023.00099,ICSE 2023,Learning to Boost Disjunctive Static Bug-Finders,"We present a new learning-based approach for accel-erating disjunctive static bug-finders. Industrial static bug-finders usually perform disjunctive analysis, differentiating program states along different execution paths of a program. Such path-sensitivity is essential for reducing false positives but it also increases analysis costs exponentially. Therefore, practical bug-finders use a state-selection heuristic to keep track of a small number of beneficial states only. However, designing a good heuristic for real-world programs is challenging; as a result, modern static bug-finders still suffer from low cost/bug-finding efficiency. In this paper, we aim to address this problem by learning effective state-selection heuristics from data. To this end, we present a novel data-driven technique that efficiently collects alarm-triggering traces, learns multiple candidate models, and adaptively chooses the best model tailored for each target program. We evaluate our approach with Infer and show that our technique significantly improves Infer's bug-finding efficiency for a range of open-source C programs.","machine learning,static analysis",学习增强虚拟静态Bug查找器,我们提出了一种新的基于学习的方法来加速析取静态错误查找器。工业静态错误查找器通常执行析取分析，沿着程序的不同执行路径区分程序状态。这种路径敏感性对于减少误报是至关重要的，但它也成倍地增加了分析成本。因此，实际的bug查找器使用状态选择启发式来只跟踪少数有益的状态。然而，为真实世界的程序设计一个好的启发式是具有挑战性的；因此，现代静态错误查找器仍然存在低成本/错误查找效率的问题。在本文中，我们旨在通过从数据中学习有效的状态选择启发式方法来解决这个问题。为此，我们提出了一种新的数据驱动技术，该技术可以有效地收集警报触发痕迹，学习多个候选模型，并自适应地为每个目标程序选择最佳模型。我们用Infer评估了我们的方法，并表明我们的技术显著提高了Infer在一系列开源C程序中的错误发现效率。,机器学习，静态分析,,,
DPITZHHT,2023,https://doi.org/10.1109/ICSE48619.2023.00139,ICSE 2023,On-Demand Security Requirements Synthesis with Relational Generative Adversarial Networks,"Security requirements engineering is a manual and error-prone activity that is often neglected due to the knowledge gap between cybersecurity professionals and software requirements engineers. In this paper, we aim to automate the process of recommending and synthesizing security requirements specifications and therefore supporting requirements engineers in soliciting and specifying security requirements. We investigate the use of Relational Generative Adversarial Networks (GANs) in automatically synthesizing security requirements specifications. We evaluate our approach using a real case study of the Court Case Management System (CCMS) developed for the Indiana Supreme Court's Division of State Court Administration. We present an approach based on RelGAN to generate security requirements specifications for the CCMS. We show that RelGAN is practical for synthesizing security requirements specifications as indicated by subject matter experts. Based on this study, we demonstrate promising results for the use of GANs in the software requirements synthesis domain. We also provide a baseline for synthesizing requirements, highlight limitations and weaknesses of RelGAN and define opportunities for further investigations.","Software Security Requirements,Requirements Engineering,Generative Adversarial Networks",基于关系生成对抗性网络的按需安全需求综合,由于网络安全专业人员和软件需求工程师之间的知识差距，安全需求工程是一项手动且容易出错的活动，经常被忽视。在本文中，我们的目标是自动化推荐和综合安全需求规范的过程，从而支持需求工程师征求和指定安全需求。我们研究了关系生成对抗性网络（GANs）在自动合成安全需求规范中的使用。我们使用为印第安纳州最高法院州法院管理司开发的法院案件管理系统（CCMS）的真实案例研究来评估我们的方法。我们提出了一种基于RelGAN的方法来生成CCMS的安全需求规范。我们证明了RelGAN在综合主题专家所指出的安全需求规范方面是实用的。基于这项研究，我们展示了在软件需求综合领域使用GANs的有希望的结果。我们还为综合需求提供了基线，强调了RelGAN的局限性和弱点，并定义了进一步调查的机会。,软件安全需求，需求工程，生成对抗性网络,,,
M4QY77LA,2023,https://doi.org/10.1109/ICSE48619.2023.00158,ICSE 2023,Automated Summarization of Stack Overflow Posts,"Software developers often resort to Stack Overflow (SO) to fill their programming needs. Given the abundance of relevant posts, navigating them and comparing different solutions is tedious and time-consuming. Recent work has proposed to automatically summarize SO posts to concise text to facilitate the navigation of SO posts. However, these techniques rely only on information retrieval methods or heuristics for text summarization, which is insufficient to handle the ambiguity and sophistication of natural language. This paper presents a deep learning based framework called Assortfor SO post summarization. Assortincludes two complementary learning methods, $\mathbf{Assort}_{S}$ and $\mathbf{Assort}_{IS}$, to address the lack of labeled training data for SO post summarization. $\mathbf{Assort}_{S}$ is designed to directly train a novel ensemble learning model with BERT embeddings and domain-specific features to account for the unique characteristics of SO posts. By contrast, $\mathbf{Assort}_{IS}$ is designed to reuse pre-trained models while addressing the domain shift challenge when no training data is present (i.e., zero-shot learning). Both $\mathbf{Assort}_{S}$ and $\mathbf{Assort}_{IS}$ outperform six existing techniques by at least 13% and 7% respectively in terms of the F1 score. Furthermore, a human study shows that participants significantly preferred summaries generated by $\mathbf{Assort}_{S}$ and $\mathbf{Assort}_{IS}$ over the best baseline, while the preference difference between $\mathbf{Assort}_{S}$ and $\mathbf{Assort}_{IS}$ was small.","Stack Overflow,Text Summarization,Deep Learning",堆栈溢出帖子的自动摘要,软件开发人员经常使用堆栈溢出（SO）来满足他们的编程需求。考虑到相关帖子的丰富性，浏览它们并比较不同的解决方案既乏味又耗时。最近的工作建议将SO帖子自动汇总为简洁的文本，以便于SO帖子的导航。然而，这些技术仅依赖于信息检索方法或启发式方法进行文本摘要，不足以处理自然语言的模糊性和复杂性。本文提出了一个基于深度学习的SO文章摘要框架AssortforSO。Assorti包括两种互补的学习方法，$\mathbf{Assort}_{S} $和$\mathbf{Assort}_｛IS｝$，以解决SO后期总结缺乏标记的训练数据的问题$\mathbf{Assort}_{S} $旨在直接训练一种具有BERT嵌入和领域特定特征的新型集成学习模型，以考虑SO帖子的独特特征。相比之下，$\mathbf{Assort}_｛IS｝$设计用于重用预先训练的模型，同时在不存在训练数据（即零样本学习）的情况下解决领域转移挑战。两个$\mathbf{Assort}_{S} $和$\mathbf{Assort}_{IS}$在F1得分方面分别比六种现有技术高出至少13%和7%。此外，一项人类研究表明，参与者更喜欢$\mathbf生成的摘要{Assort}_{S} $和$\mathbf{Assort}_｛IS｝$超过最佳基线，而$\mathbf之间的偏好差异{Assort}_{S} $和$\mathbf{Assort}_｛IS｝$很小。,堆栈溢出，文本摘要，深度学习,,,
KFD6Z96N,2023,https://doi.org/10.1109/ICSE48619.2023.00100,ICSE 2023,Predicting Bugs by Monitoring Developers During Task Execution,"Knowing which parts of the source code will be defective can allow practitioners to better allocate testing resources. For this reason, many approaches have been proposed to achieve this goal. Most state-of-the-art predictive models rely on product and process metrics, i.e., they predict the defectiveness of a component by considering what developers did. However, there is still limited evidence of the benefits that can be achieved in this context by monitoring how developers complete a development task. In this paper, we present an empirical study in which we aim at understanding whether measuring human aspects on developers while they write code can help predict the introduction of defects. First, we introduce a new developer-based model which relies on behavioral, psychophysical, and control factors that can be measured during the execution of development tasks. Then, we run a controlled experiment involving 20 software developers to understand if our developer-based model is able to predict the introduction of bugs. Our results show that a developer-based model is able to achieve a similar accuracy compared to a state-of-the-art code-based model, i.e., a model that uses only features measured from the source code. We also observed that by combining the models it is possible to obtain the best results (84% accuracy).","bug prediction,human aspects of software engineering,biometric sensors,empirical software engineering",通过在任务执行期间监视开发人员来预测Bug,知道源代码的哪些部分会有缺陷可以让从业者更好地分配测试资源。出于这个原因，已经提出了许多方法来实现这一目标。大多数最先进的预测模型都依赖于产品和流程指标，即通过考虑开发人员的行为来预测组件的缺陷。然而，通过监控开发人员如何完成开发任务，在这种情况下可以获得的好处仍然有限。在本文中，我们提出了一项实证研究，旨在了解在开发人员编写代码时测量他们的人性方面是否有助于预测缺陷的引入。首先，我们介绍了一种新的基于开发人员的模型，该模型依赖于在执行开发任务期间可以测量的行为、心理物理和控制因素。然后，我们运行了一个涉及20名软件开发人员的受控实验，以了解我们基于开发人员的模型是否能够预测错误的引入。我们的结果表明，与最先进的基于代码的模型（即仅使用源代码测量的特征的模型）相比，基于开发人员的模型能够实现类似的精度。我们还观察到，通过组合模型，可以获得最佳结果（84%的准确率）。,漏洞预测，软件工程的人性化，生物识别传感器，经验软件工程,,,
S4NM45SS,2023,https://doi.org/10.1109/ICSE48619.2023.00204,ICSE 2023,Log Parsing with Prompt-based Few-shot Learning,"Logs generated by large-scale software systems provide crucial information for engineers to understand the system status and diagnose problems of the systems. Log parsing, which converts raw log messages into structured data, is the first step to enabling automated log analytics. Existing log parsers extract the common part as log templates using statistical features. However, these log parsers often fail to identify the correct templates and parameters because: 1) they often overlook the semantic meaning of log messages, and 2) they require domain-specific knowledge for different log datasets. To address the limitations of existing methods, in this paper, we propose LogPPT to capture the patterns of templates using prompt-based few-shot learning. LogPPT utilises a novel prompt tuning method to recognise keywords and parameters based on a few labelled log data. In addition, an adaptive random sampling algorithm is designed to select a small yet diverse training set. We have conducted extensive experiments on 16 public log datasets. The experimental results show that LogPPT is effective and efficient for log parsing.","log parsing,few-shot learning,prompt-tuning,deep learning",基于提示的少镜头学习的日志解析,大型软件系统生成的日志为工程师了解系统状态和诊断系统问题提供了重要信息。日志解析将原始日志消息转换为结构化数据，是实现自动化日志分析的第一步。现有的日志解析器使用统计特征提取公共部分作为日志模板。然而，这些日志解析器通常无法识别正确的模板和参数，因为：1）它们经常忽略日志消息的语义，2）它们需要不同日志数据集的特定领域知识。为了解决现有方法的局限性，本文提出了LogPPT，使用基于提示的少镜头学习来捕捉模板的模式。LogPPT利用一种新颖的提示调整方法，基于一些标记的日志数据来识别关键词和参数。此外，还设计了一种自适应随机采样算法来选择一个小而多样的训练集。我们在16个公共日志数据集上进行了广泛的实验。实验结果表明，LogPPT对日志解析是有效的。,日志解析，少镜头学习，即时调整，深度学习,,,
QSHLWBSM,2023,https://doi.org/10.1109/ICSE48619.2023.00043,ICSE 2023,Learning Seed-Adaptive Mutation Strategies for Greybox Fuzzing,"In this paper, we present a technique for learning seed-adaptive mutation strategies for fuzzers. The performance of mutation-based fuzzers highly depends on the mutation strategy that specifies the probability distribution of selecting mutation methods. As a result, developing an effective mutation strategy has received much attention recently, and program-adaptive techniques, which observe the behavior of the target program to learn the optimized mutation strategy per program, have become a trending approach to achieve better performance. They, however, still have a major limitation; they disregard the impacts of different characteristics of seed inputs which can lead to explore deeper program locations. To address this limitation, we present SEAMFUZZ, a novel fuzzing technique that automatically captures the characteristics of individual seed inputs and applies different mutation strategies for different seed inputs. By capturing the syntactic and semantic similarities between seed inputs, SEAMFUZZ clusters them into proper groups and learns effective mutation strategies tailored for each seed cluster by using the customized Thompson sampling algorithm. Experimental results show that SEAMFUZZ improves both the path-discovering and bug-finding abilities of state-of-the-art fuzzers on real-world programs.","Fuzzing,Software Testing",Greybox引信的种子自适应变异学习策略,在本文中，我们提出了一种学习模糊器种子自适应变异策略的技术。基于突变的模糊器的性能在很大程度上取决于指定选择突变方法的概率分布的突变策略。因此，开发一种有效的变异策略最近受到了广泛关注，而程序自适应技术，即观察目标程序的行为以学习每个程序的优化变异策略，已经成为实现更好性能的一种趋势方法。然而，它们仍然有一个主要的局限性；他们忽略了种子输入的不同特征的影响，这可能会导致探索更深层次的项目位置。为了解决这一限制，我们提出了SEAMFUZZ，这是一种新的模糊技术，可以自动捕捉单个种子输入的特征，并对不同的种子输入应用不同的突变策略。通过捕捉种子输入之间的句法和语义相似性，SEAMFUZZ将它们聚类到适当的组中，并使用定制的Thompson采样算法学习为每个种子聚类量身定制的有效变异策略。实验结果表明，SEAMFUZZ在实际程序中提高了最先进的模糊器的路径发现和错误发现能力。,引信，软件测试,,,
E7TWZ7EZ,2023,https://doi.org/10.1109/ICSE48619.2023.00150,ICSE 2023,Eadro: An End-to-End Troubleshooting Framework for Microservices on Multi-source Data,"The complexity and dynamism of microservices pose significant challenges to system reliability, and thereby, automated troubleshooting is crucial. Effective root cause localization after anomaly detection is crucial for ensuring the reliability of microservice systems. However, two significant issues rest in existing approaches: (1) Microservices generate traces, system logs, and key performance indicators (KPIs), but existing approaches usually consider traces only, failing to understand the system fully as traces cannot depict all anomalies; (2) Troubleshooting microservices generally contains two main phases, i.e., anomaly detection and root cause localization. Existing studies regard these two phases as independent, ignoring their close correlation. Even worse, inaccurate detection results can deeply affect localization effectiveness. To overcome these limitations, we propose Eadro, the first end-to-end framework to integrate anomaly detection and root cause localization based on multi-source data for troubleshooting large-scale microservices. The key insights of Eadro are the anomaly manifestations on different data sources and the close connection between detection and localization. Thus, Eadro models intra-service behaviors and inter-service dependencies from traces, logs, and KPIs, all the while leveraging the shared knowledge of the two phases via multi-task learning. Experiments on two widely-used benchmark microservices demonstrate that Eadro outperforms state-of-the-art approaches by a large margin. The results also show the usefulness of integrating multi-source data. We also release our code and data to facilitate future research.","Microservices,Root Cause Localization,Anomaly Detection,Traces",Eadro:一个用于多源数据微服务的端到端故障排除框架,微服务的复杂性和动态性对系统可靠性提出了重大挑战，因此，自动化故障排除至关重要。异常检测后有效的根源定位对于确保微服务系统的可靠性至关重要。然而，现有方法存在两个重大问题：（1）微服务生成跟踪、系统日志和关键性能指标（KPI），但现有方法通常只考虑跟踪，未能完全理解系统，因为跟踪无法描述所有异常；（2） 微服务故障排除通常包括两个主要阶段，即异常检测和根本原因定位。现有研究认为这两个阶段是独立的，忽略了它们之间的密切相关性。更糟糕的是，不准确的检测结果会严重影响定位的有效性。为了克服这些限制，我们提出了Eadro，这是第一个基于多源数据集成异常检测和根本原因定位的端到端框架，用于对大规模微服务进行故障排除。Eadro的关键见解是不同数据源上的异常表现以及检测和定位之间的密切联系。因此，Eadro通过跟踪、日志和KPI对服务内行为和服务间依赖性进行建模，同时通过多任务学习利用两个阶段的共享知识。在两个广泛使用的基准微服务上的实验表明，Eadro在很大程度上优于最先进的方法。结果还显示了整合多源数据的有用性。我们还发布了我们的代码和数据，以促进未来的研究。,微服务，根本原因定位，异常检测，跟踪,,,
JWFMF8U2,2023,https://doi.org/10.1109/ICSE48619.2023.00148,ICSE 2023,Heterogeneous Anomaly Detection for Software Systems via Semi-supervised Cross-modal Attention,"Prompt and accurate detection of system anomalies is essential to ensure the reliability of software systems. Unlike manual efforts that exploit all available run-time information, existing approaches usually leverage only a single type of monitoring data (often logs or metrics) or fail to make effective use of the joint information among different types of data. Consequently, many false predictions occur. To better understand the manifestations of system anomalies, we conduct a systematical study on a large amount of heterogeneous data, i.e., logs and metrics. Our study demonstrates that logs and metrics can manifest system anomalies collaboratively and complementarily, and neither of them only is sufficient. Thus, integrating heterogeneous data can help recover the complete picture of a system's health status. In this context, we propose Hades, the first end-to-end semi-supervised approach to effectively identify system anomalies based on heterogeneous data. Our approach employs a hierarchical architecture to learn a global representation of the system status by fusing log semantics and metric patterns. It captures discriminative features and meaningful interactions from heterogeneous data via a cross-modal attention module, trained in a semi-supervised manner. We evaluate Hades extensively on large-scale simulated data and datasets from Huawei Cloud. The experimental results present the effectiveness of our model in detecting system anomalies. We also release the code and the annotated dataset for replication and future research.","Software System,Anomaly Detection,Cross-modal Learning",基于半监督跨模态注意的软件系统异构异常检测,及时准确地检测系统异常对于确保软件系统的可靠性至关重要。与利用所有可用运行时信息的手动操作不同，现有方法通常只利用单一类型的监控数据（通常是日志或度量），或者无法有效利用不同类型数据之间的联合信息。因此，出现了许多错误的预测。为了更好地理解系统异常的表现，我们对大量的异构数据，即日志和度量进行了系统的研究。我们的研究表明，日志和度量可以协同互补地显示系统异常，而且两者都不足够。因此，集成异构数据可以帮助恢复系统健康状态的全貌。在这种情况下，我们提出了Hades，这是第一种基于异构数据有效识别系统异常的端到端半监督方法。我们的方法采用分层体系结构，通过融合日志语义和度量模式来学习系统状态的全局表示。它通过以半监督方式训练的跨模态注意力模块，从异构数据中捕捉有区别的特征和有意义的交互。我们在华为云的大规模模拟数据和数据集上对哈迪斯进行了广泛的评估。实验结果表明了我们的模型在检测系统异常方面的有效性。我们还发布了代码和带注释的数据集，用于复制和未来的研究。,软件系统，异常检测，跨模态学习,,,
QJQ4QL97,2023,https://doi.org/10.1109/ICSE48619.2023.00184,ICSE 2023,Sibyl: Improving Software Engineering Tools with SMT Selection,"SMT solvers are often used in the back end of different software engineering tools─e.g., program verifiers, test generators, or program synthesizers. There are a plethora of algorithmic techniques for solving SMT queries. Among the available SMT solvers, each employs its own combination of algorithmic techniques that are optimized for different fragments of logics and problem types. The most efficient solver can change with small changes in the SMT query, which makes it nontrivial to decide which solver to use. Consequently, designers of software engineering tools often select a single solver, based on familiarity or convenience, and tailor their tool towards it. Choosing an SMT solver at design time misses the opportunity to optimize query solve times and, for tools where SMT solving is a bottleneck, the performance loss can be significant. In this work, we present Sibyl, an automated SMT selector based on graph neural networks (GNNs). Sibyl creates a graph representation of a given SMT query and uses GNNs to predict how each solver in a suite of SMT solvers would perform on said query. Sibyl learns to predict based on features of SMT queries that are specific to the population on which it is trained - avoiding the need for manual feature engineering. Once trained, Sibyl makes fast and accurate predictions which can substantially reduce the time needed to solve a set of SMT queries. We evaluate Sibyl in four scenarios in which SMT solvers are used: in competition, in a symbolic execution engine, in a bounded model checker, and in a program synthesis tool. We find that Sibyl improves upon the state of the art in nearly every case and provide evidence that it generalizes better than existing techniques. Further, we evaluate Sibyl's overhead and demonstrate that it has the potential to speedup a variety of different software engineering tools.","graph neural networks,satisfiable modulo theories,algorithm selection",Sibyl：用SMT选择改进软件工程工具,SMT求解器通常用于不同软件工程工具的后端─例如程序验证器、测试生成器或程序合成器。有过多的算法技术用于解决SMT查询。在可用的SMT解算器中，每个解算器都采用了自己的算法技术组合，这些技术针对不同的逻辑片段和问题类型进行了优化。最有效的解算器可以随着SMT查询中的微小变化而变化，这使得决定使用哪个解算器变得不重要。因此，软件工程工具的设计者通常会根据熟悉程度或方便程度选择一个求解器，并针对其定制工具。在设计时选择SMT求解器会错过优化查询求解时间的机会，而且对于SMT求解是瓶颈的工具来说，性能损失可能很大。在这项工作中，我们提出了Sibyl，一种基于图神经网络（GNN）的自动SMT选择器。Sibyl创建给定SMT查询的图形表示，并使用GNN来预测SMT解算器套件中的每个解算器将如何对所述查询执行。Sibyl学习基于SMT查询的特定于其训练对象的特征进行预测，从而避免了手动特征工程的需要。经过训练后，Sibyl可以做出快速准确的预测，这可以大大减少解决一组SMT查询所需的时间。我们在使用SMT求解器的四个场景中评估Sibyl：在竞争中，在符号执行引擎中，在有界模型检查器中，以及在程序合成工具中。我们发现Sibyl在几乎所有情况下都提高了现有技术的水平，并提供了证据表明它比现有技术推广得更好。此外，我们评估了Sibyl的开销，并证明它有可能加速各种不同的软件工程工具。,图神经网络，可满足模理论，算法选择,,,
B75G2T8G,2023,https://doi.org/10.1109/ICSE48619.2023.00085,ICSE 2023,CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models,"Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.","search based software testing,codex,test suite generation,python,large language model,automated testing",CodaMosa：用预先训练的大型语言模型在测试生成中逃离覆盖平台,基于搜索的软件测试（SBST）通过测试用例生成和突变的组合，为正在测试的程序生成高覆盖率的测试用例。SBST的性能依赖于生成测试用例的合理概率，这些测试用例行使了被测程序的核心逻辑。给定这样的测试用例，SBST可以探索它们周围的空间来练习程序的各个部分。本文探讨了代码的大型语言模型（LLM），如OpenAI的Codex，是否可以用于帮助SBST的探索。我们提出的算法CodaMosa进行SBST，直到其覆盖改进停滞，然后要求Codex为覆盖不足的函数提供示例测试用例。这些示例有助于SBST将其搜索重定向到搜索空间中更有用的区域。在对486个基准的评估中，与仅SBST和LLM的基线相比，CodaMosa在更多基准（173和279）上实现的覆盖率在统计学上显著高于在（10和4）上减少的覆盖率。,基于搜索的软件测试，codex，测试套件生成，python，大型语言模型，自动化测试,,,
6WZZD6M7,2023,https://doi.org/10.1109/ICSE48619.2023.00076,ICSE 2023,Commit Message Matters: Investigating Impact and Evolution of Commit Message Quality,"Commit messages play an important role in communication among developers. To measure the quality of commit messages, researchers have defined what semantically constitutes a Good commit message: it should have both the summary of the code change (What) and the motivation/reason behind it (Why). The presence of the issue report/pull request links referenced in a commit message has been treated as a way of providing Why information. In this study, we found several quality issues that could hamper the links' ability to provide Why information. Based on this observation, we developed a machine learning classifier for automatically identifying whether a commit message has What and Why information by considering both the commit messages and the link contents. This classifier outperforms state-of-the-art machine learning classifiers by 12 percentage points improvement in the F1 score. With the improved classifier, we conducted a mixed method empirical analysis and found that: (1) Commit message quality has an impact on software defect proneness, and (2) the overall quality of the commit messages decreases over time, while developers believe they are writing better commit messages. All the research artifacts (i.e., tools, scripts, and data) of this study are available on the accompanying website [2].","Commit message quality,software defect proneness,empirical analysis",提交消息事项：调查提交消息质量的影响和演变,提交消息在开发人员之间的沟通中起着重要作用。为了衡量提交消息的质量，研究人员定义了什么在语义上构成了好的提交消息：它应该既有代码更改的摘要（what），也有背后的动机/原因（Why）。提交消息中引用的问题报告/拉取请求链接已被视为提供“为什么”信息的一种方式。在这项研究中，我们发现了一些质量问题，这些问题可能会阻碍链接提供Why信息的能力。基于这一观察，我们开发了一个机器学习分类器，用于通过考虑提交消息和链接内容来自动识别提交消息是否具有What和Why信息。该分类器在F1分数上比最先进的机器学习分类器提高了12个百分点。使用改进的分类器，我们进行了混合方法的实证分析，发现：（1）提交消息质量对软件缺陷倾向有影响，（2）提交消息的总体质量随着时间的推移而降低，而开发人员认为他们正在编写更好的提交消息。本研究的所有研究成果（即工具、脚本和数据）可在附带的网站上获得[2]。,承诺消息质量，软件缺陷倾向，实证分析,,,
QRSMXZKB,2023,https://doi.org/10.1109/ICSE48619.2023.00078,ICSE 2023,Did We Miss Something Important? Studying and Exploring Variable-Aware Log Abstraction,"Due to the sheer size of software logs, developers rely on automated techniques for log analysis. One of the first and most important steps of automated log analysis is log abstraction, which parses the raw logs into a structured format. Prior log abstraction techniques aim to identify and abstract all the dynamic variables in logs and output a static log template for automated log analysis. However, these abstracted dynamic variables may also contain important information that is useful to different tasks in log analysis. In this paper, we investigate the characteristics of dynamic variables and their importance in practice, and explore the potential of a variable-aware log abstraction technique. Through manual investigations and surveys with practitioners, we find that different categories of dynamic variables record various information that can be important depending on the given tasks, the distinction of dynamic variables in log abstraction can further assist in log analysis. We then propose a deep learning based log abstraction approach, named VALB, which can identify different categories of dynamic variables and preserve the value of specified categories of dynamic variables along with the log templates (i.e., variable-aware log abstraction). Through the evaluation on a widely used log abstraction benchmark, we find that VALB outperforms other state-of-the-art log abstraction techniques on general log abstraction (i.e., when abstracting all the dynamic variables) and also achieves a high variable-aware log abstraction accuracy that further identifies the category of the dynamic variables. Our study highlights the potential of leveraging the important information recorded in the dynamic variables to further improve the process of log analysis.","software logs,log abstraction,deep learning",我们错过了重要的事情吗？变量感知日志抽象的研究与探索,由于软件日志的庞大规模，开发人员依赖于日志分析的自动化技术。自动化日志分析的第一个也是最重要的步骤之一是日志抽象，它将原始日志解析为结构化格式。现有的日志抽象技术旨在识别和抽象日志中的所有动态变量，并输出用于自动日志分析的静态日志模板。然而，这些抽象的动态变量也可能包含对日志分析中的不同任务有用的重要信息。在本文中，我们研究了动态变量的特征及其在实践中的重要性，并探索了变量感知日志抽象技术的潜力。通过对从业者的手动调查和调查，我们发现不同类别的动态变量记录了根据给定任务可能重要的各种信息，日志抽象中动态变量的区分可以进一步帮助日志分析。然后，我们提出了一种基于深度学习的日志抽象方法，称为VALB，它可以识别不同类别的动态变量，并将指定类别的动态变数的值与日志模板一起保留（即，变量感知日志抽象）。通过对广泛使用的日志抽象基准的评估，我们发现VALB在一般日志抽象（即，当抽象所有动态变量时）方面优于其他最先进的日志抽象技术，并且还实现了高的变量感知日志抽象精度，从而进一步识别动态变量的类别。我们的研究强调了利用动态变量中记录的重要信息来进一步改进日志分析过程的潜力。,软件日志，日志抽象，深度学习,,,
L5NMCZTB,2023,https://doi.org/10.1109/ICSE48619.2023.00179,ICSE 2023,SkCoder: A Sketch-based Approach for Automatic Code Generation,"Recently, deep learning techniques have shown great success in automatic code generation. Inspired by the code reuse, some researchers propose copy-based approaches that can copy the content from similar code snippets to obtain better performance. Practically, human developers recognize the content in the similar code that is relevant to their needs, which can be viewed as a code sketch. The sketch is further edited to the desired code. However, existing copy-based approaches ignore the code sketches and tend to repeat the similar code without necessary modifications, which leads to generating wrong results. In this paper, we propose a sketch-based code generation approach named Skcoderto mimic developers' code reuse behavior. Given a natural language requirement, Skcoderretrieves a similar code snippet, extracts relevant parts as a code sketch, and edits the sketch into the desired code. Our motivations are that the extracted sketch provides a well-formed pattern for telling models “how to write”. The post-editing further adds requirement-specific details into the sketch and outputs the complete code. We conduct experiments on two public datasets and a new dataset collected by this work. We compare our approach to 20 baselines using 5 widely used metrics. Experimental results show that (1) Skcodercan generate more correct programs, and outperforms the state-of-the-art -CodeT5-base by 30.30%, 35.39%, and 29.62% on three datasets. (2) Our approach is effective to multiple code generation models and improves them by up to 120.1% in Pass@l. (3) We investigate three plausible code sketches and discuss the importance of sketches. (4) We manually evaluate the generated code and prove the superiority of our Skcoderin three aspects.","Code Generation,Deep Learning",SkCoder:一种基于草图的代码自动生成方法,最近，深度学习技术在自动代码生成方面取得了巨大成功。受代码重用的启发，一些研究人员提出了基于复制的方法，可以复制类似代码片段中的内容，以获得更好的性能。实际上，人类开发人员可以识别与他们的需求相关的类似代码中的内容，这些内容可以被视为代码草图。草图将进一步编辑为所需的代码。然而，现有的基于副本的方法忽略了代码草图，并且倾向于在没有必要修改的情况下重复类似的代码，这导致生成错误的结果。在本文中，我们提出了一种基于草图的代码生成方法Skcoderto，以模仿开发人员的代码重用行为。给定一个自然语言需求，Skcoder检索一个类似的代码片段，提取相关部分作为代码草图，并将草图编辑为所需的代码。我们的动机是，提取的草图为告诉模型“如何写作”提供了一个良好的模式。后期编辑进一步将需求特定的细节添加到草图中，并输出完整的代码。我们在两个公共数据集和本工作收集的一个新数据集上进行了实验。我们使用5个广泛使用的指标将我们的方法与20个基线进行了比较。实验结果表明：（1）Skcodercan生成了更正确的程序，在三个数据集上分别比最先进的-CodeT5库高出30.30%、35.39%和29.62%。（2） 我们的方法对多个代码生成模型有效，并在Pass@l.（3）我们研究了三个看似合理的代码草图，并讨论了草图的重要性。（4） 我们手动评估生成的代码，并从三个方面证明了我们的Skcoderin的优越性。,代码生成，深度学习,,,
YFPYZLQU,2023,https://doi.org/10.1109/ICSE48619.2023.00110,ICSE 2023,CCTEST: Testing and Repairing Code Completion Systems,"Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems. In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTEST features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTEST repairs the code completion outputs by selecting the output that mostly reflects the “average” appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40% and 67% with respect to BLEU score and Levenshtein edit similarity.","Deep learning,Codes,Source coding,Closed box,Maintenance engineering,Task analysis,Programming profession",测试和修复代码完成系统,代码完成是软件开发领域中一个非常有价值的主题，随着大型语言模型（LLM）的最新进展，它越来越被推广使用。到目前为止，基于LLM的可见代码完成框架，如GitHub Copilot和GPT，都是在大量非结构化文本和开源代码上使用深度学习进行训练的。作为日常编程任务的重要组成部分和基石，代码完成在很大程度上提高了专业人员构建真实世界软件系统的效率。与这个繁荣的市场形成对比的是，我们发现代码完成系统经常输出可疑的结果，而且到目前为止，还没有用于代码完成系统的自动化测试和增强框架。本研究提出了CCTEST，一个在黑盒环境中测试和修复代码完成系统的框架。CCTEST具有一套新的突变策略，即程序结构一致性（PSC）突变，以生成突变代码完成输入。然后，它从所有完成的代码案例中检测不一致的输出，表示可能的错误案例。此外，CCTEST通过选择最能反映所有输出情况的“平均”外观的输出作为代码完成系统的最终输出来修复代码完成输出。在大约18K测试输入的情况下，我们从八个流行的基于LLM的代码完成系统中检测到33540个可能触发错误情况（真阳性率为86%）的输入。通过修复，我们发现代码完成系统的准确性相对于BLEU评分和Levenstein编辑相似性显著提高了40%和67%。,深度学习，代码，源代码编码，封闭盒，维护工程，任务分析，编程专业,,,
BZJ8Q3ST,2023,https://doi.org/10.1109/ICSE48619.2023.00156,ICSE 2023,Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects,"With the widespread deployment of deep neural networks (DNNs), ensuring the reliability of DNN-based systems is of great importance. Serious reliability issues such as system failures can be caused by numerical defects, one of the most frequent defects in DNNs. To assure high reliability against numerical defects, in this paper, we propose the RANUM approach including novel techniques for three reliability assurance tasks: detection of potential numerical defects, confirmation of potential-defect feasibility, and suggestion of defect fixes. To the best of our knowledge, RANUM is the first approach that confirms potential-defect feasibility with failure-exhibiting tests and suggests fixes automatically. Extensive experiments on the benchmarks of 63 real-world DNN architectures show that RANUM outperforms state-of-the-art approaches across the three reliability assurance tasks. In addition, when the RANUM-generated fixes are compared with developers' fixes on open-source projects, in 37 out of 40 cases, RANUM-generated fixes are equivalent to or even better than human fixes.","neural network,numerical defect,testing,fix",针对数值缺陷的深度神经网络结构的可靠性保证,随着深度神经网络（DNN）的广泛部署，确保基于DNN的系统的可靠性具有重要意义。数字缺陷是DNN中最常见的缺陷之一，可能会导致系统故障等严重的可靠性问题。为了确保对数值缺陷的高可靠性，在本文中，我们提出了RANUM方法，该方法包括用于三个可靠性保证任务的新技术：检测潜在的数值缺陷、确认潜在缺陷的可行性和建议缺陷修复。据我们所知，RANUM是第一种通过故障显示测试来确认潜在缺陷可行性并自动建议修复的方法。在63个真实世界DNN架构的基准测试上进行的大量实验表明，RANUM在三项可靠性保证任务中都优于最先进的方法。此外，当将RANUM生成的修复程序与开发人员对开源项目的修复程序进行比较时，在40种情况中，有37种情况下，RANUM生成修复程序相当于甚至优于人工修复程序。,神经网络，数值缺陷，测试，修复,,,
AJF5R6JL,2023,https://doi.org/10.1109/ICSE48619.2023.00106,ICSE 2023,Lightweight Approaches to DNN Regression Error Reduction: An Uncertainty Alignment Perspective,"Regression errors of Deep Neural Network (DNN) models refer to the case that predictions were correct by the old-version model but wrong by the new-version model. They frequently occur when upgrading DNN models in production systems, causing disproportionate user experience degradation. In this paper, we propose a lightweight regression error reduction approach with two goals: 1) requiring no model retraining and even data, and 2) not sacrificing the accuracy. The proposed approach is built upon the key insight rooted in the unmanaged model uncertainty, which is intrinsic to DNN models, but has not been thoroughly explored especially in the context of quality assurance of DNN models. Specifically, we propose a simple yet effective ensemble strategy that estimates and aligns the two models' uncertainty. We show that a Pareto improvement that reduces the regression errors without compromising the overall accuracy can be guaranteed in theory and largely achieved in practice. Comprehensive experiments with various representative models and datasets confirm that our approaches significantly outperform the state-of-the-art alternatives.","Software regression,deep neural networks,uncertainty alignment,model ensemble",减少DNN回归误差的轻量级方法：不确定性对齐视角,深度神经网络（DNN）模型的回归误差是指旧版本模型预测正确，而新版本模型预测错误的情况。它们经常发生在升级生产系统中的DNN模型时，导致用户体验不成比例地退化。在本文中，我们提出了一种轻量级的回归误差减少方法，目标有两个：1）不需要模型再训练，甚至不需要数据，2）不牺牲精度。所提出的方法建立在非托管模型不确定性的关键见解之上，非托管模型的不确定性是DNN模型固有的，但尚未得到彻底的探索，尤其是在DNN模型的质量保证方面。具体而言，我们提出了一种简单而有效的集成策略，用于估计和调整两个模型的不确定性。我们表明，在不影响总体精度的情况下减少回归误差的Pareto改进在理论上是可以保证的，在实践中也可以很大程度上实现。对各种具有代表性的模型和数据集进行的综合实验证实，我们的方法显著优于最先进的替代方案。,软件回归，深度神经网络，不确定性对齐，模型集成,,,
PDMPENWK,2023,https://doi.org/10.1109/ICSE48619.2023.00047,ICSE 2023,A Qualitative Study on the Implementation Design Decisions of Developers,"Decision-making is a key software engineering skill. Developers constantly make choices throughout the software development process, from requirements to implementation. While prior work has studied developer decision-making, the choices made while choosing what solution to write in code remain understudied. In this mixed-methods study, we examine the phenomenon where developers select one specific way to implement a behavior in code, given many potential alternatives. We call these decisions implementation design decisions. Our mixed-methods study includes 46 survey responses and 14 semi-structured interviews with professional developers about their decision types, considerations, processes, and expertise for implementation design decisions. We find that implementation design decisions, rather than being a natural outcome from higher levels of design, require constant monitoring of higher level design choices, such as requirements and architecture. We also show that developers have a consistent general structure to their implementation decision-making process, but no single process is exactly the same. We discuss the implications of our findings on research, education, and practice, including insights on teaching developers how to make implementation design decisions.","implementation design decisions,software design",开发人员实现设计决策的定性研究,决策是软件工程的一项关键技能。从需求到实现，开发人员在整个软件开发过程中不断做出选择。虽然之前的工作已经研究了开发人员的决策，但在选择用代码编写什么解决方案时所做的选择仍然没有得到充分的研究。在这项混合方法研究中，我们考察了开发人员在给定许多潜在替代方案的情况下，选择一种特定方式来实现代码中的行为的现象。我们将这些决策称为实现设计决策。我们的混合方法研究包括46份调查回复和14份对专业开发人员的半结构化访谈，内容涉及他们的决策类型、考虑因素、流程和实施设计决策的专业知识。我们发现，实现设计决策不是更高级别设计的自然结果，而是需要不断监控更高级别的设计选择，如需求和体系结构。我们还表明，开发人员的实现决策过程有一个一致的通用结构，但没有一个过程是完全相同的。我们讨论了我们的研究结果对研究、教育和实践的影响，包括教开发人员如何做出实现设计决策的见解。,实现设计决策，软件设计,,,
9YBDIYAM,2023,https://doi.org/10.1109/ICSE48619.2023.00119,ICSE 2023,Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing,"Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.","Text input generation,GUI testing,Android app,Large language model,Prompt-tuning",填补空白：用于移动GUI测试的上下文感知自动文本输入生成,自动化GUI测试被广泛用于帮助确保移动应用程序的质量。然而，许多GUI需要适当的文本输入才能进入下一页，这仍然是测试覆盖率的一个突出障碍。考虑到有效输入（如航班起飞、电影名称）的多样性和语义要求，自动生成文本输入具有挑战性。受预先训练的大型语言模型（LLM）在文本生成方面取得突出进展的启发，我们提出了一种基于LLM的QTypist方法，用于根据GUI上下文智能生成语义输入文本。为了提高LLM在移动测试场景中的性能，我们开发了一种基于提示的数据构建和调优方法，该方法可以自动提取模型调优的提示和答案。我们在Google Play的106个应用程序上评估了QTypist，结果显示QTypist的通过率为87%，比最佳基线高出93%。我们还将QTypist与自动化GUI测试工具集成在一起，与原始工具相比，它可以覆盖42%以上的应用程序活动，52%以上的页面，并随后帮助揭示122%以上的错误。,文本输入生成，GUI测试，Android应用程序，大型语言模型，提示调整,,,
CE9QQLAU,2023,https://doi.org/10.1109/ICSE48619.2023.00168,ICSE 2023,Ex pede Herculem: Augmenting Activity Transition Graph for Apps via Graph Convolution Network,"Mobile apps are indispensable for people's daily life. With the increase of GUI functions, apps have become more complex and diverse. As the Android app is event-driven, Activity Transition Graph (ATG) becomes an important way of app abstract and graphical user interface (GUI) modeling. Although existing works provide static and dynamic analysis to build ATG for applications, the completeness of ATG obtained is poor due to the low coverage of these techniques. To tackle this challenge, we propose a novel approach, ArchiDroid, to automatically augment the ATG via graph convolution network. It models both the semantics of activities and the graph structure of activity transitions to predict the transition between activities based on the seed ATG extracted by static analysis. The evaluation demonstrates that ArchiDroid can achieve 86% precision and 94% recall in predicting the transition between activities for augmenting ATG. We further apply the augmented ATG in two downstream tasks, i.e., guidance in automated GUI testing and assistance in app function design. Results show that the automated GUI testing tool integrated with ArchiDroid achieves 43% more activity coverage and detects 208% more bugs. Besides, ArchiDroid can predict the missing transition with 85% accuracy in real-world apps for assisting the app function design, and an interview case study further demonstrates its usefulness.","GUI testing,deep learning,program analysis,empirical study",Expede Herculem：通过图形卷积网络增强应用程序的活动转换图,移动应用程序对于人们的日常生活是必不可少的。随着GUI功能的增加，应用程序变得更加复杂和多样化。由于Android应用程序是事件驱动的，活动转换图（ATG）成为应用程序抽象和图形用户界面（GUI）建模的重要方式。尽管现有的工作提供了静态和动态分析来构建应用程序的ATG，但由于这些技术的覆盖率较低，所获得的ATG的完整性较差。为了应对这一挑战，我们提出了一种新的方法，ArchiDroid，通过图卷积网络自动增强ATG。它基于静态分析提取的种子ATG，对活动的语义和活动转换的图结构进行建模，以预测活动之间的转换。评估表明，ArchiDroid在预测增强ATG的活动之间的转换方面可以达到86%的准确率和94%的召回率。我们进一步将增强的ATG应用于两个下游任务，即自动GUI测试的指导和应用程序功能设计的辅助。结果显示，与ArchiDroid集成的自动化GUI测试工具实现了43%的活动覆盖率和208%的错误检测率。此外，ArchiDroid可以在现实世界的应用程序中以85%的准确率预测缺失的过渡，以帮助应用程序功能设计，一个访谈案例研究进一步证明了它的有用性。,GUI测试，深度学习，程序分析，实证研究,,,
FYU5XQAH,2023,https://doi.org/10.1109/ICSE48619.2023.00193,ICSE 2023,Incident-aware Duplicate Ticket Aggregation for Cloud Systems,"In cloud systems, incidents are potential threats to customer satisfaction and business revenue. When customers are affected by incidents, they often request customer support service (CSS) from the cloud provider by submitting a support ticket. Many tickets could be duplicate as they are reported in a distributed and uncoordinated manner. Thus, aggregating such duplicate tickets is essential for efficient ticket management. Previous studies mainly rely on tickets' textual similarity to detect duplication; however, duplicate tickets in a cloud system could carry semantically different descriptions due to the complex service dependency of the cloud system. To tackle this problem, we propose iPACK, an incident-aware method for aggregating duplicate tickets by fusing the failure information between the customer side (i.e., tickets) and the cloud side (i.e., incidents). We extensively evaluate iPACK on three datasets collected from the production environment of a large-scale cloud platform, Azure. The experimental results show that iPACK can precisely and comprehensively aggregate duplicate tickets, achieving an F1 score of 0.871~0.935 and outperforming state-of-the-art methods by 12.4%~31.2%.","duplicate tickets,incidents,cloud systems,reliability",云系统的事件感知重复票证聚合,在云系统中，事件是对客户满意度和业务收入的潜在威胁。当客户受到事件影响时，他们通常通过提交支持票证向云提供商请求客户支持服务（CSS）。许多门票可能是重复的，因为它们是以分布式和不协调的方式报告的。因此，聚集这样的重复票证对于有效的票证管理是必不可少的。以往的研究主要依靠票证的文本相似性来检测重复；然而，由于云系统的复杂服务依赖性，云系统中的重复票证可能携带语义不同的描述。为了解决这个问题，我们提出了iPACK，这是一种事件感知方法，通过融合客户端（即票证）和云端（即事件）之间的故障信息来聚合重复票证。我们在从大型云平台Azure的生产环境中收集的三个数据集上广泛评估了iPACK。实验结果表明，iPACK可以准确、全面地聚合重复票，F1得分为0.871~0.935，比现有方法高12.4%~31.2%。,重复票证，事件，云系统，可靠性,,,
N9AGS6HT,2023,https://doi.org/10.1109/ICSE48619.2023.00072,ICSE 2023,Syntax and Domain Aware Model for Unsupervised Program Translation,"There is growing interest in software migration as the development of software and society. Manually migrating projects between languages is error-prone and expensive. In recent years, researchers have begun to explore automatic program translation using supervised deep learning techniques by learning from large-scale parallel code corpus. However, parallel resources are scarce in the programming language domain, and it is costly to collect bilingual data manually. To address this issue, several unsupervised programming translation systems are proposed. However, these systems still rely on huge monolingual source code to train, which is very expensive. Besides, these models cannot perform well for translating the languages that are not seen during the pre-training procedure. In this paper, we propose SDA-Trans, a syntax and domain-aware model for program translation, which leverages the syntax structure and domain knowledge to enhance the cross-lingual transfer ability. SDA-Trans adopts unsupervised training on a smaller-scale corpus, including Python and Java monolingual programs. The experimental results on function translation tasks between Python, Java, and C++ show that SDA-Trans outperforms many large-scale pre-trained models, especially for unseen language translation.","program translation,neural networks,syntax structure,unsupervised learning",无监督程序翻译的语法和领域感知模型,随着软件和社会的发展，人们对软件迁移越来越感兴趣。在语言之间手动迁移项目容易出错，而且成本高昂。近年来，研究人员开始通过从大规模并行代码语料库中学习，探索使用监督深度学习技术的程序自动翻译。然而，在编程语言领域，并行资源稀缺，手动收集双语数据的成本很高。为了解决这个问题，提出了几种无监督编程翻译系统。然而，这些系统仍然依赖于庞大的单语源代码进行训练，这是非常昂贵的。此外，这些模型不能很好地翻译在预训练过程中看不到的语言。在本文中，我们提出了SDA-Trans，这是一种用于程序翻译的语法和领域感知模型，它利用语法结构和领域知识来增强跨语言迁移能力。SDA-Trans在较小规模的语料库上采用无监督训练，包括Python和Java单语程序。在Python、Java和C++之间的函数翻译任务上的实验结果表明，SDA-Trans优于许多大规模的预训练模型，尤其是在看不见的语言翻译方面。,程序翻译，神经网络，语法结构，无监督学习,,,
WFUDFL3Z,2023,https://doi.org/10.1109/ICSE48619.2023.00014,ICSE 2023,CCRep: Learning Code Change Representations via Pre-Trained Code Model and Query Back,"Representing code changes as numeric feature vectors, i.e., code change representations, is usually an essential step to automate many software engineering tasks related to code changes, e.g., commit message generation and just-in-time defect prediction. Intuitively, the quality of code change representations is crucial for the effectiveness of automated approaches. Prior work on code changes usually designs and evaluates code change representation approaches for a specific task, and little work has investigated code change encoders that can be used and jointly trained on various tasks. To fill this gap, this work proposes a novel Code Change Representation learning approach named CCRep, which can learn to encode code changes as feature vectors for diverse downstream tasks. Specifically, CCRep regards a code change as the combination of its before-change and after-change code, leverages a pre-trained code model to obtain high-quality contextual embeddings of code, and uses a novel mechanism named query back to extract and encode the changed code fragments and make them explicitly interact with the whole code change. To evaluate CCRep and demonstrate its applicability to diverse code-change-related tasks, we apply it to three tasks: commit message generation, patch correctness assessment, and just-in-time defect prediction. Experimental results show that CCRep outperforms the state-of-the-art techniques on each task.","code change,representation learning,commit message generation,patch correctness assessment,just-in-time defect prediction",CCRep：通过预先训练的代码模型和查询返回学习代码更改表示,将代码更改表示为数字特征向量，即代码更改表示，通常是自动化许多与代码更改相关的软件工程任务的重要步骤，例如，提交消息生成和实时缺陷预测。直观地说，代码更改表示的质量对于自动化方法的有效性至关重要。先前关于代码更改的工作通常为特定任务设计和评估代码更改表示方法，很少有工作研究可以在各种任务中使用和联合训练的代码更改编码器。为了填补这一空白，本工作提出了一种新的代码变化表示学习方法CCRep，该方法可以学习将代码变化编码为不同下游任务的特征向量。具体而言，CCRep将代码更改视为其更改前和更改后代码的组合，利用预先训练的代码模型来获得高质量的代码上下文嵌入，并使用一种名为“查询回”的新机制来提取和编码更改后的代码片段，使其与整个代码更改显式交互。为了评估CCRep并证明其对各种代码更改相关任务的适用性，我们将其应用于三个任务：提交消息生成、补丁正确性评估和及时缺陷预测。实验结果表明，CCRep在每项任务上都优于最先进的技术。,代码更改，表示学习，提交消息生成，补丁正确性评估，及时缺陷预测,,,
TR6DSQCG,2023,https://doi.org/10.1109/ICSE48619.2023.00207,ICSE 2023,ContraBERT: Enhancing Code Pre-trained Models via Contrastive Learning,"Large-scale pre-trained models such as CodeBERT, GraphCodeBERT have earned widespread attention from both academia and industry. Attributed to the superior ability in code representation, they have been further applied in multiple downstream tasks such as clone detection, code search and code translation. However, it is also observed that these state-of-the-art pre-trained models are susceptible to adversarial attacks. The performance of these pre-trained models drops significantly with simple perturbations such as renaming variable names. This weakness may be inherited by their downstream models and thereby amplified at an unprecedented scale. To this end, we propose an approach namely ContraBERT that aims to improve the robustness of pre-trained models via contrastive learning. Specifically, we design nine kinds of simple and complex data augmentation operators on the programming language (PL) and natural language (NL) data to construct different variants. Furthermore, we continue to train the existing pre-trained models by masked language modeling (MLM) and contrastive pre-training task on the original samples with their augmented variants to enhance the robustness of the model. The extensive ex-periments demonstrate that ContraBERT can effectively improve the robustness of the existing pre-trained models. Further study also confirms that these robustness-enhanced models provide improvements as compared to original models over four popular downstream tasks.","Code Pre-trained Models,Contrastive Learning,Model Robustness",ContraBERT：通过对比学习增强代码预训练模型,CodeBERT、GraphCodeBERT等大规模预训练模型已经引起了学术界和工业界的广泛关注。由于在代码表示方面的卓越能力，它们已被进一步应用于克隆检测、代码搜索和代码翻译等多个下游任务。然而，也观察到，这些最先进的预训练模型容易受到对抗性攻击。这些预训练模型的性能随着简单的扰动（如重命名变量名）而显著下降。这种弱点可能会被他们的下游模型继承，从而以前所未有的规模放大。为此，我们提出了一种方法，即ContraBERT，旨在通过对比学习提高预训练模型的稳健性。具体来说，我们在编程语言（PL）和自然语言（NL）数据上设计了九种简单和复杂的数据扩充算子来构造不同的变体。此外，我们通过掩蔽语言建模（MLM）和对原始样本及其增广变体的对比预训练任务，继续训练现有的预训练模型，以增强模型的鲁棒性。大量的实验表明，ContraBERT可以有效地提高现有预训练模型的鲁棒性。进一步的研究还证实，与四个流行的下游任务的原始模型相比，这些鲁棒性增强的模型提供了改进。,代码预训练模型，对比学习，模型鲁棒性,,,
UQHIMJNX,2023,https://doi.org/10.1109/ICSE48619.2023.00040,ICSE 2023,Learning Graph-based Code Representations for Source-level Functional Similarity Detection,"Detecting code functional similarity forms the basis of various software engineering tasks. However, the detection is challenging as functionally similar code fragments can be implemented differently, e.g., with irrelevant syntax. Recent studies incorporate program dependencies as semantics to identify syntactically different yet semantically similar programs, but they often focus only on local neighborhoods (e.g., one-hop dependencies), limiting the expressiveness of program semantics in modeling functionalities. In this paper, we present Tailor that explicitly exploits deep graph-structured code features for functional similarity detection. Given source-level programs, Tailor first represents them into code property graphs (CPGs) - which combine abstract syntax trees, control flow graphs, and data flow graphs - to collectively reason about program syntax and semantics. Then, Tailor learns representations of CPGs by applying a CPG-based neural network (CPGNN) to iteratively propagate information on them. It improves over prior work on code representation learning through a new graph neural network (GNN) tailored to CPG structures instead of the off-the-shelf GNNs used previously. We systematically evaluate Tailor on C and Java programs using two public benchmarks. Experimental results show that Tailor outperforms the state-of-the-art approaches, achieving 99.8% and 99.9% F-scores in code clone detection and 98.3% accuracy in source code classification.","Representation learning,Codes,Source coding,Semantics,Cloning,Syntactics,Graph neural networks",用于源级函数相似性检测的基于学习图的代码表示,检测代码功能的相似性构成了各种软件工程任务的基础。然而，检测是具有挑战性的，因为功能相似的代码片段可以以不同的方式实现，例如，使用不相关的语法。最近的研究将程序依赖性作为语义来识别语法上不同但语义上相似的程序，但它们通常只关注局部邻域（例如，单跳依赖性），限制了程序语义在建模功能中的表现力。在本文中，我们提出了Tailor，它显式地利用深度图结构化代码特征进行功能相似性检测。给定源代码级别的程序，Tailor首先将它们表示为代码属性图（CPG），它结合了抽象语法树、控制流图和数据流图，以共同推理程序语法和语义。然后，Tailor通过应用基于CPG的神经网络（CPGNN）迭代传播CPG上的信息来学习CPG的表示。它通过一种新的图神经网络（GNN）来改进先前在代码表示学习方面的工作，该网络是为CPG结构量身定制的，而不是以前使用的现成的GNN。我们使用两个公共基准对Tailor on C和Java程序进行了系统的评估。实验结果表明，Tailor优于最先进的方法，在代码克隆检测中实现了99.8%和99.9%的F分数，在源代码分类中实现了98.3%的准确率。,表示学习，编码，源代码，语义，克隆，语法，图形神经网络,,,
G72GCHI9,2023,https://doi.org/10.1109/ICSE48619.2023.00042,ICSE 2023,Reachable Coverage: Estimating Saturation in Fuzzing,"Reachable coverage is the number of code elements in the search space of a fuzzer (i.e., an automatic software testing tool). A fuzzer cannot find bugs in code that is unreachable. Hence, reachable coverage quantifies fuzzer effectiveness. Using static program analysis, we can compute an upper bound on the number of reachable coverage elements, e.g., by extracting the call graph. However, we cannot decide whether a coverage element is reachable in general. If we could precisely determine reachable coverage efficiently, we would have solved the software verification problem. Unfortunately, we cannot approach a given degree of accuracy for the static approximation, either. In this paper, we advocate a statistical perspective on the approximation of the number of elements in the fuzzer's search space, where accuracy does improve as a function of the analysis runtime. In applied statistics, corresponding estimators have been developed and well established for more than a quarter century. These estimators hold an exciting promise to finally tackle the long-standing challenge of counting reachability. In this paper, we explore the utility of these estimators in the context of fuzzing. Estimates of reachable coverage can be used to measure (a) the amount of untested code, (b) the effectiveness of the testing technique, and (c) the completeness of the ongoing fuzzing campaign (w.r.t. the asymptotic max. achievable coverage). We make all data and our analysis publicly available.","Codes,Upper bound,Runtime,Computer bugs,Fuzzing,Software",可达覆盖：引信饱和估计,可达覆盖率是模糊器（即自动软件测试工具）搜索空间中的代码元素数量。模糊器无法在无法访问的代码中找到错误。因此，可达覆盖量化了模糊有效性。使用静态程序分析，我们可以计算可达覆盖元素数量的上限，例如，通过提取调用图。然而，我们不能决定覆盖元素是否在一般情况下是可到达的。如果我们能够准确有效地确定可达覆盖范围，我们就会解决软件验证问题。不幸的是，我们也无法接近静态近似的给定精度。在本文中，我们提倡从统计学的角度来近似模糊搜索空间中的元素数量，其中准确度确实随着分析运行时间的函数而提高。在应用统计学中，四分之一个多世纪以来，相应的估计量已经得到了发展和完善。这些估计器有望最终解决计数可达性的长期挑战。在本文中，我们探讨了这些估计量在模糊上下文中的效用。可达覆盖率的估计可用于测量（a）未测试代码的数量，（b）测试技术的有效性，以及（c）正在进行的模糊化活动的完整性（w.r.t.渐近最大可达覆盖率）。我们公开所有数据和分析。,代码，上限，运行时，计算机错误，模糊，软件,,,
EPWFEN77,2023,https://doi.org/10.1109/ICSE48619.2023.00094,ICSE 2023,CHRONOS: Time-Aware Zero-Shot Identification of Libraries from Vulnerability Reports,"Tools that alert developers about library vulnerabilities depend on accurate, up-to-date vulnerability databases which are maintained by security researchers. These databases record the libraries related to each vulnerability. However, the vulnerability reports may not explicitly list every library and human analysis is required to determine all the relevant libraries. Human analysis may be slow and expensive, which motivates the need for automated approaches. Researchers and practitioners have proposed to automatically identify libraries from vulnerability reports using extreme multi-label learning (XML). While state-of-the-art XML techniques showed promising performance, their experimental settings do not practically fit what happens in reality. Previous studies randomly split the vulnerability reports data for training and testing their models without considering the chronological order of the reports. This may unduly train the models on chronologically newer reports while testing the models on chronologically older ones. However, in practice, one often receives chronologically new reports, which may be related to previously unseen libraries. Under this practical setting, we observe that the performance of current XML techniques declines substantially, e.g., F1 decreased from 0.7 to 0.24 under experiments without and with consideration of chronological order of vulnerability reports. We propose a practical library identification approach, namely Chronos, based on zero-shot learning. The novelty of Chronos is three-fold. First, Chronos fits into the practical pipeline by considering the chronological order of vulnerability reports. Second, Chronos enriches the data of the vulnerability descriptions and labels using a carefully designed data enhancement step. Third, Chronos exploits the temporal ordering of the vulnerability reports using a cache to prioritize prediction of versions of libraries that recently had reports of vulnerabilities. In our experiments, Chronos achieves an average F1-score of 0.75, 3x better than the best XML-based approach. Data enhancement and the time-aware adjustment improve Chronos over the vanilla zero-shot learning model by 27% in average F1.","zero-shot learning,library identification,unseen labels,extreme multi-label classification,vulnerability reports",CHRONOS:Time-Aware从漏洞报告中识别库的零样本,提醒开发人员库漏洞的工具依赖于安全研究人员维护的准确、最新的漏洞数据库。这些数据库记录了与每个漏洞相关的库。然而，漏洞报告可能不会明确列出每个库，需要人工分析来确定所有相关库。人工分析可能是缓慢和昂贵的，这激发了对自动化方法的需求。研究人员和从业者提出使用极端多标签学习（XML）从漏洞报告中自动识别库。虽然最先进的XML技术显示出了良好的性能，但它们的实验设置实际上并不适合实际情况。先前的研究随机分割漏洞报告数据，用于训练和测试其模型，而不考虑报告的时间顺序。这可能会在按时间顺序较新的报告上过度训练模型，而在按时间排序较旧的报告上测试模型。然而，在实践中，人们经常会收到按时间顺序排列的新报告，这些报告可能与以前看不见的库有关。在这种实际情况下，我们观察到当前XML技术的性能大幅下降，例如，在没有考虑漏洞报告的时间顺序的实验下，F1从0.7下降到0.24。我们提出了一种实用的基于零样本学习的图书馆识别方法，即Chrono。Chronos的新颖性有三个方面。首先，Chronos通过考虑漏洞报告的时间顺序，符合实际情况。其次，Chronos通过精心设计的数据增强步骤丰富了漏洞描述和标签的数据。第三，Chronos利用漏洞报告的时间顺序，使用缓存来优先预测最近有漏洞报告的库版本。在我们的实验中，Chronos获得了0.75的平均F1分数，比最好的基于XML的方法好3倍。数据增强和时间感知调整使Chrono在F1中的平均成绩比普通零样本学习模型提高了27%。,零样本学习，库识别，看不见的标签，极端多标签分类，漏洞报告,,,
PZRFHV9W,2023,https://doi.org/10.1109/ICSE48619.2023.00071,ICSE 2023,Using Reactive Synthesis: An End-to-End Exploratory Case Study,"Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. Despite its attractiveness and major research progress in the past decades, reactive synthesis is still in early-stage and has not gained popularity outside academia. We conducted an exploratory case study in which we followed students in a semester-long university workshop class on their end-to-end use of a reactive synthesizer, from writing the specifications to executing the synthesized controllers. The data we collected includes more than 500 versions of more than 80 specifications, as well as more than 2500 Slack messages, all written by the class participants. Our grounded theory analysis reveals that the use of reactive synthesis has clear benefits for certain tasks and that adequate specification language constructs assist in the specification writing process. However, inherent issues such as unrealizabilty, non-well-separation, the gap of knowledge between the users and the synthesizer, and considerable running times prevent reactive synthesis from fulfilling its promise. Based on our analysis, we propose action items in the directions of language and specification quality, tools for analysis and execution, and process and methodology, all towards making reactive synthesis more applicable for software engineers.","Reactive synthesis,Formal specifications",使用反应合成：一个端到端的探索性案例研究,反应性综合是一种自动过程，通过构建反应性系统的时间逻辑规范来获得正确的反应性系统。尽管反应合成在过去几十年中具有吸引力并取得了重大研究进展，但它仍处于早期阶段，在学术界之外尚未流行起来。我们进行了一项探索性的案例研究，在该研究中，我们跟踪了一个学期的大学研讨会班上的学生，了解他们对反应式合成器的端到端使用，从编写规范到执行合成控制器。我们收集的数据包括80多种规格的500多个版本，以及2500多条Slack消息，所有这些都是由课堂参与者编写的。我们的基础理论分析表明，反应合成的使用对某些任务有明显的好处，并且适当的规范语言结构有助于规范编写过程。然而，诸如不可行、分离不好、用户和合成器之间的知识差距以及相当长的运行时间等固有问题阻碍了反应合成实现其承诺。基于我们的分析，我们在语言和规范质量、分析和执行工具以及过程和方法论方面提出了行动项目，所有这些都是为了使反应式合成更适用于软件工程师。,反应合成，正式规范,,,
ZQCIQAZP,2023,https://doi.org/10.1109/ICSE48619.2023.00063,ICSE 2023,Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation,"Software bugs claim ≈ 50 % of development time and cost the global economy billions of dollars. Once a bug is reported, the assigned developer attempts to identify and understand the source code responsible for the bug and then corrects the code. Over the last five decades, there has been significant research on automatically finding or correcting software bugs. However, there has been little research on automatically explaining the bugs to the developers, which is essential but a highly challenging task. In this paper, we propose Bugsplainer, a transformer-based generative model, that generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits. Bugsplainer can leverage structural information and buggy patterns from the source code to generate an explanation for a bug. Our evaluation using three performance metrics shows that Bugsplainer can generate understandable and good explanations according to Google's standard, and can outperform multiple baselines from the literature. We also conduct a developer study involving 20 participants where the explanations from Bugsplainer were found to be more accurate, more precise, more concise and more useful than the baselines.","software bug,bug explanation,software engineering,software maintenance,natural language processing,deep learning,transformers",神经机器翻译中利用代码结构解释软件缺陷,软件错误占用了大约50%的开发时间，并使全球经济损失了数十亿美元。一旦报告了错误，指定的开发人员就会尝试识别和理解导致错误的源代码，然后更正代码。在过去的五十年里，人们对自动发现或纠正软件错误进行了大量研究。然而，很少有人研究自动向开发人员解释错误，这是一项重要但极具挑战性的任务。在本文中，我们提出了Bugsplainer，这是一个基于转换器的生成模型，它通过从大量的错误修复提交语料库中学习来生成软件错误的自然语言解释。Bugsplainer可以利用源代码中的结构信息和错误模式来生成错误的解释。我们使用三个性能指标进行的评估表明，Bugsplainer可以根据谷歌的标准生成可理解的良好解释，并且可以优于文献中的多个基线。我们还进行了一项涉及20名参与者的开发人员研究，发现Bugsplainer的解释比基线更准确、更精确、更简洁、更有用。,软件错误，错误解释，软件工程，软件维护，自然语言处理，深度学习，transformers,,,
8XVV4PQC,2023,https://doi.org/10.1109/ICSE48619.2023.00067,ICSE 2023,Socio-Technical Anti-Patterns in Building ML-Enabled Software: Insights from Leaders on the Forefront,"Although machine learning (ML)-enabled software systems seem to be a success story considering their rise in economic power, there are consistent reports from companies and practitioners struggling to bring ML models into production. Many papers have focused on specific, and purely technical aspects, such as testing and pipelines, but only few on socio-technical aspects. Driven by numerous anecdotes and reports from practitioners, our goal is to collect and analyze socio-technical challenges of productionizing ML models centered around and within teams. To this end, we conducted the largest qualitative empirical study in this area, involving the manual analysis of 66 hours of talks that have been recorded by the MLOps community. By analyzing talks from practitioners for practitioners of a community with over 11,000 members in their Slack workspace, we found 17 anti-patterns, often rooted in organizational or management problems. We further list recommendations to overcome these problems, ranging from technical solutions over guidelines to organizational restructuring. Finally, we contextu-alize our findings with previous research, confirming existing results, validating our own, and highlighting new insights.","Economics,Biological system modeling,Pipelines,Production,Manuals,Machine learning,Software systems",构建ML软件的社会技术反模式：前沿领导者的见解,尽管考虑到机器学习（ML）软件系统在经济实力方面的崛起，它们似乎是一个成功的故事，但仍有来自努力将ML模型投入生产的公司和从业者的一致报告。许多论文都集中在特定的纯技术方面，如测试和管道，但很少涉及社会技术方面。在从业者的众多轶事和报告的推动下，我们的目标是收集和分析围绕团队和团队内部生产ML模型的社会技术挑战。为此，我们在这一领域进行了最大规模的定性实证研究，涉及对MLOps社区记录的66小时会谈的手动分析。通过分析从业者对Slack工作区有11000多名成员的社区从业者的谈话，我们发现了17种反模式，这些模式往往源于组织或管理问题。我们进一步列出了克服这些问题的建议，从技术解决方案到指导方针，再到组织重组。最后，我们将我们的发现与以前的研究结合起来，确认现有的结果，验证我们自己的结果，并强调新的见解。,经济学，生物系统建模，管道，生产，手册，机器学习，软件系统,,,
999ZPQKK,2023,https://doi.org/10.1109/ICSE48619.2023.00166,ICSE 2023,AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces,"Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability. In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AidUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed ContextDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AidUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.","Dark Pattern,UI Analysis,UI Design",AidUI：实现用户界面中暗模式的自动识别,过去的研究表明，UI暗模式或用户界面的普遍性可能会导致最终用户（在不知不觉中）采取他们可能无意的行动。这种欺骗性的UI设计可能是有意的（为了使在线服务受益），也可能是无意的（通过共谋设计实践），并可能对最终用户造成不利影响，如过度共享个人信息或经济损失。尽管在不同软件领域开发暗模式分类法方面取得了重大研究进展，但开发人员和用户目前缺乏帮助识别、避免和导航这些往往微妙的设计主题的指导。然而，暗模式的自动识别是一项具有挑战性的任务，因为单一类型模式的实例化可以采取多种形式，导致显著的可变性。在本文中，我们迈出了理解现代软件应用程序中常见UI暗模式可以自动识别的程度的第一步。为此，我们介绍了AidUI，这是一种新颖的自动化方法，它使用计算机视觉和自然语言处理技术来识别应用程序屏幕截图中的一组视觉和文本提示，这些提示表示存在十种独特的UI暗模式，允许对其进行检测、分类和定位。为了评估我们的方法，我们构建了ContextDP，这是目前最大的完全本地化的UI暗模式数据集，涵盖175个移动和83个web UI屏幕截图，其中包含301个暗模式实例。我们的评估结果表明，AidUI在检测暗模式实例时实现了0.66的总体精度，0.67的召回率，0.65的F1分数，报告的假阳性很少，并且能够以0.84的IoU分数定位检测到的模式。此外，我们研究的暗模式的一个重要子集可以非常可靠地检测到（F1得分超过0.82），未来的研究方向可能允许改进对附加模式的检测。这项工作展示了开发工具的合理性，以帮助开发人员识别并适当纠正欺骗性的UI模式。,暗模式，UI分析，UI设计,,,
C7F4EWHK,2023,https://doi.org/10.1109/ICSE48619.2023.00181,ICSE 2023,On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot,"Software engineering research has always being concerned with the improvement of code completion approaches, which suggest the next tokens a developer will likely type while coding. The release of GitHub Copilot constitutes a big step forward, also because of its unprecedented ability to automatically generate even entire functions from their natural language description. While the usefulness of Copilot is evident, it is still unclear to what extent it is robust. Specifically, we do not know the extent to which semantic-preserving changes in the natural language description provided to the model have an effect on the generated code function. In this paper we present an empirical study in which we aim at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function. A negative answer would pose questions on the robustness of deep learning (DL)-based code generators since it would imply that developers using different wordings to describe the same code would obtain different recommendations. We asked Copilot to automatically generate 892 Java methods starting from their original Javadoc description. Then, we generated different semantically equivalent descriptions for each method both manually and automatically, and we analyzed the extent to which predictions generated by Copilot changed. Our results show that modifying the description results in different code recommendations in ∼46% of cases. Also, differences in the semantically equivalent descriptions might impact the correctness of the generated code (±28%).","Empirical Study,Recommender Systems",代码生成技术的稳健性——基于GitHub Copilot的实证研究,软件工程研究一直关注代码完成方法的改进，这表明开发人员在编码时可能会键入下一个令牌。GitHub Copilot的发布是向前迈出的一大步，这也是因为它具有前所未有的能力，可以根据自然语言描述自动生成甚至整个函数。虽然Copilot的有用性是显而易见的，但它在多大程度上是稳健的还不清楚。具体来说，我们不知道提供给模型的自然语言描述中的语义保留变化对生成的代码函数的影响程度。在本文中，我们提出了一项实证研究，旨在了解不同但语义等效的自然语言描述是否会产生相同的推荐函数。否定的答案会对基于深度学习（DL）的代码生成器的稳健性提出质疑，因为这意味着开发人员使用不同的词语来描述同一代码会获得不同的推荐。我们要求Copilot从原始Javadoc描述开始自动生成892个Java方法。然后，我们手动和自动为每种方法生成了不同的语义等效描述，并分析了Copilot生成的预测变化的程度。我们的结果表明，修改描述会在~46%的情况下产生不同的代码建议。此外，语义等效描述的差异可能会影响生成代码的正确性（±28%）。,实证研究，推荐系统,,,
JHHG5E4J,2023,https://doi.org/10.1109/ICSE48619.2023.00068,ICSE 2023,Moving on from the Software Engineers' Gambit: An Approach to Support the Defense of Software Effort Estimates,"Pressure for higher productivity and faster delivery is increasingly pervading software organizations. This can lead software engineers to act like chess players playing a gambit—making sacrifices of their technically sound estimates, thus submitting their teams to time pressure. In turn, time pressure can have varied detrimental effects, such as poor product quality and emotional distress, decreasing productivity, which leads to more time pressure and delays: a hard-to-stop vicious cycle. This reveals a need for moving on from the more passive strategy of yielding to pressure to a more active one of defending software estimates. Therefore, we propose an approach to support software estimators in acquiring knowledge on how to carry out such defense, by introducing negotiation principles encapsulated in a set of defense lenses, presented through a digital simulation. We evaluated the proposed approach through a controlled experiment with software practitioners from different companies. We collected data on participants' attitudes, subjective norms, perceived behavioral control, and intentions to perform the defense of their estimates in light of the Theory of Planned Behavior. We employed a frequentist and a bayesian approach to data analysis. Results show improved scores among experimental group participants after engaging with the digital simulation and learning about the lenses. They were also more inclined to choose a defense action when facing pressure scenarios than a control group exposed to questions to reflect on the reasons and outcomes of pressure over estimates. Qualitative evidence reveals that practitioners perceived the set of lenses as useful in their current work environments. Collectively, these results show the effectiveness of the proposed approach and its perceived relevance for the industry, despite the low amount of time required to engage with it.","Software Effort Estimation,Negotiation,Behavioral Software Engineering,Defense of Estimates",从软件工程师的甘比特开始：一种支持软件工作量估计防御的方法,要求更高生产力和更快交付的压力正日益渗透到软件组织中。这可能会导致软件工程师表现得像下棋一样——牺牲他们在技术上合理的估计，从而使他们的团队面临时间压力。反过来，时间压力会产生各种不利影响，如产品质量差和情绪困扰，生产力下降，从而导致更多的时间压力和延误：这是一个难以阻止的恶性循环。这表明需要从更被动的屈服于压力的策略转向更主动的捍卫软件估计的策略。因此，我们提出了一种方法，通过引入封装在一组防御透镜中的协商原则，支持软件评估人员获取如何进行此类防御的知识，这些透镜通过数字模拟呈现。我们通过对来自不同公司的软件从业者进行的对照实验来评估所提出的方法。我们收集了参与者的态度、主观规范、感知行为控制以及根据计划行为理论为其估计进行辩护的意图的数据。我们采用了频率分析法和贝叶斯方法进行数据分析。结果显示，实验组参与者在参与数字模拟和学习镜片后，得分有所提高。与面临问题的对照组相比，他们在面对压力情景时更倾向于选择防御行动，以反思压力超过估计的原因和结果。定性证据表明，从业者认为这套镜片在他们当前的工作环境中很有用。总的来说，这些结果表明了拟议方法的有效性及其对行业的相关性，尽管参与该方法所需的时间很短。,软件工作量估计，协商，行为软件工程，估计辩护,,,
EAMDWS24,2023,https://doi.org/10.1109/ICSE48619.2023.00169,ICSE 2023,Sustainability is Stratified: Toward a Better Theory of Sustainable Software Engineering,"Background: Sustainable software engineering (SSE) means creating software in a way that meets present needs without undermining our collective capacity to meet our future needs. It is typically conceptualized as several intersecting dimensions or “pillars”-environmental, social, economic, technical and in-dividual. However; these pillars are theoretically underdeveloped and require refinement. Objectives: The objective of this paper is to generate a better theory of SSE. Method: First, a scoping review was conducted to understand the state of research on SSE and identify existing models thereof. Next, a meta-synthesis of qualitative research on SSE was conducted to critique and improve the existing models identified. Results: 961 potentially relevant articles were extracted from five article databases. These articles were de-duplicated and then screened independently by two screeners, leaving 243 articles to examine. Of these, 109 were non-empirical, the most common empirical method was systematic review, and no randomized controlled experiments were found. Most papers focus on ecological sustainability (158) and the sustainability of software products (148) rather than processes. A meta-synthesis of 36 qualitative studies produced several key propositions, most notably, that sustainability is stratified (has different meanings at different levels of abstraction) and multisystemic (emerges from interactions among multiple social, technical, and sociotechnical systems). Conclusion: The academic literature on SSE is surprisingly non-empirical. More empirical evaluations of specific sustainability interventions are needed. The sustainability of software development products and processes should be conceptualized as multisystemic and stratified, and assessed accordingly.","Sustainable development,software engineering,sustainable software engineering,scoping review,meta-synthesis",可持续性是分层的：迈向更好的可持续软件工程理论,背景：可持续软件工程（SSE）意味着在不破坏我们满足未来需求的集体能力的情况下，以满足当前需求的方式创建软件。它通常被概念化为几个交叉的维度或“支柱”——环境、社会、经济、技术和个人。然而这些支柱在理论上还不成熟，需要改进。目的：本文的目的是生成一个更好的SSE理论。方法：首先，进行范围界定审查，以了解SSE的研究现状，并确定其现有模型。接下来，对SSE进行了定性研究的元综合，以批判和改进现有的模型。结果：从五个文章数据库中提取了961篇潜在相关文章。这些文章被消除重复，然后由两名筛选人员独立筛选，留下243篇文章需要检查。其中109个是非经验的，最常见的经验方法是系统综述，没有发现随机对照实验。大多数论文关注的是生态可持续性（158）和软件产品的可持续性（148），而不是过程。36项定性研究的元综合提出了几个关键命题，最值得注意的是，可持续性是分层的（在不同的抽象层次上有不同的含义）和多系统的（来自多个社会、技术和社会技术系统之间的互动）。结论：令人惊讶的是，关于SSE的学术文献是非实证的。需要对具体的可持续性干预措施进行更多的实证评估。软件开发产品和过程的可持续性应被概念化为多系统和分层的，并进行相应的评估。,可持续发展，软件工程，可持续软件工程，范围界定审查，元综合,,,
APPWDCE3,2023,https://doi.org/10.1109/ICSE48619.2023.00127,ICSE 2023,Template-based Neural Program Repair,"In recent years, template-based and NMT-based automated program repair methods have been widely studied and achieved promising results. However, there are still disadvantages in both methods. The template-based methods cannot fix the bugs whose types are beyond the capabilities of the templates and only use the syntax information to guide the patch synthesis, while the NMT-based methods intend to generate the small range of fixed code for better performance and may suffer from the OOV (Out-of-vocabulary) problem. To solve these problems, we propose a novel template-based neural program repair approach called TENURE to combine the template-based and NMT- based methods. First, we build two large-scale datasets for 35 fix templates from template-based method and one special fix template (single-line code generation) from NMT-based method, respectively. Second, the encoder-decoder models are adopted to learn deep semantic features for generating patch intermediate representations (IRs) for different templates. The optimized copy mechanism is also used to alleviate the OOV problem. Third, based on the combined patch IRs for different templates, three tools are developed to recover real patches from the patch IRs, replace the unknown tokens, and filter the patch candidates with compilation errors by leveraging the project-specific information. On Defects4J-vl.2, TENURE can fix 79 bugs and 52 bugs with perfect and Ochiai fault localization, respectively. It is able to repair 50 and 32 bugs as well on Defects4J-v2.0. Compared with the existing template-based and NMT-based studies, TENURE achieves the best performance in all experiments.","automated program repair,fix templates,neural machine translation,deep learning",基于模板的神经程序修复,近年来，基于模板和NMT的自动化程序修复方法得到了广泛的研究，并取得了可喜的成果。然而，这两种方法仍然存在缺点。基于模板的方法无法修复类型超出模板能力的错误，只能使用语法信息来指导补丁合成，而基于NMT的方法则倾向于生成小范围的固定代码以获得更好的性能，并可能存在OOV（词汇表外）问题。为了解决这些问题，我们提出了一种新的基于模板的神经程序修复方法，称为TENURE，将基于模板的方法和基于NMT的方法相结合。首先，我们分别为基于模板的方法的35个固定模板和基于NMT的方法的一个特殊固定模板（单行代码生成）构建了两个大规模数据集。其次，采用编码器-解码器模型来学习深层语义特征，以生成不同模板的补丁中间表示。优化的复制机制也用于缓解OOV问题。第三，基于不同模板的组合补丁IRs，开发了三种工具，用于从补丁IRs中恢复真实补丁，替换未知令牌，并通过利用项目特定信息过滤编译错误的候选补丁。在Defects4J vl.2上，TENURE可以通过完美和Ochiai故障定位分别修复79个错误和52个错误。它能够修复Defects4J-v2.0上的50个和32个错误。与现有的基于模板和基于NMT的研究相比，TENURE在所有实验中都取得了最好的性能。,自动化程序修复，修复模板，神经机器翻译，深度学习,,,
2WGVKRLV,2023,https://doi.org/10.1109/ICSE48619.2023.00142,ICSE 2023,Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX,"Intel's SGX is a confidential computing technique. It allows key functionalities of C/C++/native applications to be confidentially executed in hardware enclaves. However, numerous cloud applications are written in Java. For supporting their confidential computing, state-of-the-art approaches deploy Java Virtual Machines (JVMs) in enclaves and perform confidential computing on JVMs. Meanwhile, these JVM-in-enclave solutions still suffer from serious limitations, such as heavy overheads of running JVMs in enclaves, large attack surfaces, and deep computation stacks. To mitigate the above limitations, we for-malize a Secure Closed-World (SCW) principle and then propose Lejacon, a lightweight and efficient approach to Java confidential computing. The key idea is, given a Java application, to (1) separately compile its confidential computing tasks into a bundle of Native Confidential Computing (NCC) services; (2) run the NCC services in enclaves on the Trusted Execution Environment (TEE) side, and meanwhile run the non-confidential code on a JVM on the Rich Execution Environment (REE) side. The two sides interact with each other, protecting confidential computing tasks and as well keeping the Trusted Computing Base (TCB) size small. We implement Lejacon and evaluate it against OcclumJ (a state-of-the-art JVM-in-enclave solution) on a set of benchmarks using the BouncyCastle cryptography library. The evaluation results clearly show the strengths of Lejacon: it achieves compet-itive performance in running Java confidential code in enclaves; compared with OcclumJ, Lejacon achieves speedups by up to 16.2x in running confidential code and also reduces the TCB sizes by 90+% on average.","Software Guard Extensions,Separation Compilation,Native Confidential Computing Service,Runtime,Secure Closed-World",Lejacon:一种在SGX上实现Java机密计算的轻量级高效方法,英特尔的SGX是一种保密的计算技术。它允许C/C++/本机应用程序的关键功能在硬件包围区中秘密执行。然而，许多云应用程序都是用Java编写的。为了支持他们的机密计算，最先进的方法在飞地中部署Java虚拟机（JVM），并在JVM上执行机密计算。同时，这些飞地中JVM解决方案仍然受到严重的限制，例如在飞地中运行JVM的巨大开销、大型攻击面和深度计算堆栈。为了缓解上述限制，我们提出了一种安全封闭世界（SCW）原则，然后提出了Lejacon，一种轻量级、高效的Java机密计算方法。关键思想是，在给定Java应用程序的情况下，（1）将其机密计算任务单独编译为本地机密计算（NCC）服务包；（2） 在可信执行环境（TEE）侧的飞地中运行NCC服务，同时在富执行环境（REE）侧的JVM上运行非机密代码。双方相互作用，保护机密计算任务，并保持可信计算库（TCB）的规模较小。我们使用BouncyCastle密码库在一组基准测试上实现了Lejacon，并针对OcclumJ（一种最先进的JVM内飞地解决方案）对其进行了评估。评估结果清楚地表明了Lejacon的优势：它在飞地中运行Java机密代码方面取得了竞争性的性能；与OcclumJ相比，Lejacon在运行机密代码方面实现了高达16.2倍的加速，并且平均将TCB大小减少了90%以上。,软件保护扩展，分离编译，本地机密计算服务，运行时，安全封闭世界,,,
J67RVFGU,2023,https://doi.org/10.1109/ICSE48619.2023.00136,ICSE 2023,Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks,"The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding min-imal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions-amplifying existing biases or introducing new ones-that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids-such as severity and causal explanations-crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present Dice: an information-theoretic testing and debugging framework to discover and localize fairness defects in DNNs. The key goal of Dice is to assist software developers in triaging fairness defects by ordering them by their severity. Towards this goal, we quantify fairness in terms of protected information (in bits) used in decision making. A quantitative view of fairness defects not only helps in ordering these defects, our empirical evaluation shows that it improves the search efficiency due to resulting smoothness of the search space. Guided by the quan-titative fairness, we present a causal debugging framework to localize inadequately trained layers and neurons responsible for fairness defects. Our experiments over ten DNNs, developed for socially critical tasks, show that Dice efficiently characterizes the amounts of discrimination, effectively generates discriminatory instances (vis-a-vis the state-of-the-art techniques), and localizes layers/neurons with significant biases.","Algorithmic Fairness,Information Theory,Software Testing,Fairness Defect Localization,Bias Mitigation",深度神经网络公平性缺陷的信息论测试与调试,深度前馈神经网络（DNN）越来越多地应用于社会经济关键决策支持软件系统中。DNN非常善于在其训练数据中找到最小、足够的统计模式。因此，DNN可能会学会对放大现有偏见或引入新偏见的决策进行编码，这些偏见可能对受保护的个人/群体不利，并可能违反法律保护。虽然现有的基于搜索的软件测试方法在发现公平性缺陷方面是有效的，但它们并没有用调试辅助工具来补充这些缺陷，例如严重性和因果解释，这对帮助开发人员分类和决定下一步行动至关重要。我们能衡量DNN中公平性缺陷的严重程度吗？这些缺陷是不当训练的症状，还是仅仅反映了训练数据中存在的偏见？为了回答这些问题，我们提出了Dice：一个信息论测试和调试框架，用于发现和定位DNN中的公平性缺陷。Dice的主要目标是帮助软件开发人员通过按严重程度排序来测试公平性缺陷。为了实现这一目标，我们量化了决策中使用的受保护信息（以位为单位）的公平性。公平性缺陷的定量观点不仅有助于对这些缺陷进行排序，我们的经验评估表明，由于搜索空间的平滑性，它提高了搜索效率。在概念公平的指导下，我们提出了一个因果调试框架来定位训练不足的层和导致公平缺陷的神经元。我们针对社会批判任务开发的十多个DNN的实验表明，Dice有效地表征了歧视的数量，有效地生成了歧视实例（相对于最先进的技术），并定位了具有显著偏见的层/神经元。,算法公平，信息论，软件测试，公平缺陷定位，偏差缓解,,,
J3X6GQGZ,2023,https://doi.org/10.1109/ICSE48619.2023.00056,ICSE 2023,ECSTATIC: An Extensible Framework for Testing and Debugging Configurable Static Analysis,"Testing and debugging the implementation of static analysis is a challenging task, often involving significant manual effort from domain experts in a tedious and unprincipled process. In this work, we propose an approach that greatly improves the automation of this process for static analyzers with configuration options. At the core of our approach is the novel adaptation of the theoretical partial order relations that exist between these options to reason about the correctness of actual results from running the static analyzer with different configurations. This allows for automated testing of static analyzers with clearly defined oracles, followed by automated delta debugging, even in cases where ground truths are not defined over the input programs. To apply this approach to many static analysis tools, we design and implement ECSTATIC, an easy-to-extend, open-source framework. We have integrated four popular static analysis tools, SOOT, WALA, DOOP, and FlowDroid, into ECSTATIC. Our evaluation shows running ECSTATIC detects 74 partial order bugs in the four tools and produces reduced bug-inducing programs to assist debugging. We reported 42 bugs; in all cases where we received responses, the tool developers confirmed the reported tool behavior was unintended. So far, three bugs have been fixed and there are ongoing discussions to fix more.","Program analysis,testing and debugging",ECSTATIC：一个可扩展的可配置静态分析测试和调试框架,测试和调试静态分析的实现是一项具有挑战性的任务，通常需要领域专家在一个乏味且无原则的过程中进行大量的手动工作。在这项工作中，我们提出了一种方法，该方法大大提高了具有配置选项的静态分析器的自动化程度。我们方法的核心是对这些选项之间存在的理论偏序关系进行新颖的调整，以推断运行不同配置的静态分析器的实际结果的正确性。这允许使用明确定义的oracles对静态分析器进行自动测试，然后进行自动delta调试，即使在输入程序中没有定义基本事实的情况下也是如此。为了将这种方法应用于许多静态分析工具，我们设计并实现了ECSTATIC，这是一个易于扩展的开源框架。我们已经将四种流行的静态分析工具SOOT、WALA、DOOP和FlowDroid集成到ECSTATIC中。我们的评估显示，运行ECSTATIC可以在四个工具中检测到74个偏序错误，并生成减少的错误引发程序来帮助调试。我们报告了42个错误；在我们收到回复的所有情况下，工具开发人员都确认报告的工具行为是无意的。到目前为止，已经修复了三个错误，目前正在讨论如何修复更多错误。,程序分析，测试和调试,,,
7IC3C6PR,2023,https://doi.org/10.1109/ICSE48619.2023.00109,ICSE 2023,Better Automatic Program Repair by Using Bug Reports and Tests Together,"Automated program repair is already deployed in industry, but concerns remain about repair quality. Recent research has shown that one of the main reasons repair tools produce incorrect (but seemingly correct) patches is imperfect fault localization (FL). This paper demonstrates that combining information from natural-language bug reports and test executions when localizing faults can have a significant positive impact on repair quality. For example, existing repair tools with such FL are able to correctly repair 7 defects in the Defects4J benchmark that no prior tools have repaired correctly. We develop, Blues, the first information-retrieval-based, statement-level FL technique that requires no training data. We further develop RAFL, the first unsupervised method for combining multiple FL techniques, which outperforms a supervised method. Using RAFL, we create SBIR by combining Blues with a spectrum-based (SBFL) technique. Evaluated on 815 real-world defects, SBIR consistently ranks buggy statements higher than its underlying techniques. We then modify three state-of-the-art repair tools, Arja, SequenceR, and SimFix, to use SBIR, SBFL, and Blues as their internal FL. We evaluate the quality of the produced patches on 689 real-world defects. Arja and SequenceR significantly benefit from SBIR: Arja using SBIR correctly repairs 28 defects, but only 21 using SBFL, and only 15 using Blues; SequenceR using SBIR correctly repairs 12 defects, but only 10 using SBFL, and only 4 using Blues. SimFix, (which has internal mechanisms to overcome poor FL), correctly repairs 30 defects using SBIR and SBFL, but only 13 using Blues. Our work is the first investigation of simultaneously using multiple software artifacts for automated program repair, and our promising findings suggest future research in this directions is likely to be fruitful.","Automatic Program repair,Information retrieval based fault localization,Debugging,fault localization",将Bug报告和测试结合使用，实现更好的程序自动修复,自动化程序修复已经在工业中部署，但对修复质量的担忧仍然存在。最近的研究表明，修复工具产生不正确（但看似正确）补丁的主要原因之一是故障定位不完善。本文证明，在定位故障时，结合自然语言错误报告和测试执行中的信息可以对修复质量产生显著的积极影响。例如，具有这种FL的现有修复工具能够正确修复Defects4J基准中的7个缺陷，而以前的工具没有正确修复过这些缺陷。我们开发了第一种不需要训练数据的基于信息检索的语句级外语技术Blues。我们进一步开发了RAFL，这是第一种用于组合多种FL技术的无监督方法，其性能优于有监督方法。使用RAFL，我们通过将Blues与基于频谱的（SBFL）技术相结合来创建SBIR。SBIR对815个真实世界的缺陷进行了评估，始终将bug语句列为高于其底层技术的级别。然后，我们修改了三种最先进的修复工具，Arja、SequenceR和SimFix，以使用SBIR、SBFL和Blues作为其内部FL。我们评估了689个真实世界缺陷的补丁质量。Arja和SequenceR显著受益于SBIR：Arja使用SBIR正确修复了28个缺陷，但使用SBFL仅修复了21个，使用Blues仅修复了15个；SequenceR使用SBIR正确修复了12个缺陷，但使用SBFL仅修复了10个，使用Blues仅修复了4个。SimFix（具有克服不良FL的内部机制）使用SBIR和SBFL正确修复了30个缺陷，但使用Blues仅修复了13个。我们的工作是第一次同时使用多个软件工件进行自动程序修复的研究，我们有希望的发现表明，未来在这方面的研究可能会富有成效。,自动程序修复，基于信息检索的故障定位，调试，故障定位,,,
W7AK4B3V,2023,https://doi.org/10.1109/ICSE48619.2023.00073,ICSE 2023,Developer-Intent Driven Code Comment Generation,"Existing automatic code comment generators mainly focus on producing a general description of functionality for a given code snippet without considering developer intentions. However, in real-world practice, comments are complicated, which often contain information reflecting various intentions of developers, e.g., functionality summarization, design rationale, implementation details, code properties, etc. To bridge the gap between automatic code comment generation and real-world comment practice, we define Developer-Intent Driven Code Comment Generation, which can generate intent-aware comments for the same source code with different intents. To tackle this challenging task, we propose DOME, an approach that utilizes Intent-guided Selective Attention to explicitly select intent-relevant information from the source code, and produces various comments reflecting different intents. Our approach is evaluated on two real-world Java datasets, and the experimental results show that our approach outperforms the state-of-the-art baselines. A human evaluation also confirms the significant potential of applying DOME in practical usage, enabling developers to comment code effectively according to their own needs.","Code Comment Generation,Intent-Controllable Comment Generation,Automated Comment-Intent Labeling",开发人员意图驱动的代码注释生成,现有的自动代码注释生成器主要专注于为给定的代码片段生成功能的一般描述，而不考虑开发人员的意图。然而，在现实世界的实践中，注释是复杂的，通常包含反映开发人员各种意图的信息，例如功能摘要、设计原理、实现细节、代码属性等。为了弥合自动代码注释生成和现实世界注释实践之间的差距，我们定义了开发人员意图驱动的代码注释生成，其可以为具有不同意图的相同源代码生成意图感知评论。为了解决这一具有挑战性的任务，我们提出了DOME，这是一种利用意图引导的选择性注意从源代码中明确选择意图相关信息的方法，并产生反映不同意图的各种注释。我们的方法在两个真实世界的Java数据集上进行了评估，实验结果表明，我们的方法优于最先进的基线。人工评估也证实了在实际使用中应用DOME的巨大潜力，使开发人员能够根据自己的需求有效地注释代码。,代码注释生成，意图可控注释生成，自动注释意图标记,,,
6MLMSMKY,2023,https://doi.org/10.1109/ICSE48619.2023.00176,ICSE 2023,Analysing the Impact of Workloads on Modeling the Performance of Configurable Software Systems,"Modern software systems often exhibit numerous configuration options to tailor them to user requirements, including the system's performance behavior. Performance models derived via machine learning are an established approach for estimating and optimizing configuration-dependent software performance. Most existing approaches in this area rely on software performance measurements conducted with a single workload (i.e., input fed to a system). This single workload, however, is often not representative of a software system's real-world application scenarios. Understanding to what extent configuration and workload-individually and combined-cause a software system's performance to vary is key to understand whether performance models are generalizable across different configurations and workloads. Yet, so far, this aspect has not been systematically studied. To fill this gap, we conducted a systematic empirical study across 25 258 configurations from nine real-world configurable software systems to investigate the effects of workload variation at system-level performance and for individual configuration options. We explore driving causes for workload-configuration interactions by enriching performance observations with option-specific code coverage information. Our results demonstrate that workloads can induce substantial performance variation and interact with configuration options, often in non-monotonous ways. This limits not only the generalizability of single-workload models, but also challenges assumptions for existing transfer-learning techniques. As a result, workloads should be considered when building performance prediction models to maintain and improve representativeness and reliability.","Codes,Systematics,Sensitivity,System performance,Software performance,Predictive models,Software systems",分析工作负载对可配置软件系统性能建模的影响,现代软件系统经常展示许多配置选项，以根据用户需求进行调整，包括系统的性能行为。通过机器学习导出的性能模型是一种用于估计和优化配置相关软件性能的既定方法。该领域的大多数现有方法都依赖于使用单个工作负载（即，输入到系统）进行的软件性能测量。然而，这种单一的工作负载通常不能代表软件系统的真实应用场景。了解配置和工作负载单独或组合在多大程度上导致软件系统的性能变化，是了解性能模型是否可在不同配置和工作负荷之间通用的关键。然而，到目前为止，这方面还没有得到系统的研究。为了填补这一空白，我们对来自九个真实世界可配置软件系统的25258种配置进行了系统的实证研究，以调查工作负载变化对系统级性能和单个配置选项的影响。我们通过使用特定于选项的代码覆盖率信息丰富性能观察结果，来探索工作负载配置交互的驱动原因。我们的研究结果表明，工作负载会导致显著的性能变化，并与配置选项交互，通常是以非单调的方式。这不仅限制了单一工作量模型的可推广性，而且也挑战了现有迁移学习技术的假设。因此，在构建性能预测模型时应考虑工作负载，以保持和提高代表性和可靠性。,代码，系统学，灵敏度，系统性能，软件性能，预测模型，软件系统,,,
CV2UEDX3,2023,https://doi.org/10.1109/ICSE48619.2023.00161,ICSE 2023,Improving API Knowledge Discovery with ML: A Case Study of Comparable API Methods,"Developers constantly learn new APIs, but often lack necessary information from documentation, resorting instead to popular question-and-answer platforms such as Stack Overflow. In this paper, we investigate how to use recent machine-Iearning-based knowledge extraction techniques to automatically identify pairs of comparable API methods and the sentences describing the comparison from Stack Overflow answers. We first built a prototype that can be stocked with a dataset of comparable API methods and provides tool-tips to users in search results and in API documentation. We conducted a user study with this tool based on a dataset of TensorFlow comparable API methods spanning 198 hand-annotated facts from Stack Overflow posts. This study confirmed that providing comparable API methods can be useful for helping developers understand the design space of APIs: developers using our tool were significantly more aware of the comparable API methods and better understood the differences between them. We then created SOREL, an comparable API methods knowledge extraction tool trained on our hand-annotated corpus, which achieves a 71% precision and 55% recall at discovering our manually extracted facts and discovers 433 pairs of comparable API methods from thousands of unseen Stack Overflow posts. This work highlights the merit of jointly studying programming assistance tools and constructing machine learning techniques to power them.","API,Knowledge Discovery,Pre trained Language Models,Stack Overflow",用ML改进API知识发现——以API可比方法为例,开发人员不断学习新的API，但往往缺乏文档中的必要信息，而是求助于流行的问答平台，如Stack Overflow。在本文中，我们研究了如何使用最新的基于机器学习的知识提取技术来自动识别可比较的API方法对和从Stack Overflow答案中描述比较的句子。我们首先构建了一个原型，该原型可以存储可比较的API方法的数据集，并在搜索结果和API文档中为用户提供工具时间。我们使用该工具进行了一项用户研究，该研究基于TensorFlow可比API方法的数据集，涵盖Stack Overflow帖子中的198个手工注释事实。这项研究证实，提供可比的API方法有助于帮助开发人员了解API的设计空间：使用我们工具的开发人员明显更了解可比的API方法，并更好地理解它们之间的差异。然后，我们创建了SOREL，这是一种可比的API方法知识提取工具，在我们的手绘语料库上进行了训练，在发现手动提取的事实时，该工具的准确率和召回率分别为71%和55%，并从数千个看不见的Stack Overflow帖子中发现了433对可比的API方法。这项工作突出了联合研究编程辅助工具和构建机器学习技术为其提供动力的优点。,API，知识发现，预先训练的语言模型，堆栈溢出,,,
JI6J93EP,2023,https://doi.org/10.1109/ICSE48619.2023.00205,ICSE 2023,Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning,"Large language models trained on massive code corpora can generalize to new tasks without the need for task-specific fine-tuning. In few-shot learning, these models take as input a prompt, composed of natural language instructions, a few instances of task demonstration, and a query and generate an output. However, the creation of an effective prompt for code-related tasks in few-shot learning has received little attention. We present a technique for prompt creation that automatically retrieves code demonstrations similar to the developer task, based on embedding or frequency analysis. We apply our approach, Cedar, to two different programming languages, statically and dynamically typed, and two different tasks, namely, test assertion generation and program repair. For each task, we compare Cedar with state-of-the-art task-specific and fine-tuned models. The empirical results show that, with only a few relevant code demonstrations, our prompt creation technique is effective in both tasks with an accuracy of 76% and 52% for exact matches in test assertion generation and program repair tasks, respectively. For assertion generation, Cedar outperforms existing task-specific and fine-tuned models by 333% and 11%, respectively. For program repair, Cedar yields 189% better accuracy than task-specific models and is competitive with recent fine-tuned models. These findings have practical implications for practitioners, as Cedar could potentially be applied to multilingual and multitask settings without task or language-specific training with minimal examples and effort.","Large Language Models,Transformers,Few-shot learning,Program repair,Test assertion generation",基于检索的代码相关少镜头学习提示选择,在大量代码语料库上训练的大型语言模型可以推广到新任务，而不需要特定于任务的微调。在少镜头学习中，这些模型将由自然语言指令、一些任务演示实例和查询组成的提示作为输入，并生成输出。然而，在少镜头学习中为代码相关任务创建有效提示却很少受到关注。我们提出了一种基于嵌入或频率分析的即时创建技术，该技术可以自动检索类似于开发人员任务的代码演示。我们将我们的方法Cedar应用于静态和动态类型的两种不同编程语言，以及两种不同的任务，即测试断言生成和程序修复。对于每个任务，我们将Cedar与最先进的特定任务和微调模型进行比较。实证结果表明，在只有少量相关代码演示的情况下，我们的即时创建技术在这两个任务中都是有效的，在测试断言生成和程序修复任务中，准确率分别为76%和52%。在断言生成方面，Cedar分别比现有的任务特定模型和微调模型高出333%和11%。在程序修复方面，Cedar的准确率比特定任务模型高189%，与最近微调的模型相比具有竞争力。这些发现对从业者有实际意义，因为Cedar可以应用于多语言和多任务环境，而无需任务或特定语言的培训，只需最少的例子和努力。,大型语言模型，转换器，少量镜头学习，程序修复，测试断言生成,,,
K2SQI9UV,2023,https://doi.org/10.1109/ICSE48619.2023.00108,ICSE 2023,"Code Review of Build System Specifications: Prevalence, Purposes, Patterns, and Perceptions","Build systems automate the integration of source code into executables. Maintaining build systems is known to be challenging. Lax build maintenance can lead to costly build breakages or unexpected software behaviour. Code review is a broadly adopted practice to improve software quality. Yet, little is known about how code review is applied to build specifications. In this paper, we present the first empirical study of how code review is practiced in the context of build specifications. Through quantitative analysis of 502,931 change sets from the Qt and Eclipse communities, we observe that changes to build specifications are at least two times less frequently discussed during code review when compared to production and test code changes. A qualitative analysis of 500 change sets reveals that (i) comments on changes to build specifications are more likely to point out defects than rates reported in the literature for production and test code, and (ii) evolvability and dependency-related issues are the most frequently raised patterns of issues. Follow-up interviews with nine developers with 1–40 years of experience point out social and technical factors that hinder rigorous review of build specifications, such as a prevailing lack of understanding of and interest in build systems among developers, and the lack of dedicated tooling to support the code review of build specifications.","build systems,build specifications,code review",构建系统规范的代码审查：流行性、目的、模式和感知,构建系统自动将源代码集成到可执行文件中。众所周知，维护构建系统具有挑战性。松懈的构建维护可能导致代价高昂的构建损坏或意外的软件行为。代码评审是一种广泛采用的提高软件质量的做法。然而，对于如何将代码评审应用于构建规范，我们知之甚少。在本文中，我们提出了第一个关于如何在构建规范的背景下实践代码评审的实证研究。通过对来自Qt和Eclipse社区的502931个变更集的定量分析，我们观察到，与生产和测试代码变更相比，在代码审查期间讨论构建规范变更的频率至少低两倍。对500个变更集的定性分析表明，（i）对构建规范变更的评论比生产和测试代码文献中报告的比率更有可能指出缺陷，（ii）可进化性和依赖性相关问题是最常见的问题模式。对九位具有1-40年经验的开发人员的后续采访指出了阻碍严格审查构建规范的社会和技术因素，例如开发人员普遍缺乏对构建系统的理解和兴趣，以及缺乏支持构建规范代码审查的专用工具。,构建系统，构建规范，代码审查,,,
XKK8JUFP,2023,https://doi.org/10.1109/ICSE48619.2023.00065,ICSE 2023,From Organizations to Individuals: Psychoactive Substance Use By Professional Programmers,"Psychoactive substances, which influence the brain to alter perceptions and moods, have the potential to have positive and negative effects on critical software engineering tasks. They are widely used in software, but that use is not well understood. We present the results of the first qualitative investigation of the experiences of, and challenges faced by, psychoactive substance users in professional software communities. We conduct a the-matic analysis of hour-long interviews with 26 professional pro-grammers who use psychoactive substances at work. Our results provide insight into individual motivations and impacts, including mental health and the relationships between various substances and productivity. Our findings elaborate on socialization effects, including soft skills, stigma, and remote work. The analysis also highlights implications for organizational policy, including positive and negative impacts on recruitment and retention. By exploring individual usage motivations, social and cultural ramifications, and organizational policy, we demonstrate how substance use can permeate all levels of software development.","software engineering,mental health,drug use,productivity,qualitative methods",从组织到个人：专业程序员使用精神活性物质,精神活性物质影响大脑改变感知和情绪，有可能对关键的软件工程任务产生积极和消极的影响。它们在软件中被广泛使用，但这种使用还没有得到很好的理解。我们介绍了对专业软件社区中精神活性物质用户的经历和面临的挑战进行的第一次定性调查的结果。我们对26名在工作中使用精神活性物质的专业程序员进行了长达一小时的访谈，并进行了实证分析。我们的研究结果深入了解了个人的动机和影响，包括心理健康以及各种物质与生产力之间的关系。我们的研究结果详细阐述了社会化效应，包括软技能、耻辱感和远程工作。该分析还强调了对组织政策的影响，包括对招聘和留用的积极和消极影响。通过探索个人使用动机、社会和文化影响以及组织政策，我们展示了物质使用如何渗透到软件开发的各个层面。,软件工程，心理健康，药物使用，生产力，定性方法,,,
V5HWB396,2023,https://doi.org/10.1109/ICSE48619.2023.00178,ICSE 2023,Learning Deep Semantics for Test Completion,"Writing tests is a time-consuming yet essential task during software development. We propose to leverage recent advances in deep learning for text and code generation to assist developers in writing tests. We formalize the novel task of test completion to automatically complete the next statement in a test method based on the context of prior statements and the code under test. We develop TECo-a deep learning model using code semantics for test completion. The key insight underlying TECO is that predicting the next statement in a test method requires reasoning about code execution, which is hard to do with only syntax-level data that existing code completion models use. Teco extracts and uses six kinds of code semantics data, including the execution result of prior statements and the execution context of the test method. To provide a testbed for this new task, as well as to evaluate TECO, we collect a corpus of 130,934 test methods from 1,270 open-source Java projects. Our results show that Teco achieves an exact-match accuracy of 18, which is 29% higher than the best baseline using syntax-level data only. When measuring functional correctness of generated next statement, Teco can generate runnable code in 29% of the cases compared to 18% obtained by the best baseline. Moreover, Teco is sianificantly better than prior work on test oracle generation.","test completion,deep neural networks,programming language semantics",学习用于测试完成的深层语义,在软件开发过程中，编写测试是一项耗时但必不可少的任务。我们建议利用深度学习的最新进展来生成文本和代码，以帮助开发人员编写测试。我们将测试完成这一新任务形式化，以在基于先前语句和被测试代码的上下文的测试方法中自动完成下一个语句。我们开发了TECo——一个使用代码语义完成测试的深度学习模型。TECO背后的关键见解是，预测测试方法中的下一个语句需要对代码执行进行推理，而这很难只使用现有代码完成模型使用的语法级数据。Teco提取并使用六种代码语义数据，包括先前语句的执行结果和测试方法的执行上下文。为了为这项新任务提供一个测试平台，并评估TECO，我们从1270个开源Java项目中收集了130934种测试方法的语料库。我们的结果表明，Teco实现了18的精确匹配精度，比仅使用语法级别数据的最佳基线高29%。在衡量生成的下一条语句的功能正确性时，Teco可以在29%的情况下生成可运行的代码，而最佳基线获得的情况为18%。此外，Teco在测试预言机生成方面明显优于之前的工作。,测试完成，深度神经网络，编程语言语义,,,
WN9HVPJ5,2023,https://doi.org/10.1109/ICSE48619.2023.00028,ICSE 2023,RAT: A Refactoring-Aware Traceability Model for Bug Localization,"A large number of bug reports are created during the evolution of a software system. Locating the source code files that need to be changed in order to fix these bugs is a challenging task. Information retrieval-based bug localization techniques do so by correlating bug reports with historical information about the source code (e.g., previously resolved bug reports, commit logs). These techniques have shown to be efficient and easy to use. However, one flaw that is nearly omnipresent in all these techniques is that they ignore code refactorings. Code refactorings are common during software system evolution, but from the perspective of typical version control systems, they break the code history. For example, a class when renamed then appears as two separate classes with separate histories. Obviously, this is a problem that affects any technique that leverages code history. This paper proposes a refactoring-aware traceability model to keep track of the code evolution history. With this model, we reconstruct the code history by analyzing the impact of code refactorings to correctly stitch together what would otherwise be a fragmented history. To demonstrate that a refactoring aware history is indeed beneficial, we investigated three widely adopted bug localization techniques that make use of code history, which are important components in existing approaches. Our evaluation on 11 open source projects shows that taking code refactorings into account significantly improves the results of these bug localization techniques without significant changes to the techniques themselves. The more refactorings are used in a project, the stronger the benefit we observed. Based on our findings, we believe that much of the state of the art leveraging code history should benefit from our work.","bug localization,bug report similarity,code refactoring,traceability,commit history,information retrieval",RAT：一个用于错误定位的可重构跟踪模型,大量的错误报告是在软件系统的进化过程中创建的。查找需要更改的源代码文件以修复这些错误是一项具有挑战性的任务。基于信息检索的错误定位技术通过将错误报告与源代码的历史信息（例如，以前解决的错误报告、提交日志）相关联来实现这一点。这些技术已被证明是有效且易于使用的。然而，在所有这些技术中几乎无处不在的一个缺陷是，它们忽略了代码重构。代码重构在软件系统进化过程中很常见，但从典型的版本控制系统的角度来看，它们打破了代码历史。例如，一个类在重命名后会显示为两个具有不同历史记录的独立类。显然，这是一个影响任何利用代码历史记录的技术的问题。本文提出了一个具有重构意识的可追溯性模型来跟踪代码的演化历史。有了这个模型，我们通过分析代码重构的影响来重构代码历史，从而正确地将原本支离破碎的历史拼接在一起。为了证明重构感知历史确实是有益的，我们研究了三种广泛采用的错误定位技术，它们利用了代码历史，这是现有方法中的重要组成部分。我们对11个开源项目的评估表明，在不对技术本身进行重大更改的情况下，将代码重构考虑在内可以显著改进这些缺陷定位技术的结果。项目中使用的重构越多，我们观察到的好处就越大。根据我们的发现，我们相信利用代码历史的大部分最新技术应该从我们的工作中受益。,bug本地化，bug报告相似性，代码重构，可追溯性，提交历史记录，信息检索,,,
HKN2ISKD,2023,https://doi.org/10.1109/ICSE48619.2023.00180,ICSE 2023,An Empirical Comparison of Pre-Trained Models of Source Code,"While a large number of pre-trained models of source code have been successfully developed and applied to a variety of software engineering (SE) tasks in recent years, our understanding of these pre-trained models is arguably fairly limited. With the goal of advancing our understanding of these models, we perform the first systematic empirical comparison of 19 recently-developed pre-trained models of source code on 13 SE tasks. To gain additional insights into these models, we adopt a recently -developed 4-dimensional categorization of pre-trained models, and subsequently investigate whether there are correlations between different categories of pre-trained models and their performances on different SE tasks.","Pre-training of Source Code,AI for SE",源代码预训练模型的实证比较,虽然近年来已经成功开发了大量预训练的源代码模型，并将其应用于各种软件工程（SE）任务，但我们对这些预训练模型的理解可以说相当有限。为了提高我们对这些模型的理解，我们对最近开发的19个预训练的源代码模型在13个SE任务中进行了首次系统的实证比较。为了深入了解这些模型，我们采用了最近开发的预训练模型的4维分类，随后研究了不同类别的预训练模式与其在不同SE任务中的表现之间是否存在相关性。,SE的源代码，AI预培训,,,
2QPKPTB7,2023,https://doi.org/10.1109/ICSE48619.2023.00055,ICSE 2023,CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models,"Despite the recent advances showing that a model pre-trained on large-scale source code data is able to gain appreciable generalization capability, it still requires a sizeable amount of data on the target task for fine-tuning. And the effectiveness of the model generalization is largely affected by the size and quality of the fine-tuning data, which is detrimental for target tasks with limited or unavailable resources. Therefore, cross-task generalization, with the goal of improving the generalization of the model to unseen tasks that have not been seen before, is of strong research and application value. In this paper, we propose a large-scale benchmark that includes 216 existing code-related tasks. Then, we annotate each task with the corresponding meta information such as task description and instruction, which contains detailed information about the task and a solution guide. This also helps us to easily create a wide variety of “training/evaluation” task splits to evaluate the various cross-task generalization capabilities of the model. Then we perform some preliminary experiments to demonstrate that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross-task learning research on our benchmark. We hope that the collection of the datasets and our benchmark will facilitate future work that is not limited to cross-task generalization.","Pre-training of source code,cross-task transfer learning,few-shot learning,AI for SE",CrossCodeBench:源代码模型的基准跨任务泛化,尽管最近的进展表明，在大规模源代码数据上预先训练的模型能够获得可观的泛化能力，但它仍然需要关于目标任务的大量数据来进行微调。模型泛化的有效性在很大程度上受到微调数据的大小和质量的影响，这对资源有限或不可用的目标任务是不利的。因此，以提高模型对以前从未见过的看不见任务的泛化能力为目标的跨任务泛化具有很强的研究和应用价值。在本文中，我们提出了一个大规模的基准测试，其中包括216个现有的代码相关任务。然后，我们用相应的元信息（如任务描述和说明）注释每个任务，其中包含有关任务的详细信息和解决方案指南。这也有助于我们轻松创建各种各样的“训练/评估”任务划分，以评估模型的各种跨任务泛化能力。然后，我们进行了一些初步实验，证明通过上下文学习方法，如少镜头学习和从任务指令中学习，可以大大提高模型的跨任务泛化能力，这表明在我们的基准上进行跨任务学习研究是有前景的。我们希望数据集和基准测试的收集将有助于未来的工作，而不仅仅局限于跨任务泛化。,源代码预训练，跨任务迁移学习，少镜头学习，SE人工智能,,,
6QPA9V92,2023,https://doi.org/10.1109/ICSE48619.2023.00211,ICSE 2023,VULGEN: Realistic Vulnerability Generation Via Pattern Mining and Deep Learning,"Building new, powerful data-driven defenses against prevalent software vulnerabilities needs sizable, quality vulnerability datasets, so does large-scale benchmarking of existing defense solutions. Automatic data generation would promisingly meet the need, yet there is little work aimed to generate much-needed quality vulnerable samples. Meanwhile, existing similar and adaptable techniques suffer critical limitations for that purpose. In this paper, we present VULGEN, the first injection-based vulnerability-generation technique that is not limited to a particular class of vulnerabilities. VULGEN combines the strengths of deterministic (pattern-based) and probabilistic (deep-learning/DL-based) program transformation approaches while mutually overcoming respective weaknesses. This is achieved through close collaborations between pattern mining/application and DL-based injection localization, which separates the concerns with how and where to inject. By leveraging large, pretrained programming language modeling and only learning locations, VULGEN mitigates its own needs for quality vulnerability data (for training the localization model). Extensive evaluations show that VULGEN significantly outperforms a state-of-the-art (SOTA) pattern-based peer technique as well as both Transformer- and GNN-based approaches in terms of the percentages of generated samples that are vulnerable and those also exactly matching the ground truth (by 38.0-430.1% and 16.3-158.2%, respectively). The VULGEN-generated samples led to substantial performance improvements for two SOTA DL-based vulnerability detectors (by up to 31.8% higher in F1), close to those brought by the ground-truth real-world samples and much higher than those by the same numbers of existing synthetic samples.","Software vulnerability,data generation,bug injection,pattern mining,deep learning,vulnerability detection",VULGEN：通过模式挖掘和深度学习生成现实脆弱性,针对普遍存在的软件漏洞构建新的、强大的数据驱动防御需要大规模、高质量的漏洞数据集，现有防御解决方案的大规模基准测试也是如此。自动数据生成有望满足需求，但很少有工作旨在生成急需的高质量易受攻击的样本。与此同时，现有的类似和适应性强的技术在这方面受到严重限制。在本文中，我们介绍了VULGEN，这是第一种基于注入的漏洞生成技术，不限于特定类别的漏洞。VULGEN结合了确定性（基于模式）和概率性（基于深度学习/DL）程序转换方法的优势，同时相互克服各自的弱点。这是通过模式挖掘/应用程序和基于DL的注入本地化之间的密切合作来实现的，这将如何注入以及在哪里注入的问题分开。VULGEN通过利用大型预训练编程语言建模和仅学习地点，缓解了其对高质量漏洞数据的需求（用于训练本地化模型）。广泛的评估表明，VULGEN在生成的易受攻击样本的百分比和与基本事实完全匹配的样本的百分比方面显著优于最先进的基于SOTA模式的对等技术以及基于Transformer和GNN的方法（分别为38.0-430.1%和16.3-158.2%）。VULGEN生成的样本显著提高了两个基于SOTA DL的漏洞检测器的性能（F1中高达31.8%），接近真实世界样本的性能，远高于相同数量的现有合成样本的性能。,软件漏洞，数据生成，漏洞注入，模式挖掘，深度学习，漏洞检测,,,
ZEGLADJZ,2023,https://doi.org/10.1109/ICSE48619.2023.00021,ICSE 2023,The untold story of code refactoring customizations in practice,"Refactoring is a common software maintenance practice. The literature defines standard code modifications for each refactoring type and popular IDEs provide refactoring tools aiming to support these standard modifications. However, previous studies indicated that developers either frequently avoid using these tools or end up modifying and even reversing the code automatically refactored by IDEs. Thus, developers are forced to manually apply refactorings, which is cumbersome and error-prone. This means that refactoring support may not be entirely aligned with practical needs. The improvement of tooling support for refactoring in practice requires understanding in what ways developers tailor refactoring modifications. To address this issue, we conduct an analysis of 1,162 refactorings composed of more than 100k program modifications from 13 software projects. The results reveal that developers recurrently apply patterns of additional modifications along with the standard ones, from here on called patterns of customized refactorings. For instance, we found customized refactorings in 80.77% of the Move Method instances observed in the software projects. We also investigated the features of refactoring tools in popular IDEs and observed that most of the customization patterns are not fully supported by them. Additionally, to understand the relevance of these customizations, we conducted a survey with 40 developers about the most frequent customization patterns we found. Developers confirm the relevance of customization patterns and agree that improvements in IDE's refactoring support are needed. These observations highlight that refactoring guidelines must be updated to reflect typical refactoring customizations. Also, IDE builders can use our results as a basis to enable a more flexible application of automated refactorings. For example, developers should be able to choose which method must handle exceptions when extracting an exception code into a new method.","Refactoring,Custom Refactoring,Refactoring Tooling Support",代码重构定制在实践中的不为人知的故事,重构是一种常见的软件维护实践。文献定义了每种重构类型的标准代码修改，流行的IDE提供了旨在支持这些标准修改的重构工具。然而，先前的研究表明，开发人员要么经常避免使用这些工具，要么最终修改甚至反转IDE自动重构的代码。因此，开发人员被迫手动应用重构，这既麻烦又容易出错。这意味着重构支持可能与实际需求不完全一致。在实践中改进工具对重构的支持需要了解开发人员如何定制重构修改。为了解决这个问题，我们对1162次重构进行了分析，这些重构由13个软件项目中超过10万次的程序修改组成。结果表明，开发人员会反复应用附加修改模式和标准修改模式，从现在起，这些模式被称为自定义重构模式。例如，我们在软件项目中观察到的移动方法实例中发现了80.77%的自定义重构。我们还研究了流行IDE中重构工具的特性，并观察到大多数定制模式并没有得到它们的完全支持。此外，为了了解这些定制的相关性，我们对40名开发人员进行了一项调查，了解我们发现的最常见的定制模式。开发人员确认了定制模式的相关性，并同意需要改进IDE的重构支持。这些观察结果强调，重构准则必须更新，以反映典型的重构定制。此外，IDE构建者可以使用我们的结果作为基础，实现更灵活的自动化重构应用程序。例如，当将异常代码提取到新方法中时，开发人员应该能够选择必须处理异常的方法。,重构，自定义重构，重构工具支持,,,
663QWULN,2023,https://doi.org/10.1109/ICSE48619.2023.00023,ICSE 2023,Do code refactorings influence the merge effort?,"In collaborative software development, multiple contributors frequently change the source code in parallel to implement new features, fix bugs, refactor existing code, and make other changes. These simultaneous changes need to be merged into the same version of the source code. However, the merge operation can fail, and developer intervention is required to resolve the conflicts. Studies in the literature show that 10 to 20 percent of all merge attempts result in conflicts, which require the manual developer's intervention to complete the process. In this paper, we concern about a specific type of change that affects the structure of the source code and has the potential to increase the merge effort: code refactorings. We analyze the relationship between the occurrence of refactorings and the merge effort. To do so, we applied a data mining technique called association rule extraction to find patterns of behavior that allow us to analyze the influence of refactorings on the merge effort. Our experiments extracted association rules from 40,248 merge commits that occurred in 28 popular open-source projects. The results indicate that: (i) the occurrence of refactorings increases the chances of having merge effort; (ii) the more refactorings, the greater the chances of effort; (iii) the more refactorings, the greater the effort; and (iv) parallel refactorings increase even more the chances of having effort, as well as the intensity of it. The results obtained may suggest behavioral changes in the way refactorings are implemented by developer teams. In addition, they can indicate possible ways to improve tools that support code merging and those that recommend refactorings, considering the number of refactorings and merge effort attributes.","Software Merge,Merge Effort,Refactoring,Association Rules,Data Mining",代码重构会影响合并工作吗？,在协作软件开发中，多个贡献者经常并行地更改源代码，以实现新功能、修复错误、重构现有代码以及进行其他更改。这些同时发生的更改需要合并到同一版本的源代码中。但是，合并操作可能会失败，需要开发人员介入以解决冲突。文献中的研究表明，所有合并尝试中有10%到20%会导致冲突，这需要手动开发人员的干预才能完成过程。在本文中，我们关注的是一种特定类型的更改，它会影响源代码的结构，并有可能增加合并工作量：代码重构。我们分析了重构的发生与合并工作之间的关系。为此，我们应用了一种名为关联规则提取的数据挖掘技术来寻找行为模式，从而分析重构对合并工作的影响。我们的实验从28个流行的开源项目中发生的40248次合并提交中提取了关联规则。结果表明：（i）重构的发生增加了进行合并的机会；（ii）重构越多，努力的机会就越大；（iii）重构越多，工作量就越大；以及（iv）并行重构增加了付出努力的机会和强度。所获得的结果可能表明开发团队实现重构的方式发生了行为变化。此外，考虑到重构的数量和合并工作属性，它们可以指示改进支持代码合并的工具和建议重构的工具的可能方法。,软件合并，合并工作，重构，关联规则，数据挖掘,,,
QNX7Y4JK,2023,https://doi.org/10.1109/ICSE48619.2023.00210,ICSE 2023,MirrorTaint: Practical Non-intrusive Dynamic Taint Tracking for JVM-based Microservice Systems,"Taint analysis, i.e., labeling data and propagating the labels through data flows, has been widely used for analyzing program information flows and ensuring system/data security. Due to its important applications, various taint analysis techniques have been proposed, including static and dynamic taint analysis. However, existing taint analysis techniques can be hardly applied to the rising microservice systems for industrial applications. To address such a problem, in this paper, we proposed the first practical non-intrusive dynamic taint analysis technique MirrorTaint for extensively supporting microservice systems on JVMs. In particular, by instrumenting the microservice systems, MirrorTaint constructs a set of data structures with their respective policies for labeling/propagating taints in its mirrored space. Such data structures are essentially non-intrusive, i.e., modifying no program meta-data or runtime system. Then, during program execution, MirrorTaint replicates the stack-based JVM instruction execution in its mirrored space on-the-fly for dynamic taint tracking. We have evaluated MirrorTaint against state-of-the-art dynamic and static taint analysis systems on various popular open-source microservice systems. The results demonstrate that MirrorTaint can achieve better compatibility, quite close precision and higher recall (97.9%/100.0%) than state-of-the-art Phosphor (100.0%/9.9%) and FlowDroid (100%/28.2%). Also, MirrorTaint incurs lower runtime overhead than Phosphor (although both are dynamic techniques). Moreover, we have performed a case study in Ant Group, a global billion-user FinTech company, to compare MirrorTaint and their mature developer-experience-based data checking system for automatically generated fund documents. The result shows that the developer experience can be incomplete, causing the data checking system to only cover 84.0% total data relations, while MirrorTaint can automatically find 99.0% relations with 100.0% precision. Lastly, we also applied MirrorTaint to successfully detect a recently wide-spread Log4j2 security vulnerability.","dynamic taint analysis,microservice,JVM",MirrorTaint：基于JVM的微服务系统的实用非侵入动态Taint跟踪,污点分析，即标记数据并通过数据流传播标签，已被广泛用于分析程序信息流和确保系统/数据安全。由于其重要的应用，人们提出了各种污染分析技术，包括静态和动态污染分析。然而，现有的污染分析技术很难应用于新兴的工业应用微服务系统。为了解决这个问题，在本文中，我们提出了第一种实用的非侵入式动态污染分析技术MirrorTaint，用于在JVM上广泛支持微服务系统。特别是，通过对微服务系统进行检测，MirrorTaint构建了一组数据结构，这些结构具有各自的策略，用于在其镜像空间中标记/传播污点。这种数据结构本质上是非侵入性的，即不修改程序元数据或运行时系统。然后，在程序执行期间，MirrorTaint在其镜像空间中动态复制基于堆栈的JVM指令执行，以进行动态污染跟踪。我们针对各种流行的开源微服务系统上最先进的动态和静态污染分析系统对MirrorTaint进行了评估。结果表明，MirrorTaint可以实现比最先进的Phosphor（100.0%/9.9%）和FlowDroid（100%/28.2%）更好的兼容性、更接近的精度和更高的召回率（97.9%/100.0%）。此外，MirrorTaint的运行时开销比Phosphor低（尽管两者都是动态技术）。此外，我们还对全球数十亿用户的金融科技公司蚂蚁集团进行了案例研究，以比较MirrorTaint和他们成熟的基于开发者经验的自动生成基金文档数据检查系统。结果表明，开发人员的体验可能是不完整的，导致数据检查系统只能覆盖84.0%的总数据关系，而MirrorTaint可以以100.0%的精度自动找到99.0%的关系。最后，我们还应用MirrorTaint成功地检测到了最近广泛传播的Log4j2安全漏洞。,动态污染分析，微服务，JVM,,,
YZTKDJB8,2023,https://doi.org/10.1109/ICSE48619.2023.00202,ICSE 2023,MorphQ: Metamorphic Testing of the Qiskit Quantum Computing Platform,"As quantum computing is becoming increasingly popular, the underlying quantum computing platforms are growing both in ability and complexity. Unfortunately, testing these platforms is challenging due to the relatively small number of existing quantum programs and because of the oracle problem, i.e., a lack of specifications of the expected behavior of programs. This paper presents MorphQ, the first metamorphic testing approach for quantum computing platforms. Our two key contributions are (i) a program generator that creates a large and diverse set of valid (i.e., non-crashing) quantum programs, and (ii) a set of program transformations that exploit quantum-specific metamorphic relationships to alleviate the oracle problem. Evaluating the approach by testing the popular Qiskit platform shows that the approach creates over 8k program pairs within two days, many of which expose crashes. Inspecting the crashes, we find 13 bugs, nine of which have already been confirmed. MorphQ widens the slim portfolio of testing techniques of quantum computing platforms, helping to create a reliable software stack for this increasingly important field.","quantum computing,metamorphic testing,software engineering,compiler testing,quantum computing platforms,quantum bugs,Qiskit,MorphQ,quantum software reliability,quality assurance,quantum program generator,differential testing,fuzz testing",MorphQ：Qiskit量子计算平台的变形测试,随着量子计算越来越流行，底层量子计算平台的能力和复杂性都在增长。不幸的是，由于现有量子程序的数量相对较少，以及oracle问题，即缺乏程序预期行为的规范，测试这些平台具有挑战性。本文介绍了第一种用于量子计算平台的变形测试方法MorphQ。我们的两个关键贡献是（i）一个程序生成器，它创建了一组庞大而多样的有效（即非崩溃）量子程序，以及（ii）一组利用量子特定变质关系来缓解预言机问题的程序转换。通过测试流行的Qiskit平台对该方法进行评估表明，该方法在两天内创建了超过8k个程序对，其中许多都暴露了崩溃。通过检查崩溃，我们发现了13个错误，其中9个已经得到确认。MorphQ扩展了量子计算平台的测试技术组合，有助于为这个日益重要的领域创建可靠的软件堆栈。,量子计算，变形测试，软件工程，编译器测试，量子计算平台，量子bug，Qiskit，MorphQ，量子软件可靠性，质量保证，量子程序生成器，差分测试，模糊测试,,,
6KM7DDCZ,2023,https://doi.org/10.1109/ICSE48619.2023.00088,ICSE 2023,Fine-grained Commit-level Vulnerability Type Prediction by CWE Tree Structure,"Identifying security patches via code commits to allow early warnings and timely fixes for Open Source Software (OSS) has received increasing attention. However, the existing detection methods can only identify the presence of a patch (i.e., a binary classification) but fail to pinpoint the vulnerability type. In this work, we take the first step to categorize the security patches into fine-grained vulnerability types. Specifically, we use the Common Weakness Enumeration (CWE) as the label and perform fine-grained classification using categories at the third level of the CWE tree. We first formulate the task as a Hierarchical Multi-label Classification (HMC) problem, i.e., inferring a path (a sequence of CWE nodes) from the root of the CWE tree to the node at the target depth. We then propose an approach named TreeVul with a hierarchical and chained architecture, which manages to utilize the structure information of the CWE tree as prior knowledge of the classification task. We further propose a tree structure aware and beam search based inference algorithm for retrieving the optimal path with the highest merged probability. We collect a large security patch dataset from NVD, consisting of 6,541 commits from 1,560 GitHub OSS repositories. Experimental results show that Tree-vulsignificantly outperforms the best performing baselines, with improvements of 5.9%, 25.0%, and 7.7% in terms of weighted F1-score, macro F1-score, and MCC, respectively. We further conduct a user study and a case study to verify the practical value of TreeVul in enriching the binary patch detection results and improving the data quality of NVD, respectively.","Software Security,Vulnerability Type,CWE",基于CWE树结构的细粒度提交级漏洞类型预测,通过代码提交来识别安全补丁，以便对开源软件（OSS）进行早期警告和及时修复，这一点越来越受到关注。然而，现有的检测方法只能识别补丁的存在（即二进制分类），但无法精确定位漏洞类型。在这项工作中，我们首先将安全补丁分类为细粒度的漏洞类型。具体来说，我们使用通用弱点枚举（CWE）作为标签，并使用CWE树的第三级类别执行细粒度分类。我们首先将任务公式化为分层多标签分类（HMC）问题，即推断从CWE树的根到目标深度的节点的路径（CWE节点的序列）。然后，我们提出了一种名为TreeVul的方法，该方法具有分层和链式结构，能够利用CWE树的结构信息作为分类任务的先验知识。我们进一步提出了一种树结构感知和基于波束搜索的推理算法，用于检索具有最高合并概率的最优路径。我们从NVD收集了一个大型安全补丁数据集，由1560个GitHub OSS存储库中的6541个提交组成。实验结果表明，Tree Vul显著优于性能最好的基线，在加权F1得分、宏F1得分和MCC方面分别提高了5.9%、25.0%和7.7%。我们进一步进行了用户研究和案例研究，分别验证了TreeVul在丰富二进制补丁检测结果和提高NVD数据质量方面的实用价值。,软件安全，漏洞类型，CWE,,,
6AMGFET5,2023,https://doi.org/10.1109/ICSE48619.2023.00160,ICSE 2023,Identifying Key Classes for Initial Software Comprehension: Can We Do It Better?,"Key classes are excellent starting points for developers, especially newcomers, to comprehend an unknown software system. Though many unsupervised key class identification approaches have been proposed in the literature by representing software as class dependency networks (aka software networks) and using some network metrics (e.g., h-index, a-index, and coreness), they are never aware of the field where the nodes exist and the effect of the field on the importance of the nodes in it. According to the classic field theory in physics, every material particle is in a field through which they exert an impact on other particles in the field via non-contact interactions (e.g., electromagnetic force, gravity, and nuclear force). Similarly, every node in a software network might also exist in a field, which might affect the importance of class nodes in it. In this paper, we propose an approach, iFit, to identify key classes in object-oriented software systems. First, we represent software as a CSNWD (Weighted Directed Class-level Software Network) to capture the topological structure of software, including classes, their couplings, and the direction and strength of couplings. Second, we assume that the nodes in the CSNWD exist in a gravitation-like field and propose a new metric, CG (Cumulative Gravitation-like importance), to measure the importance of classes. CG is inspired by Newton's gravitational formula and uses the PageRank value computed by a biased-PageRank algorithm as the masses of classes. Finally, classes in the system are sorted in descending order according to their CG values, and a cutoff is utilized, that is, the top-ranked classes are recommended as key classes. The experiments were performed on a data set composed of six open-source Java systems from the literature. The results show that iFit is superior to the baseline approaches on 93.75% of the total cases, and is scalable to large-scale software systems. Besides, we find that iFit is neutral to the weighting mechanisms used to assign the weights for different coupling types in the CSNWD, that is, when applying iFit to identify key classes, we can use any one of the weighting mechanisms.","complex networks,field theory,key classes,PageRank,program comprehension",识别初始软件理解的关键类：我们能做得更好吗？,关键类是开发人员，尤其是新手，理解未知软件系统的绝佳起点。尽管文献中已经提出了许多无监督的关键类识别方法，将软件表示为类依赖网络（也称为软件网络）并使用一些网络度量（例如，h-index、a-index和coreness），但他们从未意识到节点存在的领域以及该领域对其中节点重要性的影响。根据物理学中的经典场论，每个材料粒子都处于一个场中，通过该场，它们通过非接触相互作用（例如电磁力、重力和核力）对场中的其他粒子施加影响。同样，软件网络中的每个节点也可能存在于一个领域中，这可能会影响类节点在其中的重要性。本文提出了一种识别面向对象软件系统中关键类的方法iFit。首先，我们将软件表示为CSNWD（加权有向类级软件网络），以捕获软件的拓扑结构，包括类、它们的耦合以及耦合的方向和强度。其次，我们假设CSNWD中的节点存在于类引力场中，并提出了一个新的度量CG（类累积引力重要性）来衡量类的重要性。CG受到牛顿引力公式的启发，使用有偏PageRank算法计算的PageRank值作为类的质量。最后，系统中的类根据它们的CG值按降序排序，并使用截止值，即排名靠前的类被推荐为关键类。实验是在由文献中的六个开源Java系统组成的数据集上进行的。结果表明，iFit在93.75%的总案例中优于基线方法，并且可扩展到大型软件系统。此外，我们发现iFit对用于为CSNWD中的不同耦合类型分配权重的加权机制是中立的，也就是说，当应用iFit来识别关键类时，我们可以使用任何一种加权机制。,复杂网络，场论，关键类，PageRank，程序理解,,,
2RGA2K97,2023,https://doi.org/10.1109/ICSE48619.2023.00146,ICSE 2023,ATM: Black-box Test Case Minimization based on Test Code Similarity and Evolutionary Search,"Executing large test suites is time and resource consuming, sometimes impossible, and such test suites typically contain many redundant test cases. Hence, test case (suite) minimization is used to remove redundant test cases that are unlikely to detect new faults. However, most test case minimization techniques rely on code coverage (white-box), model-based features, or requirements specifications, which are not always (entirely) accessible by test engineers. Code coverage analysis also leads to scalability issues, especially when applied to large industrial systems. Recently, a set of novel techniques was proposed, called FAST-R, relying solely on test case code for test case minimization, which appeared to be much more efficient than white-box techniques. However, it achieved a comparable low fault detection capability for Java projects, thus making its application challenging in practice. In this paper, we propose ATM (AST-based Test case Minimizer), a similarity-based, search-based test case minimization technique, taking a specific budget as input, that also relies exclusively on the source code of test cases but attempts to achieve higher fault detection through finer-grained similarity analysis and a dedicated search algorithm. ATM transforms test case code into Abstract Syntax Trees (AST) and relies on four tree-based similarity measures to apply evolutionary search, specifically genetic algorithms, to minimize test cases. We evaluated the effectiveness and efficiency of ATM on a large dataset of 16 Java projects with 661 faulty versions using three budgets ranging from 25% to 75% of test suites. ATM achieved significantly higher fault detection rates (0.82 on average), compared to FAST-R (0.61 on average) and random minimization (0.52 on average), when running only 50% of the test cases, within practically acceptable time (1.1 - 4.3 hours, on average, per project version), given that minimization is only occasionally applied when many new test cases are created (major releases). Results achieved for other budgets were consistent.","Test case minimization,Test suite reduction,Tree-based similarity,AST,Genetic algorithm,Black-box testing",ATM：基于测试代码相似性和进化搜索的黑盒测试用例最小化,执行大型测试套件耗费时间和资源，有时是不可能的，而且此类测试套件通常包含许多冗余的测试用例。因此，测试用例（套件）最小化用于删除不太可能检测到新故障的冗余测试用例。然而，大多数测试用例最小化技术依赖于代码覆盖率（白盒）、基于模型的特性或需求规范，而测试工程师并不总是（完全）可以访问这些特性。代码覆盖率分析也会导致可扩展性问题，尤其是在应用于大型工业系统时。最近，有人提出了一套新的技术，称为FAST-R，它完全依赖于测试用例代码来最小化测试用例，这似乎比白盒技术更有效。然而，它在Java项目中实现了相当低的故障检测能力，因此使其应用在实践中具有挑战性。在本文中，我们提出了ATM（基于AST的测试用例最小化器），这是一种基于相似性、基于搜索的测试用例最小技术，以特定预算为输入，它也完全依赖于测试用例的源代码，但试图通过更细粒度的相似性分析和专用搜索算法来实现更高的故障检测。ATM将测试用例代码转换为抽象语法树（AST），并依靠四个基于树的相似性度量来应用进化搜索，特别是遗传算法，以最小化测试用例。我们在一个由16个Java项目组成的大型数据集上评估了ATM的有效性和效率，该数据集包含661个错误版本，使用了25%到75%的测试套件的三个预算。与FAST-R（平均0.61）和随机最小化（平均0.52）相比，ATM在实际可接受的时间内（每个项目版本平均1.1-4.3小时）仅运行50%的测试用例时，实现了显著更高的故障检测率（平均0.82），因为在创建许多新的测试用例（主要版本）时，最小化只是偶尔应用。其他预算取得的成果是一致的。,测试用例最小化，测试套件缩减，基于树的相似性，AST，遗传算法，黑盒测试,,,
9S6MZQG5,2023,https://doi.org/10.1109/ICSE48619.2023.00112,ICSE 2023,Rete: Learning Namespace Representation for Program Repair,"A key challenge of automated program repair is finding correct patches in the vast search space of candidate patches. Real-world programs define large namespaces of variables that considerably contributes to the search space explosion. Existing program repair approaches neglect information about the program namespace, which makes them inefficient and increases the chance of test-overfitting. We propose Rete, a new program repair technique, that learns project-independent information about program namespace and uses it to navigate the search space of patches. Rete uses a neural network to extract project-independent information about variable CDU chains, def-use chains augmented with control flow. Then, it ranks patches by jointly ranking variables and the patch templates into which the variables are inserted. We evaluated Rete on 142 bugs extracted from two datasets, ManyBugs and BugsInPy. Our experiments demonstrate that ReTe generates six new correct patches that fix bugs that previous tools did not repair, an improvement of 31% and 59% over the existing state of the art.","Program Repair,Deep Learning,Patch Prioritisation,Variable Representation",Rete：用于程序修复的学习命名空间表示,自动程序修复的一个关键挑战是在候选补丁的巨大搜索空间中找到正确的补丁。现实世界中的程序定义了大量的变量名称空间，这对搜索空间的爆炸有很大的贡献。现有的程序修复方法忽略了有关程序名称空间的信息，这使得它们效率低下，并增加了测试过拟合的机会。我们提出了Rete，这是一种新的程序修复技术，它学习有关程序名称空间的独立于项目的信息，并使用它来导航补丁的搜索空间。Rete使用神经网络来提取关于可变CDU链的独立于项目的信息，定义使用控制流增强的链。然后，它通过联合对变量和插入变量的补丁模板进行排名来对补丁进行排名。我们对从两个数据集ManyBugs和BugsInPy中提取的142个Bug进行了Rete评估。我们的实验表明，ReTe生成了六个新的正确补丁，修复了以前的工具没有修复的错误，比现有技术提高了31%和59%。,程序修复，深度学习，补丁优先级，变量表示,,,
3W3F4XZ4,2023,https://doi.org/10.1109/ICSE48619.2023.00124,ICSE 2023,Flexible and Optimal Dependency Management via Max-SMT,"Package managers such as NPM have become essential for software development. The NPM repository hosts over 2 million packages and serves over 43 billion downloads every week. Unfortunately, the NPM dependency solver has several shortcomings. 1) NPM is greedy and often fails to install the newest versions of dependencies; 2) NPM's algorithm leads to duplicated dependencies and bloated code, which is particularly bad for web applications that need to minimize code size; 3) NPM's vulnerability fixing algorithm is also greedy, and can even introduce new vulnerabilities; and 4) NPM's ability to duplicate dependencies can break stateful frameworks and requires a lot of care to workaround. Although existing tools try to address these problems they are either brittle, rely on post hoc changes to the dependency tree, do not guarantee optimality, or are not composable. We present Pacsolve, a unifying framework and implementation for dependency solving which allows for customizable constraints and optimization goals. We use Pacsolve to build Maxnpm, a complete, drop-in replacement for NPM, which empowers developers to combine multiple objectives when installing dependencies. We evaluate Maxnpm with a large sample of packages from the NPM ecosystem and show that it can: 1) reduce more vulnerabilities in dependencies than NPM's auditing tool in 33% of cases; 2) chooses newer dependencies than NPM in 14% of cases; and 3) chooses fewer dependencies than NPM in 21% of cases. All our code and data is open and available.","package-management,Max-SMT,NPM,Rosette,dependency-management,JavaScript",通过Max SMT实现灵活优化的依赖关系管理,像NPM这样的包管理器已经成为软件开发的关键。NPM存储库拥有200多万个软件包，每周下载量超过430亿次。不幸的是，NPM依赖性求解器有几个缺点。1） NPM贪婪，经常无法安装最新版本的依赖项；2） NPM的算法导致重复的依赖关系和臃肿的代码，这对于需要最小化代码大小的web应用程序来说尤其糟糕；3） NPM的漏洞修复算法也是贪婪的，甚至可以引入新的漏洞；和4）NPM复制依赖关系的能力可能会破坏有状态的框架，需要大量的精力来解决。尽管现有的工具试图解决这些问题，但它们要么很脆弱，要么依赖于对依赖树的事后更改，不能保证最优性，要么不可组合。我们介绍了Pacsolve，这是一个用于依赖关系解决的统一框架和实现，它允许自定义约束和优化目标。我们使用Pacsolve来构建Maxnpm，这是NPM的一个完整的替代品，它使开发人员能够在安装依赖项时组合多个目标。我们用来自NPM生态系统的大量包样本评估了Maxnpm，并表明它可以：1）在33%的情况下，减少比NPM审计工具更多的依赖性漏洞；2） 在14%的情况下选择比NPM更新的依赖关系；以及3）在21%的情况下选择比NPM更少的依赖性。我们所有的代码和数据都是开放的。,包管理，Max SMT，NPM，Rosette，依赖关系管理，JavaScript,,,
24TDFLP9,2023,https://doi.org/10.1109/ICSE48619.2023.00010,ICSE 2023,Future Software for Life in Trusted Futures,"How will people, other species, software and hardware live together in as yet unknown futures? How can we work towards trusted and safe futures where human values and the environment are supported by emerging technologies? Research demonstrates that human values and everyday life priorities, ethics, routines and activities will shape our possible futures. I will draw on ethnographic research to outline how people anticipate and imagine everyday life futures with emerging technologies in their homes and neighbourhoods, and how technology workers envisage futures in their professional lives. If, as social science research shows, technologies cannot solve human and societal problems, what roles should they play in future life? What are the implications for future software? What values should underpin its design? Where should it be developed? By and in collaboration with whom? What role can software play in generating the circumstances for trusted futures?",Software Engineering and Society,可信未来中的未来软件,在未知的未来，人类、其他物种、软件和硬件将如何共同生活？在新兴技术支持人类价值观和环境的情况下，我们如何努力实现可信和安全的未来？研究表明，人类的价值观和日常生活的优先事项、道德、日常生活和活动将塑造我们可能的未来。我将利用人种学研究，概述人们如何在家里和社区中预测和想象新兴技术的日常生活未来，以及技术工作者如何在职业生活中展望未来。如果正如社会科学研究所表明的那样，技术不能解决人类和社会问题，那么它们在未来生活中应该扮演什么角色？对未来的软件有什么影响？什么价值观应该支撑它的设计？它应该在哪里开发？由谁和谁合作？软件在为可信的未来创造环境方面可以发挥什么作用？,软件工程与社会,,,
4T95B8ED,2023,https://doi.org/10.1109/ICSE48619.2023.00183,ICSE 2023,Safe Low-Level Code Without Overhead is Practical,"Developers write low-level systems code in unsafe programming languages due to performance concerns. The lack of safety causes bugs and vulnerabilities that safe languages avoid. We argue that safety without run-time overhead is possible through type invariants that prove the safety of potentially unsafe operations. We empirically show that Rust and C# can be extended with such features to implement safe network device drivers without run-time overhead, and that Ada has these features already.","programming languages,safety,performance",无开销的安全低级别代码是实用的,出于性能考虑，开发人员使用不安全的编程语言编写低级系统代码。缺乏安全性会导致安全语言所避免的错误和漏洞。我们认为，通过证明潜在不安全操作的安全性的类型不变量，在没有运行时开销的情况下实现安全是可能的。我们的经验表明，Rust和C#可以用这样的功能进行扩展，以在没有运行时开销的情况下实现安全的网络设备驱动程序，并且Ada已经具备了这些功能。,编程语言，安全性，性能,,,
XRYJEFLM,2023,https://doi.org/10.1109/ICSE48619.2023.00006,ICSE 2023,Message from the ICSE 2023 Program Co-Chairs,"Welcome to ICSE 2023! It is our great pleasure to introduce the program of the 45th IEEE/ACM International Conference on Software Engineering (ICSE 2023), which will be held in Melbourne, Australia, on May 14-20, 2023.",,ICSE 2023项目联合主席致辞,欢迎来到ICSE 2023！我们非常高兴地介绍将于2023年5月14日至20日在澳大利亚墨尔本举行的第45届IEEE/ACM国际软件工程会议（ICSE 2023）的计划。,,,,
TKQ5JBWR,2023,https://doi.org/10.1109/ICSE48619.2023.00143,ICSE 2023,Keyword Extraction From Specification Documents for Planning Security Mechanisms,"Software development companies heavily invest both time and money to provide post-production support to fix security vulnerabilities in their products. Current techniques identify vulnerabilities from source code using static and dynamic analyses. However, this does not help integrate security mechanisms early in the architectural design phase. We develop VDocScan, a technique for predicting vulnerabilities based on specification documents, even before the development stage. We evaluate VDocScan using an extensive dataset of CVE vulnerability reports mapped to over 3600 product documentations. An evaluation of 8 CWE vulnerability pillars shows that even interpretable whitebox classifiers predict vulnerabilities with up to 61.1% precision and 78% recall. Further, using strategies to improve the relevance of extracted keywords, addressing class imbalance, segregating products into categories such as Operating Systems, Web applications, and Hardware, and using blackbox ensemble models such as the random forest classifier improves the performance to 96% precision and 91.1% recall. The high precision and recall shows that VDocScan can anticipate vulnerabilities detected in a product's lifetime ahead of time during the Design phase to incorporate necessary security mechanisms. The performance is consistently high for vulnerabilities with the mode of introduction: architecture and design.","Security,Vulnerability Prediction,CVE,CWE,Keyword Extraction,Documentation",从规划安全机制规范文档中提取关键字,软件开发公司投入大量时间和金钱来提供后期生产支持，以修复其产品中的安全漏洞。当前的技术使用静态和动态分析从源代码中识别漏洞。然而，这无助于在体系结构设计阶段早期集成安全机制。我们开发了VDocScan，这是一种基于规范文档预测漏洞的技术，甚至在开发阶段之前。我们使用映射到3600多份产品文档的CVE漏洞报告的广泛数据集来评估VDocScan。对8个CWE漏洞支柱的评估表明，即使是可解释的白盒分类器也能预测高达61.1%的准确率和78%的召回率的漏洞。此外，使用策略来提高提取的关键词的相关性，解决类别不平衡问题，将产品分为操作系统、Web应用程序和硬件等类别，并使用黑盒集成模型（如随机森林分类器），将性能提高到96%的准确率和91.1%的召回率。高精度和召回率表明，VDocScan可以在设计阶段提前预测产品生命周期中检测到的漏洞，以纳入必要的安全机制。采用引入模式：体系结构和设计，漏洞的性能始终很高。,安全性，漏洞预测，CVE，CWE，关键字提取，文档,,,
UGIMYT8A,2023,https://doi.org/10.1109/ICSE48619.2023.00215,ICSE 2023,Finding Causally Different Tests for an Industrial Control System,"Industrial control systems (ICSs) are types of cyber-physical systems in which programs, written in languages such as ladder logic or structured text, control industrial processes through sensing and actuating. Given the use of ICSs in critical infrastructure, it is important to test their resilience against manipulations of sensor/actuator inputs. Unfortunately, existing methods fail to test them comprehensively, as they typically focus on finding the simplest-to-craft manipulations for a testing goal, and are also unable to determine when a test is simply a minor permutation of another, i.e. based on the same causal events. In this work, we propose a guided fuzzing approach for finding 'meaningfully different’ tests for an ICS via a general formalisation of sensor/actuator-manipulation strategies. Our algorithm identifies the causal events in a test, generalises them to an equivalence class, and then updates the fuzzing strategy so as to find new tests that are causally different from those already identified. An evaluation of our approach on a real-world water treatment system shows that it is able to find 106% more causally different tests than the most comparable fuzzer. While we focus on diversifying the test suite of an ICS, our formalisation may be useful for other fuzzers that intercept communication channels.","Cyber-physical systems,fuzzing,test diversity,equivalence classes,causality",寻找工业控制系统的不同测试,工业控制系统（ICSs）是一种网络物理系统，其中用梯形逻辑或结构化文本等语言编写的程序通过传感和执行来控制工业过程。考虑到ICSs在关键基础设施中的使用，测试其对传感器/致动器输入操作的弹性是很重要的。不幸的是，现有的方法未能对其进行全面的测试，因为它们通常专注于为测试目标寻找最简单的操作，并且也无法确定测试何时只是另一个测试的微小排列，即基于相同的因果事件。在这项工作中，我们提出了一种引导模糊方法，通过传感器/执行器操作策略的一般形式化，为ICS找到“有意义的不同”测试。我们的算法识别测试中的因果事件，将它们推广到等价类，然后更新模糊策略，以便找到与已经识别的测试有因果差异的新测试。对我们的方法在真实世界的水处理系统上的评估表明，与最具可比性的模糊器相比，它能够找到106%的原因不同的测试。当我们专注于使ICS的测试套件多样化时，我们的形式化可能对拦截通信信道的其他模糊器有用。,网络物理系统，模糊，测试多样性，等价类，因果关系,,,
RK8YYHIE,2023,https://doi.org/10.1109/ICSE48619.2023.00090,ICSE 2023,Reusing Deep Neural Network Models through Model Re-engineering,"Training deep neural network (DNN) models, which has become an important task in today's software development, is often costly in terms of computational resources and time. With the inspiration of software reuse, building DNN models through reusing existing ones has gained increasing attention recently. Prior approaches to DNN model reuse have two main limitations: 1) reusing the entire model, while only a small part of the model's functionalities (labels) are required, would cause much overhead (e.g., computational and time costs for inference), and 2) model reuse would inherit the defects and weaknesses of the reused model, and hence put the new system under threats of security attack. To solve the above problem, we propose SeaM, a tool that re-engineers a trained DNN model to improve its reusability. Specifically, given a target problem and a trained model, SeaM utilizes a gradient-based search method to search for the model's weights that are relevant to the target problem. The re-engineered model that only retains the relevant weights is then reused to solve the target problem. Evaluation results on widely-used models show that the re-engineered models produced by SeaM only contain 10.11% weights of the original models, resulting 42.41% reduction in terms of inference time. For the target problem, the re-engineered models even outperform the original models in classification accuracy by 5.85%. Moreover, reusing the re-engineered models inherits an average of 57% fewer defects than reusing the entire model. We believe our approach to reducing reuse overhead and defect inheritance is one important step forward for practical model reuse.","model reuse,deep neural network,re-engineering,DNN modularization",通过模型重组重用深度神经网络模型,训练深度神经网络（DNN）模型已经成为当今软件开发中的一项重要任务，在计算资源和时间方面往往是昂贵的。在软件复用的启发下，通过复用现有的DNN模型来构建DNN模型近年来越来越受到关注。DNN模型重用的现有方法有两个主要限制：1）重用整个模型，而只需要模型的一小部分功能（标签），这将导致大量开销（例如，推理的计算和时间成本）；2）模型重用将继承重用模型的缺陷和弱点，从而使新系统面临安全攻击的威胁。为了解决上述问题，我们提出了SeaM，这是一种重新设计经过训练的DNN模型以提高其可重用性的工具。具体而言，给定目标问题和训练的模型，SeaM利用基于梯度的搜索方法来搜索与目标问题相关的模型权重。然后，只保留相关权重的重新设计模型被重用以解决目标问题。对广泛使用的模型的评估结果表明，SeaM重新设计的模型只包含原始模型的10.11%的权重，从而使推理时间减少了42.41%。对于目标问题，重新设计的模型在分类准确率上甚至比原始模型高5.85%。此外，重新设计模型的重用比整个模型的重用平均少57%的缺陷。我们相信，我们减少重用开销和缺陷继承的方法是实现实际模型重用的重要一步。,模型重用，深度神经网络，重新设计，DNN模块化,,,
Y5B8TMTP,2023,https://doi.org/10.1109/ICSE48619.2023.00083,ICSE 2023,Badge: Prioritizing UI Events with Hierarchical Multi-Armed Bandits for Automated UI Testing,"To assure high quality of mobile applications (apps for short), automated UI testing triggers events (associated with UI elements on app UIs) without human intervention, aiming to maximize code coverage and find unique crashes. To achieve high test effectiveness, automated UI testing prioritizes a UI event based on its exploration value (e.g., the increased code coverage of future exploration rooted from the UI event). Various strategies have been proposed to estimate the exploration value of a UI event without considering its exploration diversity (reflecting the variance of covered code entities achieved by explorations rooted from this UI event across its different triggerings), resulting in low test effectiveness, especially on complex mobile apps. To address the preceding problem, in this paper, we propose a new approach named Badge to prioritize UI events considering both their exploration values and exploration diversity for effective automated UI testing. In particular, we design a hierarchical multi-armed bandit model to effectively estimate the exploration value and exploration diversity of a UI event based on its historical explorations along with historical explorations rooted from UI events in the same UI group. We evaluate Badge on 21 highly popular industrial apps widely used by previous related work. Experimental results show that Badge outperforms state-of-the-art/practice tools with 18%-146% relative code coverage improvement and finding 1.19-5.20 × unique crashes, demonstrating the effectiveness of Badge. Further experimental studies confirm the benefits brought by Badge's individual algorithms.","GUI testing,mobile testing,mobile app,Android,multi-armed bandits,reinforcement learning",徽章：使用分层多武装Bandits对UI事件进行优先级排序，以进行自动UI测试,为了确保移动应用程序（简称应用程序）的高质量，自动UI测试在没有人为干预的情况下触发事件（与应用程序UI上的UI元素相关），旨在最大限度地提高代码覆盖率并发现独特的崩溃。为了实现高测试效率，自动UI测试根据UI事件的探索价值（例如，源于UI事件的未来探索的代码覆盖率增加）对其进行优先级排序。已经提出了各种策略来估计UI事件的探索价值，而不考虑其探索多样性（反映了源于该UI事件的探究在其不同触发中实现的覆盖代码实体的差异），导致测试有效性较低，尤其是在复杂的移动应用程序上。为了解决前面的问题，在本文中，我们提出了一种名为Badge的新方法来区分UI事件的优先级，同时考虑它们的探索价值和探索多样性，以实现有效的自动化UI测试。特别是，我们设计了一个分层的多武装土匪模型，以基于UI事件的历史探索以及源于同一UI组中UI事件的传统探索，有效地估计UI事件的探索价值和探索多样性。我们在先前相关工作中广泛使用的21个非常流行的工业应用程序上评估Badge。实验结果表明，Badge优于最先进的/实践工具，相对代码覆盖率提高了18%-146%，发现了1.19-5.20×的独特崩溃，证明了Badge的有效性。进一步的实验研究证实了Badge的单独算法带来的好处。,GUI测试，手机测试，手机应用程序，Android，多武装匪徒，强化学习,,,
7XEQCSCP,2023,https://doi.org/10.1109/ICSE48619.2023.00131,ICSE 2023,Testability Refactoring in Pull Requests: Patterns and Trends,"To create unit tests, it may be necessary to refactor the production code, e.g. by widening access to specific methods or by decomposing classes into smaller units that are easier to test independently. We report on an extensive study to understand such composite refactoring procedures for the purpose of improving testability. We collected and studied 346,841 java pull requests from 621 GitHub projects. First, we compared the atomic refactorings in two populations: pull requests with changed test-pairs (i.e. with co-changes in production and test code and thus potentially including testability refactoring) and pull requests without test-pairs. We found significantly more atomic refactorings in test-pairs pull requests, such as Change Variable Type Operation or Change Parameter Type. Second, we manually analyzed the code changes of 200 pull requests, where developers explicitly mention the terms “testability” or “refactor + test”. We identified ten composite refactoring procedures for the purpose of testability, which we call testability refactoring patterns. Third, we manually analyzed additional 524 test-pairs pull requests: both randomly selected and where we assumed to find testability refactorings, e.g. in pull requests about dependency or concurrency issues. About 25% of all analyzed pull requests actually included testability refactoring patterns. The most frequent were extract a method for override or for invocation, widen access to a method for invocation, and extract a class for invocation. We also report on frequent atomic refactorings which co-occur with the patterns and discuss the implications of our findings for research, practice, and education.","Pull request mining,software quality,refactoring patterns,software testability,mining software repositories",Pull请求中的可测试性重构：模式和趋势,要创建单元测试，可能需要重构生产代码，例如，通过扩大对特定方法的访问权限，或者通过将类分解为更容易独立测试的较小单元。我们报告了一项广泛的研究，以了解这种复合重构过程，从而提高可测试性。我们收集并研究了621个GitHub项目的346841个java拉取请求。首先，我们比较了两种群体中的原子重构：具有更改的测试对的拉取请求（即生产和测试代码中的共同更改，因此可能包括可测试性重构）和没有测试对的拉取请求。我们在测试对拉请求中发现了明显更多的原子重构，如更改变量类型操作或更改参数类型。其次，我们手动分析了200个pull请求的代码更改，其中开发人员明确提到了术语“可测试性”或“重构+测试”。为了可测试性，我们确定了十个复合重构过程，我们称之为可测试性重构模式。第三，我们手动分析了额外的524个测试对拉取请求：这两个请求都是随机选择的，并且我们假设在哪里可以找到可测试性重构，例如在关于依赖性或并发性问题的拉取请求中。在所有分析的pull请求中，大约25%实际上包含可测试性重构模式。最常见的是提取用于重写或调用的方法，扩大对用于调用的方法的访问权限，以及提取用于调用的类。我们还报告了与模式同时发生的频繁的原子重构，并讨论了我们的发现对研究、实践和教育的影响。,拉式请求挖掘，软件质量，重构模式，软件可测试性，挖掘软件存储库,,,
JC4UYG8I,2023,https://doi.org/10.1109/ICSE48619.2023.00058,ICSE 2023,Does the Stream API Benefit from Special Debugging Facilities? A Controlled Experiment on Loops and Streams with Specific Debuggers,"Java's Stream API, that massively makes use of lambda expressions, permits a more declarative way of defining operations on collections in comparison to traditional loops. While experimental results suggest that the use of the Stream API has measurable benefits with respect to code readability (in comparison to loops), a remaining question is whether it has other implications. And one of such implications is, for example, tooling in general and debugging in particular because of the following: While the traditional loop-based approach applies filters one after another to single elements, the Stream API applies filters on whole collections. In the meantime there are dedicated debuggers for the Stream API, but it remains unclear whether such a debugger (on the Stream API) has a measurable benefit in comparison to the traditional stepwise debugger (on loops). The present papers introduces a controlled experiment on the debugging of filter operations using a stepwise debugger versus a stream debugger. The results indicate that under the experiment's settings the stream debugger has a significant ($\mathrm{p} &lt; .001$) and large, positive effect $(\eta_{p}^{2}=.899;\ \frac{M_{stepwise}}{M_{stream}} \sim 204\%)$. However, the experiment reveals that additional factors interact with the debugger treatment such as whether or not the failing object is known upfront. The mentioned factor has a strong and large disordinal interaction effect with the debugger ($\mathrm{p} &lt; .001; \eta_{p}^{2}=.928$): In case an object is known upfront that can be used to identify a failing filter, the stream debugger is even less efficient than the stepwise debugger $(\frac{M_{stepwise}}{M_{stream}}\sim 72\%)$. Hence, while we found overall a positive effect of the stream debugger, the answer whether or not debugging is easier on loops or streams cannot be answered without taking the other variables into account. Consequently, we see a contribution of the present paper not only in the comparison of different debuggers but in the identification of additional factors.","Software Engineering,Programming Techniques,Debugging aids,Usability testing",流API是否受益于特殊调试设施？使用特定调试器对循环和流进行控制实验,Java的Stream API大量使用lambda表达式，与传统循环相比，它允许以更具声明性的方式定义集合上的操作。虽然实验结果表明，使用Stream API在代码可读性方面（与循环相比）具有可测量的好处，但剩下的问题是它是否具有其他含义。其中一个影响是，例如，由于以下原因，通常使用工具，特别是调试：虽然传统的基于环的方法将过滤器一个接一个地应用于单个元素，但Stream API将过滤器应用于整个集合。与此同时，有专门的调试器用于Stream API，但目前尚不清楚这种调试器（在Stream API上）与传统的逐步调试器（在循环上）相比是否具有可测量的优势。本文介绍了一个使用逐步调试器与流调试器调试过滤操作的对照实验。结果表明，在实验设置下，流调试器具有显著的（$\mathrm｛p｝&lt；.001$）和大的正效应$（\eta_{p｝^{2｝=.899；\\frac｛M_｛逐步｝｝｛M_{stream｝｝\sim 204\%）$。然而，实验表明，其他因素与调试器处理相互作用，例如是否预先知道失败的对象。上述因素与调试器有着强烈而大的无序交互效应（$\mathrm｛p｝&lt；.001；\eta_｛p}^｛2｝=.928$）：如果一个对象是预先已知的，可以用来识别失败的筛选器，则流调试器的效率甚至不如步进调试器$（\frac｛M_｛steppedic｝｝｛M_｛stream｝\sim 72\%）。因此，虽然我们发现流调试器的总体效果是积极的，但如果不考虑其他变量，就无法回答对循环或流的调试是否更容易。因此，我们不仅在比较不同的调试器方面，而且在识别其他因素方面看到了本文的贡献。,软件工程，编程技术，调试辅助工具，可用性测试,,,
ZDHV2SMK,2023,https://doi.org/10.1109/ICSE48619.2023.00092,ICSE 2023,DeepArc: Modularizing Neural Networks for the Model Maintenance,"Neural networks are an emerging data-driven programming paradigm widely used in many areas. Unlike traditional software systems consisting of decomposable modules, a neural network is usually delivered as a monolithic package, raising challenges for some maintenance tasks such as model restructure and re-adaption. In this work, we propose DeepArc, a novel modularization method for neural networks, to reduce the cost of model maintenance tasks. Specifically, DeepArc decomposes a neural network into several consecutive modules, each of which encapsulates consecutive layers with similar semantics. The network modularization facilitates practical tasks such as refactoring the model to preserve existing features (e.g., model compression) and enhancing the model with new features (e.g., fitting new samples). The modularization and encapsulation allow us to restructure or retrain the model by only pruning and tuning a few localized neurons and layers. Our experiments show that (1) DeepArc can boost the runtime efficiency of the state-of-the-art model compression techniques by 14.8%; (2) compared to the traditional model retraining, DeepArc only needs to train less than 20% of the neurons on average to fit adversarial samples and repair under-performing models, leading to 32.85% faster training performance while achieving similar model prediction performance.","architecture,modularization,neural networks",DeepArc：用于模型维护的模块化神经网络,神经网络是一种新兴的数据驱动编程范式，广泛应用于许多领域。与由可分解模块组成的传统软件系统不同，神经网络通常是作为一个整体包提供的，这给一些维护任务（如模型重构和重新适应）带来了挑战。在这项工作中，我们提出了一种新的神经网络模块化方法DeepArc，以降低模型维护任务的成本。具体来说，DeepArc将神经网络分解为几个连续的模块，每个模块封装具有相似语义的连续层。网络模块化促进了实际任务，例如重构模型以保留现有特征（例如，模型压缩）和用新特征增强模型（例如，拟合新样本）。模块化和封装使我们能够仅通过修剪和调整少数局部神经元和层来重构或重新训练模型。我们的实验表明：（1）DeepArc可以将最先进的模型压缩技术的运行效率提高14.8%；（2） 与传统的模型再训练相比，DeepArc平均只需要训练不到20%的神经元就可以拟合对抗性样本并修复性能较差的模型，从而使训练性能提高32.85%，同时实现类似的模型预测性能。,架构，模块化，神经网络,,,
CBZKZ3K5,2023,https://doi.org/10.1109/ICSE48619.2023.00104,ICSE 2023,When and Why Test Generators for Deep Learning Produce Invalid Inputs: an Empirical Study,"Testing Deep Learning (DL) based systems inherently requires large and representative test sets to evaluate whether DL systems generalise beyond their training datasets. Diverse Test Input Generators (TIGs) have been proposed to produce artificial inputs that expose issues of the DL systems by triggering misbehaviours. Unfortunately, such generated inputs may be invalid, i.e., not recognisable as part of the input domain, thus providing an unreliable quality assessment. Automated validators can ease the burden of manually checking the validity of inputs for human testers, although input validity is a concept difficult to formalise and, thus, automate. In this paper, we investigate to what extent TIGs can generate valid inputs, according to both automated and human validators. We conduct a large empirical study, involving 2 different automated validators, 220 human assessors, 5 different TIGs and 3 classification tasks. Our results show that 84% artificially generated inputs are valid, according to automated validators, but their expected label is not always preserved. Automated validators reach a good consensus with humans (78% accuracy), but still have limitations when dealing with feature-rich datasets.","software testing,deep learning",深度学习的测试生成器何时以及为什么产生无效输入：一项实证研究,测试基于深度学习（DL）的系统本质上需要大型且具有代表性的测试集来评估DL系统是否在其训练数据集之外具有泛化能力。已经提出了多种测试输入生成器（TIG）来产生人工输入，通过触发不当行为来暴露DL系统的问题。不幸的是，这样生成的输入可能是无效的，即，不能被识别为输入域的一部分，从而提供不可靠的质量评估。自动化验证器可以减轻人工测试人员手动检查输入有效性的负担，尽管输入有效性是一个很难正式化的概念，因此很难自动化。在本文中，我们研究了根据自动化和人工验证器，TIG可以在多大程度上生成有效输入。我们进行了一项大型实证研究，涉及2个不同的自动验证器、220个人工评估员、5个不同的TIG和3个分类任务。根据自动验证器的数据，我们的结果表明，84%的人工生成的输入是有效的，但它们的预期标签并不总是保留下来。自动验证器与人类达成了良好的共识（78%的准确率），但在处理功能丰富的数据集时仍然存在局限性。,软件测试，深度学习,,,
RZKGS2G9,2023,https://doi.org/10.1109/ICSE48619.2023.00080,ICSE 2023,How Do Developers' Profiles and Experiences Influence their Logging Practices? An Empirical Study of Industrial Practitioners,"Logs record the behavioral data of running programs and are typically generated by executing log statements. Software developers generally carry out logging practices with clear intentions and associated concerns (I&Cs). However, I&Cs may not be properly fulfilled in source code as log placement - specifically determination of a log statement's context and content - is often susceptible to an individual's profile and experience. Some industrial studies have been conducted to discern developers' main logging I&Cs and the way I&Cs are fulfilled. However, the findings are only based on the developers from a single company in each individual study and hence have limited generalizability. More importantly, there lacks a comprehensive and deep understanding of the relationships between developers' profiles and experiences and their logging practices from a wider perspective. To fill this significant gap, we conducted an empirical study using mixed methods comprising questionnaire surveys, semi-structured interviews, and code analyses with practitioners from a wide range of companies across a variety of industrial domains. Results reveal that while developers share common logging I&Cs and conduct logging practices mainly in the coding stage, their profiles and experiences profoundly influence their logging I&Cs and the way the I&Cs are fulfilled. These findings pave the way to facilitate the acceptance of important logging I&Cs and the adoption of good logging practices by developers","Logging practice,Intention,Concern,Fulfill",开发人员的配置文件和经验如何影响他们的日志记录实践？产业从业者的实证研究,日志记录运行程序的行为数据，通常通过执行日志语句生成。软件开发人员通常以明确的意图和相关的关注点（I&C）来执行日志记录实践。然而，I&C可能无法在源代码中正确实现，因为日志放置（特别是确定日志语句的上下文和内容）通常容易受到个人资料和经验的影响。已经进行了一些行业研究，以了解开发人员的主要日志I&C以及I&C的实现方式。然而，这些发现仅基于每个单独研究中单个公司的开发人员，因此具有有限的可推广性。更重要的是，从更广泛的角度对开发人员的个人资料和经验以及他们的日志记录实践之间的关系缺乏全面而深入的理解。为了填补这一重大空白，我们使用混合方法进行了一项实证研究，包括问卷调查、半结构化访谈和代码分析，参与者来自各行各业的广泛公司。结果表明，虽然开发人员共享常见的日志记录I&C，并主要在编码阶段进行日志记录实践，但他们的配置文件和经验深刻影响了他们的日志记录I/C和I&C的实现方式。这些发现为促进开发人员接受重要的日志记录I&C和采用良好的日志记录实践铺平了道路,测井实践，意图，关注，实现,,,
9SBF4VSE,2023,https://doi.org/10.1109/ICSE48619.2023.00159,ICSE 2023,"Semi-Automatic, Inline and Collaborative Web Page Code Curations","Software developers spend about a quarter of their workday using the web to fulfill various information needs. Searching for relevant information online can be time-consuming, yet acquired information is rarely systematically persisted for later reference. In this work, we introduce SALI, an approach for semi-automated inline linking of web pages to source code locations. SALI helps developers naturally capture high-quality, explicit links between web pages and specific source code lo-cations by recommending links for curation within the IDE. Through two laboratory studies, we examined the developer's ability to both curate and consume links between web pages and specific source code locations while performing software development tasks. The studies were performed with 20 subjects working on realistic software change tasks from widely-used open-source projects. Results show that developers continuously and concisely curate web pages at meaningful locations in the code with little effort. Additionally, we found that other developers could use these curations while performing new and different change tasks to speed up relevant information gathering within unfamiliar codebases by a factor of 2.4.","Semi-automated link curation,knowledge management,web browsing,collaboration",半自动、内联和协作式网页代码编排,软件开发人员大约四分之一的工作日都在使用网络来满足各种信息需求。在线搜索相关信息可能很耗时，但获取的信息很少系统地保存下来供日后参考。在这项工作中，我们介绍了SALI，一种将网页半自动内联链接到源代码位置的方法。SALI通过在IDE中推荐管理链接，帮助开发人员自然地捕获网页和特定源代码位置之间的高质量、明确的链接。通过两项实验室研究，我们检查了开发人员在执行软件开发任务时策划和使用网页和特定源代码位置之间的链接的能力。这项研究由20名受试者进行，他们从事广泛使用的开源项目中的现实软件变更任务。结果表明，开发人员可以毫不费力地在代码中有意义的位置连续、简洁地策划网页。此外，我们发现其他开发人员可以在执行新的和不同的更改任务时使用这些策略，以将不熟悉的代码库中的相关信息收集速度提高2.4倍。,半自动化链接管理，知识管理，网页浏览，协作,,,
UFAHKJS2,2023,https://doi.org/10.1109/ICSE48619.2023.00140,ICSE 2023,Measuring Secure Coding Practice and Culture: A Finger Pointing at the Moon is not the Moon,"Software security research has a core problem: it is impossible to prove the security of complex software. A low number of known defects may simply indicate that the software has not been attacked yet, or that successful attacks have not been detected. A high defect count may be the result of white-hat hacker targeting, or of a successful bug bounty program which prevented insecurities from persisting in the wild. This makes it difficult to measure the security of non-trivial software. Researchers instead usually measure effort directed towards ensuring software security. However, different researchers use their own tailored measures, usually devised from industry secure coding guidelines. Not only is there no agreed way to measure effort, there is also no agreement on what effort entails. Qualitative studies emphasise the importance of security culture in an organisation. Where software security practices are introduced solely to ensure compliance with legislative or industry standards, a box-ticking attitude to security may result. The security culture may be weak or non-existent, making it likely that precautions not explicitly mentioned in the standards will be missed. Thus, researchers need both a way to assess software security practice and a way to measure software security culture. To assess security practice, we converted the empirically-established 12 most common software security activities into questions. To assess security culture, we devised a number of questions grounded in prior literature. We ran a secure development survey with both sets of questions, obtaining organic responses from 1,100 software coders in 59 countries. We used proven common activities to assess security practice, and made a first attempt to quantitatively assess aspects of security culture in the broad developer population. Our results show that some coders still work in environments where there is little to no attempt to ensure code security. Security practice and culture do not always correlate, and some organisations with strong secure coding practice have weak secure coding culture. This may lead to problems in defect prevention and sustained software security effort.","Security,secure coding,security compliance",测量安全编码实践与文化：手指指向月亮不是月亮,软件安全研究有一个核心问题：无法证明复杂软件的安全性。少量的已知缺陷可能只是表明软件尚未受到攻击，或者尚未检测到成功的攻击。高缺陷数可能是白帽黑客瞄准的结果，也可能是成功的漏洞奖励计划阻止了不安全感在野外持续存在的结果。这使得很难衡量非平凡软件的安全性。相反，研究人员通常会衡量旨在确保软件安全的努力。然而，不同的研究人员使用他们自己量身定制的措施，通常是根据行业安全编码指南设计的。不仅没有商定的方法来衡量努力，而且也没有就努力的内容达成一致。定性研究强调了安全文化在组织中的重要性。如果引入软件安全实践仅仅是为了确保遵守立法或行业标准，那么可能会导致对安全的态度打勾。安全文化可能薄弱或根本不存在，因此可能会错过标准中未明确提及的预防措施。因此，研究人员既需要一种评估软件安全实践的方法，也需要一种衡量软件安全文化的方法。为了评估安全实践，我们将根据经验建立的12种最常见的软件安全活动转化为问题。为了评估安全文化，我们设计了一些基于先前文献的问题。我们对这两组问题进行了安全开发调查，从59个国家的1100名软件程序员那里获得了有机的回答。我们使用已证实的常见活动来评估安全实践，并首次尝试在广大开发人员中定量评估安全文化的各个方面。我们的研究结果表明，一些编解码器仍然可以在几乎没有尝试确保代码安全的环境中工作。安全实践和文化并不总是相互关联的，一些具有强大安全编码实践的组织的安全编码文化较弱。这可能会导致缺陷预防和持续的软件安全工作出现问题。,安全性，安全编码，安全合规性,,,
GFH5TGAT,2023,https://doi.org/10.1109/ICSE48619.2023.00097,ICSE 2023,On Privacy Weaknesses and Vulnerabilities in Software Systems,"In this digital era, our privacy is under constant threat as our personal data and traceable online/offline activities are frequently collected, processed and transferred by many software applications. Privacy attacks are often formed by exploiting vulnerabilities found in those software applications. The Common Weakness Enumeration (CWE) and Common Vulnerabilities and Exposures (CVE) systems are currently the main sources that software engineers rely on for understanding and preventing publicly disclosed software vulnerabilities. However, our study on all 922 weaknesses in the CWE and 156,537 vulnerabilities registered in the CVE to date has found a very small coverage of privacy-related vulnerabilities in both systems, only 4.45% in CWE and 0.1% in CVE. These also cover only a small number of areas of privacy threats that have been raised in existing privacy software engineering research, privacy regulations and frameworks, and relevant reputable organisations. The actionable insights generated from our study led to the introduction of 11 new common privacy weaknesses to supplement the CWE system, making it become a source for both security and privacy vulnerabilities.","Privacy,Vulnerabilities,Threats,CWE,CVE,Software",论软件系统中的隐私弱点,在这个数字时代，我们的隐私不断受到威胁，因为我们的个人数据和可追踪的在线/离线活动经常被许多软件应用程序收集、处理和传输。隐私攻击通常是通过利用这些软件应用程序中的漏洞而形成的。常见弱点枚举（CWE）和常见漏洞和暴露（CVE）系统是目前软件工程师了解和防止公开披露的软件漏洞的主要来源。然而，我们对迄今为止CWE中的922个弱点和CVE中登记的156537个弱点的研究发现，这两个系统中与隐私相关的漏洞覆盖率都很小，CWE中只有4.45%，CVE中只有0.1%。这些也只涵盖了现有隐私软件工程研究、隐私法规和框架以及相关声誉良好的组织中提出的隐私威胁的一小部分领域。从我们的研究中产生的可操作的见解导致引入了11个新的常见隐私弱点，以补充CWE系统，使其成为安全和隐私漏洞的来源。,隐私，漏洞，威胁，CWE，CVE，软件,,,
SJDIJQ4S,2023,https://doi.org/10.1109/ICSE48619.2023.00030,ICSE 2023,Which of My Assumptions are Unnecessary for Realizability and Why Should I Care?,"Specifications for reactive systems synthesis consist of assumptions and guarantees. However, some specifications may include unnecessary assumptions, i.e., assumptions that are not necessary for realizability. While the controllers that are synthesized from such specifications are correct, they are also inflexible and fragile; their executions will satisfy the specification's guarantees in only very specific environments. In this work we show how to detect unnecessary assumptions, and to transform any realizable specification into a corresponding realizable core specification, one that includes the same guarantees but no unnecessary assumptions. We do this by computing an assumptions core, a locally minimal subset of assumptions that suffices for realizability. Controllers that are synthesized from a core specification are not only correct but, importantly, more general; their executions will satisfy the specification's guarantees in more environments. We implemented our ideas in the Spectra synthesis environment, and evaluated their impact over different benchmarks from the literature. The evaluation provides evidence for the motivation and significance of our work, by showing (1) that unnecessary assumptions are highly prevalent, (2) that in almost all cases the fully-automated removal of unnecessary assumptions pays off in total synthesis time, and (3) that core specifications induce more general controllers whose reachable state space is larger but whose representation more memory efficient.","Reactive synthesis,Formal specifications",我的哪些假设对可实现性来说是不必要的，为什么我应该关心？,无功系统综合规范由假设和保证组成。然而，一些规范可能包括不必要的假设，即对可实现性不必要的假定。虽然从这些规范合成的控制器是正确的，但它们也不灵活和脆弱；它们的执行将仅在非常特定的环境中满足规范的保证。在这项工作中，我们展示了如何检测不必要的假设，并将任何可实现的规范转换为相应的可实现核心规范，该规范包括相同的保证，但没有不必要的假定。我们通过计算一个假设核心来实现这一点，这是一个足以实现的假设的局部极小子集。根据核心规范合成的控制器不仅是正确的，而且更重要的是，更通用；它们的执行将在更多的环境中满足规范的保证。我们在Spectra合成环境中实现了我们的想法，并评估了它们对文献中不同基准的影响。该评估为我们工作的动机和意义提供了证据，表明（1）不必要的假设非常普遍，（2）在几乎所有情况下，完全自动删除不必要的假定在总合成时间内都会得到回报，以及（3）核心规范引入了更通用的控制器，其可达状态空间更大，但其表示更高效。,反应合成，正式规范,,,
7HFWS96Y,2023,https://doi.org/10.1109/ICSE48619.2023.00185,ICSE 2023,CoCoSoDa: Effective Contrastive Learning for Code Search,"Code search aims to retrieve semantically relevant code snippets for a given natural language query. Recently, many approaches employing contrastive learning have shown promising results on code representation learning and greatly improved the performance of code search. However, there is still a lot of room for improvement in using contrastive learning for code search. In this paper, we propose CoCoSoDa to effectively utilize contrastive learning for code search via two key factors in contrastive learning: data augmentation and negative samples. Specifically, soft data augmentation is to dynamically masking or replacing some tokens with their types for input sequences to generate positive samples. Momentum mechanism is used to generate large and consistent representations of negative samples in a mini-batch through maintaining a queue and a momentum encoder. In addition, multimodal contrastive learning is used to pull together representations of code-query pairs and push apart the unpaired code snippets and queries. We conduct extensive experiments to evaluate the effectiveness of our approach on a large-scale dataset with six programming languages. Experimental results show that: (1) CoCoSoDa outperforms 18 baselines and especially exceeds CodeBERT, GraphCodeBERT, and UniXcoder by 13.3%, 10.5%, and 5.9% on average MRR scores, respectively. (2) The ablation studies show the effectiveness of each component of our approach. (3) We adapt our techniques to several different pre-trained models such as RoBERTa, CodeBERT, and GraphCodeBERT and observe a significant boost in their performance in code search. (4) Our model performs robustly under different hyper-parameters. Furthermore, we perform qualitative and quantitative analyses to explore reasons behind the good performance of our model.","code search,contrastive learning,soft data augmentation,momentum mechanism",CoCoSoDa：用于代码搜索的有效对比学习,代码搜索旨在为给定的自然语言查询检索语义相关的代码片段。近年来，许多采用对比学习的方法在代码表示学习方面取得了很好的效果，极大地提高了代码搜索的性能。然而，在使用对比学习进行代码搜索方面仍有很大的改进空间。在本文中，我们提出了CoCoSoDa，通过对比学习中的两个关键因素：数据扩充和负样本，有效地利用对比学习进行代码搜索。具体地说，软数据扩充是动态地屏蔽或替换输入序列中的某些标记，以生成正样本。动量机制用于通过维护队列和动量编码器，在小批量中生成负样本的大而一致的表示。此外，多模态对比学习用于将代码-查询对的表示拉到一起，并将未配对的代码片段和查询推开。我们在使用六种编程语言的大规模数据集上进行了广泛的实验，以评估我们的方法的有效性。实验结果表明：（1）CoCoSoDa优于18个基线，尤其是在平均MRR得分上分别超过CodeBERT、GraphCodeBERT和UniXcoder 13.3%、10.5%和5.9%。（2） 消融研究显示了我们方法各组成部分的有效性。（3） 我们将我们的技术应用于几种不同的预训练模型，如RoBERTa、CodeBERT和GraphCodeBERT，并观察到它们在代码搜索中的性能显著提高。（4） 我们的模型在不同的超参数下表现稳健。此外，我们进行了定性和定量分析，以探索我们的模型良好性能背后的原因。,代码搜索，对比学习，软数据扩充，动量机制,,,
J8Q9HF9J,2023,https://doi.org/10.1109/ICSE48619.2023.00175,ICSE 2023,Testing Database Systems via Differential Query Execution,"Database Management Systems (DBMSs) provide efficient data retrieval and manipulation for many applications through Structured Query Language (SQL). Incorrect implementations of DBMSs can result in logic bugs, which cause SELECT queries to fetch incorrect results, or UPDATE and DELETE queries to generate incorrect database states. Existing approaches mainly focus on detecting logic bugs in SELECT queries. However, logic bugs in UPDATE and DELETE queries have not been tackled. In this paper, we propose a novel and general approach, which we have termed Differential Query Execution (DQE), to detect logic bugs in SELECT, UPDATE and DELETE queries of DBMSs. The core idea of DQE is that different SQL queries with the same predicate usually access the same rows in a database. For example, a row updated by an UPDATE query with a predicate φ should also be fetched by a SELECT query with the same predicate φ, If not, a logic bug is revealed in the target DBMS. To evaluate the effectiveness and generality of DQE, we apply DQE on five production-level DBMSs, i.e., MySQL, MariaDB, TiDB, CockroachDB and SQLite. In total, we have detected 50 unique bugs in these DBMSs, 41 of which have been confirmed, and 11 have been fixed. We expect that the simplicity and generality of DQE can greatly improve the reliability of DBMSs.","Database system,DBMS testing,logic bug",通过差异查询执行测试数据库系统,数据库管理系统（DBMS）通过结构化查询语言（SQL）为许多应用程序提供高效的数据检索和操作。DBMS的不正确实现可能会导致逻辑错误，导致SELECT查询获取不正确的结果，或者UPDATE和DELETE查询生成不正确的数据库状态。现有的方法主要集中于检测SELECT查询中的逻辑错误。然而，UPDATE和DELETE查询中的逻辑错误尚未得到解决。在本文中，我们提出了一种新的通用方法，称为差分查询执行（DQE），用于检测DBMS的SELECT、UPDATE和DELETE查询中的逻辑错误。DQE的核心思想是，具有相同谓词的不同SQL查询通常访问数据库中的相同行。例如，由具有谓词φ的UPDATE查询更新的行也应该由具有相同谓词φ的SELECT查询获取。如果没有，则在目标DBMS中显示逻辑错误。为了评估DQE的有效性和通用性，我们将DQE应用于五个生产级数据库管理系统，即MySQL、MariaDB、TiDB、CockratchDB和SQLite。我们总共在这些DBMS中检测到了50个独特的错误，其中41个已被确认，11个已被修复。我们期望DQE的简单性和通用性可以大大提高DBMS的可靠性。,数据库系统，DBMS测试，逻辑错误,,,
S3HFUNUN,2023,https://doi.org/10.1109/ICSE48619.2023.00025,ICSE 2023,Evaluating the Impact of Experimental Assumptions in Automated Fault Localization,"Much research on automated program debugging often assumes that bug fix location(s) indicate the faults' root causes and that root causes of faults lie within single code elements (statements). It is also often assumed that the number of statements a developer would need to inspect before finding the first faulty statement reflects debugging effort. Although intuitive, these three assumptions are typically used (55% of experiments in surveyed publications make at least one of these three assumptions) without any consideration of their effects on the debugger's effectiveness and potential impact on developers in practice. To deal with this issue, we perform controlled experimentation, split testing in particular, using 352 bugs from 46 open-source C programs, 19 Automated Fault Localization (AFL) techniques (18 statistical debugging formulas and dynamic slicing), two (2) state-of-the-art automated program repair (APR) techniques (GenProg and Angelix) and 76 professional developers. Our results show that these assumptions conceal the difficulty of debugging. They make AFL techniques appear to be (up to 38%) more effective, and make APR tools appear to be (2X) less effective. We also find that most developers (83%) consider these assumptions to be unsuitable for debuggers and, perhaps worse, that they may inhibit development productivity. The majority (66%) of developers prefer debugging diagnoses without these assumptions twice as much as with the assumptions. Our findings motivate the need to assess debuggers conservatively, i.e., without these assumptions.","fault localization,program repair,user study",评估自动故障定位中实验假设的影响,许多关于自动程序调试的研究通常认为，错误修复位置指示故障的根本原因，并且故障的根源位于单个代码元素（语句）中。人们还经常认为，开发人员在发现第一个错误语句之前需要检查的语句数量反映了调试工作。尽管这三个假设很直观，但通常会使用（调查出版物中55%的实验至少做出了这三个假定中的一个），而没有考虑它们对调试器有效性的影响以及在实践中对开发人员的潜在影响。为了解决这个问题，我们使用了来自46个开源C程序的352个bug、19个自动故障定位（AFL）技术（18个统计调试公式和动态切片）、两（2）种最先进的自动程序修复（APR）技术（GenProg和Angelix）和76个专业开发人员进行了控制实验，特别是分割测试。我们的结果表明，这些假设掩盖了调试的困难。它们使AFL技术看起来更有效（高达38%），使APR工具看起来更无效（2倍）。我们还发现，大多数开发人员（83%）认为这些假设不适合调试器，也许更糟的是，它们可能会抑制开发效率。大多数（66%）的开发人员更喜欢在没有这些假设的情况下调试诊断，这是有这些假设的两倍。我们的发现促使我们需要保守地评估调试器，即在没有这些假设的情况下。,故障定位，程序修复，用户研究,,,
PI2QJQDV,2023,https://doi.org/10.1109/ICSE48619.2023.00188,ICSE 2023,An Empirical Study of Deep Learning Models for Vulnerability Detection,"Deep learning (DL) models of code have recently reported great progress for vulnerability detection. In some cases, DL-based models have outperformed static analysis tools. Although many great models have been proposed, we do not yet have a good understanding of these models. This limits the further advancement of model robustness, debugging, and deployment for the vulnerability detection. In this paper, we surveyed and reproduced 9 state-of-the-art (SOTA) deep learning models on 2 widely used vulnerability detection datasets: Devign and MSR. We investigated 6 research questions in three areas, namely model capabilities, training data, and model interpretation. We experimentally demonstrated the variability between different runs of a model and the low agreement among different models' outputs. We investigated models trained for specific types of vulnerabilities compared to a model that is trained on all the vulnerabilities at once. We explored the types of programs DL may consider “hard” to handle. We investigated the relations of training data sizes and training data composition with model performance. Finally, we studied model interpretations and analyzed important features that the models used to make predictions. We believe that our findings can help better understand model results, provide guidance on preparing training data, and improve the robustness of the models. All of our datasets, code, and results are available at https://doi.org/10.6084/m9.figshare.20791240.","deep learning,vulnerability detection,empirical study",漏洞检测的深度学习模型的实证研究,最近，代码的深度学习（DL）模型在漏洞检测方面取得了巨大进展。在某些情况下，基于DL的模型的性能优于静态分析工具。尽管已经提出了许多伟大的模型，但我们对这些模型还没有很好的理解。这限制了模型健壮性、调试和漏洞检测部署的进一步发展。在本文中，我们在两个广泛使用的漏洞检测数据集上调查并复制了9个最先进的（SOTA）深度学习模型：Devign和MSR。我们调查了三个领域的6个研究问题，即模型能力、训练数据和模型解释。我们通过实验证明了模型不同运行之间的可变性，以及不同模型输出之间的低一致性。与同时针对所有漏洞进行训练的模型相比，我们调查了针对特定类型漏洞训练的模型。我们探讨了DL可能认为“难以”处理的程序类型。我们研究了训练数据大小和训练数据组成与模型性能的关系。最后，我们研究了模型解释，并分析了模型用于预测的重要特征。我们相信，我们的发现可以帮助更好地理解模型结果，为准备训练数据提供指导，并提高模型的稳健性。我们所有的数据集、代码和结果都可以在https://doi.org/10.6084/m9.figshare.20791240.,深度学习，漏洞检测，实证研究,,,
GTU8U58Z,2023,https://doi.org/10.1109/ICSE48619.2023.00089,ICSE 2023,Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation,"Due to convenience, open-source software is widely used. For beneficial reasons, open-source maintainers often fix the vulnerabilities silently, exposing their users unaware of the updates to threats. Previous works all focus on black-box binary detection of the silent dependency alerts that suffer from high false-positive rates. Open-source software users need to analyze and explain AI prediction themselves. Explainable AI becomes remarkable as a complementary of black-box AI models, providing details in various forms to explain AI decisions. Noticing there is still no technique that can discover silent dependency alert on time, in this work, we propose a framework using an encoder-decoder model with a binary detector to provide explainable silent dependency alert prediction. Our model generates 4 types of vulnerability key aspects including vulnerability type, root cause, attack vector, and impact to enhance the trustworthiness and users' acceptance to alert prediction. By experiments with several models and inputs, we confirm CodeBERT with both commit messages and code changes achieves the best results. Our user study shows that explainable alert predictions can help users find silent dependency alert more easily than black-box predictions. To the best of our knowledge, this is the first research work on the application of Explainable AI in silent dependency alert prediction, which opens the door of the related domains.","Codes,Soft sensors,Closed box,Detectors,Predictive models,Generators,Data models",带漏洞关键方面解释的静默漏洞依赖性警报预测,由于方便，开源软件被广泛使用。出于有利的原因，开源维护人员经常默默地修复漏洞，让他们的用户在没有意识到更新的情况下受到威胁。以前的工作都集中在无声依赖警报的黑盒二进制检测上，这些警报的假阳性率很高。开源软件用户需要自己分析和解释人工智能预测。可解释人工智能作为黑匣子人工智能模型的补充，以各种形式提供细节来解释人工智能决策，变得引人注目。注意到仍然没有能够及时发现无声依赖警报的技术，在这项工作中，我们提出了一个框架，使用带有二进制检测器的编码器-解码器模型来提供可解释的无声依赖警报预测。我们的模型生成了4种类型的漏洞关键方面，包括漏洞类型、根本原因、攻击向量和影响，以提高可信度和用户对警报预测的接受度。通过对几个模型和输入的实验，我们证实了具有提交消息和代码更改的CodeBERT实现了最佳结果。我们的用户研究表明，可解释的警报预测比黑匣子预测更容易帮助用户找到无声依赖警报。据我们所知，这是可解释人工智能在无声依赖警报预测中的首次应用研究，为相关领域打开了大门。,代码，软测量，封闭盒子，检测器，预测模型，生成器，数据模型,,,
GU4MQ8T5,2023,https://doi.org/10.1109/ICSE48619.2023.00018,ICSE 2023,Validating SMT Solvers via Skeleton Enumeration Empowered by Historical Bug-Triggering Inputs,"SMT solvers check the satisfiability of logic formulas over first-order theories, which have been utilized in a rich number of critical applications, such as software verification, test case generation, and program synthesis. Bugs hidden in SMT solvers would severely mislead those applications and further cause severe consequences. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Although many approaches have been proposed to test SMT solvers, it is still a challenge to discover bugs effectively. To tackle such a challenge, we conduct an empirical study on the historical bug-triggering formulas in SMT solvers' bug tracking systems. We observe that the historical bug-triggering formulas contain valuable skeletons (i.e., core structures of formulas) as well as associated atomic formulas which can cast significant impacts on formulas' ability in triggering bugs. Therefore, we propose a novel approach that utilizes the skeletons extracted from the historical bug-triggering formulas and enumerates atomic formulas under the guidance of association rules derived from historical formulas. In this study, we realized our approach as a practical fuzzing tool HistFuzz and conducted extensive testing on the well-known SMT solvers Z3 and cvc5. To date, HistFuzz has found 111 confirmed new bugs for Z3 and cvc5, of which 108 have been fixed by the developers. More notably, out of the confirmed bugs, 23 are soundness bugs and invalid model bugs found in the solvers' default mode, which are essential for SMT solvers. In addition, our experiments also demonstrate that HistFuzz outperforms the state-of-the-art SMT solver fuzzers in terms of achieved code coverage and effectiveness.","SMT solver,fuzzing,skeleton enumeration,association rules,bug detection",通过历史错误触发输入授权的骨架枚举验证SMT解决方案,SMT求解器检查逻辑公式相对于一阶理论的可满足性，一阶理论已被用于大量关键应用，如软件验证、测试用例生成和程序合成。SMT解决程序中隐藏的错误会严重误导这些应用程序，并进一步造成严重后果。因此，确保SMT求解器的可靠性和稳健性至关重要。尽管已经提出了许多方法来测试SMT求解器，但有效地发现错误仍然是一个挑战。为了应对这一挑战，我们对SMT求解器的错误跟踪系统中的历史错误触发公式进行了实证研究。我们观察到，历史错误触发公式包含有价值的骨架（即公式的核心结构）以及相关的原子公式，这些原子公式会对公式触发错误的能力产生重大影响。因此，我们提出了一种新的方法，利用从历史错误触发公式中提取的骨架，并在从历史公式中导出的关联规则的指导下枚举原子公式。在本研究中，我们将我们的方法实现为一个实用的模糊工具HistFuzz，并对著名的SMT求解器Z3和cvc5进行了广泛的测试。到目前为止，HistFuzz已经为Z3和cvc5发现了111个已确认的新错误，其中108个已由开发人员修复。更值得注意的是，在已确认的错误中，有23个是在解算器的默认模式中发现的健全性错误和无效模型错误，这对SMT解算器至关重要。此外，我们的实验还表明，HistFuzz在代码覆盖率和有效性方面优于最先进的SMT求解器模糊器。,SMT求解器，模糊化，骨架枚举，关联规则，错误检测,,,
ZZRG2YFN,2023,https://doi.org/10.1109/ICSE48619.2023.00064,ICSE 2023,Is It Enough to Recommend Tasks to Newcomers? Understanding Mentoring on Good First Issues,"Newcomers are critical for the success and continuity of open source software (OSS) projects. To attract newcomers and facilitate their onboarding, many OSS projects recommend tasks for newcomers, such as good first issues (GFIs). Previous studies have preliminarily investigated the effects of GFIs and techniques to identify suitable GFIs. However, it is still unclear whether just recommending tasks is enough and how significant mentoring is for newcomers. To better understand mentoring in OSS communities, we analyze the resolution process of 48,402 GFIs from 964 repositories through a mix-method approach. We investigate the extent, the mentorship structures, the discussed topics, and the relevance of expert involvement. We find that ~70% of GFIs have expert participation, with each GFI usually having one expert who makes two comments. Half of GFIs will receive their first expert comment within 8.5 hours after a newcomer comment. Through analysis of the collaboration networks of newcomers and experts, we observe that community mentorship presents four types of structure: centralized mentoring, decentralized mentoring, collaborative mentoring, and distributed mentoring. As for discussed topics, we identify 14 newcomer challenges and 18 expert mentoring content. By fitting the generalized linear models, we find that expert involvement positively correlates with newcomers' successful contributions but negatively correlates with newcomers' retention. Our study manifests the status and significance of mentoring in the OSS projects, which provides rich practical implications for optimizing the mentoring process and helping newcomers contribute smoothly and suecessfully,","newcomer,mentoring,open source,good first issue",向新人推荐任务就够了吗？理解良好开端问题的指导,新加入者对于开放源码软件（OSS）项目的成功和连续性至关重要。为了吸引新来者并促进他们的入职，许多OSS项目向新来者推荐任务，例如良好的第一期（GFI）。先前的研究已经初步调查了GFI的效果和确定合适GFI的技术。然而，目前尚不清楚仅仅推荐任务是否足够，以及辅导对新人的意义有多大。为了更好地理解OSS社区中的辅导，我们通过混合方法分析了964个存储库中48402个GFI的解决过程。我们调查了专家参与的程度、指导结构、讨论的主题和相关性。我们发现，约70%的GFI有专家参与，每个GFI通常有一位专家发表两条评论。一半的GFI将在收到新评论后8.5小时内收到他们的第一条专家评论。通过分析新来者和专家的协作网络，我们观察到社区导师制呈现出四种类型的结构：集中导师制、分散导师制、协作导师制和分布式导师制。关于讨论的主题，我们确定了14个新挑战和18个专家指导内容。通过拟合广义线性模型，我们发现专家参与与新人的成功贡献呈正相关，但与新人的保留率呈负相关。我们的研究表明了辅导在OSS项目中的地位和意义，这为优化辅导过程和帮助新来者顺利、成功地做出贡献提供了丰富的实践意义，,新人，指导，开源，好的第一期,,,
6FVLHTVI,2023,https://doi.org/10.1109/ICSE48619.2023.00050,ICSE 2023,PTPDroid: Detecting Violated User Privacy Disclosures to Third-Parties of Android Apps,"Android apps frequently access personal information to provide customized services. Since such information is sensitive in general, regulators require Android app vendors to publish privacy policies that describe what information is collected and why it is collected. Existing work mainly focuses on the types of the collected data but seldom considers the entities that collect user privacy, which could falsely classify problematic declarations about user privacy collected by third-parties into clear disclosures. To address this problem, we propose PTPDroid, a flow-to-policy consistency checking approach and an automated tool, to comprehensively uncover from the privacy policy the violated disclosures to third-parties. Our experiments on real-world apps demonstrate the effectiveness and superiority of PTPDroid, and our empirical study on 1,000 popular real-world apps reveals that violated user privacy disclosures to third-parties are prevalent in practice.","Android app,privacy policy,third-party entities,violation detection,taint analysis,empirical study",PTPDroid:检测违反用户隐私向安卓应用程序第三方披露,安卓应用程序经常访问个人信息以提供定制服务。由于此类信息通常是敏感的，监管机构要求安卓应用程序供应商发布隐私政策，说明收集的信息以及收集原因。现有工作主要关注收集的数据类型，但很少考虑收集用户隐私的实体，这可能会将第三方收集的关于用户隐私的有问题的声明错误地归类为明确的披露。为了解决这个问题，我们提出了一种流到策略一致性检查方法和自动化工具PTPDroid，以从隐私政策中全面揭示向第三方披露的违规信息。我们在现实世界应用程序上的实验证明了PTPDroid的有效性和优越性，我们对1000个流行的现实世界应用的实证研究表明，向第三方披露侵犯用户隐私的行为在实践中很普遍。,安卓应用程序，隐私政策，第三方实体，违规检测，污染分析，实证研究,,,
MKE2A7QL,2023,https://doi.org/10.1109/ICSE48619.2023.00011,ICSE 2023,The Road Toward Dependable AI Based Systems,"With the advent of deep learning, AI components have achieved unprecedented performance on complex, human competitive tasks, such as image, video, text and audio processing. Hence, they are increasingly integrated into sophisticated software systems, some of which (e.g., autonomous vehicles) are required to deliver certified dependability warranties. In this talk, I will consider the unique features of AI based systems and of the faults possibly affecting them, in order to revise the testing fundamentals and redefine the overall goal of testing, taking a statistical view on the dependability warranties that can be actually delivered. Then, I will consider the key elements of a revised testing process for AI based systems, including the test oracle and the test input generation problems. I will also introduce the notion of runtime supervision, to deal with unexpected error conditions that may occur in the field. Finally, I will identify the future steps that are essential to close the loop from testing to operation, proposing an empirical framework that reconnects the output of testing to its original goals.","Software Testing,Deep Learning,Reliability and Dependability",迈向可靠的基于人工智能的系统之路,随着深度学习的出现，人工智能组件在图像、视频、文本和音频处理等复杂的、具有人类竞争力的任务上取得了前所未有的性能。因此，它们越来越多地集成到复杂的软件系统中，其中一些系统（如自动驾驶汽车）需要提供经认证的可靠性保证。在本次演讲中，我将考虑基于人工智能的系统的独特特征以及可能影响它们的故障，以修订测试基本原理并重新定义测试的总体目标，从统计角度看待实际可提供的可靠性保证。然后，我将考虑基于人工智能的系统的修订测试流程的关键要素，包括测试预言机和测试输入生成问题。我还将介绍运行时监督的概念，以处理现场可能发生的意外错误情况。最后，我将确定未来的步骤，这些步骤对于结束从测试到操作的循环至关重要，并提出一个经验框架，将测试的输出与其原始目标重新连接起来。,软件测试，深度学习，可靠性和可靠性,,,
TB8WEZIQ,2023,https://doi.org/10.1109/ICSE48619.2023.00038,ICSE 2023,Do I Belong? Modeling Sense of Virtual Community Among Linux Kernel Contributors,"The sense of belonging to a community is a basic human need that impacts an individual's behavior, long-term engagement, and job satisfaction, as revealed by research in disciplines such as psychology, healthcare, and education. Despite much research on how to retain developers in Open Source Software (OSS) projects and other virtual, peer-production communities, there is a paucity of research investigating what might contribute to a sense of belonging in these communities. To that end, we develop a theoretical model that seeks to understand the link between OSS developer motives and a Sense of Virtual Community (SVC). We test the model with a dataset collected in the Linux Kernel developer community (N=225), using structural equation modeling techniques. Our results for this case study show that intrinsic motivations (social or hedonic motives) are positively associated with a sense of virtual community, but living in an authoritative country and being paid to contribute can reduce the sense of virtual community. Based on these results, we offer suggestions for open source projects to foster a sense of virtual community, with a view to retaining contributors and Improving projects' sustainability.","sense of virtual community,belonging,open source,software developers,human factors,survey,PLS-SEM",我属于吗？Linux内核贡献者的虚拟社区建模意识,心理学、医疗保健和教育等学科的研究表明，对社区的归属感是影响个人行为、长期参与和工作满意度的人类基本需求。尽管有很多关于如何在开源软件（OSS）项目和其他虚拟的同行生产社区中留住开发人员的研究，但很少有研究调查什么可能有助于这些社区的归属感。为此，我们开发了一个理论模型，试图理解OSS开发者的动机和虚拟社区感（SVC）之间的联系。我们使用结构方程建模技术，用Linux内核开发人员社区（N=225）中收集的数据集测试了该模型。我们在这个案例研究中的结果表明，内在动机（社会或享乐动机）与虚拟社区感呈正相关，但生活在权威国家并获得贡献报酬会降低虚拟社区感。基于这些结果，我们为开源项目提供了建议，以培养虚拟社区意识，从而留住贡献者并提高项目的可持续性。,虚拟社区感，归属感，开源，软件开发人员，人为因素，调查，PLS-SEM,,,
J4XUQRWC,2023,https://doi.org/10.1109/ICSE48619.2023.00203,ICSE 2023,Automating Code-Related Tasks Through Transformers: The Impact of Pre-training,"Transformers have gained popularity in the software engineering (SE) literature. These deep learning models are usually pre-trained through a self-supervised objective, meant to provide the model with basic knowledge about a language of interest (e.g., Java). A classic pre-training objective is the masked language model (MLM), in which a percentage of tokens from the input (e.g., a Java method) is masked, with the model in charge of predicting them. Once pre-trained, the model is then fine-tuned to support the specific downstream task of interest (e.g., code summarization). While there is evidence suggesting the boost in performance provided by pre-training, little is known about the impact of the specific pre-training objective(s) used. Indeed, MLM is just one of the possible pre-training objectives and recent work from the natural language processing field suggest that pre-training objectives tailored for the specific downstream task of interest may substantially boost the model's performance. For example, in the case of code summarization, a tailored pre-training objective could be the identification of an appropriate name for a given method, considering the method name to generate as an extreme summary. In this study, we focus on the impact of pre-training objectives on the performance of trans-formers when automating code-related tasks. We start with a systematic literature review aimed at identifying the pre-training objectives used in SE. Then, we pre-train 32 transformers using both (i) generic pre-training objectives usually adopted in SE; and (ii) pre-training objectives tailored to specific code-related tasks subject of our experimentation, namely bug-fixing, code summarization, and code completion. We also compare the pre-trained models with non pre-trained ones and show the advantage brought by pre-training in different scenarios, in which more or less fine-tuning data are available. Our results show that: (i) pre-training helps in boosting performance only if the amount of fine-tuning data available is small; (ii) the MLM objective is usually sufficient to maximize the prediction performance of the model, even when comparing it with pre-training objectives specialized for the downstream task at hand.","Pre-training,Code Recommenders",通过变压器实现代码相关任务的自动化：预培训的影响,变压器在软件工程（SE）文献中越来越受欢迎。这些深度学习模型通常是通过自我监督目标预先训练的，旨在为模型提供有关感兴趣的语言（例如Java）的基本知识。一个经典的预训练目标是掩蔽语言模型（MLM），其中来自输入（例如Java方法）的令牌的百分比被掩蔽，由模型负责预测它们。一旦进行了预训练，则对模型进行微调，以支持感兴趣的特定下游任务（例如，代码摘要）。虽然有证据表明预训练可以提高成绩，但人们对所使用的特定预训练目标的影响知之甚少。事实上，传销只是可能的预训练目标之一，自然语言处理领域的最新工作表明，为感兴趣的特定下游任务量身定制的预训练目的可能会大大提高模型的性能。例如，在代码摘要的情况下，定制的预训练目标可以是识别给定方法的适当名称，将要生成的方法名称视为极端摘要。在这项研究中，我们重点关注在自动化代码相关任务时，预训练目标对转换器性能的影响。我们从系统的文献综述开始，旨在确定SE中使用的预培训目标。然后，我们使用以下两种方法对32台变压器进行预培训：（i）SE中通常采用的通用预培训目标；以及（ii）针对我们实验的特定代码相关任务量身定制的预训练目标，即错误修复、代码摘要和代码完成。我们还比较了预训练模型和未预训练模型，并展示了在不同场景下预训练带来的优势，在不同场景中，或多或少都有微调数据可用。我们的结果表明：（i）只有当可用的微调数据量很小时，预训练才有助于提高成绩；（ii）MLM目标通常足以使模型的预测性能最大化，即使将其与专门用于手头下游任务的预训练目标进行比较也是如此。,预培训，代码推荐人,,,
QT8IUFSN,2023,https://doi.org/10.1109/ICSE48619.2023.00052,ICSE 2023,Bad Snakes: Understanding and Improving Python Package Index Malware Scanning,"Open-source, community-driven package repositories see thousands of malware packages each year, but do not currently run automated malware detection systems. In this work, we explore the security goals of the repository administrators and the requirements for deploying such malware scanners via a case study of the Python ecosystem and PyPI repository, including interviews with administrators and maintainers. Further, we evaluate existing malware detection techniques for deployment in this setting by creating a benchmark dataset and comparing several existing tools: the malware checks implemented in PyPI, Bandit4Mal, and OSSGadget's OSS Detect Backdoor. We find that repository administrators have exacting requirements for such malware detection tools. Specifically, they consider a false positive rate of even 0.1% to be unacceptably high, given the large number of package releases that might trigger false alerts. Measured tools have false positive rates between 15% and 97%; increasing thresholds for detection rules to reduce this rate renders the true positive rate useless. While automated tools are far from reaching these demands, we find that a socio-technical malware detection system has emerged to meet these needs: external security researchers perform repository malware scans, filter for useful results, and report the results to repository administrators. These parties face different incentives and constraints on their time and tooling. We conclude with recommendations for improving detection capabilities and strengthening the collaboration between security researchers and software repository administrators.","Open-source software (OSS) Supply Chain,Malware Detection,PyPI,Qualitative Study,Quantitative Study",Bad Snakes：理解和改进Python包索引恶意软件扫描,开源、社区驱动的软件包存储库每年都会看到数千个恶意软件包，但目前还没有运行自动恶意软件检测系统。在这项工作中，我们通过对Python生态系统和PyPI存储库的案例研究，包括对管理员和维护人员的采访，探讨了存储库管理员的安全目标以及部署此类恶意软件扫描仪的要求。此外，我们通过创建一个基准数据集并比较几种现有工具来评估在这种情况下部署的现有恶意软件检测技术：PyPI、Bandit4Mal和OSSGadget的OSS Detect Backdoor中实现的恶意软件检查。我们发现，存储库管理员对此类恶意软件检测工具有严格的要求。具体而言，他们认为，考虑到可能引发虚假警报的大量软件包发布，即使是0.1%的假阳性率也高得令人无法接受。测量工具的假阳性率在15%到97%之间；增加检测规则的阈值以降低该比率使得真实阳性率变得无用。虽然自动化工具远未达到这些需求，但我们发现，已经出现了一种社会技术恶意软件检测系统来满足这些需求：外部安全研究人员执行存储库恶意软件扫描，筛选有用的结果，并将结果报告给存储库管理员。这些政党在时间和工具方面面临着不同的激励和限制。最后，我们提出了改进检测能力和加强安全研究人员与软件存储库管理员之间合作的建议。,开源软件（OSS）供应链，恶意软件检测，PyPI，定性研究，定量研究,,,
P7GKWLWN,2023,https://doi.org/10.1109/ICSE48619.2023.00013,ICSE 2023,One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization,"As pre-trained models automate many code intel-ligence tasks, a widely used paradigm is to fine-tune a model on the task dataset for each programming language. A recent study reported that multilingual fine-tuning benefits a range of tasks and models. However, we find that multilingual fine-tuning leads to performance degradation on recent models UniXcoder and CodeT5. To alleviate the potentially catastrophic forgetting issue in multilingual models, we fix all pre-trained model parameters, insert the parameter-efficient structure adapter, and fine-tune it. Updating only 0.6% of the overall parameters compared to full-model fine-tuning for each programming language, adapter tuning yields consistent improvements on code search and sum-marization tasks, achieving state-of-the-art results. In addition, we experimentally show its effectiveness in cross-lingual and low-resource scenarios. Multilingual fine-tuning with 200 samples per programming language approaches the results fine-tuned with the entire dataset on code summarization. Our experiments on three probing tasks show that adapter tuning significantly outperforms full-model fine-tuning and effectively overcomes catastrophic forgetting.","transfer learning,adapter,multilingual task",适用于所有编程语言的一个适配器？用于代码搜索和摘要的适配器调整,由于预先训练的模型使许多代码智能任务自动化，因此一种广泛使用的范式是在每个编程语言的任务数据集上微调模型。最近的一项研究报告称，多语言微调有利于一系列任务和模型。然而，我们发现多语言微调会导致最新型号UniXcoder和CodeT5的性能下降。为了缓解多语言模型中潜在的灾难性遗忘问题，我们修复了所有预先训练好的模型参数，插入参数有效的结构适配器，并对其进行微调。与每种编程语言的全模型微调相比，适配器调整只更新了0.6%的总体参数，从而在代码搜索和求和任务上产生了一致的改进，实现最先进的结果。此外，我们通过实验证明了它在跨语言和低资源场景中的有效性。每种编程语言200个样本的多语言微调接近于在代码摘要上对整个数据集进行微调的结果。我们在三个探测任务上的实验表明，适配器调优显著优于全模型微调，并有效克服了灾难性遗忘。,迁移学习，适配器，多语言任务,,,
TTJFQWTV,2023,https://doi.org/10.1109/ICSE48619.2023.00117,ICSE 2023,Verifying Data Constraint Equivalence in FinTech Systems,"Data constraints are widely used in FinTech systems for monitoring data consistency and diagnosing anomalous data manipulations. However, many equivalent data constraints are created redundantly during the development cycle, slowing down the FinTech systems and causing unnecessary alerts. We present EQDAC, an efficient decision procedure to determine the data constraint equivalence. We first propose the symbolic representation for semantic encoding and then introduce two light-weighted analyses to refute and prove the equivalence, respectively, which are proved to achieve in polynomial time. We evaluate EQDAC upon 30,801 data constraints in a FinTech system. It is shown that EQDAC detects 11,538 equivalent data constraints in three hours. It also supports efficient equivalence searching with an average time cost of 1.22 seconds, enabling the system to check new data constraints upon submission.","Equivalence Verification,Data Constraints,Fin-Tech Systems",验证金融科技系统中的数据约束等价性,数据约束在金融科技系统中被广泛用于监测数据一致性和诊断异常数据操作。然而，在开发周期中，许多等效的数据约束是冗余创建的，这会减慢FinTech系统的速度，并导致不必要的警报。我们提出了EQDAC，这是一种确定数据约束等价性的有效决策过程。我们首先提出了语义编码的符号表示，然后引入两个轻权分析来分别反驳和证明等价性，这两个分析被证明是在多项式时间内实现的。我们根据金融科技系统中30801个数据约束对EQDAC进行了评估。结果表明，EQDAC在三小时内检测到11538个等效数据约束。它还支持高效的等价搜索，平均时间成本为1.22秒，使系统能够在提交时检查新的数据约束。,等效性验证，数据约束，财务技术系统,,,
GB8IZ5JY,2023,https://doi.org/10.1109/ICSE48619.2023.00200,ICSE 2023,MTTM: Metamorphic Testing for Textual Content Moderation Software,"The exponential growth of social media platforms such as Twitter and Facebook has revolutionized textual communication and textual content publication in human society. However, they have been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography, which can lead to highly negative impacts (e.g., harmful effects on teen mental health). Researchers and practitioners have been enthusiastically developing and extensively deploying textual content moderation software to address this problem. However, we find that malicious users can evade moderation by changing only a few words in the toxic content. Moreover, modern content moderation software's performance against malicious inputs remains underexplored. To this end, we propose MTTM, a Metamorphic Testing framework for Textual content Moderation software. Specifically, we conduct a pilot study on 2, 000 text messages collected from real users and summarize eleven metamorphic relations across three perturbation levels: character, word, and sentence. MTTM employs these metamorphic relations on toxic textual contents to generate test cases, which are still toxic yet likely to evade moderation. In our evaluation, we employ MTTM to test three commercial textual content moderation software and two state-of-the-art moderation algorithms against three kinds of toxic content. The results show that MTTM achieves up to 83.9%, 51%, and 82.5% error finding rates (EFR) when testing commercial moderation software provided by Google, Baidu, and Huawei, respectively, and it obtains up to 91.2% EFR when testing the state-of-the-art algorithms from the academy. In addition, we leverage the test cases generated by MTTM to retrain the model we explored, which largely improves model robustness 0% ~ 5.9% EFR) while maintaining the accuracy on the original test set. A demo can be found in this link1.","Software testing,metamorphic relations,NLP software,textual content moderation",MTTM：文本内容调节软件的变形测试,Twitter和Facebook等社交媒体平台的指数级增长彻底改变了人类社会的文本交流和文本内容发布。然而，它们越来越多地被用来传播有毒内容，如仇恨言论、恶意广告和色情，这可能会导致高度负面的影响（例如，对青少年心理健康的有害影响）。研究人员和从业者一直在积极开发和广泛部署文本内容审核软件来解决这个问题。然而，我们发现恶意用户可以通过只更改有毒内容中的几个单词来逃避审核。此外，现代内容审核软件对恶意输入的性能仍然没有得到充分的研究。为此，我们提出了MTTM，一个用于文本内容调节软件的变形测试框架。具体来说，我们对从真实用户那里收集的2000条短信进行了试点研究，总结了三个扰动级别的11种变形关系：字符、单词和句子。MTTM在有毒的文本内容上使用这些变质关系来生成测试用例，这些测试用例仍然是有毒的，但可能会逃避审核。在我们的评估中，我们使用MTTM测试了三种商业文本内容审核软件和两种最先进的针对三种有毒内容的审核算法。结果表明，MTTM在测试谷歌、百度和华为提供的商业审核软件时，分别获得了83.9%、51%和82.5%的错误发现率，在测试该院最先进的算法时，其错误发现率高达91.2%。此外，我们利用MTTM生成的测试用例来重新训练我们探索的模型，这在很大程度上提高了模型的鲁棒性0%~5.9%EFR），同时保持了原始测试集的准确性。演示可在此链接1中找到。,软件测试，变形关系，NLP软件，文本内容审核,,,
6YKHS49L,2023,https://doi.org/10.1109/ICSE48619.2023.00062,ICSE 2023,Understanding and Detecting On-The-Fly Configuration Bugs,"Software systems introduce an increasing number of configuration options to provide flexibility, and support updating the options on the fly to provide persistent services. This mechanism, however, may affect the system reliability, leading to unexpected results like software crashes or functional errors. In this paper, we refer to the bugs caused by on-the-fly configuration updates as on-the-fly configuration bugs, or OCBugs for short. In this paper, we conducted the first in-depth study on 75 real-world OCBugs from 5 widely used systems to understand the symptoms, root causes, and triggering conditions of OCBugs. Based on our study, we designed and implemented Parachute, an automated testing framework to detect OCBugs. Our key insight is that the value of one configuration option, either loaded at the startup phase or updated on the fly, should have the same effects on the target program. Parachute generates tests for on-the-fly configuration updates by mutating the existing tests and conducts differential analysis to identify OCBugs. We evaluated Parachute on 7 real-world software systems. The results show that Parachute detected 75% (42/56) of the known OCBugs, and reported 13 unknown bugs, 11 of which have been confirmed or fixed by developers until the time of writing.","on-the-fly configuration updates,bug detection,metamorphic testing",了解和检测动态配置错误,软件系统引入了越来越多的配置选项以提供灵活性，并支持动态更新选项以提供持久服务。然而，这种机制可能会影响系统的可靠性，导致软件崩溃或功能错误等意外结果。在本文中，我们将动态配置更新引起的错误称为动态配置错误，简称OCBug。在本文中，我们对5个广泛使用的系统中的75个真实世界的OCBug进行了首次深入研究，以了解OCBug的症状、根本原因和触发条件。基于我们的研究，我们设计并实现了Parachute，这是一个检测OCBug的自动化测试框架。我们的关键见解是，一个配置选项的值，无论是在启动阶段加载还是在运行中更新，都应该对目标程序产生相同的影响。降落伞通过改变现有测试来生成飞行中配置更新的测试，并进行差异分析以识别OCBug。我们在7个真实世界的软件系统上评估了降落伞。结果显示，Parachute检测到75%（42/56）的已知OCBug，并报告了13个未知Bug，其中11个已被开发人员确认或修复，直到撰写本文时为止。,动态配置更新，错误检测，变形测试,,,
RLANNLWL,2023,https://doi.org/10.1109/ICSE48619.2023.00086,ICSE 2023,Taintmini: Detecting Flow of Sensitive Data in Mini-Programs with Static Taint Analysis,"Mini-programs, which are programs running inside mobile super apps such as WeChat, often have access to privacy-sensitive information, such as location data and phone numbers, through APUs provided by the super apps. This access poses a risk of privacy sensitive data leaks, either accidentally from carelessly programmed mini-programs or intentionally from malicious ones. To address this concern, it is crucial to track the flow of sensitive data in mini-programs for either human analysis or automated tools. Although existing taint analysis techniques have been widely studied, they face unique challenges in tracking sensitive data flows in mini-programs, such as cross-language, cross-page, and cross-mini-program data flows. This paper presents a novel framework, Taintmini, which addresses these challenges by using a novel universal data flow graph approach that captures data flows within and across mini-programs. We have evaluated Taintminiwith 238,866 mini-programs and detect 27,184 that contain sensitive data flows. We have also applied Taintminito detect privacy leakage colluding mini-programs and identify 455 such programs from them that clearly violate privacy policy.","Mini-programs,Taint analysis,Privacy leaks detection,Security,Empirical Study",Taintmini:用静态Taint分析检测小程序中的敏感数据流,迷你程序是在微信等移动超级应用程序中运行的程序，通常可以通过超级应用程序提供的APU访问位置数据和电话号码等隐私敏感信息。这种访问带来了隐私敏感数据泄露的风险，无论是不小心编程的迷你程序意外泄露的，还是恶意程序故意泄露的。为了解决这一问题，跟踪用于人工分析或自动化工具的迷你程序中的敏感数据流至关重要。尽管现有的污染分析技术已经得到了广泛的研究，但它们在跟踪小程序中的敏感数据流方面面临着独特的挑战，例如跨语言、跨页面和跨小程序的数据流。本文提出了一个新的框架Taintmini，它通过使用一种新的通用数据流图方法来捕捉迷你程序内和跨迷你程序的数据流，从而解决了这些挑战。我们已经用238866个迷你程序评估了Taintmini，并检测到27184个包含敏感数据流的程序。我们还应用Taintminit来检测串通迷你程序的隐私泄露，并从中识别出455个明显违反隐私政策的程序。,迷你程序，污点分析，隐私泄露检测，安全，实证研究,,,
9WWAMRK2,2023,https://doi.org/10.1109/ICSE48619.2023.00145,ICSE 2023,Test Selection for Unified Regression Testing,"Today's software failures have two dominating root causes: code bugs and misconfigurations. To combat failure-inducing software changes, unified regression testing (URT) is needed to synergistically test the changed code and all changed production configurations for deployment reliability. However, URT could incur high cost, as it needs to run a large number of tests under multiple configurations. Regression test selection (RTS) can reduce regression testing cost. Unfortunately, no existing RTS technique reasons about code and configuration changes collectively. We introduce Unified Regression Test Selection (uRTS) to effectively reduce the cost of URT. uRTS supports project changes on 1) code only, 2) configurations only, and 3) both code and configurations. It selects regular tests and configuration tests with a unified selection algorithm. The uRTS algorithm analyzes code and configuration dependencies of each test across runs and across configurations. uRTS provides the same safety guarantee as the state-of-the-art RTS while selecting fewer tests and, more importantly, reducing the end-to-end testing time. We implemented uRTS on top of Ekstazi (a RTS tool for code changes) and Ctest (a configuration testing framework). We evaluate uRTS on hundreds of code revisions and dozens of configurations of five large projects. The results show that uRTS reduces the end-to-end testing time, on average, by 3.64X compared to executing all tests and 1.87X compared to a competitive reference solution that directly extends RTS for URT.","Codes,Costs,Computer bugs,Production,Reliability engineering,Software,Software reliability",统一回归测试的测试选择,今天的软件故障有两个主要的根本原因：代码错误和错误配置。为了对抗引起故障的软件更改，需要统一回归测试（URT）来协同测试更改后的代码和所有更改后的生产配置，以提高部署可靠性。然而，URT可能会产生高昂的成本，因为它需要在多种配置下运行大量测试。回归测试选择（RTS）可以降低回归测试成本。不幸的是，没有任何现有的RTS技术会对代码和配置进行集体更改。为了有效降低URT的成本，我们引入了统一回归测试选择（uRTS）。uRTS支持对1）仅代码、2）仅配置以及3）代码和配置进行项目更改。它使用统一的选择算法选择定期测试和配置测试。uRTS算法分析每个测试在运行和配置之间的代码和配置依赖关系。uRTS提供了与最先进的RTS相同的安全保障，同时选择了更少的测试，更重要的是，减少了端到端测试时间。我们在Ekstazi（一个用于代码更改的RTS工具）和Ctest（一个配置测试框架）之上实现了uRTS。我们在五个大型项目的数百个代码修订和数十个配置上评估了uRTS。结果表明，与执行所有测试相比，uRTS平均减少了3.64倍的端到端测试时间，与直接扩展RTS for URT的竞争参考解决方案相比，uRTS平均减少了1.87倍。,代码，成本，计算机错误，生产，可靠性工程，软件，软件可靠性,,,
P2623DR4,2023,https://doi.org/10.1109/ICSE48619.2023.00189,ICSE 2023,DeepVD: Toward Class-Separation Features for Neural Network Vulnerability Detection,"The advances of machine learning (ML) including deep learning (DL) have enabled several approaches to implicitly learn vulnerable code patterns to automatically detect software vulnerabilities. A recent study showed that despite successes, the existing ML/DL-based vulnerability detection (VD) models are limited in the ability to distinguish between the two classes of vulnerability and benign code. We propose DeepVD, a graph-based neural network VD model that emphasizes on class-separation features between vulnerability and benign code. DeepVDleverages three types of class-separation features at different levels of abstraction: statement types (similar to Part-of-Speech tagging), Post-Dominator Tree (covering regular flows of execution), and Exception Flow Graph (covering the exception and error-handling flows). We conducted several experiments to evaluate DeepVD in a real-world vulnerability dataset of 303 projects with 13,130 vulnerable methods. Our results show that DeepVD relatively improves over the state-of-the-art ML/DL-based VD approaches 13%–29.6% in precision, 15.6%–28.9% in recall, and 16.4%–25.8% in F-score. Our ablation study confirms that our designed features and components help DeepVDachieve high class-separability for vulnerability and benign code.","neural vulnerability detection,graph neural network,class separation",DeepVD：面向神经网络漏洞检测的类分离特征,包括深度学习（DL）在内的机器学习（ML）的进步使几种方法能够隐式学习易受攻击的代码模式，从而自动检测软件漏洞。最近的一项研究表明，尽管取得了成功，但现有的基于ML/DL的漏洞检测（VD）模型在区分两类漏洞和良性代码方面的能力有限。我们提出了DeepVD，这是一种基于图的神经网络VD模型，强调漏洞和良性代码之间的类分离特征。DeepVD在不同抽象级别利用了三种类型的类分离功能：语句类型（类似于词性标记）、后支配树（涵盖常规执行流）和异常流图（涵盖异常和错误处理流）。我们在303个项目的真实世界漏洞数据集中进行了几个实验来评估DeepVD，使用了13130种漏洞方法。我们的结果表明，与最先进的基于ML/DL的VD相比，DeepVD的准确率相对提高了13%–29.6%，召回率接近15.6%–28.9%，F得分接近16.4%–25.8%。我们的消融研究证实，我们设计的功能和组件有助于DeepVD实现漏洞和良性代码的高度可分离性。,神经脆弱性检测，图神经网络，类分离,,,
W39QYF7W,2023,https://doi.org/10.1109/ICSE48619.2023.00039,ICSE 2023,Comparison and Evaluation of Clone Detection Techniques with Different Code Representations,"As one of bad smells in code, code clones may increase the cost of software maintenance and the risk of vulnerability propagation. In the past two decades, numerous clone detection technologies have been proposed. They can be divided into text-based, token-based, tree-based, and graph-based approaches according to their code representations. Different code representations abstract the code details from different perspectives. However, it is unclear which code representation is more effective in detecting code clones and how to combine different code representations to achieve ideal performance. In this paper, we present an empirical study to compare the clone detection ability of different code representations. Specifically, we reproduce 12 clone detection algorithms and divide them into different groups according to their code representations. After analyzing the empirical results, we find that token and tree representations can perform better than graph representation when detecting simple code clones. However, when the code complexity of a code pair increases, graph representation becomes more effective. To make our findings more practical, we perform manual analysis on open-source projects to seek a possible distribution of different clone types in the open-source community. Through the results, we observe that most clone pairs belong to simple code clones. Based on this observation, we discard heavyweight graph-based clone detection algorithms and conduct combination experiments to find out a suitable combination of token-based and tree-based approaches for achieving scalable and effective code clone detection. We develop the suitable combination into a tool called TACC and evaluate it with other state-of-the-art code clone detectors. Experimental results indicate that TACC performs better and has the ability to detect large-scale code clones.","Clone Detection,Empirical Study,Code Representation,Large Scale",不同代码表示的克隆检测技术的比较与评价,作为代码中的一种臭味，代码克隆可能会增加软件维护成本和漏洞传播的风险。在过去的二十年里，已经提出了许多克隆检测技术。根据代码表示，它们可以分为基于文本、基于标记、基于树和基于图的方法。不同的代码表示从不同的角度抽象代码细节。然而，目前尚不清楚哪种代码表示在检测代码克隆方面更有效，以及如何组合不同的代码表示以实现理想的性能。在本文中，我们提出了一个实证研究来比较不同代码表示的克隆检测能力。具体来说，我们复制了12种克隆检测算法，并根据它们的代码表示将它们分为不同的组。在分析经验结果后，我们发现在检测简单代码克隆时，令牌和树表示可以比图表示表现得更好。然而，当代码对的代码复杂性增加时，图表示变得更加有效。为了使我们的发现更加实用，我们对开源项目进行了手动分析，以寻求不同克隆类型在开源社区中的可能分布。通过结果，我们观察到大多数克隆对属于简单的代码克隆。基于这一观察，我们放弃了基于重量级图的克隆检测算法，并进行了组合实验，以找到基于令牌和基于树的方法的合适组合，从而实现可扩展和有效的代码克隆检测。我们将合适的组合开发成一个名为TACC的工具，并与其他最先进的代码克隆检测器进行评估。实验结果表明，TACC具有较好的检测大规模代码克隆的能力。,克隆检测，实证研究，代码表示，大规模,,,
S3VN4899,2023,https://doi.org/10.1109/ICSE48619.2023.00032,ICSE 2023,APICAD: Augmenting API Misuse Detection through Specifications from Code and Documents,"Using API should follow its specifications. Otherwise, it can bring security impacts while the functionality is damaged. To detect API misuse, we need to know what its specifications are. In addition to being provided manually, current tools usually mine the majority usage in the existing codebase as specifications, or capture specifications from its relevant texts in human language. However, the former depends on the quality of the codebase itself, while the latter is limited to the irregularity of the text. In this work, we observe that the information carried by code and documents can complement each other. To mitigate the demand for a high-quality codebase and reduce the pressure to capture valid information from texts, we present APICAD to detect API misuse bugs of C/C++ by combining the specifications mined from code and documents. On the one hand, we effectively build the contexts for API invocations and mine specifications from them through a frequency-based method. On the other hand, we acquire the specifications from documents by using lightweight keyword-based and NLP-assisted techniques. Finally, the combined specifications are generated for bug detection. Experiments show that APICAD can handle diverse API usage semantics to deal with different types of API misuse bugs. With the help of APICAD, we report 153 new bugs in Curl, Httpd, OpenSSL and Linux kernel, 145 of which have been confirmed and 126 have applied our patches.","Codes,Text analysis,Linux,Computer bugs,Semantics,Prototypes,Benchmark testing",APICAD:通过代码和文档中的规范增强API误用检测,使用API应遵循其规范。否则，它可能会在功能受损的同时带来安全影响。为了检测API的误用，我们需要知道它的规范是什么。除了手动提供外，当前的工具通常会挖掘现有代码库中的大多数用法作为规范，或者从人类语言的相关文本中捕获规范。然而，前者取决于代码库本身的质量，而后者仅限于文本的不规则性。在这项工作中，我们观察到代码和文档所携带的信息可以相互补充。为了减少对高质量代码库的需求，减轻从文本中获取有效信息的压力，我们提出了APICAD，通过结合从代码和文档中挖掘的规范来检测C/C++的API误用错误。一方面，我们有效地构建了API调用的上下文，并通过基于频率的方法从中挖掘规范。另一方面，我们使用基于轻量级关键字和NLP辅助的技术从文档中获取规范。最后，生成用于错误检测的组合规范。实验表明，APICAD可以处理多种API使用语义，以处理不同类型的API误用错误。在APICAD的帮助下，我们报告了Curl、Httpd、OpenSSL和Linux内核中的153个新错误，其中145个已被确认，126个已应用我们的补丁。,代码，文本分析，Linux，计算机错误，语义，原型，基准测试,,,
KXU9P8AK,2023,https://doi.org/10.1109/ICSE48619.2023.00208,ICSE 2023,DStream: A Streaming-Based Highly Parallel IFDS Framework,"The IFDS framework supports interprocedural dataflow analysis with distributive flow functions over finite domains. A large class of interprocedural dataflow analysis problems can be formulated as IFDS problems and thus can be solved with the IFDS framework precisely. Unfortunately, scaling IFDS analysis to large-scale programs is challenging in terms of both massive memory consumption and low analysis efficiency. This paper presents DStream, a scalable system dedicated to precise and highly parallel IFDS analysis for large-scale programs. DStream leverages a streaming-based out-of-core computation model to reduce memory footprint significantly and adopts fine-grained data parallelism to achieve efficiency. We implemented a taint analysis as a DStream instance analysis and compared DStream with three state-of-the-art tools. Our exper-iments validate that DStream outperforms all other tools with average speedups from 4.37x to 14.46x on a commodity PC with limited available memory. Meanwhile, the experiments confirm that DStream successfully scales to large-scale programs which the state-of-the-art tools (e.g., FlowDroid and/or DiskDroid) fail to analyze.","interprocedural static analysis,IFDS analysis,streaming,data-parallel computation",DStream：一个基于流的高度并行IFDS框架,IFDS框架支持在有限域上使用分布式流函数进行过程间数据流分析。一大类过程间数据流分析问题可以被公式化为IFDS问题，因此可以用IFDS框架精确地解决。不幸的是，将IFDS分析扩展到大型程序在大量内存消耗和低分析效率方面都具有挑战性。本文介绍了DStream，这是一个可扩展的系统，专门用于大规模程序的精确和高度并行的IFDS分析。DStream利用基于流的核心外计算模型来显著减少内存占用，并采用细粒度数据并行来实现效率。我们实现了一个污点分析作为DStream实例分析，并将DStream与三个最先进的工具进行了比较。我们的实验证明，在可用内存有限的商用电脑上，DStream的平均加速率从4.37倍提高到14.46倍，优于所有其他工具。同时，实验证实，DStream成功地扩展到了大型程序，而最先进的工具（例如FlowDroid和/或DiskDroid）无法分析这些程序。,过程间静态分析，IFDS分析，流式传输，数据并行计算,,,
EI78UXZM,2023,https://doi.org/10.1109/ICSE48619.2023.00177,ICSE 2023,Twins or False Friends? A Study on Energy Consumption and Performance of Configurable Software,"Reducing energy consumption of software is an increasingly important objective, and there has been extensive research for data centers, smartphones, and embedded systems. However, when it comes to software, we lack working tools and methods to directly reduce energy consumption. For performance, we can resort to configuration options for tuning response time or throughput of a software system. For energy, it is still unclear whether the underlying assumption that runtime performance correlates with energy consumption holds, especially when it comes to optimization via configuration. To evaluate whether and to what extent this assumption is valid for configurable software systems, we conducted the largest empirical study of this kind to date. First, we searched the literature for reports on whether and why runtime performance correlates with energy consumption. We obtained a mixed, even contradicting picture from positive to negative correlation, and that configurability has not been considered yet as a factor for this variance. Second, we measured and analyzed both the runtime performance and energy consumption of 14 real-world software systems. We found that, in many cases, it depends on the software system's configuration whether runtime performance and energy consumption correlate and that, typically, only few configuration options influence the degree of correlation. A fine-grained analysis at the function level revealed that only few functions are relevant to obtain an accurate proxy for energy consumption and that, knowing them, allows one to infer individual transfer factors between runtime performance and energy consumption.","energy consumption,performance,configurability,correlation",双胞胎还是假朋友？可配置软件的能耗与性能研究,降低软件能耗是一个越来越重要的目标，对数据中心、智能手机和嵌入式系统进行了广泛的研究。然而，当涉及到软件时，我们缺乏直接降低能耗的工作工具和方法。为了提高性能，我们可以使用配置选项来调整软件系统的响应时间或吞吐量。对于能源，目前尚不清楚运行时性能与能源消耗相关的基本假设是否成立，尤其是在通过配置进行优化时。为了评估这种假设是否以及在多大程度上适用于可配置软件系统，我们进行了迄今为止规模最大的此类实证研究。首先，我们在文献中搜索关于运行时性能是否以及为什么与能耗相关的报告。我们得到了一个从正相关到负相关的混合甚至矛盾的画面，这种可配置性还没有被认为是这种方差的一个因素。其次，我们测量并分析了14个真实世界软件系统的运行时性能和能耗。我们发现，在许多情况下，运行时性能和能耗是否相关取决于软件系统的配置，通常只有少数配置选项会影响相关程度。在函数级别进行的细粒度分析表明，只有少数函数与获得准确的能源消耗代理相关，并且了解这些函数后，可以推断运行时性能和能源消耗之间的个别传递因素。,能耗，性能，可配置性，相关性,,,
Q2HFL735,2023,https://doi.org/10.1109/ICSE48619.2023.00116,ICSE 2023,Compiling Parallel Symbolic Execution with Continuations,"Symbolic execution is a powerful program analysis and testing technique. Symbolic execution engines are usually implemented as interpreters, and the induced interpretation over-head can dramatically inhibit performance. Alternatively, implementation choices based on instrumentation provide a limited ability to transform programs. However, the use of compilation and code generation techniques beyond simple instrumentation remains underexplored for engine construction, leaving potential performance gains untapped. In this paper, we show how to tap some of these gains using sophisticated compilation techniques: We present Gensym, an optimizing symbolic-execution compiler that generates symbolic code which explores paths and generates tests in parallel. The key insight of GensYmis to compile symbolic execution tasks into cooperative concurrency via continuation-passing style, which further enables efficient parallelism. The design and implementation of Gensym is based on partial evaluation and generative programming techniques, which make it high-level and performant at the same time. We compare the performance of Gensym against the prior symbolic-execution compiler LLSC and the state-of-the-art symbolic interpreter KLEE. The results show an average 4.6× speedup for sequential execution and 9.4× speedup for parallel execution on 20 benchmark programs.","symbolic execution,compiler,code generation,metaprogramming,continuation",用Continuations编译并行符号执行,符号执行是一种强大的程序分析和测试技术。符号执行引擎通常被实现为解释器，而从头引发的解释会极大地抑制性能。或者，基于插装的实现选择提供了转换程序的有限能力。然而，在引擎构建中，除了简单的工具之外，编译和代码生成技术的使用仍然没有得到充分的开发，潜在的性能收益尚未得到开发。在本文中，我们展示了如何使用复杂的编译技术来挖掘其中的一些收益：我们介绍了Gensym，一种优化的符号执行编译器，它可以生成并行探索路径和生成测试的符号代码。GensYmis的关键见解是通过延续传递风格将符号执行任务编译为协同并发，这进一步实现了高效的并行性。Gensym的设计和实现基于部分评估和生成编程技术，使其具有高水平和高性能。我们将Gensym的性能与先前的符号执行编译器LLSC和最先进的符号解释器KLEE进行了比较。结果表明，在20个基准程序上，顺序执行平均加速4.6倍，并行执行平均加速9.4倍。,符号执行，编译器，代码生成，元编程，延续,,,
NYXNMK28,2023,https://doi.org/10.1109/ICSE48619.2023.00191,ICSE 2023,Vulnerability Detection with Graph Simplification and Enhanced Graph Representation Learning,"Prior studies have demonstrated the effectiveness of Deep Learning (DL) in automated software vulnerability detection. Graph Neural Networks (GNNs) have proven effective in learning the graph representations of source code and are commonly adopted by existing DL-based vulnerability detection methods. However, the existing methods are still limited by the fact that GNNs are essentially difficult to handle the connections between long-distance nodes in a code structure graph. Besides, they do not well exploit the multiple types of edges in a code structure graph (such as edges representing data flow and control flow). Consequently, despite achieving state-of-the-art performance, the existing GNN-based methods tend to fail to capture global information (i.e., long-range dependencies among nodes) of code graphs. To mitigate these issues, in this paper, we propose a novel vulnerability detection framework with grAph siMplification and enhanced graph rePresentation LEarning, named AMPLE. AMPLE mainly contains two parts: 1) graph simplification, which aims at reducing the distances between nodes by shrinking the node sizes of code structure graphs; 2) enhanced graph representation learning, which involves one edge-aware graph convolutional network module for fusing heterogeneous edge information into node representations and one kernel-scaled representation module for well capturing the relations between distant graph nodes. Experiments on three public benchmark datasets show that AMPLE outperforms the state-of-the-art methods by 0.39%-35.32% and 7.64%-199.81% with respect to the accuracy and F1 score metrics, respectively. The results demonstrate the effectiveness of AMPLE in learning global information of code graphs for vulnerability detection.","Software vulnerability,graph simplification,graph representation learning",基于图简化和增强图表示学习的漏洞检测,先前的研究已经证明了深度学习（DL）在自动软件漏洞检测中的有效性。图神经网络（GNN）已被证明在学习源代码的图表示方面是有效的，并且通常被现有的基于DL的漏洞检测方法所采用。然而，现有的方法仍然受到这样一个事实的限制，即GNN本质上很难处理代码结构图中长距离节点之间的连接。此外，它们不能很好地利用代码结构图中的多种类型的边（例如表示数据流和控制流的边）。因此，尽管实现了最先进的性能，但现有的基于GNN的方法往往无法捕获代码图的全局信息（即节点之间的长程依赖关系）。为了缓解这些问题，在本文中，我们提出了一种新的漏洞检测框架，称为AMPLE，该框架具有grAph-siM放大和增强的图重新呈现学习。AMPLE主要包括两个部分：1）图简化，旨在通过缩小代码结构图的节点大小来减少节点之间的距离；2） 增强的图表示学习，包括一个边缘感知图卷积网络模块，用于将异构边缘信息融合到节点表示中，以及一个内核缩放表示模块，用于很好地捕捉远处图节点之间的关系。在三个公共基准数据集上的实验表明，AMPLE在准确性和F1得分指标方面分别优于最先进的方法0.39%-35.32%和7.64%-199.81%。结果证明了AMPLE在学习用于漏洞检测的代码图的全局信息方面的有效性。,软件漏洞，图形简化，图形表示学习,,,
WEKNRGHZ,2023,https://doi.org/10.1109/ICSE48619.2023.00017,ICSE 2023,JITfuzz: Coverage-guided Fuzzing for JVM Just-in-Time Compilers,"As a widely-used platform to support various Java-bytecode-based applications, Java Virtual Machine (JVM) incurs severe performance loss caused by its real-time program interpretation mechanism. To tackle this issue, the Just-in- Time compiler (JIT) has been widely adopted to strengthen the efficacy of JVM. Therefore, how to effectively and efficiently detect JIT bugs becomes critical to ensure the correctness of JVM. In this paper, we propose a coverage-guided fuzzing framework, namely JITfuzz, to automatically detect JIT bugs. In particular, JITfuzz adopts a set of optimization-activating mutators to trigger the usage of typical JIT optimizations, e.g., function inlining and simplification. Meanwhile, given JIT optimizations are closely coupled with program control flows, JITfuzz also adopts mutators to enrich the control flows of target programs. Moreover, JITfuzz also proposes a mutator scheduler which iteratively schedules mutators according to the coverage updates to maximize the code coverage of JIT. To evaluate the effectiveness of JITfuzz, we conduct a set of experiments based on a benchmark suite with 16 popular JVM-based projects from GitHub. The experimental results suggest that JITfuzz outperforms the state-of-the-art mutation-based and generation-based JVM fuzzers by 27.9 % and 18.6 % respectively in terms of edge coverage on average. Furthermore, JITfuzz also successfully detects 36 previously unknown bugs (including 23 JIT bugs) and 27 bugs (including 18 JIT bugs) have been confirmed by the developers.","Java,Schedules,Image edge detection,Computer bugs,Fuzzing,Benchmark testing,Virtual machining",JITfuzz：JVM实时编译器的覆盖引导模糊化,Java虚拟机（JVM）作为一个广泛使用的支持各种基于Java字节码的应用程序的平台，其实时程序解释机制会导致严重的性能损失。为了解决这个问题，实时编译器（JIT）被广泛采用来增强JVM的有效性。因此，如何有效、高效地检测JIT错误成为保证JVM正确性的关键。在本文中，我们提出了一个覆盖引导的模糊化框架，即JITfuzz，用于自动检测JIT错误。特别是，JITfuzz采用了一组优化激活变量来触发典型JIT优化的使用，例如函数内联和简化。同时，由于JIT优化与程序控制流紧密耦合，JITfuzz还采用了变异函数来丰富目标程序的控制流。此外，JITfuzz还提出了一种变异器调度器，该调度器根据覆盖更新迭代调度变异器，以最大限度地提高JIT的代码覆盖率。为了评估JITfuzz的有效性，我们在GitHub的16个流行的基于JVM的项目的基准套件上进行了一组实验。实验结果表明，JITfuzz在边缘覆盖方面平均分别比最先进的基于突变和基于生成的JVM模糊器高27.9%和18.6%。此外，JITfuzz还成功地检测到了36个以前未知的bug（包括23个JIT bug），27个bug（包括18个JIT Bug）已被开发人员确认。,Java，时间表，图像边缘检测，计算机错误，模糊，基准测试，虚拟加工,,,
IPQLJFDV,2023,https://doi.org/10.1109/ICSE48619.2023.00034,ICSE 2023,OSSFP: Precise and Scalable C/C++ Third-Party Library Detection using Fingerprinting Functions,"Third-party libraries (TPLs) are frequently used in software to boost efficiency by avoiding repeated developments. However, the massive using TPLs also brings security threats since TPLs may introduce bugs and vulnerabilities. Therefore, software composition analysis (SCA) tools have been proposed to detect and manage TPL usage. Unfortunately, due to the presence of common and trivial functions in the bloated feature dataset, existing tools fail to precisely and rapidly identify TPLs in C/C++ real-world projects. To this end, we propose OSSFP, a novel SCA framework for effective and efficient TPL detection in large-scale real-world projects via generating unique fingerprints for open source software. By removing common and trivial functions and keeping only the core functions to build the fingerprint index for each TPL project, OSSFP significantly reduces the database size and accelerates the detection process. It also improves TPL detection accuracy since noises are excluded from the fingerprints. We applied OSSFP on a large data set containing 23,427 C/C++ repositories, which included 585,683 versions and 90 billion lines of code. The result showed that it could achieve 90.84% of recall and 90.34% of precision, which outperformed the state-of-the-art tool by 35.31% and 3.71%, respectively. OSSFP took only 0.12 seconds on average to identify all TPLs per project, which was 22 times faster than the other tool. OSSFP has proven to be highly scalable on large-scale datasets.","Codes,Databases,Computer bugs,Fingerprint recognition,Libraries,Security,Indexes",OSSFP：使用指纹功能进行精确且可扩展的C/C++第三方库检测,第三方库（TPL）经常用于软件中，以通过避免重复开发来提高效率。然而，TPL的大量使用也带来了安全威胁，因为TPL可能会引入漏洞和漏洞。因此，已经提出了软件组成分析（SCA）工具来检测和管理TPL的使用。不幸的是，由于臃肿的特征数据集中存在常见和琐碎的函数，现有的工具无法准确快速地识别C/C++现实世界项目中的TPL。为此，我们提出了OSSFP，这是一种新的SCA框架，通过为开源软件生成唯一指纹，在大型现实世界项目中实现有效和高效的TPL检测。通过删除常见和琐碎的功能，只保留核心功能来为每个TPL项目构建指纹索引，OSSFP显著减少了数据库大小，加快了检测过程。它还提高了TPL检测精度，因为从指纹中排除了噪声。我们在一个包含23427个C/C++存储库的大型数据集上应用了OSSFP，其中包括585683个版本和900亿行代码。结果表明，它可以实现90.84%的召回率和90.34%的准确率，分别比最先进的工具高出35.31%和3.71%。OSSFP识别每个项目的所有TPL平均只需0.12秒，比其他工具快22倍。OSSFP已被证明在大规模数据集上具有高度可扩展性。,代码，数据库，计算机错误，指纹识别，库，安全性，索引,,,
6HMEIBBY,2023,https://doi.org/10.1109/ICSE48619.2023.00095,ICSE 2023,Understanding the Threats of Upstream Vulnerabilities to Downstream Projects in the Maven Ecosystem,"Modern software systems are increasingly relying on dependencies from the ecosystem. A recent estimation shows that around 35% of an open-source project's code come from its depended libraries. Unfortunately, open-source libraries are often threatened by various vulnerability issues, and the number of disclosed vulnerabilities is increasing steadily over the years. Such vulnerabilities can pose significant security threats to the whole ecosystem, not only to the vulnerable libraries themselves, but also to the corresponding downstream projects. Many Software Composition Analysis (SCA) tools have been proposed, aiming to detect vulnerable libraries or components referring to existing vulnerability databases. However, recent studies report that such tools often generate a large number of false alerts. Particularly, up to 73.3% of the projects depending on vulnerable libraries are actually safe. Aiming to devise more precise tools, understanding the threats of vulnerabilities holistically in the ecosystem is significant, as already performed by a number of existing studies. However, previous researches either analyze at a very coarse granularity (e.g., without analyzing the source code) or are limited by the study scales. This study aims to bridge such gaps. In particular, we collect 44,450 instances of (CVE, upstream, downstream) relations and analyze around 50 million invocations made from downstream to upstream projects to understand the potential threats of upstream vulnerabilities to downstream projects in the Maven ecosystem. Our investigation makes interesting yet significant findings with respect to multiple aspects, including the reach-ability of vulnerabilities, the complexities of the reachable paths as well as how downstream projects and developers perceive upstream vulnerabilities. We believe such findings can not only provide a holistic understanding towards the threats of upstream vulnerabilities in the Maven ecosystem, but also can guide future researches in this field.","Maven,Ecosystem Security,Vulnerability",了解Maven生态系统中上游漏洞对下游项目的威胁,现代软件系统越来越依赖于生态系统的依赖性。最近的一项估计显示，一个开源项目大约35%的代码来自其依赖的库。不幸的是，开源库经常受到各种漏洞问题的威胁，多年来公开的漏洞数量稳步增加。这样的漏洞不仅会对易受攻击的库本身，还会对相应的下游项目，对整个生态系统构成重大的安全威胁。已经提出了许多软件组合分析（SCA）工具，旨在检测引用现有漏洞数据库的易受攻击的库或组件。然而，最近的研究报告称，此类工具往往会产生大量虚假警报。特别是，高达73.3%的依赖于易受攻击的库的项目实际上是安全的。正如一些现有研究所做的那样，为了设计更精确的工具，全面了解生态系统中脆弱性的威胁意义重大。然而，以前的研究要么以非常粗略的粒度进行分析（例如，不分析源代码），要么受到研究规模的限制。这项研究旨在弥合这些差距。特别是，我们收集了44450个（CVE、上游、下游）关系实例，并分析了从下游到上游项目的约5000万次调用，以了解Maven生态系统中上游漏洞对下游项目的潜在威胁。我们的调查在多个方面做出了有趣但重要的发现，包括漏洞的可达性、可达路径的复杂性以及下游项目和开发人员如何感知上游漏洞。我们相信，这些发现不仅可以全面了解Maven生态系统上游脆弱性的威胁，而且可以指导该领域未来的研究。,Maven，生态系统安全，漏洞,,,
7N93C6IL,2023,https://doi.org/10.1109/ICSE48619.2023.00054,ICSE 2023,Practical and Efficient Model Extraction of Sentiment Analysis APIs,"Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.","model extraction,sentiment analysis APIs,active learning,architecture search",情绪分析API的实用高效模型提取,尽管它们的性能令人惊叹，但从头开始开发深度学习模型是一项艰巨的任务。因此，它推广了机器学习即服务（MLaaS），普通用户可以在按查询付费的基础上通过应用程序编程接口（API）访问MLaaS提供商的培训模型。不幸的是，MLaaS的成功受到模型提取攻击的威胁，攻击者打算提取与目标MLaaS模型功能等效的本地模型。然而，现有的关于文本分析API的模型提取的研究经常假设对手对受害者模型有很强的了解，比如它的架构和参数，而这在实践中很难成立。此外，由于攻击者和受害者的训练数据可能存在很大差异，因此执行有效的模型提取并非易事。在本文中，为了加深对此类攻击的理解，我们提出了一个框架PEEP，用于仅使用查询访问的情感分析API的实用高效的模型提取。具体而言，PEEP采用了一种基于学习的方案，该方案采用了域外公共语料库和一种新的查询策略来构建用于模型提取的代理训练数据。此外，PEEP引入了贪婪搜索算法来为提取的模型确定合适的架构。我们在三个数据集和两个现实生活中的商业情绪分析API中对两个受害者模型进行了广泛的实验。实验结果证实，PEEP在有效性和效率方面始终优于最先进的基线。,模型提取，情感分析API，主动学习，架构搜索,,,
ZKCJJUPN,2023,https://doi.org/10.1109/ICSE48619.2023.00219,ICSE 2023,An Empirical Study on Software Bill of Materials: Where We Stand and the Road Ahead,"The rapid growth of software supply chain attacks has attracted considerable attention to software bill of materials (SBOM). SBOMs are a crucial building block to ensure the transparency of software supply chains that helps improve software supply chain security. Although there are significant efforts from academia and industry to facilitate SBOM development, it is still unclear how practitioners perceive SBOMs and what are the challenges of adopting SBOMs in practice. Furthermore, existing SBOM-related studies tend to be ad-hoc and lack software engineering focuses. To bridge this gap, we conducted the first empirical study to interview and survey SBOM practitioners. We applied a mixed qualitative and quantitative method for gathering data from 17 interviewees and 65 survey respondents from 15 countries across five continents to understand how practitioners perceive the SBOM field. We summarized 26 statements and grouped them into three topics on SBOM's states of practice. Based on the study results, we derived a goal model and highlighted future directions where practitioners can put in their effort.","software bill of materials,SBOM,bill of materials,responsible AI,empirical study",软件材料清单的实证研究：我们的立场和未来之路,软件供应链攻击的快速增长引起了软件物料清单（SBOM）的极大关注。SBOM是确保软件供应链透明度的关键组成部分，有助于提高软件供应链的安全性。尽管学术界和工业界为促进SBOM的发展做出了重大努力，但目前尚不清楚从业者如何看待SBOM，以及在实践中采用SBOM的挑战是什么。此外，现有的SBOM相关研究往往是专门的，缺乏软件工程重点。为了弥补这一差距，我们进行了第一次实证研究，对SBOM从业者进行了采访和调查。我们采用定性和定量相结合的方法收集了来自五大洲15个国家的17名受访者和65名调查受访者的数据，以了解从业者对SBOM领域的看法。我们总结了26项声明，并将其分为三个主题，分别讨论SBOM的实践状况。根据研究结果，我们得出了一个目标模型，并强调了从业者可以努力的未来方向。,软件材料清单，SBOM，材料清单，负责任的人工智能，实证研究,,,
BA9ATKXC,2023,https://doi.org/10.1109/ICSE48619.2023.00154,ICSE 2023,Balancing Effectiveness and Flakiness of Non-Deterministic Machine Learning Tests,"Testing Machine Learning (ML) projects is challenging due to inherent non-determinism of various ML algorithms and the lack of reliable ways to compute reference results. Developers typically rely on their intuition when writing tests to check whether ML algorithms produce accurate results. However, this approach leads to conservative choices in selecting assertion bounds for comparing actual and expected results in test assertions. Because developers want to avoid false positive failures in tests, they often set the bounds to be too loose, potentially leading to missing critical bugs. We present FASER - the first systematic approach for balancing the trade-off between the fault-detection effectiveness and flakiness of non-deterministic tests by computing optimal assertion bounds. FASER frames this trade-off as an optimization problem between these competing objectives by varying the assertion bound. FASER leverages 1) statistical methods to estimate the flakiness rate, and 2) mutation testing to estimate the fault-detection effectiveness. We evaluate FASER on 87 non-deterministic tests collected from 22 popular ML projects. FASER finds that 23 out of 87 studied tests have conservative bounds and proposes tighter assertion bounds that maximizes the fault-detection effectiveness of the tests while limiting flakiness. We have sent 19 pull requests to developers, each fixing one test, out of which 14 pull requests have already been accepted.","Machine learning algorithms,Systematics,Limiting,Statistical analysis,Machine learning,Writing,Reliability",非确定性机器学习测试的平衡有效性和片状性,由于各种机器学习算法固有的不确定性以及缺乏计算参考结果的可靠方法，测试机器学习（ML）项目具有挑战性。开发人员在编写测试时通常依靠直觉来检查ML算法是否产生准确的结果。然而，在选择断言边界以比较测试断言中的实际结果和预期结果时，这种方法会导致保守的选择。因为开发人员希望避免测试中的假阳性失败，所以他们经常将界限设置得过于宽松，这可能会导致关键错误的丢失。我们提出了FASER——第一种通过计算最佳断言边界来平衡非确定性测试的故障检测有效性和片状性之间的权衡的系统方法。FASER通过改变断言界限，将这种权衡定义为这些竞争目标之间的优化问题。FASER利用1）统计方法来估计片状率，以及2）突变测试来估计故障检测的有效性。我们对从22个流行的ML项目中收集的87个非确定性测试进行了FASER评估。FASER发现，在87项研究的测试中，有23项具有保守界限，并提出了更严格的断言界限，最大限度地提高了测试的故障检测效率，同时限制了片状。我们已经向开发人员发送了19个拉请求，每个请求都修复了一个测试，其中14个拉请求已经被接受。,机器学习算法，系统学，限制，统计分析，机器学习，写作，可靠性,,,
FFCNYHRX,2023,https://doi.org/10.1109/ICSE48619.2023.00129,ICSE 2023,Automated Program Repair in the Era of Large Pre-trained Language Models,"Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed. In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.","Automated Program Repair,Machine Learning",大型预训练语言模型时代的自动程序修复,自动程序修复（APR）旨在帮助开发人员自动修补软件错误。然而，目前最先进的传统和基于学习的APR技术面临着补丁种类有限的问题，无法修复复杂的错误。这主要是由于依赖漏洞修复数据集来制作修复模板（传统）或直接预测潜在补丁（基于学习）。使用数十亿文本/代码令牌训练的大型预训练语言模型（LLM）可能有助于避免这个问题。最近，研究人员在不依赖任何错误修复数据集的情况下，直接利用LLM进行APR。同时，这些现有工作要么没有包括最先进的LLM，要么没有在现实的数据集上进行评估。因此，现代LLM在重要的APR问题上的真正力量尚待揭示。在这项工作中，我们首次对直接将LLM应用于APR进行了广泛的研究。我们选择了9个最新的最先进的LLM，包括生成和填充模型，大小从125M到20B不等。我们设计了3种不同的修复设置来评估使用LLM生成补丁的不同方式：1）生成整个补丁函数，2）在给定前缀和后缀的情况下填充一块代码3）输出单行修复。我们将这些修复设置下的LLM应用于3种不同语言的5个数据集，并在修复的错误数量、生成速度和编译率方面比较不同的LLM。我们还将LLM与最近最先进的APR工具进行了比较。我们的研究表明，在我们所有的数据集上，直接应用最先进的LLM已经大大优于所有现有的APR技术。在所研究的LLM中，APR存在缩放效应，其中较大的模型往往获得更好的性能。此外，我们首次表明，bug行后的后缀代码（在填充样式APR中采用）不仅在生成更多修复程序方面很重要，而且在生成更多编译率更高的补丁程序方面也很重要。除了生成补丁外，LLM还认为正确的补丁比其他补丁更自然，甚至可以用于有效的补丁排名或补丁正确性检查。最后，我们表明，基于LLM的APR可以通过以下方式进一步大幅提高：1）增加样本量，2）结合固定模板信息。,自动化程序修复，机器学习,,,
88AQXMER,2023,https://doi.org/10.1109/ICSE48619.2023.00201,ICSE 2023,Metamorphic Shader Fusion for Testing Graphics Shader Compilers,"Computer graphics are powered by graphics APIs (e.g., OpenGL, Direct3D) and their associated shader compilers, which render high-quality images by compiling and optimizing user-written high-level shader programs into GPU machine code. Graphics rendering is extensively used in production scenarios like virtual reality (VR), gaming, autonomous driving, and robotics. Despite the development by industrial manufacturers such as Intel, Nvidia, and AMD, shader compilers - like traditional software - may produce ill-rendered outputs. In turn, these errors may result in negative results, from poor user experience in entertainment to accidents in driving assistance systems. This paper introduces FSHADER, a metamorphic testing (MT) framework designed specifically for shader compilers to uncover erroneous compilations and optimizations. FSHADER tests shader compilers by mutating input shader programs via four carefully-designed metamorphic relations (MRs). In particular, FSHADER fuses two shader programs via an MR and checks the visual consistency between the image rendered from the fused shader program with the output of fusing individually rendered images. Our study of 12 shader compilers covers five mainstream GPU vendors, including Intel, AMD, Nvidia, ARM, and Apple. We successfully uncover over 16K error-triggering inputs that generate incorrect rendering outputs. We manually locate and characterize buggy optimization places, and developers have confirmed representative bugs.","Compiler testing,Metamorphic testing,Shader compilers,Graphics compilers,GPU driver,Virtual reality",用于测试图形着色器编译器的变形着色器融合,计算机图形由图形API（例如OpenGL、Direct3D）及其相关着色器编译器提供动力，这些编译器通过将用户编写的高级着色器程序编译和优化为GPU机器代码来渲染高质量图像。图形渲染广泛应用于虚拟现实（VR）、游戏、自动驾驶和机器人等生产场景。尽管英特尔、英伟达和AMD等工业制造商进行了开发，但着色器编译器与传统软件一样，可能会产生渲染不良的输出。反过来，这些错误可能会导致负面结果，从娱乐中的不良用户体验到驾驶辅助系统中的事故。本文介绍了FSHADER，这是一个专门为着色器编译器设计的变形测试（MT）框架，用于发现错误的编译和优化。FSHADER通过四个精心设计的变形关系（MR）来改变输入着色器程序，从而测试着色器编译器。特别地，FSHADER通过MR融合两个着色器程序，并检查融合着色器程序渲染的图像与融合单独渲染的图像的输出之间的视觉一致性。我们对12个着色器编译器的研究涵盖了五家主流GPU供应商，包括英特尔、AMD、英伟达、ARM和苹果。我们成功地发现了超过16K的错误触发输入，这些输入会产生不正确的渲染输出。我们手动定位和描述bug优化的地方，开发人员已经确认了有代表性的bug。,编译器测试，变形测试，着色器编译器，图形编译器，GPU驱动程序，虚拟现实,,,
YAPGKVNC,2023,https://doi.org/10.1109/ICSE48619.2023.00074,ICSE 2023,Data Quality Matters: A Case Study of Obsolete Comment Detection,"Machine learning methods have achieved great success in many software engineering tasks. However, as a data-driven paradigm, how would the data quality impact the effectiveness of these methods remains largely unexplored. In this paper, we explore this problem under the context of just-in-time obsolete comment detection. Specifically, we first conduct data cleaning on the existing benchmark dataset, and empirically observe that with only 0.22% label corrections and even 15.0% fewer data, the existing obsolete comment detection approaches can achieve up to 10.7% relative accuracy improvement. To further mitigate the data quality issues, we propose an adversarial learning framework to simultaneously estimate the data quality and make the final predictions. Experimental evaluations show that this adversarial learning framework can further improve the relative accuracy by up to 18.1% compared to the state-of-the-art method. Although our current results are from the obsolete comment detection problem, we believe that the proposed two-phase solution, which handles the data quality issues through both the data aspect and the algorithm aspect, is also generalizable and applicable to other machine learning based software engineering tasks.","Obsolete comment detection,machine learning for software engineering,data quality",数据质量问题：一个过时评论检测的案例研究,机器学习方法在许多软件工程任务中取得了巨大成功。然而，作为一种数据驱动的范式，数据质量将如何影响这些方法的有效性在很大程度上仍有待探索。在本文中，我们在即时过时评论检测的背景下探讨了这个问题。具体来说，我们首先对现有的基准数据集进行数据清理，并从经验上观察到，只有0.22%的标签校正，甚至减少15.0%的数据，现有的过时评论检测方法可以实现高达10.7%的相对准确率提高。为了进一步缓解数据质量问题，我们提出了一个对抗性学习框架，以同时估计数据质量并做出最终预测。实验评估表明，与最先进的方法相比，这种对抗性学习框架可以进一步提高18.1%的相对准确性。尽管我们目前的结果来自过时评论检测问题，但我们相信，所提出的两阶段解决方案通过数据方面和算法方面处理数据质量问题，也是可推广的，适用于其他基于机器学习的软件工程任务。,过时的评论检测，软件工程的机器学习，数据质量,,,
6DMYTMG3,2023,https://doi.org/10.1109/ICSE48619.2023.00209,ICSE 2023,(Partial) Program Dependence Learning,"Code fragments from developer forums often migrate to applications due to the code reuse practice. Owing to the incomplete nature of such programs, analyzing them to early determine the presence of potential vulnerabilities is challenging. In this work, we introduce NeuralPDA, a neural network-based program dependence analysis tool for both complete and partial programs. Our tool efficiently incorporates intra-statement and inter-statement contextual features into statement representations, thereby modeling program dependence analysis as a statement-pair dependence decoding task. In the empirical evaluation, we report that NeuralPDA predicts the CFG and PDG edges in complete Java and C/C++ code with combined F-scores of 94.29% and 92.46%, respectively. The F-score values for partial Java and C/C++ code range from 94.29%-97.17% and 92.46%-96.01%, respectively. We also test the usefulness of the PDGs predicted by NeuralPDA (i.e., PDG*) on the downstream task of method-level vulnerability detection. We discover that the performance of the vulnerability detection tool utilizing PDG* is only 1.1% less than that utilizing the PDGs generated by a program analysis tool. We also report the detection of 14 real-world vulnerable code snippets from StackOverflow by a machine learning-based vulnerability detection tool that employs the PDGs predicted by NeuralPDA for these code snippets.","neural partial program analysis,neural program dependence analysis,neural networks,deep learning",（部分）程序依赖性学习,由于代码重用实践，来自开发人员论坛的代码片段经常迁移到应用程序中。由于此类程序的不完整性，分析它们以尽早确定潜在漏洞的存在具有挑战性。在这项工作中，我们介绍了NeuralPDA，一种基于神经网络的程序依赖性分析工具，用于完整程序和部分程序。我们的工具有效地将语句内和语句间上下文特征结合到语句表示中，从而将程序依赖性分析建模为语句对依赖性解码任务。在经验评估中，我们报告NeuralPDA预测了完整Java和C/C++代码中的CFG和PDG边，组合F得分分别为94.29%和92.46%。部分Java和C/C++代码的F分值分别为94.29%至97.17%和92.46%至96.01%。我们还测试了NeuralPDA预测的PDGs（即PDG*）在方法级漏洞检测的下游任务中的有用性。我们发现，使用PDG*的漏洞检测工具的性能仅比使用程序分析工具生成的PDG低1.1%。我们还报告了一个基于机器学习的漏洞检测工具从StackOverflow中检测到的14个真实世界中的漏洞代码片段，该工具使用NeuralPDA为这些代码片段预测的PDG。,神经部分程序分析，神经程序依赖性分析，神经网络，深度学习,,,
KGB4LYWT,2023,https://doi.org/10.1109/ICSE48619.2023.00170,ICSE 2023,DLInfer: Deep Learning with Static Slicing for Python Type Inference,"Python programming language has gained enor-mous popularity in the past decades. While its flexibility signifi-cantly improves software development productivity, the dynamic typing feature challenges software maintenance and quality assurance. To facilitate programming and type error checking, the Python programming language has provided a type hint mechanism enabling developers to annotate type information for variables. However, this manual annotation process often requires plenty of resources and may introduce errors. In this paper, we propose a deep learning type inference technique, namely DLInfer, to automatically infer the type infor-mation for Python programs. DLInfer collects slice statements for variables through static analysis and then vectorizes them with the Unigram Language Model algorithm. Based on the vectorized slicing features, we designed a bi-directional gated recurrent unit model to learn the type propagation information for inference. To validate the effectiveness of DLInfer, we conduct an extensive empirical study on 700 open-source projects. We evaluate its accuracy in inferring three kinds of fundamental types, including built-in, library, and user-defined types. By training with a large-scale dataset, DLInfer achieves an average of 98.79% Top-1 accuracy for the variables that can get type information through static analysis and manual annotation. Further, DLInfer achieves 83.03% type inference accuracy on average for the variables that can only obtain the type information through dynamic analysis. The results indicate DLInfer is highly effective in inferring types. It is promising to apply it to assist in various software engineering tasks for Python programs.","type inference,Python,static slicing",DLInfer：Python类型推理的静态切片深度学习,Python编程语言在过去几十年中获得了极大的流行。虽然它的灵活性显著提高了软件开发生产力，但动态类型特性对软件维护和质量保证提出了挑战。为了便于编程和类型错误检查，Python编程语言提供了一种类型提示机制，使开发人员能够注释变量的类型信息。然而，这种手动注释过程通常需要大量资源，并且可能会引入错误。在本文中，我们提出了一种深度学习类型推断技术，即DLInfer，用于自动推断Python程序的类型信息。DLInfer通过静态分析收集变量的切片语句，然后使用Unigram语言模型算法对其进行矢量化。基于矢量化切片特征，我们设计了一个双向门控递归单元模型来学习用于推理的类型传播信息。为了验证DLInfer的有效性，我们对700个开源项目进行了广泛的实证研究。我们评估了它在推断三种基本类型时的准确性，包括内置类型、库类型和用户定义类型。通过使用大规模数据集进行训练，DLInfer对可以通过静态分析和手动注释获得类型信息的变量实现了平均98.79%的Top-1准确率。此外，对于只能通过动态分析获得类型信息的变量，DLInfer平均达到83.03%的类型推断准确率。结果表明，DLInfer在推断类型方面是非常有效的。它有望应用于Python程序的各种软件工程任务。,类型推理，Python，静态切片,,,
6WS6T5CY,2023,https://doi.org/10.1109/ICSE48619.2023.00173,ICSE 2023,Generating Test Databases for Database-Backed Applications,"Database-backed applications are widely used. To effectively test these applications, one needs to design not only user inputs but also database states, which imposes unique challenges. First, valid database states have to satisfy complicated constraints determined by application semantics, and hence are difficult to synthesize. Second, the state space of a database is huge, as an application can contain tens to hundreds of tables with up to tens of fields per table. Making things worse, each test involving database operations takes significant time to run. Consequently, unhelpful database states and running tests on them can severely waste testing resources. We propose DBGRILLER, a tool that generates database states to facilitate thorough testing of database-backed applications. To effectively generate valid database states, DBGRILLER strategically injects minor mutation into existing database states and transforms part of the application-under-test into a stand-alone validity checker. To tackle the huge database state space and save testing time, DBGRILLER uses program analysis to identify a novel branch-projected DB view that can be used to filter out database states that are unlikely to increase the testing branch coverage. Our evaluation on 9 popular open-source database applications shows that DBGRILLER can effectively increase branch coverage of existing tests and expose previously unknown bugs.","Automated testing,Test data generation,database-backed application,database-state generation",为数据库支持的应用程序生成测试数据库,数据库支持的应用程序被广泛使用。为了有效地测试这些应用程序，不仅需要设计用户输入，还需要设计数据库状态，这带来了独特的挑战。首先，有效的数据库状态必须满足由应用程序语义确定的复杂约束，因此很难进行综合。其次，数据库的状态空间是巨大的，因为一个应用程序可以包含几十到几百个表，每个表最多有几十个字段。更糟糕的是，每个涉及数据库操作的测试都需要花费大量时间来运行。因此，无用的数据库状态和对它们运行测试可能会严重浪费测试资源。我们提出了DBGRILLER，这是一种生成数据库状态的工具，有助于对数据库支持的应用程序进行彻底测试。为了有效地生成有效的数据库状态，DBGRILLER策略性地将微小的突变注入现有的数据库状态中，并将测试中的部分应用程序转换为独立的有效性检查器。为了解决巨大的数据库状态空间并节省测试时间，DBGRILLER使用程序分析来确定一个新的分支投影数据库视图，该视图可用于筛选不太可能增加测试分支覆盖率的数据库状态。我们对9个流行的开源数据库应用程序的评估表明，DBGRILLER可以有效地增加现有测试的分支覆盖率，并暴露以前未知的错误。,自动化测试，测试数据生成，数据库支持的应用程序，数据库状态生成,,,
SWMFTYLA,2023,https://doi.org/10.1109/ICSE48619.2023.00026,ICSE 2023,Locating Framework-specific Crashing Faults with Compact and Explainable Candidate Set,"Nowadays, many applications do not exist independently but rely on various frameworks or libraries. The frequent evolution and the complex implementation of framework APIs induce lots of unexpected post-release crashes. Starting from the crash stack traces, existing approaches either perform application-level call graph (CG) tracing or construct datasets with similar crash-fixing records to locate buggy methods. However, these approaches are limited by the completeness of CG or dependent on historical fixing records, and some of them only focus on specific manually modeled exception types. To achieve effective debugging on complex framework-specific crashes, we propose a code-separation-based locating approach that weakly relies on CG tracing and does not require any prior knowledge. Our key insight is that one crash trace with the description message can be mapped to a definite exception-thrown point in the framework, the semantics analysis of which can help to figure out the root causes of the crash-triggering procedure. Thus, we can pre-construct reusable summaries for all the framework-specific exceptions to support fault localization in application code. Based on that idea, we design the exception-thrown summary (ETS) that describes both the key variables and key APIs related to the exception triggering. Then, we perform static analysis to automatically compute such summaries and make a data-tracking of key variables and APIs in the application code to get the ranked buggy candidates. In the scenario of locating Android framework-specific crashing faults, our tool CrashTracker exhibited an overall MRR value of 0.91 and outperforms the state-of-the-art tool Anchor with higher precision. It only provides a compact candidate set and gives user-friendly reports with explainable reasons for each candidate.","Fault Localization,Framework-specific Exception,Crash Stack Trace,Android Application",用紧凑可解释候选集定位框架特定的碰撞故障,如今，许多应用程序并不是独立存在的，而是依赖于各种框架或库。框架API的频繁进化和复杂实现导致了许多意外的发布后崩溃。从崩溃堆栈跟踪开始，现有的方法要么执行应用程序级调用图（CG）跟踪，要么构建具有类似崩溃修复记录的数据集来定位有缺陷的方法。然而，这些方法受到CG完整性的限制或依赖于历史修复记录，其中一些方法只关注特定的手动建模异常类型。为了实现对复杂框架特定崩溃的有效调试，我们提出了一种基于代码分离的定位方法，该方法弱地依赖于CG跟踪，不需要任何先验知识。我们的关键见解是，一个带有描述消息的崩溃跟踪可以映射到框架中的一个特定异常抛出点，对其进行语义分析可以帮助找出崩溃触发过程的根本原因。因此，我们可以为所有特定于框架的异常预先构建可重用的摘要，以支持应用程序代码中的故障定位。基于这一思想，我们设计了异常抛出摘要（ETS），该摘要描述了与异常触发相关的关键变量和关键API。然后，我们执行静态分析来自动计算这些摘要，并对应用程序代码中的关键变量和API进行数据跟踪，以获得排名的候选bug。在定位Android框架特定崩溃故障的场景中，我们的工具CrashTracker显示出0.91的总体MRR值，并以更高的精度优于最先进的工具Anchor。它只提供了一个紧凑的候选集，并为每个候选提供了用户友好的报告，其中包含可解释的原因。,故障定位，特定于框架的异常，崩溃堆栈跟踪，Android应用程序,,,
V8VAFKL4,2023,https://doi.org/10.1109/ICSE48619.2023.00051,ICSE 2023,Adhere: Automated Detection and Repair of Intrusive Ads,"Today, more than 3 million websites rely on online advertising revenue. Despite the monetary incentives, ads often frustrate users by disrupting their experience, interrupting content, and slowing browsing. To improve ad experiences, leading media associations define Better Ads Standards for ads that are below user expectations. However, little is known about how well websites comply with these standards and whether existing approaches are sufficient for developers to quickly resolve such issues. In this paper, we propose Adhere, a technique that can detect intrusive ads that do not comply with Better Ads Standards and suggest repair proposals. Adhere works by first parsing the initial web page to a DOM tree to search for potential static ads, and then using mutation observers to monitor and detect intrusive (dynamic/static) ads on the fly. To handle ads' volatile nature, Adhere includes two detection algorithms for desktop and mobile ads to identify different ad violations during three phases of page load events. It recursively applies the detection algorithms to resolve nested layers of DOM elements inserted by ad delegations. We evaluate Adhere on Alexa Top 1 Million Websites. The results show that Adhere is effective in detecting violating ads and suggesting repair proposals. Comparing to the current available alternative, Adhere detected intrusive ads on 4,656 more mobile websites and 3,911 more desktop websites, and improved recall by 16.6% and accuracy by 4.2%.","ad experience,advertising practice,Better Ads Standards",坚持：入侵广告的自动检测和修复,如今，超过300万个网站依赖在线广告收入。尽管有金钱激励，但广告往往会扰乱用户的体验、中断内容和减缓浏览速度，从而让用户感到沮丧。为了改善广告体验，领先的媒体协会为低于用户期望的广告定义了更好的广告标准。然而，对于网站在多大程度上符合这些标准，以及现有的方法是否足以让开发人员快速解决这些问题，人们知之甚少。在本文中，我们提出了坚持，这是一种可以检测不符合更好的广告标准的侵入性广告并提出修复建议的技术。坚持的工作原理是首先将初始网页解析为DOM树，以搜索潜在的静态广告，然后使用突变观测器实时监控和检测侵入性（动态/静态）广告。为了处理广告的波动性，坚持包括两种针对桌面和移动广告的检测算法，以在页面加载事件的三个阶段识别不同的广告违规行为。它递归地应用检测算法来解析由广告委托插入的DOM元素的嵌套层。我们评估了在Alexa排名前100万的网站上坚持。结果表明，坚持在发现违规广告和提出修复建议方面是有效的。与目前可用的替代方案相比，坚持在4656个移动网站和3911个桌面网站上检测到侵入性广告，召回率提高了16.6%，准确率提高了4.2%。,广告体验，广告实践，更好的广告标准,,,
XXP2YISV,2023,https://doi.org/10.1109/ICSE48619.2023.00167,ICSE 2023,Carving UI Tests to Generate API Tests and API Specification,"Modern web applications make extensive use of API calls to update the UI state in response to user events or server-side changes. For such applications, API-level testing can play an important role, in-between unit-level testing and UI-level (or end-to-end) testing. Existing API testing tools require API specifications (e.g., OpenAPI), which often may not be available or, when available, be inconsistent with the API implementation, thus limiting the applicability of automated API testing to web applications. In this paper, we present an approach that leverages UI testing to enable API-level testing for web applications. Our technique navigates the web application under test and automatically generates an API-level test suite, along with an OpenAPI specification that describes the application's server-side APIs (for REST-based web applications). A key element of our solution is a dynamic approach for inferring API endpoints with path parameters via UI navigation and directed API probing. We evaluated the technique for its accuracy in inferring API specifications and the effectiveness of the “carved” API tests. Our results on seven open-source web applications show that the technique achieves 98% precision and 56% recall in inferring endpoints. The carved API tests, when added to test suites generated by two automated REST API testing tools, increase statement coverage by 52% and 29% and branch coverage by 99% and 75%, on average. The main benefits of our technique are: (1) it enables API-level testing of web applications in cases where existing API testing tools are inapplicable and (2) it creates API-level test suites that cover server-side code efficiently while exercising APIs as they would be invoked from an application's web UI, and that can augment existing API test suites.","Web Application Testing,API Testing,Test Generation,UI Testing,End-to-end Testing,Test Carving,API Specification Inference",雕刻UI测试以生成API测试和API规范,现代web应用程序广泛使用API调用来更新UI状态，以响应用户事件或服务器端更改。对于此类应用程序，API级测试可以在单元级测试和UI级（或端到端）测试之间发挥重要作用。现有的API测试工具需要API规范（例如OpenAPI），这些规范通常可能不可用，或者在可用时与API实现不一致，从而限制了自动化API测试对web应用程序的适用性。在本文中，我们提出了一种利用UI测试实现web应用程序API级测试的方法。我们的技术导航测试中的web应用程序，并自动生成API级测试套件，以及描述应用程序服务器端API的OpenAPI规范（用于基于REST的web应用）。我们解决方案的一个关键元素是通过UI导航和定向API探测，使用路径参数推断API端点的动态方法。我们评估了该技术在推断API规范方面的准确性以及“雕刻”API测试的有效性。我们在七个开源web应用程序上的结果表明，该技术在推断端点方面实现了98%的准确率和56%的召回率。将经过雕刻的API测试添加到由两个自动化REST API测试工具生成的测试套件中时，语句覆盖率平均提高了52%和29%，分支覆盖率平均增加了99%和75%。我们的技术的主要优点是：（1）在现有API测试工具不适用的情况下，它能够对web应用程序进行API级测试；（2）它创建API级测试套件，在从应用程序的web UI调用API时，有效地覆盖服务器端代码，并可以扩展现有的API测试套件。,Web应用程序测试，API测试，测试生成，UI测试，端到端测试，测试雕刻，API规范推理,,,
MB5CV86L,2023,https://doi.org/10.1109/ICSE48619.2023.00033,ICSE 2023,Compatibility Issue Detection for Android Apps Based on Path-Sensitive Semantic Analysis,"Android API-related compatibility issues have be-come a severe problem and significant challenge for app devel-opers due to the well-known Android fragmentation issues. To address this problem, many effective approaches such as app-based and API lifetime-based methods have been proposed to identify incompatible API usages. However, due to the various implementations of API usages and different API invoking paths, there is still a significant weakness of existing approaches, i.e., introducing a massive number of false positives (FP) and false negatives (FN). To this end, in this paper, we propose PSDroid, an automated compatibility detection approach for Android apps, which aims to reduce FPs and FNs by overcoming several technical bottlenecks. Firstly, we make substantial efforts to carry out a preliminary study to summarize a set of novel API usages with diverse checking implementations. Secondly, we construct a refined API lifetime database by leveraging a semantic resolving analysis on all existing Android SDK frameworks. Based on the above two key phases, we design and implement a novel path-sensitive semantic approach to effectively and automatically detect incompatibility issues. To demonstrate the performance, we compared with five existing approaches (i.e., FicFinder, ACRYL, CIDER, IctAPIFinder, and CID) and the results show that PSDroid outperforms existing tools. We also conducted an in-depth root cause analysis to comprehensively explain the ability of PSDroid in reducing FPs and FNs. Finally, 18/30 reported issues have been confirmed and further fixed by app developers.","Compatibility detection,Android app,Path-sensitive analysis,Semantic analysis",基于路径敏感语义分析的Android应用程序兼容性问题检测,由于众所周知的Android碎片问题，与Android API相关的兼容性问题一直是应用程序开发人员面临的严重问题和重大挑战。为了解决这个问题，已经提出了许多有效的方法，如基于应用程序和基于API生命周期的方法，以识别不兼容的API使用。然而，由于API使用的各种实现方式和不同的API调用路径，现有方法仍然存在显著的弱点，即引入了大量的假阳性（FP）和假阴性（FN）。为此，在本文中，我们提出了PSDroid，这是一种针对Android应用程序的自动兼容性检测方法，旨在通过克服几个技术瓶颈来减少FPs和FNs。首先，我们做了大量的努力来进行初步研究，总结出一组具有不同检查实现的API新用法。其次，我们通过利用对所有现有Android SDK框架的语义解析分析，构建了一个改进的API生命周期数据库。基于以上两个关键阶段，我们设计并实现了一种新的路径敏感语义方法，以有效地自动检测不兼容问题。为了证明性能，我们将其与五种现有方法（即FicFinder、ACRYL、CIDER、IctAPIFinder和CID）进行了比较，结果表明PSDroid优于现有工具。我们还进行了深入的根本原因分析，以全面解释PSDroid减少FPs和FNs的能力。最后，18/30报告的问题已得到应用程序开发人员的确认和进一步修复。,兼容性检测，Android应用程序，路径敏感分析，语义分析,,,
I57IFXPH,2023,https://doi.org/10.1109/ICSE48619.2023.00105,ICSE 2023,Fuzzing Automatic Differentiation in Deep-Learning Libraries,"Deep learning (DL) has attracted wide attention and has been widely deployed in recent years. As a result, more and more research efforts have been dedicated to testing DL libraries and frameworks. However, existing work largely overlooked one crucial component of any DL system, automatic differentiation (AD), which is the basis for the recent development of DL. To this end, we propose ∇Fuzz, the first general and practical approach specifically targeting the critical AD component in DL libraries. Our key insight is that each DL library API can be abstracted into a function processing tensors/vectors, which can be differentially tested under various execution scenarios (for computing outputs/gradients with different implementations). We have implemented $\nabla \text{Fuzz}$ as a fully automated API-level fuzzer targeting AD in DL libraries, which utilizes differential testing on different execution scenarios to test both first-order and high-order gradients, and also includes automated filtering strategies to remove false positives caused by numerical instability. We have performed an extensive study on four of the most popular and actively-maintained DL libraries, PyTorch, TensorFlow, JAX, and OneFlow. The result shows that $\nabla \text{Fuzz}$ substantially outperforms state-of-the-art fuzzers in terms of both code coverage and bug detection. To date, $\nabla \text{Fuzz}$ has detected 173 bugs for the studied DL libraries, with 144 already confirmed by developers (117 of which are previously unknown bugs and 107 are related to AD). Remarkably, $\nabla \text{Fuzz}$ contributed 58.3% (7/12) of all high-priority AD bugs for PyTorch and JAX during a two-month period. None of the confirmed AD bugs were detected by existing fuzzers.","Deep learning,Codes,Filtering,Computer bugs,Fuzzing,Libraries,Engines",模糊化深度学习图书馆的自动区分,近年来，深度学习（DL）引起了广泛关注并得到了广泛部署。因此，越来越多的研究工作致力于测试DL库和框架。然而，现有的工作在很大程度上忽略了任何DL系统的一个关键组成部分，即自动微分（AD），这是DL最近发展的基础。为此，我们提出了ŞFuzz，这是第一个专门针对DL库中关键AD组件的通用实用方法。我们的关键见解是，每个DL库API都可以抽象为处理张量/向量的函数，这些张量/向量可以在各种执行场景下进行差异测试（用于计算具有不同实现的输出/梯度）。我们已经实现了$\nabla\text｛Fuzz｝$，作为一个针对DL库中AD的全自动API级模糊器，它利用对不同执行场景的差异测试来测试一阶和高阶梯度，还包括自动过滤策略来消除由数值不稳定性引起的假阳性。我们对PyTorch、TensorFlow、JAX和OneFlow这四个最流行、维护最积极的DL库进行了广泛的研究。结果表明，$\nabla\text{Fuzz}$在代码覆盖率和错误检测方面都大大优于最先进的模糊器。到目前为止，$\nabla\text｛Fuzz｝$已经为所研究的DL库检测到173个错误，其中144个已经被开发人员确认（其中117个是以前未知的错误，107个与AD有关）。值得注意的是，在两个月的时间里，$\nabla\text｛Fuzz｝$为PyTorch和JAX贡献了58.3%（7/12）的所有高优先级AD错误。现有的模糊器没有检测到任何已确认的AD错误。,深度学习，代码，过滤，计算机错误，模糊，库，引擎,,,
LPDX3Q76,2023,https://doi.org/10.1109/ICSE48619.2023.00157,ICSE 2023,"Demystifying Issues, Challenges, and Solutions for Multilingual Software Development","Developing a software project using multiple languages together has been a dominant practice for years. Yet it remains unclear what issues developers encounter during the development, which challenges cause the issues, and what solutions developers receive. In this paper, we aim to answer these questions via a study on developer discussions on Stack Overflow. By manually analyzing 586 highly relevant posts spanning 14 years, we observed a large variety (11 categories) of issues, dominated by those with interfacing and data handling among different languages. Behind these issues, we found that a major challenge developers faced is the diversity and complexity in multilingual code building and interoperability. Another key challenge lies in developers' lack of particular technical background on the diverse features of various languages (e.g., threading and memory management mechanisms). Meanwhile, Stack Overflow itself served as a key source of solutions to these challenges-the majority (73%) of the posts received accepted answers eventually, and most in a week (36.5% within 24 hours and 25% in the next 6 days). Based on our findings on these issues, challenges, and solutions, we provide actionable insights and suggestions for both multi-language software researchers and developers.","Multilingual software,development issues,language interfacing,software build,data format,interoperability",解开多语言软件开发的问题、挑战和解决方案,多年来，使用多种语言共同开发软件项目一直是一种主流做法。然而，目前尚不清楚开发人员在开发过程中遇到了什么问题，是什么挑战导致了这些问题，以及开发人员收到了什么解决方案。在本文中，我们旨在通过研究开发人员对Stack Overflow的讨论来回答这些问题。通过手动分析跨越14年的586个高度相关的帖子，我们观察到了各种各样（11类）的问题，主要是不同语言之间的接口和数据处理问题。在这些问题背后，我们发现开发人员面临的一个主要挑战是多语言代码构建和互操作性的多样性和复杂性。另一个关键挑战在于开发人员缺乏关于各种语言的不同功能（例如线程和内存管理机制）的特定技术背景。与此同时，Stack Overflow本身是解决这些挑战的关键来源——大多数（73%）帖子最终都得到了认可，大多数帖子在一周内得到了认可（36.5%在24小时内，25%在接下来的6天内）。基于我们对这些问题、挑战和解决方案的发现，我们为多语言软件研究人员和开发人员提供了可操作的见解和建议。,多语言软件，开发问题，语言接口，软件构建，数据格式，互操作性,,,
XWUDWLVA,2023,https://doi.org/10.1109/ICSE48619.2023.00192,ICSE 2023,Does data sampling improve deep learning-based vulnerability detection? Yeas! and Nays!,"Recent progress in Deep Learning (DL) has sparked interest in using DL to detect software vulnerabilities automatically and it has been demonstrated promising results at detecting vulnerabilities. However, one prominent and practical issue for vulnerability detection is data imbalance. Prior study observed that the performance of state-of-the-art (SOTA) DL-based vulnerability detection (DLVD) approaches drops precipitously in real world imbalanced data and a 73% drop of F1-score on average across studied approaches. Such a significant performance drop can disable the practical usage of any DLVD approaches. Data sampling is effective in alleviating data imbalance for machine learning models and has been demonstrated in various software engineering tasks. Therefore, in this study, we conducted a systematical and extensive study to assess the impact of data sampling for data imbalance problem in DLVD from two aspects: i) the effectiveness of DLVD, and ii) the ability of DLVD to reason correctly (making a decision based on real vulnerable statements). We found that in general, oversampling outperforms undersampling, and sampling on raw data outperforms sampling on latent space, typically random oversampling on raw data performs the best among all studied ones (including advanced one SMOTE and OSS). Surprisingly, OSS does not help alleviate the data imbalance issue in DLVD. If the recall is pursued, random undersampling is the best choice. Random oversampling on raw data also improves the ability of DLVD approaches for learning real vulnerable patterns. However, for a significant portion of cases (at least 33% in our datasets), DVLD approach cannot reason their prediction based on real vulnerable statements. We provide actionable suggestions and a roadmap to practitioners and researchers.","Vulnerability detection,deep learning,data sampling,interpretable AI",数据采样是否改进了基于深度学习的漏洞检测？是的！不！,深度学习（DL）的最新进展引发了人们对使用DL自动检测软件漏洞的兴趣，并且在检测漏洞方面取得了很好的结果。然而，漏洞检测的一个突出而实际的问题是数据不平衡。先前的研究观察到，在现实世界中不平衡的数据中，最先进的（SOTA）基于DL的漏洞检测（DLVD）方法的性能急剧下降，所研究的方法的F1得分平均下降73%。这种显著的性能下降可能会使任何DLVD方法的实际使用失效。数据采样在缓解机器学习模型的数据不平衡方面是有效的，并已在各种软件工程任务中得到证明。因此，在本研究中，我们进行了系统而广泛的研究，从两个方面评估数据采样对DLVD中数据不平衡问题的影响：i）DLVD的有效性，以及ii）DLVD正确推理的能力（根据真实的脆弱性陈述做出决策）。我们发现，一般来说，过采样优于欠采样，对原始数据的采样优于对潜在空间的采样，通常对原始数据进行随机过采样在所有研究中表现最好（包括高级过采样SMOTE和OSS）。令人惊讶的是，OSS并没有帮助缓解DLVD中的数据不平衡问题。如果进行召回，随机欠采样是最佳选择。对原始数据的随机过采样还提高了DLVD方法学习真实易受攻击模式的能力。然而，对于很大一部分病例（在我们的数据集中至少有33%），DVLD方法无法根据真实的脆弱性陈述来推断其预测。我们为从业者和研究人员提供可行的建议和路线图。,漏洞检测，深度学习，数据采样，可解释AI,,,
P3AHHQVU,2023,https://doi.org/10.1109/ICSE48619.2023.00066,ICSE 2023,On the Self-Governance and Episodic Changes in Apache Incubator Projects: An Empirical Study,"Sustainable Open Source Software (OSS) projects are characterized by the ability to attract new project members and maintain an energetic project community. Building sustainable OSS projects from a nascent state requires effective project governance and socio-technical structure to be interleaved, in a complex and dynamic process. Although individual disciplines have studied each separately, little is known about how governance and software development work together in practice toward sustainability. Prior work has shown that many OSS projects experience large, episodic changes over short periods of time, which can propel them or drag them down. However, sustainable projects typically manage to come out unscathed from such changes, while others do not. The natural questions arise: Can we identify the back-and-forth between governance and socio-technical structure that lead to sustainability following episodic events? And, how about those that do not lead to sustainability? From a data set of social, technical, and policy digital traces from 262 sustainability-labeled ASF incubator projects, here we employ a large-scale empirical study to characterize episodic changes in socio-technical aspects measured by Change Intervals (CI), governance rules and regulations in a form of Institutional Statements (IS), and the temporal relationships between them. We find that sustainable projects during episodic changes can adapt themselves to institutional statements more efficiently, and that institutional discussions can lead to episodic changes intervals in socio-technical aspects of the projects, and vice versa. In practice, these results can provide timely guidance beyond socio-technical considerations, adding rules and regulations in the mix, toward a unified analytical framework for OSS project sustainability.","Open Source Software,Sustainability,Governance,Apache Software Foundation",Apache孵化器项目的自治与阶段性变化——一项实证研究,可持续开放源码软件（OSS）项目的特点是能够吸引新的项目成员并维持一个充满活力的项目社区。从一个新生状态构建可持续的开放源码软件项目需要在一个复杂而动态的过程中交织有效的项目治理和社会技术结构。尽管各个学科分别进行了研究，但对于治理和软件开发如何在实践中共同实现可持续性，人们知之甚少。先前的工作表明，许多OSS项目在短时间内经历了巨大的、偶发性的变化，这可能会推动或拖累它们。然而，可持续项目通常能够在这些变化中毫发无损，而其他项目则不然。自然产生的问题是：我们能否确定治理和社会技术结构之间的来回关系，从而在偶发事件之后实现可持续性？那么，那些不能带来可持续性的呢？从262个可持续性标记的ASF孵化器项目的社会、技术和政策数字痕迹数据集中，我们采用了一项大规模的实证研究来描述社会技术方面的偶发性变化，这些变化通过变化间隔（CI）、以制度声明（IS）形式存在的治理规则和条例以及它们之间的时间关系来衡量。我们发现，在阶段性变化中的可持续项目可以更有效地适应机构声明，而机构讨论可以导致项目的社会技术方面的阶段性变化间隔，反之亦然。在实践中，这些结果可以提供超越社会技术考虑的及时指导，在组合中添加规则和条例，为OSS项目的可持续性提供统一的分析框架。,开源软件，可持续发展，治理，Apache软件基金会,,,
LPVUY523,2023,https://doi.org/10.1109/ICSE48619.2023.00019,ICSE 2023,Regression Fuzzing for Deep Learning Systems,"Deep learning (DL) Systems have been widely used in various domains. Similar to traditional software, DL system evolution may also incur regression faults. To find the regression faults between versions of a DL system, we propose a novel regression fuzzing technique called DRFuzz, which facilitates generating inputs that trigger diverse regression faults and have high fidelity. To enhance the diversity of the found regression faults, DRFuzz proposes a diversity-oriented test criterion to explore as many faulty behaviors as possible. Then, DRFuzz incorporates the GAN model to guarantee the fidelity of generated test inputs. We conduct an extensive study on four subjects in four regression scenarios of DL systems. The experimental results demonstrate the superiority of DRFuzz over the two compared state-of-the-art approaches, with an average improvement of 1,177% and 539% in terms of the number of detected regression faults.","Regression,Fuzzing,Deep Learning",深度学习系统的回归模糊化,深度学习（DL）系统已被广泛应用于各个领域。与传统软件类似，DL系统进化也可能导致回归错误。为了找到DL系统版本之间的回归故障，我们提出了一种称为DRFuzz的新的回归模糊技术，该技术有助于生成触发不同回归故障并具有高保真度的输入。为了增强发现的回归错误的多样性，DRFuzz提出了一种面向多样性的测试标准，以探索尽可能多的错误行为。然后，DRFuzz结合了GAN模型，以保证生成的测试输入的保真度。我们在DL系统的四个回归场景中对四个受试者进行了广泛的研究。实验结果表明，DRFuzz优于两种最先进的方法，在检测到的回归错误数量方面，平均改进了1177%和539%。,回归，模糊化，深度学习,,,
2MSU7A5A,2023,https://doi.org/10.1109/ICSE48619.2023.00151,ICSE 2023,LogReducer: Identify and Reduce Log Hotspots in Kernel on the Fly,"Modern systems generate a massive amount of logs to detect and diagnose system faults, which incurs expensive storage costs and runtime overhead. After investigating real-world production logs, we observe that most of the logging overhead is due to a small number of log templates, referred to as log hotspots. Therefore, we conduct a systematical study about log hotspots in an industrial system WeChat, which motivates us to identify log hotspots and reduce them on the fly. In this paper, we propose LogReducer, a non-intrusive and language-independent log reduction framework based on eBPF (Extended Berkeley Packet Filter), consisting of both online and offline processes. After two months of serving the offline process of LogReducer in WeChat, the log storage overhead has dropped from 19.7 PB per day to 12.0 PB (i.e., about a 39.08% decrease). Practical implementation and experimental evaluations in the test environment demonstrate that the online process of LogReducer can control the logging overhead of hotspots while preserving logging effectiveness. Moreover, the log hotspot handling time can be reduced from an average of 9 days in production to 10 minutes in the test with the help of LogReducer,","Log Hotspot,eBPF,Log Reduction,Log Parsing",LogReducer：动态识别和减少内核中的日志热点,现代系统会生成大量日志来检测和诊断系统故障，这会带来昂贵的存储成本和运行时开销。在研究了真实世界的生产日志后，我们发现大部分日志开销都是由于少数日志模板造成的，这些模板被称为日志热点。因此，我们在工业系统微信中对日志热点进行了系统的研究，这激励我们识别日志热点并动态减少它们。在本文中，我们提出了LogReducer，这是一个基于eBPF（Extended Berkeley Packet Filter）的非侵入性和语言无关的日志缩减框架，由在线和离线进程组成。在微信中服务LogReducer离线进程两个月后，日志存储开销从每天19.7 PB下降到12.0 PB（即下降约39.08%）。在测试环境中的实际实现和实验评估表明，LogReducer的在线过程可以在保持日志有效性的同时控制热点的日志开销。此外，在LogReducer的帮助下，日志热点处理时间可以从生产时的平均9天减少到测试时的10分钟，,日志热点，eBPF，日志缩减，日志解析,,,
6M6JHHTD,2023,https://doi.org/10.1109/ICSE48619.2023.00171,ICSE 2023,ViolationTracker: Building Precise Histories for Static Analysis Violations,"Automatic static analysis tools (ASATs) detect source code violations to static analysis rules and are usually used as a guard for source code quality. The adoption of ASATs, however, is often challenged because of several problems such as a large number of false alarms, invalid rule priorities, and inappropriate rule configurations. Research has shown that tracking the history of the violations is a promising way to solve the above problems because the facts of violation fixing may reflect the developers' subjective expectations on the violation detection results. Precisely identifying the revisions that induce or fix a violation is however challenging because of the imprecise matching of violations between code revisions and ignorance of merge commits in the maintenance history. In this paper, we propose ViolationTracker, an approach to precisely matching the violation instances between adjacent revisions and building the life cycle of violations with the identification of inducing, fixing, deleting, and reopening of each violation case. The approach employs code entity anchoring heuristics for violation matching and considers merge commits that used to be ignored in existing research. We evaluate ViolationTracker with a manually-validated dataset that consists of 500 violation instances and 158 threads of 30 violation cases with detailed evolution history from open-source projects. Violation Tracker achieves over 93 % precision and 98 % recall on violation matching, outperforming the state-of-the-art approach, and 99.4 % precision on rebuilding the histories of violation cases. We also show that ViolationTracker is useful to identify actionable violations. A preliminary empirical study reveals the possibility to prioritize static analysis rules according to further analysis on the actionable rates of the rules.","Codes,Source coding,Buildings,Static analysis,Manuals,Maintenance engineering,History",ViolationTracker：为静态分析违规建立精确的历史记录,自动静态分析工具（ASAT）检测源代码违反静态分析规则的行为，通常用作源代码质量的保护。然而，ASAT的采用经常受到挑战，因为存在大量错误警报、无效规则优先级和不适当的规则配置等问题。研究表明，跟踪违规历史是解决上述问题的一种很有前途的方法，因为违规修复的事实可能反映了开发人员对违规检测结果的主观期望。然而，准确识别导致或修复违规的修订是具有挑战性的，因为代码修订之间的违规匹配不精确，并且在维护历史中不知道合并提交。在本文中，我们提出了ViolationTracker，这是一种精确匹配相邻修订之间的违规实例的方法，并通过识别每个违规案例的诱导、修复、删除和重新打开来构建违规的生命周期。该方法采用代码实体锚定启发式方法进行违规匹配，并考虑了在现有研究中被忽视的合并提交。我们使用手动验证的数据集来评估ViolationTracker，该数据集由500个违规实例和158个线程组成，这些线程包含30个违规案例，并具有开源项目的详细进化历史。违规跟踪器在违规匹配方面实现了超过93%的精度和98%的召回率，优于最先进的方法，在重建违规案例历史方面实现了99.4%的精度。我们还展示了ViolationTracker在识别可起诉的违规行为方面非常有用。一项初步的实证研究揭示了根据对静态分析规则的可操作率的进一步分析来确定静态分析规则优先级的可能性。,代码，源代码编码，建筑，静态分析，手册，维护工程，历史,,,
YTFBYL3L,2023,https://doi.org/10.1109/ICSE48619.2023.00122,ICSE 2023,GameRTS: A Regression Testing Framework for Video Games,"Continuous game quality assurance is of great importance to satisfy the increasing demands of users. To respond to game issues reported by users timely, game com-panies often create and maintain a large number of releases, updates, and tweaks in a short time. Regression testing is an essential technique adopted to detect regression issues during the evolution of the game software. However, due to the special characteristics of game software (e.g., frequent updates and long-running tests), traditional regression testing techniques are not directly applicable. To bridge this gap, in this paper, we perform an early exploratory study to investigate the challenges in regression testing of video games. We first performed empirical studies to better understand the game development process, bugs introduced during game evolution, and the context sensitivity. Based on the results of the study, we proposed the first regression test selection (RTS) technique for game software, which is a compromise between safety and practicality. In particular, we model the test suite of game software as a State Transition Graph (STG) and then perform the RTS on the STG. We establish the dependencies between the states/actions of STG and game files, including game art resources, game design files, and source code, and perform change impact analysis to identify the states/actions (in the STG) that potentially execute such changes. We implemented our framework in a tool, named GameRTS, and evaluated its usefulness on 10 tasks of a large-scale commercial game, including a total of 1,429 commits over three versions. The experimental results demonstrate the usefulness and effectiveness of GameRTS in game RTS. For most tasks, GameRTS only selected one trace from STG, which can significantly reduce the testing time. Furthermore, GameRTS detects all the regression bugs from the test evaluation suites. Compared with the file-level RTS, GameRTS selected fewer states/actions/traces (i.e., 13.77%, 23.97%, 6.85%). In addition, GameRTS identified 2 new critical regression bugs in the game.","Game Testing,Regression Testing,Testing Cases Selection,State Transition Graph",GameRTS：一个用于电子游戏的回归测试框架,持续的游戏质量保证对于满足用户日益增长的需求至关重要。为了及时回应用户报告的游戏问题，游戏公司通常会在短时间内创建和维护大量的发布、更新和调整。回归测试是在游戏软件进化过程中检测回归问题的一项重要技术。然而，由于游戏软件的特殊特性（如频繁更新和长时间测试），传统的回归测试技术并不直接适用。为了弥补这一差距，在本文中，我们进行了一项早期的探索性研究，以调查电子游戏回归测试中的挑战。我们首先进行了实证研究，以更好地了解游戏开发过程、游戏进化过程中引入的漏洞以及上下文敏感性。基于研究结果，我们提出了游戏软件的第一回归测试选择（RTS）技术，这是安全性和实用性之间的折衷。特别地，我们将游戏软件的测试套件建模为状态转换图（STG），然后在STG上执行RTS。我们建立了STG和游戏文件（包括游戏美术资源、游戏设计文件和源代码）的状态/动作之间的依赖关系，并执行更改影响分析，以确定可能执行此类更改的状态/行动（在STG中）。我们在一个名为GameRTS的工具中实现了我们的框架，并评估了它在大型商业游戏的10个任务中的有用性，包括三个版本的1429次提交。实验结果证明了游戏RTS在游戏RTS中的有效性和实用性。对于大多数任务，GameRTS只从STG中选择了一个跟踪，这可以显著减少测试时间。此外，GameRTS从测试评估套件中检测所有回归错误。与文件级RTS相比，GameRTS选择的状态/动作/轨迹更少（即13.77%、23.97%、6.85%）。此外，GameRTS在游戏中发现了2个新的关键回归错误。,游戏测试，回归测试，测试用例选择，状态转换图,,,
BHQ895DC,2023,https://doi.org/10.1109/ICSE48619.2023.00190,ICSE 2023,Enhancing Deep Learning-based Vulnerability Detection by Building Behavior Graph Model,"Software vulnerabilities have posed huge threats to the cyberspace security, and there is an increasing demand for automated vulnerability detection (VD). In recent years, deep learning-based (DL-based) vulnerability detection systems have been proposed for the purpose of automatic feature extraction from source code. Although these methods can achieve ideal performance on synthetic datasets, the accuracy drops a lot when detecting real-world vulnerability datasets. Moreover, these approaches limit their scopes within a single function, being not able to leverage the information between functions. In this paper, we attempt to extract the function's abstract behaviors, figure out the relationships between functions, and use this global information to assist DL-based VD to achieve higher performance. To this end, we build a Behavior Graph Model and use it to design a novel framework, namely VulBG. To examine the ability of our constructed Behavior Graph Model, we choose several existing DL-based VD models (e.g., TextCNN, ASTGRU, CodeBERT, Devign, and VulCNN) as our baseline models and conduct evaluations on two real-world datasets: the balanced $\text{FFMpeg}+\text{Qemu}$ dataset and the unbalanced $\text{Chrome} +\text{Debian}$ dataset. Experimental results indicate that VulBG enables all baseline models to detect more real vulnerabilities, thus improving the overall detection performance.","Vulnerability Detection,Behavior Graph,Deep Learning",通过建立行为图模型增强基于深度学习的漏洞检测,软件漏洞对网络空间安全构成了巨大威胁，对自动漏洞检测（VD）的需求越来越大。近年来，为了从源代码中自动提取特征，已经提出了基于深度学习（DL）的漏洞检测系统。尽管这些方法可以在合成数据集上实现理想的性能，但在检测真实世界的漏洞数据集时，准确性会下降很多。此外，这些方法将其范围限制在单个函数内，无法利用函数之间的信息。在本文中，我们试图提取函数的抽象行为，找出函数之间的关系，并利用这些全局信息来帮助基于DL的VD实现更高的性能。为此，我们建立了一个行为图模型，并用它来设计一个新的框架，即VulBG。为了检验我们构建的行为图模型的能力，我们选择了几个现有的基于DL的VD模型（例如，TextCNN、ASTGRU、CodeBERT、Devign和VulCNN）作为我们的基线模型，并在两个真实世界的数据集上进行评估：平衡的$\text{FFMpeg}+\text{Qemu}$数据集和不平衡的$_text{Chrome}+\text{Debian}$数据集中。实验结果表明，VulBG使所有基线模型都能检测到更真实的漏洞，从而提高了整体检测性能。,漏洞检测，行为图，深度学习,,,
GEEZW75Z,2023,https://doi.org/10.1109/ICSE48619.2023.00107,ICSE 2023,Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and Distribution-Aware Criterion,"Various deep neural network (DNN) coverage criteria have been proposed to assess DNN test inputs and steer input mutations. The coverage is characterized via neurons having certain outputs, or the discrepancy between neuron outputs. Nevertheless, recent research indicates that neuron coverage criteria show little correlation with test suite quality. In general, DNNs approximate distributions, by incorporating hierarchical layers, to make predictions for inputs. Thus, we champion to deduce DNN behaviors based on its approximated distributions from a layer perspective. A test suite should be assessed using its induced layer output distributions. Accordingly, to fully examine DNN behaviors, input mutation should be directed toward diversifying the approximated distributions. This paper summarizes eight design requirements for DNN coverage criteria, taking into account distribution properties and practical concerns. We then propose a new criterion, Neural Coverage (nlc),that satisfies all design requirements. NLC treats a single DNN layer as the basic computational unit (rather than a single neuron) and captures four critical properties of neuron output distributions. Thus, NL C accurately describes how DNNs comprehend inputs via approximated distributions. We demonstrate that NLC is significantly correlated with the diversity of a test suite across a number of tasks (classification and generation) and data formats (image and text). Its capacity to discover DNN prediction errors is promising. Test input mutation guided by NLC results in a greater quality and diversity of exposed erroneous behaviors.","machine learning testing,coverage",DNN测试中神经元覆盖率的修正：分层和分布感知准则,已经提出了各种深度神经网络（DNN）覆盖标准来评估DNN测试输入和引导输入突变。覆盖率通过具有特定输出的神经元或神经元输出之间的差异来表征。然而，最近的研究表明，神经元覆盖标准与测试套件的质量几乎没有相关性。通常，DNN通过结合层次层来近似分布，以对输入进行预测。因此，我们主张从层的角度基于其近似分布来推导DNN行为。测试套件应使用其诱导层输出分布进行评估。因此，为了充分检查DNN行为，输入突变应该指向使近似分布多样化。考虑到分布特性和实际问题，本文总结了DNN覆盖标准的八项设计要求。然后，我们提出了一个新的标准，神经覆盖（nlc），以满足所有的设计要求。NLC将单个DNN层作为基本计算单元（而不是单个神经元），并捕获神经元输出分布的四个关键特性。因此，NLC准确地描述了DNN如何通过近似分布来理解输入。我们证明了NLC与测试套件在许多任务（分类和生成）和数据格式（图像和文本）中的多样性显著相关。它发现DNN预测误差的能力是有希望的。NLC引导的测试输入突变导致暴露的错误行为具有更高的质量和多样性。,机器学习测试，覆盖范围,,,
TGKNYVZW,2023,https://doi.org/10.1109/ICSE48619.2023.00098,ICSE 2023,Detecting Exception Handling Bugs in C++ Programs,"Exception handling is a mechanism in modern programming languages. Studies have shown that the exception handling code is error-prone. However, there is still limited research on detecting exception handling bugs, especially for C++ programs. To tackle the issue, we try to precisely represent the exception control flow in C++ programs and propose an analysis method that makes use of the control flow to detect such bugs. More specifically, we first extend control flow graph by introducing the concepts of five different kinds of basic blocks, and then modify the classic symbolic execution framework by extending the program state to a quadruple and properly processing try, throw and catch statements. Based on the above techniques, we develop a static analysis tool on the top of Clang Static Analyzer to detect exception handling bugs. We run our tool on projects with high stars from GitHub and find 36 exception handling bugs in 8 projects, with a precision of 84%. We compare our tool with four state-of-the-art static analysis tools (Cppcheck, Clang Static Analyzer, Facebook Infer and IKOS) on projects from GitHub and handmade benchmarks. On the GitHub projects, other tools are not able to detect any exception handling bugs found by our tool. On the handmade benchmarks, our tool has a significant higher recall.","static analysis,exception handling,bug finding",检测C++程序中的异常处理错误,异常处理是现代编程语言中的一种机制。研究表明，异常处理代码容易出错。然而，关于检测异常处理错误的研究仍然有限，尤其是对于C++程序。为了解决这个问题，我们试图准确地表示C++程序中的异常控制流，并提出了一种利用控制流来检测此类错误的分析方法。更具体地说，我们首先通过引入五种不同类型的基本块的概念来扩展控制流图，然后通过将程序状态扩展到四元组并正确处理try、throw和catch语句来修改经典的符号执行框架。基于上述技术，我们在Clang静态分析器的顶部开发了一个静态分析工具来检测异常处理错误。我们在GitHub的高星项目上运行了我们的工具，在8个项目中发现了36个异常处理错误，准确率为84%。我们将我们的工具与四种最先进的静态分析工具（Cppcheck、Clang static Analyzer、Facebook Infer和IKOS）在GitHub项目和手工基准测试中进行了比较。在GitHub项目中，其他工具无法检测到我们的工具发现的任何异常处理错误。在手工制作的基准测试中，我们的工具具有显著更高的召回率。,静态分析，异常处理，错误查找,,,
499DXFW8,2023,https://doi.org/10.1109/ICSE48619.2023.00049,ICSE 2023,FedSlice: Protecting Federated Learning Models from Malicious Participants with Model Slicing,"Crowdsourcing Federated learning (CFL) is a new crowdsourcing development paradigm for the Deep Neural Network (DNN) models, also called “software 2.0”. In practice, the privacy of CFL can be compromised by many attacks, such as free-rider attacks, adversarial attacks, gradient leakage attacks, and inference attacks. Conventional defensive techniques have low efficiency because they deploy heavy encryption techniques or rely on Trusted Execution Environments (TEEs). To improve the efficiency of protecting CFL from these attacks, this paper proposes FedSlice to prevent malicious participants from getting the whole server-side model while keeping the performance goal of CFL. FedSlice breaks the server-side model into several slices and delivers one slice to each participant. Thus, a malicious participant can only get a subset of the server-side model, preventing them from effectively conducting effective attacks. We evaluate FedSlice against these attacks, and results show that FedSlice provides effective defense: the server-side model leakage is reduced from 100% to 43.45%, the success rate of adversarial attacks is reduced from 100% to 11.66%, the average accuracy of membership inference is reduced from 71.91% to 51.58%, and the data leakage from shared gradients is reduced to the level of random guesses. Besides, FedSlice only introduces less than 2% accuracy loss and about 14% computation overhead. To the best of our knowledge, this is the first paper to discuss defense methods against these attacks to the CFL framework.","Deep Neural Networks,Software Engineering,Crowdsourcing,Federated Learning",FedSlice：通过模型切片保护联合学习模型免受恶意参与者的攻击,众包联合学习（CFL）是深度神经网络（DNN）模型的一种新的众包开发模式，也称为“软件2.0”。在实践中，CFL的隐私可能会受到许多攻击，如搭便车攻击、对抗性攻击、梯度泄漏攻击和推理攻击。传统的防御技术效率较低，因为它们部署了繁重的加密技术或依赖于可信执行环境（TEE）。为了提高保护CFL免受这些攻击的效率，本文提出了FedSlice来防止恶意参与者获得整个服务器端模型，同时保持CFL的性能目标。FedSlice将服务器端模型分解为多个切片，并向每个参与者提供一个切片。因此，恶意参与者只能获得服务器端模型的一个子集，从而阻止他们有效地进行有效的攻击。我们对FedSlice针对这些攻击进行了评估，结果表明FedSlice提供了有效的防御：服务器端模型泄漏从100%降低到43.45%，对抗性攻击的成功率从100%下降到11.66%，成员推断的平均准确率从71.91%下降到51.58%，并且来自共享梯度的数据泄漏被减少到随机猜测的水平。此外，FedSlice只引入了不到2%的精度损失和大约14%的计算开销。据我们所知，这是第一篇讨论针对CFL框架的这些攻击的防御方法的论文。,深度神经网络，软件工程，众包，联合学习,,,
6N4RBSNL,2023,https://doi.org/10.1109/ICSE48619.2023.00212,ICSE 2023,Compatible Remediation on Vulnerabilities from Third-Party Libraries for Java Projects,"With the increasing disclosure of vulnerabilities in open-source software, software composition analysis (SCA) has been widely applied to reveal third-party libraries and the associated vulnerabilities in software projects. Beyond the revelation, SCA tools adopt various remediation strategies to fix vulnerabilities, the quality of which varies substantially. However, ineffective remediation could induce side effects, such as compi-lation failures, which impede acceptance by users. According to our studies, existing SCA tools could not correctly handle the concerns of users regarding the compatibility of remediated projects. To this end, we propose Compatible Remediation of Third-party libraries (CORAL) for Maven projects to fix vulnerabilities without breaking the projects. The evaluation proved that Coralnot only fixed 87.56% of vulnerabilities which outperformed other tools (best 75.32%) and achieved a 98.67% successful compilation rate and a 92.96% successful unit test rate. Furthermore, we found that 78.45% of vulnerabilities in popular Maven projects could be fixed without breaking the compilation, and the rest of the vulnerabilities (21.55%) could either be fixed by upgrades that break the compilations or even be impossible to fix by upgrading.","Remediation,Compatibility,Java,Open-source software",Java项目第三方库漏洞的兼容修复,随着开源软件中漏洞的日益暴露，软件组成分析（SCA）已被广泛应用于揭示软件项目中的第三方库和相关漏洞。除此之外，SCA工具还采用了各种补救策略来修复漏洞，这些漏洞的质量差异很大。然而，无效的补救措施可能会引发副作用，如组装失败，从而阻碍用户的接受。根据我们的研究，现有的SCA工具无法正确处理用户对补救项目兼容性的担忧。为此，我们为Maven项目提出了第三方库的兼容修复（CORAL），以在不破坏项目的情况下修复漏洞。评估证明，Coralnot只修复了87.56%的漏洞，优于其他工具（最好为75.32%），编译成功率为98.67%，单元测试成功率为92.96%。此外，我们发现流行的Maven项目中78.45%的漏洞可以在不破坏编译的情况下修复，其余的漏洞（21.55%）可以通过破坏编译的升级来修复，甚至不可能通过升级来修复。,补救，兼容性，Java，开源软件,,,
8MKQTAPR,2023,https://doi.org/10.1109/ICSE48619.2023.00046,ICSE 2023,Robustification of Behavioral Designs against Environmental Deviations,"Modern software systems are deployed in a highly dynamic, uncertain environment. Ideally, a system that is robust should be capable of establishing its most critical requirements even in the presence of possible deviations in the environment. We propose a technique called behavioral robustification, which involves systematically and rigorously improving the robustness of a design against potential deviations. Given behavioral models of a system and its environment, along with a set of user-specified deviations, our robustification method produces a redesign that is capable of satisfying a desired property even when the environment exhibits those deviations. In particular, we describe how the robustification problem can be formulated as a multi-objective optimization problem, where the goal is to restrict the deviating environment from causing a violation of a desired property, while maximizing the amount of existing functionality and minimizing the cost of changes to the original design. We demonstrate the effectiveness of our approach on case studies involving the robustness of an electronic voting machine and safety-critical interfaces.","robustness,robustification,labeled transition systems",行为设计对环境偏差的稳健性,现代软件系统部署在高度动态、不确定的环境中。理想情况下，一个稳健的系统应该能够建立其最关键的需求，即使在环境中存在可能的偏差的情况下也是如此。我们提出了一种称为行为鲁棒性的技术，该技术涉及系统地、严格地提高设计对潜在偏差的鲁棒性。给定系统及其环境的行为模型，以及一组用户指定的偏差，我们的鲁棒性方法产生了一种重新设计，即使环境表现出这些偏差，也能够满足所需的特性。特别地，我们描述了如何将鲁棒性问题公式化为多目标优化问题，其中目标是限制偏离的环境导致对所需属性的破坏，同时最大限度地增加现有功能的数量并最小化原始设计的更改成本。我们在涉及电子投票机稳健性和安全关键接口的案例研究中证明了我们的方法的有效性。,鲁棒性，鲁棒性，标记过渡系统,,,
AWK8M6YF,2023,https://doi.org/10.1109/ICSE48619.2023.00130,ICSE 2023,Faster or Slower? Performance Mystery of Python Idioms Unveiled with Empirical Evidence,"The usage of Python idioms is popular among Python developers in a formative study of 101 Python idiom performance related questions on Stack Overflow, we find that developers often get confused about the performance impact of Python idioms and use anecdotal toy code or rely on personal project experience which is often contradictory in performance outcomes. There has been no large-scale, systematic empirical evidence to reconcile these performance debates. In the paper, we create a large synthetic dataset with 24,126 pairs of non-idiomatic and functionally-equivalent idiomatic code for the nine unique Python idioms identified in [1], and reuse a large real-project dataset of 54,879 such code pairs provided in [1]. We develop a reliable performance measurement method to compare the speedup or slowdown by idiomatic code against non-idiomatic counterpart, and analyze the performance discrepancies between the synthetic and real-project code, the relationships between code features and performance changes, and the root causes of performance changes at the bytecode level. We summarize our findings as some actionable suggestions for using Python idioms.","Measurement,Codes,Systematics,Toy manufacturing industry,Reliability,Python,Synthetic data",更快还是更慢？Python习语表演之谜的实证揭示,Python习语的使用在Python开发人员中很受欢迎。在对Stack Overflow上101个Python习语性能相关问题的形成性研究中，我们发现开发人员经常对Python习语的性能影响感到困惑，并使用轶事玩具代码或依赖于个人项目经验，而这往往与性能结果相矛盾。没有大规模、系统的经验证据来调和这些绩效辩论。在本文中，我们为[1]中确定的九个独特的Python习惯用法创建了一个包含24126对非习惯用法和功能等效的习惯用法代码的大型合成数据集，并重用[1]中提供的54879对此类代码的大型真实项目数据集。我们开发了一种可靠的性能测量方法，将惯用代码与非惯用代码的加速或减速进行比较，并分析合成代码与真实项目代码之间的性能差异、代码特性与性能变化之间的关系，以及字节码级别性能变化的根本原因。我们将我们的发现总结为使用Python习语的一些可行建议。,测量，代码，系统学，玩具制造业，可靠性，Python，合成数据,,,
CZ9K8WEH,2023,https://doi.org/10.1109/ICSE48619.2023.00061,ICSE 2023,Demystifying Exploitable Bugs in Smart Contracts,"Exploitable bugs in smart contracts have caused significant monetary loss. Despite the substantial advances in smart contract bug finding, exploitable bugs and real-world attacks are still trending. In this paper we systematically investigate 516 unique real-world smart contract vulnerabilities in years 2021–2022, and study how many can be exploited by malicious users and cannot be detected by existing analysis tools. We further categorize the bugs that cannot be detected by existing tools into seven types and study their root causes, distributions, difficulties to audit, consequences, and repair strategies. For each type, we abstract them to a bug model (if possible), facilitating finding similar bugs in other contracts and future automation. We leverage the findings in auditing real world smart contracts, and so far we have been rewarded with $102,660 bug bounties for identifying 15 critical zero-day exploitable bugs, which could have caused up to $22.52 millions monetary loss if exploited.","Blockchain,Smart Contract,Vulnerability,Security,Empirical Study",揭开智能合约中可利用漏洞的神秘面纱,智能合约中的可利用漏洞造成了巨大的金钱损失。尽管在智能合约漏洞发现方面取得了重大进展，但可利用的漏洞和现实世界中的攻击仍呈趋势。在本文中，我们系统地调查了2021年至2022年516个独特的现实世界智能合约漏洞，并研究了有多少漏洞可以被恶意用户利用，而现有的分析工具无法检测到。我们将现有工具无法检测到的错误进一步分类为七种类型，并研究其根本原因、分布、审计困难、后果和修复策略。对于每种类型，我们都将它们抽象为一个bug模型（如果可能的话），以便于在其他合同和未来的自动化中找到类似的bug。我们在审计现实世界的智能合约时利用了这些发现，到目前为止，我们已经获得了102660美元的漏洞奖励，因为我们发现了15个关键的零日可利用漏洞，如果被利用，可能会造成高达2252万美元的金钱损失。,区块链，智能合约，漏洞，安全，实证研究,,,
T2TCT6NG,2023,https://doi.org/10.1109/ICSE48619.2023.00165,ICSE 2023,SeeHow: Workflow Extraction from Programming Screencasts through Action-Aware Video Analytics,"Programming screencasts (e.g., video tutorials on Youtube or live coding stream on Twitch) are important knowledge source for developers to learn programming knowledge, especially the workflow of completing a programming task. Nonetheless, the image nature of programming screencasts limits the accessibility of screencast content and the workflow embedded in it, resulting in a gap to access and interact with the content and workflow in programming screencasts. Existing non-intrusive methods are limited to extract either primitive human-computer interaction (HCI) actions or coarse-grained video fragments. In this work, we leverage Computer Vision (CV) techniques to build a programming screencast analysis tool which can automatically extract code-line editing steps (enter text, delete text, edit text and select text) from screencasts. Given a programming screencast, our approach outputs a sequence of coding steps and code snippets involved in each step, which we refer to as programming workflow. The proposed method is evaluated on 41 hours of tutorial videos and live coding screencasts with diverse programming environments. The results demonstrate our tool can extract code-line editing steps accurately and the extracted workflow steps can be intuitively understood by developers.","Screencast,Computer vision,Workflow extraction,Action recognition",请参阅如何：通过动作感知视频分析从编程屏幕截图中提取工作流,编程屏幕广播（例如，Youtube上的视频教程或Twitch上的实时编码流）是开发人员学习编程知识的重要知识来源，尤其是完成编程任务的工作流程。尽管如此，编程屏幕广播的图像性质限制了屏幕广播内容及其嵌入的工作流程的可访问性，导致在访问和与编程屏幕广播中的内容和工作流程交互方面存在差距。现有的非侵入性方法仅限于提取原始人机交互（HCI）动作或粗粒度视频片段。在这项工作中，我们利用计算机视觉（CV）技术构建了一个编程屏幕广播分析工具，该工具可以从屏幕广播中自动提取代码行编辑步骤（输入文本、删除文本、编辑文本和选择文本）。给定编程屏幕广播，我们的方法输出一系列编码步骤和每个步骤中涉及的代码片段，我们称之为编程工作流。所提出的方法在41小时的教程视频和具有不同编程环境的实时编码屏幕上进行了评估。结果表明，我们的工具可以准确地提取代码行编辑步骤，开发人员可以直观地理解提取的工作流步骤。,屏幕广播，计算机视觉，工作流提取，动作识别,,,
YUQEMU6F,2023,https://doi.org/10.1109/ICSE48619.2023.00137,ICSE 2023,Demystifying Privacy Policy of Third-Party Libraries in Mobile Apps,"The privacy of personal information has received significant attention in mobile software. Although researchers have designed methods to identify the conflict between app behavior and privacy policies, little is known about the privacy compliance issues relevant to third-party libraries (TPLs). The regulators enacted articles to regulate the usage of personal information for TPLs (e.g., the CCPA requires businesses clearly notify consumers if they share consumers' data with third parties or not). However, it remains challenging to investigate the privacy compliance issues of TPLs due to three reasons: 1) Difficulties in collecting TPLs' privacy policies. In contrast to Android apps, which are distributed through markets like Google Play and must provide privacy policies, there is no unique platform for collecting privacy policies of TPLs. 2) Difficulties in analyzing TPL's user privacy access behaviors. TPLs are mainly provided in binary files, such as jar or aar, and their whole functionalities usually cannot be executed independently without host apps. 3) Difficulties in identifying consistency between TPL's functionalities and privacy policies, and host app's privacy policy and data sharing with TPLs. This requires analyzing not only the privacy policies of TPLs and host apps but also their functionalities. In this paper, we propose an automated system named ATPChecker to analyze whether Android TPLs comply with the privacy-related regulations. We construct a data set that contains a list of 458 TPLs, 247 TPL's privacy policies, 187 TPL's binary files and 641 host apps and their privacy policies. Then, we analyze the bytecode of TPLs and host apps, design natural language processing systems to analyze privacy policies, and implement an expert system to identify TPL usage-related regulation compliance. The experimental results show that 23% TPLs violate regulation requirements for providing privacy policies. Over 47% TPLs miss disclosing data usage in their privacy policies. Over 65% host apps share user data with TPLs while 65% of them miss disclosing interactions with TPLs. Our findings remind developers to be mindful of TPL usage when developing apps or writing privacy policies to avoid violating regulations,","Privacy policy,third-party library,Android",移动应用程序中第三方图书馆隐私政策的神秘化,个人信息的隐私在移动软件中受到了极大的关注。尽管研究人员已经设计了识别应用程序行为和隐私政策之间冲突的方法，但对与第三方库（TPL）相关的隐私合规问题知之甚少。监管机构制定了条款来规范TPL的个人信息使用（例如，CCPA要求企业在是否与第三方共享消费者数据时明确通知消费者）。然而，由于三个原因，调查TPL的隐私合规问题仍然具有挑战性：1）收集TPL隐私政策的困难。与安卓应用不同，安卓应用通过Google Play等市场分发，必须提供隐私政策，但没有唯一的平台来收集TPL的隐私政策。2） 第三方物流用户隐私访问行为分析难点。TPL主要以二进制文件形式提供，如jar或aar，如果没有主机应用程序，它们的整个功能通常无法独立执行。3） 在确定TPL的功能和隐私政策、主机应用程序的隐私政策以及与TPL的数据共享之间的一致性方面存在困难。这不仅需要分析TPL和主机应用程序的隐私策略，还需要分析它们的功能。在本文中，我们提出了一个名为ATPChecker的自动化系统来分析Android TPL是否符合隐私相关法规。我们构建了一个数据集，其中包含458个TPL、247个TPL的隐私策略、187个TPL二进制文件和641个主机应用程序及其隐私策略的列表。然后，我们分析了TPL和主机应用程序的字节码，设计了自然语言处理系统来分析隐私政策，并实现了一个专家系统来识别TPL使用相关法规的合规性。实验结果表明，23%的TPL违反了提供隐私政策的法规要求。超过47%的TPL未在其隐私政策中披露数据使用情况。超过65%的主机应用程序与TPL共享用户数据，而其中65%的应用程序未披露与TPL的交互。我们的研究结果提醒开发者在开发应用程序或制定隐私政策时要注意TPL的使用，以避免违反规定，,隐私政策，第三方库，安卓,,,
IV3JARJJ,2023,https://doi.org/10.1109/ICSE48619.2023.00037,ICSE 2023,BSHUNTER: Detecting and Tracing Defects of Bitcoin Scripts,"Supporting the most popular cryptocurrency, the Bitcoin platform allows its transactions to be programmable via its scripts. Defects in Bitcoin scripts will make users lose their bitcoins. However, there are few studies on the defects of Bitcoin scripts. In this paper, we conduct the first systematic investigation on the defects of Bitcoin scripts through three steps, including defect definition, defect detection, and exploitation tracing. First, we define six typical defects of scripts in Bitcoin history, namely unbinded-txid, simple-key, useless-sig, uncertain-sig, impossible-key, and never-true. Three are inspired by the community, and three are new from us. Second, we develop a tool to discover Bitcoin scripts with any of typical defects based on symbolic execution and enhanced by historical exact scripts. By analyzing all Bitcoin transactions from Oct. 2009 to Aug. 2022, we find that 383,544 transaction outputs are paid to the Bitcoin scripts with defects. The total amount of them is 3,115.43 BTC, which is around 60 million dollars at present. Third, in order to trace the exploitation of the defects, we instrument the Bitcoin VM to record the traces of the real-world spending transactions of the buggy scripts. We find that 84,130 output scripts are exploited. The implementation and non-harmful datasets are released.","bitcoin,blockchain,smart contract",BSHUNTER：检测和追踪比特币脚本的缺陷,比特币平台支持最流行的加密货币，允许通过其脚本对其交易进行编程。比特币脚本中的缺陷会让用户丢失他们的比特币。然而，关于比特币脚本缺陷的研究却很少。在本文中，我们通过缺陷定义、缺陷检测和利用追踪三个步骤，首次对比特币脚本的缺陷进行了系统的调查。首先，我们定义了比特币历史上脚本的六个典型缺陷，即无绑定txid、简单密钥、无用sig、不确定sig、无法实现密钥和从不为真。三个来自社区，三个来自我们。第二，我们开发了一种工具，可以基于符号执行发现具有任何典型缺陷的比特币脚本，并通过历史精确脚本进行增强。通过分析2009年10月至2022年8月的所有比特币交易，我们发现383544笔交易输出被支付给了有缺陷的比特币脚本。它们的总金额为3115.43 BTC，目前约为6000万美元。第三，为了跟踪缺陷的利用，我们对比特币虚拟机进行了测试，以记录缺陷脚本的真实支出交易的痕迹。我们发现84130个输出脚本被利用。发布了实施和无害数据集。,比特币，区块链，智能合约,,,
V4K33S94,2023,https://doi.org/10.1109/ICSE48619.2023.00036,ICSE 2023,Turn the Rudder: A Beacon of Reentrancy Detection for Smart Contracts on Ethereum,"Smart contracts are programs deployed on a blockchain and are immutable once deployed. Reentrancy, one of the most important vulnerabilities in smart contracts, has caused millions of dollars in financial loss. Many reentrancy detection approaches have been proposed. It is necessary to investigate the performance of these approaches to provide useful guidelines for their application. In this work, we conduct a large-scale empirical study on the capability of five well-known or recent reentrancy detection tools such as Mythril and Sailfish. We collect 230,548 verified smart contracts from Etherscan and use detection tools to analyze 139,424 contracts after deduplication, which results in 21,212 contracts with reentrancy issues. Then, we manually examine the defective functions located by the tools in the contracts. From the examination results, we obtain 34 true positive contracts with reentrancy and 21,178 false positive contracts without reentrancy. We also analyze the causes of the true and false positives. Finally, we evaluate the tools based on the two kinds of contracts. The results show that more than 99.8% of the reentrant contracts detected by the tools are false positives with eight types of causes, and the tools can only detect the reentrancy issues caused by call.value(), 58.8% of which can be revealed by the Ethereum's official IDE, Remix. Furthermore, we collect real-world reentrancy attacks reported in the past two years and find that the tools fail to find any issues in the corresponding contracts. Based on the findings, existing works on reentrancy detection appear to have very limited capability, and researchers should turn the rudder to discover and detect new reentrancy patterns except those related to call.value().","Smart contract,Reentrancy,Empirical study",转向舵：以太坊智能合约再入检测的灯塔,智能合约是部署在区块链上的程序，一旦部署就不可变。重入是智能合约中最重要的漏洞之一，已造成数百万美元的财务损失。已经提出了许多重入检测方法。有必要调查这些方法的性能，为其应用提供有用的指导方针。在这项工作中，我们对Mythril和Sailfish等五种知名或最新的再入检测工具的能力进行了大规模的实证研究。我们从Etherscan收集了230548个经过验证的智能合约，并使用检测工具分析了重复数据消除后的139424个合约，这导致21212个合约存在可重入性问题。然后，我们手动检查由合同中的工具定位的缺陷函数。从检验结果中，我们获得了34个具有可重入性的真阳性契约和21178个不具有可重进性的假阳性契约。我们还分析了真阳性和假阳性的原因。最后，我们基于这两种合同对工具进行了评估。结果表明，该工具检测到的可重入合约中，超过99.8%是由八种原因引起的误报，并且该工具只能检测到由call.value（）引起的可重进入问题，其中58.8%可以由以太坊的官方IDE Remix揭示。此外，我们收集了过去两年中报告的真实世界的可重入攻击，发现这些工具在相应的合同中没有发现任何问题。基于这些发现，现有的重入检测工作似乎能力非常有限，研究人员应该转向发现和检测新的重入模式，但与call.value（）相关的模式除外。,智能合约，再进入，实证研究,,,
VG6TJYQZ,2023,https://doi.org/10.1109/ICSE48619.2023.00214,ICSE 2023,CoLeFunDa: Explainable Silent Vulnerability Fix Identification,"It is common practice for OSS users to leverage and monitor security advisories to discover newly disclosed OSS vulnerabilities and their corresponding patches for vulnerability remediation. It is common for vulnerability fixes to be publicly available one week earlier than their disclosure. This gap in time provides an opportunity for attackers to exploit the vulnerability. Hence, OSS users need to sense the fix as early as possible so that the vulnerability can be remediated before it is exploited. However, it is common for OSS to adopt a vulnerability disclosure policy which causes the majority of vulnerabilities to be fixed silently, meaning the commit with the fix does not indicate any vulnerability information. In this case even if a fix is identified, it is hard for OSS users to understand the vulnerability and evaluate its potential impact. To improve early sensing of vulnerabilities, the identification of silent fixes and their corresponding explanations (e.g., the corresponding common weakness enumeration (CWE) and exploitability rating) are equally important. However, it is challenging to identify silent fixes and provide explanations due to the limited and diverse data. To tackle this challenge, we propose CoLeFunDa: a framework consisting of a Contrastive Learner and FunDa, which is a novel approach for Function change Data augmentation. FunDa first increases the fix data (i.e., code changes) at the function level with unsupervised and supervised strategies. Then the contrastive learner leverages contrastive learning to effectively train a function change encoder, FCBERT, from diverse fix data. Finally, we leverage FCBERT to further fine-tune three downstream tasks, i.e., silent fix identification, CWE category classification, and exploitability rating classification, respectively. Our result shows that CoLeFunDa outperforms all the state-of-art baselines in all downstream tasks. We also conduct a survey to verify the effectiveness of CoLeFunDa in practical usage. The result shows that CoLeFunDa can categorize 62.5% (25 out of 40) CVEs with correct CWE categories within the top 2 recommendations.","OSS Vulnerabilities,Contrastive Learning",CoLeFunDa：可解释的静默漏洞修复标识,OSS用户通常会利用和监控安全咨询来发现新披露的OSS漏洞及其相应的补丁，以进行漏洞修复。漏洞修复通常会在披露前一周公开。这种时间间隔为攻击者利用该漏洞提供了机会。因此，OSS用户需要尽早检测到修复，以便在漏洞被利用之前对其进行补救。然而，OSS通常采用漏洞公开策略，导致大多数漏洞被静默修复，这意味着修复后的提交不会显示任何漏洞信息。在这种情况下，即使确定了修复，OSS用户也很难理解该漏洞并评估其潜在影响。为了提高对漏洞的早期感知，识别静默修复及其相应的解释（例如，相应的常见弱点列举（CWE）和可利用性评级）同样重要。然而，由于数据有限且多样，确定静默修复并提供解释是一项挑战。为了应对这一挑战，我们提出了CoLeFunDa：一个由对比学习者和FunDa组成的框架，这是一种新的函数变化数据扩充方法。FunDa首先使用无监督和有监督的策略在函数级别增加修复数据（即代码更改）。然后，对比学习器利用对比学习从不同的固定数据中有效地训练函数变化编码器FCBERT。最后，我们利用FCBERT进一步微调三个下游任务，分别是静默修复识别、CWE类别分类和可利用性评级分类。我们的结果表明，CoLeFunDa在所有下游任务中都优于所有最先进的基线。我们还进行了一项调查，以验证CoLeFunDa在实际使用中的有效性。结果表明，CoLeFunDa可以在前2个推荐中对62.5%（40个中的25个）具有正确CWE类别的CVE进行分类。,OSS漏洞，对比学习,,,
MJRFT9VL,2023,https://doi.org/10.1109/ICSE48619.2023.00012,ICSE 2023,Software Engineering as the Linchpin of Responsible AI,"From humanity's existential risks to safety risks in critical systems to ethical risks, responsible AI, as the saviour, has become a major research challenge with significant real-world consequences. However, achieving responsible AI remains elusive despite the plethora of high-level ethical principles, risk frameworks and progress in algorithmic assurance. In the meantime, software engineering (SE) is being upended by AI, grappling with building system-level quality and alignment from inscrutable machine learning models and code generated from natural language prompts. The upending poses new challenges and opportunities for engineering AI systems responsibly. This talk will share our experiences in helping the industry achieve responsible AI systems by inventing new SE approaches. It will dive into industry challenges (such as risk silos and principle-algorithm gaps) and research challenges (such as lack of requirements, emerging properties and inscrutable systems) and make the point that SE is the linchpin of responsible AI. But SE also requires some fundamental rethinking - shifting from building functions into AI systems to discovering and managing emerging functions from AI systems. Only by doing so can SE take on critical new roles, from understanding human intelligence to building a thriving human-AI symbiosis.","Responsible AI,Ethical AI,Trustworthy AI,AI Engineering,SE4AI",软件工程是负责任人工智能的核心,从人类的生存风险到关键系统的安全风险，再到道德风险，负责任的人工智能作为救世主，已经成为一项重大的研究挑战，并在现实世界中产生重大影响。然而，尽管有过多的高级道德原则、风险框架和算法保证方面的进展，但实现负责任的人工智能仍然难以捉摸。与此同时，软件工程（SE）正被人工智能颠覆，努力从难以理解的机器学习模型和自然语言提示生成的代码中构建系统级的质量和一致性。这种颠覆为负责任地设计人工智能系统带来了新的挑战和机遇。本次演讲将分享我们通过发明新的SE方法帮助行业实现负责任的人工智能系统的经验。它将深入探讨行业挑战（如风险竖井和原理算法差距）和研究挑战（如缺乏需求、新兴特性和难以理解的系统），并指出SE是负责任的人工智能的关键。但SE也需要一些根本性的反思——从构建功能到人工智能系统，再到发现和管理人工智能系统中的新兴功能。只有这样，SE才能扮演关键的新角色，从理解人类智能到构建繁荣的人类-人工智能共生关系。,负责任人工智能，道德人工智能，值得信赖的人工智能，人工智能工程，SE4AI,,,
RPCYHGX5,2023,https://doi.org/10.1109/ICSE48619.2023.00118,ICSE 2023,Tolerate Control-Flow Changes for Sound Data Race Prediction,"Data races seriously threaten the correctness of concurrent programs. Earlier works can report false positives. Recently, trace-based predictive analysis has achieved sound results by inferring feasible traces based on sound partial orders or constraint solvers. However, they hold the same assumption: any read event may affect the control-flow of a predicted trace. Thus, being control-flow sensitive, they have to enforce any read event (in an inferred trace) to either read the same value or a value from the same event as that in the original trace, albeit some slightly relax this. This (even with relaxation) severely limits their predictive ability and many true data races can be missed. We introduce the concept of Fix-Point Event and propose a new partial order model. This allows us to not only predict races with witness traces (like existing works with no control-flow changes) but also soundly infer existences of witness traces with potential control-flow changes. Thus, we can achieve a higher concurrency coverage and detect more data races soundly. We have implemented above as a tool ToccRACE and conducted a set of experiments on a benchmark of seven real-world programs and a large-scale software MySQL, where MySQL produced 427 traces with a total size of 3.4TB. Compared with the state-of-the-art sound data race detector SeqCheck,ToccRACE is significantly more effective by detecting 84.4%/200% more unique/dynamic races on the benchmark programs and 52.22%/49.8% more unique/dynamic races on MySQL, incurring reasonable time and memory costs (about 1.1x×43.5x on the benchmark programs and 10x/1.03x on MySQL). Furthermore, ToccRACE is sound and is comnlcte on two threads.","Concurrency bugs,data races,control flow,static information",用于声音数据竞赛预测的容忍控制流变化,数据竞赛严重威胁并发程序的正确性。早期的工作可能会报告误报。最近，基于轨迹的预测分析通过基于合理偏序或约束求解器推断可行轨迹，取得了良好的结果。然而，他们持有相同的假设：任何读取事件都可能影响预测跟踪的控制流。因此，由于对控制流敏感，它们必须强制任何读取事件（在推断的跟踪中）读取与原始跟踪中相同的值或来自相同事件的值，尽管有些稍微放松了这一点。这（即使是放松）严重限制了他们的预测能力，许多真实的数据竞赛可能会被错过。我们引入了不动点事件的概念，并提出了一个新的偏序模型。这使我们不仅能够预测有见证痕迹的种族（就像没有控制流变化的现有作品一样），而且能够可靠地推断出有潜在控制流变化见证痕迹的存在。因此，我们可以实现更高的并发覆盖率，并可靠地检测更多的数据竞赛。我们已经将上述作为ToccRACE工具进行了实现，并在七个真实世界程序和一个大型软件MySQL的基准上进行了一组实验，MySQL产生了427个轨迹，总大小为3.4TB。与最先进的声音数据竞赛检测器SeqCheck相比，ToccRACE在基准程序上检测到84.4%/200%的唯一/动态竞争，在MySQL上检测到52.22%/49.8%的唯一/动力学竞争，从而显著提高了效率，从而产生了合理的时间和内存成本（基准程序上约为1.1x×43.5x，MySQL上约为10x/1.03x）。此外，ToccRACE是健全的，并且由两个线程组成。,并发错误，数据竞赛，控制流，静态信息,,,
SQLGVMDG,2023,https://doi.org/10.1109/ICSE48619.2023.00195,ICSE 2023,On the Reproducibility of Software Defect Datasets,"Software defect datasets are crucial to facilitating the evaluation and comparison of techniques in fields such as fault localization, test generation, and automated program repair. However, the reproducibility of software defect artifacts is not immune to breakage. In this paper, we conduct a study on the reproducibility of software defect artifacts. First, we study five state-of-the-art Java defect datasets. Despite the multiple strategies applied by dataset maintainers to ensure reproducibility, all datasets are prone to breakages. Second, we conduct a case study in which we systematically test the reproducibility of 1,795 software artifacts during a 13-month period. We find that 62.6% of the artifacts break at least once, and 15.3% artifacts break multiple times. We manually investigate the root causes of breakages and handcraft 10 patches, which are automatically applied to 1,055 distinct artifacts in 2,948 fixes. Based on the nature of the root causes, we propose automated dependency caching and artifact isolation to prevent further breakage. In particular, we show that isolating artifacts to eliminate external dependencies increases reproducibility to 95% or higher, which is on par with the level of reproducibility exhibited by the most reliable manually curated dataset.","software reproducibility,software defects,software maintenance,software quality",软件缺陷数据集的可再现性,软件缺陷数据集对于促进故障定位、测试生成和自动程序修复等领域技术的评估和比较至关重要。然而，软件缺陷伪影的再现性也不能免受破坏。在本文中，我们对软件缺陷伪影的再现性进行了研究。首先，我们研究了五个最先进的Java缺陷数据集。尽管数据集维护者采用了多种策略来确保再现性，但所有数据集都容易损坏。其次，我们进行了一个案例研究，在13个月的时间里，我们系统地测试了1795个软件工件的再现性。我们发现62.6%的伪影至少破裂一次，15.3%的伪影破裂多次。我们手动调查损坏的根本原因，并手工制作了10个补丁，这些补丁在2948个修复中自动应用于1055个不同的工件。基于根本原因的性质，我们提出了自动依赖缓存和工件隔离，以防止进一步的破坏。特别是，我们发现，隔离伪影以消除外部依赖性可将再现性提高到95%或更高，这与最可靠的手动策划数据集所表现出的再现性水平持平。,软件再现性，软件缺陷，软件维护，软件质量,,,
V7UCD6C6,2023,https://doi.org/10.1109/ICSE48619.2023.00126,ICSE 2023,Tare: Type-Aware Neural Program Repair,"Automated program repair (APR) aims to reduce the effort of software development. With the development of deep learning, lots of DL-based APR approaches have been proposed using an encoder-decoder architecture. Despite the promising performance, these models share the same limitation: generating lots of untypable patches. The main reason for this phenomenon is that the existing models do not consider the constraints of code captured by a set of typing rules. In this paper, we propose, Tare, a type-aware model for neural program repair to learn the typing rules. To encode an individual typing rule, we introduce three novel components: (1) a novel type of grammars, T-Grammar, that integrates the type information into a standard grammar, (2) a novel representation of code, T-Graph, that integrates the key information needed for type checking an AST, and (3) a novel type-aware neural program repair approach, Tare, that encodes the T-Graph and generates the patches guided by T-Grammar. The experiment was conducted on three benchmarks, 393 bugs from Defects4J v1.2, 444 additional bugs from Defects4J v2.0, and 40 bugs from QuixBugs. Our results show that Tare repairs 62, 32, and 27 bugs on these benchmarks respectively, and outperforms the existing APR approaches on all benchmarks. Further analysis also shows that Tare tends to generate more compilable patches than the existing DL-based APR approaches with the typing rule information.","program repair,neural networks",Tare：类型感知神经程序修复,自动程序修复（APR）旨在减少软件开发的工作量。随着深度学习的发展，已经提出了许多使用编码器-解码器架构的基于DL的APR方法。尽管性能很好，但这些模型都有相同的局限性：生成大量不可类型的补丁。出现这种现象的主要原因是，现有的模型没有考虑由一组类型规则捕获的代码的约束。在本文中，我们提出了Tare，一个用于神经程序修复的类型感知模型来学习类型规则。为了对单个类型规则进行编码，我们引入了三个新颖的组件：（1）一种新颖的语法类型T-Grammar，它将类型信息集成到标准语法中；（2）一种新的代码表示T-Graph，它集成了AST类型检查所需的关键信息，其对T-图进行编码并生成由T-语法引导的补丁。该实验在三个基准上进行，分别是Defects4J v1.2中的393个bug、Defects4Jv2.0中的444个额外bug和QuixBugs中的40个bug。我们的结果表明，Tare在这些基准测试中分别修复了62个、32个和27个错误，并且在所有基准测试中都优于现有的APR方法。进一步的分析还表明，Tare倾向于生成比现有的具有键入规则信息的基于DL的APR方法更多的可编译补丁。,程序修复，神经网络,,,
LTK3I8KI,2022,https://doi.org/10.1145/3540250.3549080,ESEC/FSE 2022,SEDiff: scope-aware differential fuzzing to test internal function models in symbolic execution,"Symbolic execution has become a foundational program analysis technique.  
Performing symbolic execution unavoidably encounters internal functions (e.g., library functions) that provide basic operations such as string processing.  
Many symbolic execution engines construct internal function models that abstract function behaviors for scalability and compatibility concerns.  
Due to the high complexity of constructing the models,  
developers intentionally summarize only partial behaviors of a function, namely modeled functionalities, in the models.  
The correctness of the internal function models is critical because  
it would impact all applications of symbolic execution, e.g., bug detection and model checking.  
","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Formal methods,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",SEDiff:范围感知差分模糊，用于测试符号执行中的内部函数模型,符号执行已经成为一种基础的程序分析技术。,安全与隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和验证，正式软件验证，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，形式化方法，计算理论，语义和推理，程序推理，程序验证,,,
4DHYPUJ2,2022,https://doi.org/10.1145/3540250.3549158,ESEC/FSE 2022,An exploratory study on the predominant programming paradigms in Python code,"Python is a multi-paradigm programming language that fully supports object-oriented (OO) programming. The language allows writing code in a non-procedural imperative manner, using procedures, using classes, or in a functional style. To date, no one has studied what paradigm(s), if any, are predominant in Python code and projects. In this work, we first define a technique to classify Python files into predominant paradigm(s). We then automate our approach and evaluate it against human judgements, showing over 80% agreement. We then analyze over 100k open-source Python projects, automatically classifying each source file and investigating the paradigm distributions. The results indicate Python developers tend to heavily favor OO features. We also observed a positive correlation between OO and procedural paradigms and the size of the project. And despite few files or projects being predominantly functional, we still found many functional feature uses.","Social and professional topics,Professional topics,Software and its engineering,Software creation and management,Software development techniques,Software notations and tools,General programming languages,Language types,Functional languages,Imperative languages,Multiparadigm languages,Object oriented languages",Python代码中主要编程范式的探索性研究,Python是一种完全支持面向对象（OO）编程的多范式编程语言。该语言允许以非过程命令式、使用过程、使用类或函数式的方式编写代码。到目前为止，还没有人研究过Python代码和项目中占主导地位的范式（如果有的话）。在这项工作中，我们首先定义了一种将Python文件分类为主要范式的技术。然后，我们将我们的方法自动化，并根据人类的判断进行评估，显示出超过80%的一致性。然后，我们分析了超过10万个开源Python项目，自动对每个源文件进行分类，并调查了范式分布。结果表明Python开发人员倾向于大力支持OO特性。我们还观察到OO和过程范式与项目规模之间存在正相关关系。尽管很少有文件或项目主要是功能性的，但我们仍然发现了许多功能性特性的用途。,社会和专业主题，专业主题，软件及其工程，软件创建和管理，软件开发技术，软件符号和工具，通用编程语言，语言类型，函数式语言，命令式语言，多范例语言，面向对象语言,,,
SS3H5APH,2022,https://doi.org/10.1145/3540250.3569448,ESEC/FSE 2022,Performing large-scale mining studies: from start to finish (tutorial),"Modern software engineering research often relies on mining open-source software repositories, to either provide motivation for their research problems and/or evaluation of the proposed approach. Mining ultra-large-scale software repositories is still a difficult task, requiring substantial expertise and access to significant hardware. Tools such as Boa can help researchers easily mine large numbers of open-source repositories. There has also recently been more of a push toward open science, with an emphasis on making replication packages available. Building such replication packages incurs additional workload for researchers. In this tutorial, we teach how to use the Boa infrastructure for mining software repository data. We leverage Boa’s VS Code IDE extension to help write and submit Boa queries, and also leverage Boa’s study template to show how researchers can more easily analyze the output from Boa and automatically produce a suitable replication package that is published on Zenodo.","Human-centered computing,Collaborative and social computing,Collaborative and social computing systems and tools,Open source software,Information systems,Information systems applications,Data mining,Social and professional topics,Professional topics,Software and its engineering,Software creation and management,Software notations and tools,Context specific languages,Domain specific languages,Software configuration management and version control systems",执行大规模挖掘研究：从头至尾（教程）,现代软件工程研究通常依赖于挖掘开源软件存储库，为其研究问题提供动力和/或评估所提出的方法。挖掘超大型软件存储库仍然是一项艰巨的任务，需要大量的专业知识和对重要硬件的访问。Boa等工具可以帮助研究人员轻松挖掘大量开源存储库。最近，开放科学也得到了更多的推动，重点是提供复制包。构建这样的复制包会给研究人员带来额外的工作量。在本教程中，我们将教授如何使用Boa基础设施来挖掘软件存储库数据。我们利用Boa的VS Code IDE扩展来帮助编写和提交Boa查询，还利用Boa研究模板来展示研究人员如何更容易地分析Boa的输出，并自动生成在Zenodo上发布的合适的复制包。,以人为中心的计算，协作和社会计算，协作和社会计算系统和工具，开源软件，信息系统，信息系统应用，数据挖掘，社会和专业主题，专业主题，软件及其工程，软件创建和管理，软件符号和工具，上下文特定语言，领域特定语言，软件配置管理和版本控制系统,,,
IJAKTTCV,2022,https://doi.org/10.1145/3540250.3569450,ESEC/FSE 2022,"Dynamic data race prediction: fundamentals, theory, and practice (tutorial)","Data races are the most common concurrency bugs and considerable efforts are put in ensuring data-race-free (DRF) programs. The most popular approach is via dynamic analyses, which soundly report DRF violations by analyzing program executions. Recently, there has been a prevalent shift to predictive analysis techniques. Such techniques attempt to predict DRF violations even in unobserved program executions, while making sure that the analysis is sound (does not raise false positives).  
","Computing methodologies,Concurrent computing methodologies,General and reference,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software defect analysis,Software testing and debugging,Software notations and tools,Theory of computation,Semantics and reasoning,Program reasoning",动态数据竞赛预测：基础、理论和实践（教程）,数据竞赛是最常见的并发错误，在确保无数据竞赛（DRF）程序方面付出了相当大的努力。最流行的方法是通过动态分析，通过分析程序执行来可靠地报告DRF违规行为。最近，人们普遍转向预测分析技术。这种技术试图预测DRF违规，即使在未观察到的程序执行中也是如此，同时确保分析是正确的（不会产生误报）。,计算方法论，并行计算方法论，概论和参考文献，软件及其工程，软件创建和管理，软件验证和确认，正式软件验证，软件缺陷分析，软件测试和调试，软件符号和工具，计算理论，语义和推理，程序推理,,,
WAB5R6QJ,2022,https://doi.org/10.1145/3540250.3549172,ESEC/FSE 2022,A retrospective study of one decade of artifact evaluations,"Most software engineering research involves the development of a prototype, a proof of concept, or a measurement apparatus. Together with the data collected in the research process, they are collectively referred to as research artifacts and are subject to artifact evaluation (AE) at scientific conferences. Since its initiation in the SE community at ESEC/FSE 2011, both the goals and the process of AE have evolved and today expectations towards AE are strongly linked with reproducible research results and reusable tools that other researchers can build their work on. However, to date little evidence has been provided that artifacts which have passed AE actually live up to these high expectations, i.e., to which degree AE processes contribute to AE's goals and whether the overhead they impose is justified.  
","General and reference,Cross-computing tools and techniques,Empirical studies,Information systems,Information systems applications,Digital libraries and archives,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software development process management,Software post-development issues,Software verification and validation,Empirical software validation",十年伪影评估的回顾性研究,大多数软件工程研究涉及原型、概念验证或测量仪器的开发。与研究过程中收集的数据一起，它们被统称为研究人工制品，并在科学会议上接受人工制品评估（AE）。自2011年ESEC/FSE在SE社区启动以来，AE的目标和过程都发生了变化，今天对AE的期望与可重复的研究结果和其他研究人员可以建立工作的可重复使用的工具密切相关。然而，到目前为止，几乎没有证据表明通过AE的工件实际上达到了这些高期望，即AE过程在多大程度上有助于AE的目标以及它们施加的开销是否合理。,概述和参考，交叉计算工具和技术，经验研究，信息系统，信息系统应用，数字图书馆和档案馆，社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件开发过程管理，软件开发后问题，软件验证和确认，经验软件确认,,,
B5253LIS,2022,https://doi.org/10.1145/3540250.3558923,ESEC/FSE 2022,SolSEE: a source-level symbolic execution engine for solidity,"Most of the existing smart contract symbolic execution tools perform analysis on bytecode, which loses high-level semantic information presented in source code. This makes interactive analysis tasks—such as visualization and debugging—extremely challenging, and significantly limits the tool usability. In this paper, we present SolSEE, a source-level symbolic execution engine for Solidity smart contracts. We describe the design of SolSEE, highlight its key features, and demonstrate its usages through a Web-based user interface. SolSEE demonstrates advantages over other existing source-level analysis tools in the advanced Solidity language features it supports and analysis flexibility. A demonstration video is available at: https://sites.google.com/view/solsee/.","Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software notations and tools,Development frameworks and environments,Software organization and properties,Software functional properties,Theory of computation,Semantics and reasoning,Program reasoning",SolSEE：用于solidity的源代码级符号执行引擎,现有的大多数智能合约符号执行工具都对字节码进行分析，这会丢失源代码中呈现的高级语义信息。这使得交互式分析任务（如可视化和调试）极具挑战性，并大大限制了工具的可用性。在本文中，我们介绍了SolSEE，一个用于Solidity智能合约的源代码级符号执行引擎。我们描述了SolSEE的设计，强调了它的主要功能，并通过基于Web的用户界面演示了它的用法。SolSEE在其支持的高级Solidity语言功能和分析灵活性方面展示了与其他现有源代码级分析工具相比的优势。演示视频位于：https://sites.google.com/view/solsee/.,软件及其工程，软件创建和管理，软件验证和确认，正式软件验证，软件符号和工具，开发框架和环境，软件组织和属性，软件功能属性，计算理论，语义和推理，程序推理,,,
7CINJF82,2022,https://doi.org/10.1145/3540250.3549151,ESEC/FSE 2022,Semi-supervised pre-processing for learning-based traceability framework on real-world software projects,"The traceability of software artifacts has been recognized as an important factor to support various activities in software development processes. However, traceability can be difficult and time-consuming to create and maintain manually, thereby automated approaches have gained much attention. Unfortunately, existing automated approaches for traceability suffer from practical issues. This paper aims to gain an understanding of the potential challenges for the underperforming of the state-of-the-art, ML-based trace link classifiers applied in real-world projects. By investigating different industrial datasets, we found that two critical (and classic) challenges, i.e. data imbalance and sparse problems, lie in real-world projects’ traceability automation. To overcome these challenges, we developed a framework called SPLINT to incorporate hybrid textual similarity measures and semi-supervised learning strategies as enhancements to the learning-based traceability approaches. We carried out experiments with six open-source platforms and ten industry datasets. The results confirm that SPLINT is able to operate at higher performance on two communities’ datasets. Specifically, the industrial datasets, which significantly suffer from data imbalance and sparsity problems, show an increase in F2-score over 14% and AUC over 8% on average. The adjusted class-balancing and self-training policies used in SPLINT (CBST-Adjust) also work effectively for the selection of pseudo-labels on minor classes from unlabeled trace sets, demonstrating SPLINT’s practicability.","Computing methodologies,Machine learning,Learning paradigms,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software verification and validation,Software notations and tools,Software configuration management and version control systems",基于学习的可追溯性框架在实际软件项目中的半监督预处理,软件工件的可追溯性已被公认为支持软件开发过程中各种活动的重要因素。然而，手动创建和维护可追溯性可能既困难又耗时，因此自动化方法受到了广泛关注。不幸的是，现有的可追溯性自动化方法存在实际问题。本文旨在了解在现实世界项目中应用的最先进的基于ML的跟踪链接分类器表现不佳的潜在挑战。通过调查不同的工业数据集，我们发现现实世界项目的可追溯性自动化存在两个关键（也是经典）挑战，即数据不平衡和稀疏问题。为了克服这些挑战，我们开发了一个名为SPLINT的框架，将混合文本相似性度量和半监督学习策略作为对基于学习的可追溯性方法的增强。我们用六个开源平台和十个行业数据集进行了实验。结果证实，SPLINT能够在两个社区的数据集上以更高的性能运行。具体而言，工业数据集明显存在数据不平衡和稀疏性问题，其F2得分平均增加了14%以上，AUC平均增加了8%以上。SPLINT（CBST-Adjust）中使用的调整后的类平衡和自训练策略也有效地从未标记的跟踪集中选择小类上的伪标签，证明了SPLINT的实用性。,计算方法，机器学习，学习范式，社会和专业主题，专业主题，计算和信息系统的管理，软件及其工程，软件创建和管理，软件开发后问题，维护软件，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统,,,
SP9JYSTJ,2022,https://doi.org/10.1145/3540250.3558951,ESEC/FSE 2022,Trace analysis based microservice architecture measurement,"Microservice architecture design highly relies on expert experience and may often result in improper service decomposition. Moreover, a microservice architecture is likely to degrade with the continuous evolution of services. Architecture measurement is thus important for the long-term evolution of microservice architectures. Due to the independent and dynamic nature of services, source code analysis based approaches cannot well capture the interactions between services. In this paper, we propose a trace analysis based microservice architecture measurement approach. We define a trace data model for microservice architecture measurement, which enables fine-grained analysis of the execution processes of requests and the interactions between interfaces and services. Based on the data model, we define 14 architectural metrics to measure the service independence and invocation chain complexity of a microservice system. We implement the approach and conduct three case studies with a student course project, an open-source microservice benchmark system, and three industrial microservice systems. The results show that our approach can well characterize the independence and invocation chain complexity of microservice architectures and help developers to identify microservice architecture issues caused by improper service decomposition and architecture degradation.","General and reference,Cross-computing tools and techniques,Metrics,Performance,Social and professional topics,Professional topics,Software and its engineering,Software creation and management,Designing software,Software design tradeoffs,Software organization and properties,Software system structures,Distributed systems organizing principles,Cloud computing,Software architectures",基于跟踪分析的微服务架构测量,微服务架构设计高度依赖于专家经验，并且可能导致不适当的服务分解。此外，微服务架构可能会随着服务的不断发展而退化。因此，架构测量对于微服务架构的长期发展非常重要。由于服务的独立性和动态性，基于源代码分析的方法无法很好地捕捉服务之间的交互。在本文中，我们提出了一种基于跟踪分析的微服务架构测量方法。我们为微服务架构测量定义了一个跟踪数据模型，该模型能够对请求的执行过程以及接口和服务之间的交互进行细粒度分析。基于数据模型，我们定义了14个体系结构指标来衡量微服务系统的服务独立性和调用链复杂性。我们实施了该方法，并对一个学生课程项目、一个开源微服务基准系统和三个工业微服务系统进行了三个案例研究。结果表明，我们的方法可以很好地刻画微服务架构的独立性和调用链复杂性，并帮助开发人员识别由不当的服务分解和架构退化引起的微服务架构问题。,一般和参考，交叉计算工具和技术，度量，性能，社会和专业主题，专业主题，软件及其工程，软件创建和管理，设计软件，软件设计权衡，软件组织和属性，软件系统结构，分布式系统组织原则，云计算，软件体系结构,,,
WFQRQACZ,2022,https://doi.org/10.1145/3540250.3558930,ESEC/FSE 2022,JSIMutate: understanding performance results through mutations,"Understanding the performance characteristics of software systems is particular relevant when looking at design alternatives. However, it is a very challenging problem, due to the complexity of interpreting the role and incidence of the different system elements on performance metrics of interest, such as system response time or resources utilisation. This work introduces JSIMutate, a tool that makes use of queueing network performance models and enables the analysis of mutations of a model reflecting possible design changes to support designers in identifying the model elements that contribute to improving or worsening the system's performance.","Computing methodologies,Modeling and simulation,General and reference,Cross-computing tools and techniques,Metrics,Performance,Social and professional topics,Software and its engineering,Software creation and management,Software verification and validation,Software organization and properties,Extra-functional properties,Software performance",JSIMutate：通过突变了解性能结果,在考虑设计备选方案时，了解软件系统的性能特征尤其重要。然而，由于解释不同系统元素在感兴趣的性能指标（如系统响应时间或资源利用率）上的作用和发生率的复杂性，这是一个非常具有挑战性的问题。这项工作介绍了JSIMutate，这是一种利用排队网络性能模型的工具，能够分析反映可能的设计变化的模型的突变，以支持设计者识别有助于改善或恶化系统性能的模型元素。,计算方法，建模和模拟，通则和参考，交叉计算工具和技术，度量，性能，社会和专业主题，软件及其工程，软件创建和管理，软件验证和确认，软件组织和属性，额外功能属性，软件性能,,,
P7K66J7S,2022,https://doi.org/10.1145/3540250.3549145,ESEC/FSE 2022,Are we building on the rock? on the importance of data preprocessing for code summarization,"Code summarization, the task of generating useful comments given the code, has long been of interest. Most of the existing code summarization models are trained and validated on widely-used code comment benchmark datasets. However, little is known about the quality of the benchmark datasets built from real-world projects. Are the benchmark datasets as good as expected? 
To bridge the gap, we conduct a systematic research to assess and improve the quality of four benchmark datasets widely used for code summarization tasks. First, we propose an automated code-comment cleaning tool that can accurately detect noisy data caused by inappropriate data preprocessing operations from existing benchmark datasets. Then, we apply the tool to further assess the data quality of the four benchmark datasets, based on the detected noises. Finally, we conduct comparative experiments to investigate the impact of noisy data on the performance of code summarization models. The results show that these data preprocessing noises widely exist in all four benchmark datasets, and removing these noisy data leads to a significant improvement on the performance of code summarization. We believe that the findings and insights will enable a better understanding of data quality in code summarization tasks, and pave the way for relevant research and practice.","General and reference,Cross-computing tools and techniques,Empirical studies,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Software post-development issues,Software verification and validation,Software defect analysis,Software notations and tools,Software configuration management and version control systems",我们在岩石上建造吗？论数据预处理在代码摘要中的重要性,代码摘要，即在给定代码的情况下生成有用注释的任务，长期以来一直备受关注。大多数现有的代码摘要模型都是在广泛使用的代码注释基准数据集上进行训练和验证的。然而，人们对从真实世界项目构建的基准数据集的质量知之甚少。基准数据集是否与预期一样好？,一般和参考，交叉计算工具和技术，实证研究，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发协作，开源模型，软件开发后问题，软件验证和确认，软件缺陷分析，软件符号和工具，软件配置管理和版本控制系统,,,
EI3IY39L,2022,https://doi.org/10.1145/3540250.3558937,ESEC/FSE 2022,Context-aware code recommendation in Intellij IDEA,"Developers spend a lot of time online, searching for code to help them implement their desired features. While code recommenders help improve developers’ productivity, there is currently no support for context-aware code recommendation for opportunistic code reuse on-the-go. Typical code recommendation systems provide recommendations against a search query, whereas a code recommender that supports opportunistic reuse can recommend related code snippets that represent features that the developer may want to implement next. In this paper, we present a novel Context-aware Feature-driven API usage-based Code Recommender (CA-FACER) tool, which is an Intellij IDEA plugin that leverages a developer’s development context to recommend related code snippets. We consider the methods having API usages in a developer’s active project as part of the development context. Our approach uses contextual data from a developer’s active project to find similar projects and recommends code from popular features of those projects. The popular features are identified as frequently occurring API usage based Method Clone Classes. From our experimental evaluation on 120 Android Java projects from GitHub, we observe a 46% improvement of precision using our proposed context-aware approach over a baseline system. Our technique recommends related code examples with an average precision (P@5) of 94% and 83% and a success rate of 90% and 95% for initial and evolved development stages respectively. A video demonstration of our tool is available at https://youtu.be/UjuM8WRc318.","Information systems,Information retrieval,Retrieval tasks and goals,Recommender systems,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software development techniques,Reusability,Software post-development issues,Software notations and tools,General programming languages",Intellij IDEA中的上下文感知代码推荐,开发人员花费大量时间在线搜索代码，以帮助他们实现所需的功能。虽然代码推荐器有助于提高开发人员的生产力，但目前还不支持上下文感知的代码推荐，以在旅途中进行机会主义的代码重用。典型的代码推荐系统针对搜索查询提供推荐，而支持机会重用的代码推荐器可以推荐表示开发人员接下来可能想要实现的功能的相关代码片段。在本文中，我们提出了一种新颖的上下文软件功能驱动的基于API使用的代码推荐器（CA-FACER）工具，这是一种利用开发人员的开发上下文来推荐相关代码片段的Intellij IDEA插件。我们将在开发人员的活动项目中使用API的方法视为开发上下文的一部分。我们的方法使用来自开发人员活动项目的上下文数据来查找类似的项目，并推荐这些项目的流行功能中的代码。流行的功能被识别为经常出现的基于API使用的方法克隆类。从我们对GitHub的120个Android Java项目的实验评估中，我们观察到，与基线系统相比，使用我们提出的上下文感知方法的精度提高了46%。我们的技术推荐了相关的代码示例，其初始和进化开发阶段的平均精度（P@5）分别为94%和83%，成功率分别为90%和95%。我们的工具的视频演示可在https://youtu.be/UjuM8WRc318.,信息系统，信息检索，检索任务和目标，推荐系统，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发技术，可重用性，软件开发后问题，软件符号和工具，通用编程语言,,,
GLML3MKA,2022,https://doi.org/10.1145/3540250.3558926,ESEC/FSE 2022,COREQQA: a COmpliance REQuirements understanding using question answering tool,"We introduce COREQQA, a tool for assisting requirements engineers in acquiring a better understanding of compliance requirements by means of automated Question Answering. Extracting compliance-related requirements by manually navigating through a legal document is both time-consuming and error-prone. COREQQA enables requirements engineers to pose questions in natural language about a compliance-related topic given some legal document, e.g., asking about data breach. The tool then automatically navigates through the legal document and returns to the requirements engineer a list of text passages containing the possible answers to the input question. For better readability, the tool also highlights the likely answers in these passages. The engineer can then use this output for specifying compliance requirements. COREQQA is developed using advanced large-scale language models from BERT’s family. COREQQA has been evaluated on four legal documents. The results of this evaluation are briefly presented in the paper. The tool is publicly available on Zenodo (https://doi.org/10.5281/zenodo.6653514).","Computing methodologies,Artificial intelligence,Natural language processing,Information extraction,Human-centered computing,Software and its engineering,Software creation and management,Designing software,Requirements analysis",COREQQA：使用问答工具进行合规性测验的理解,我们介绍了COREQQA，这是一种通过自动问答帮助需求工程师更好地了解合规性需求的工具。通过手动浏览法律文档来提取与法规遵从性相关的要求既耗时又容易出错。COREQQA使需求工程师能够在给定一些法律文件的情况下，用自然语言就合规性相关主题提出问题，例如，询问数据泄露。然后，该工具自动浏览法律文档，并向需求工程师返回一份文本段落列表，其中包含输入问题的可能答案。为了更好的可读性，该工具还突出显示了这些段落中可能的答案。然后，工程师可以使用此输出来指定合规性要求。COREQQA是使用BERT家族的高级大规模语言模型开发的。COREQQA已经对四份法律文件进行了评估。文中简要介绍了评价结果。该工具在Zenodo上公开(https://doi.org/10.5281/zenodo.6653514)。,计算方法论，人工智能，自然语言处理，信息提取，以人为中心的计算，软件及其工程，软件创建和管理，设计软件，需求分析,,,
PNX8LL3U,2022,https://doi.org/10.1145/3540250.3549095,ESEC/FSE 2022,Using graph neural networks for program termination,"Termination analyses investigate the termination behavior of programs, intending to detect nontermination, which is known to cause a variety of program bugs (e.g. hanging programs,  
denial-of-service vulnerabilities). Beyond formal approaches, various attempts have been made to estimate the termination behavior of programs using neural networks. However, the majority of these  
approaches continue to rely on formal methods to provide strong soundness guarantees and consequently suffer from similar limitations. In this paper, we move away from formal methods and embrace the stochastic nature of machine learning models. Instead of aiming for rigorous guarantees  
that can be interpreted by solvers, our objective is to provide an estimation of a program's termination behavior and of the likely reason for nontermination (when applicable) that a programmer can use for debugging purposes. Compared to previous approaches using neural networks for program termination, we also take advantage of the graph representation of programs by employing Graph Neural Networks. To further assist programmers in understanding and debugging nontermination bugs, we adapt the notions of attention and semantic segmentation, previously used for other application domains, to programs. Overall, we designed and implemented classifiers for program termination based on Graph Convolutional Networks and Graph Attention Networks, as well as a semantic segmentation Graph Neural Network that localizes AST nodes likely to cause nontermination. We also  
illustrated how the information provided by semantic segmentation can be combined with program slicing to further aid debugging.","Computing methodologies,Artificial intelligence,Knowledge representation and reasoning,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation",使用图神经网络终止程序,终止分析调查程序的终止行为，旨在检测非终止，已知非终止会导致各种程序错误（例如，挂起程序，,计算方法论，人工智能，知识表示和推理，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论,,,
6ZVCJD6Q,2022,https://doi.org/10.1145/3540250.3559080,ESEC/FSE 2022,Automated generation of test oracles for RESTful APIs,"Test case generation tools for RESTful APIs have proliferated in recent years. However, despite their promising results, they all share the same limitation: they can only detect crashes (i.e., server errors) and disconformities with the API specification. In this paper, we present a technique for the automated generation of test oracles for RESTful APIs through the detection of invariants. In practice, our approach aims to learn the expected properties of the output by analysing previous API requests and their corresponding responses. For this, we extended the popular tool Daikon for dynamic detection of likely invariants. A preliminary evaluation conducted on a set of 8 operations from 6 industrial APIs reveals a total precision of 66.5% (reaching 100% in 2 operations). Moreover, our approach revealed 6 reproducible bugs in APIs with millions of users: Amadeus, GitHub and OMDb.","Computer systems organization,Dependable and fault-tolerant systems and networks,Redundancy,Embedded and cyber-physical systems,Embedded systems,Robotics,Networks,Network properties,Network reliability,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",RESTful API测试预言机的自动生成,近年来，RESTful API的测试用例生成工具激增。然而，尽管它们的结果很有希望，但它们都有相同的限制：它们只能检测崩溃（即服务器错误）和与API规范的不一致。在本文中，我们提出了一种通过检测不变量来自动生成RESTful API测试预言库的技术。在实践中，我们的方法旨在通过分析以前的API请求及其相应的响应来了解输出的预期属性。为此，我们扩展了流行的工具Daikon，用于动态检测可能的不变量。对6种工业原料药的8种操作进行的初步评估显示，总精度为66.5%（2种操作达到100%）。此外，我们的方法揭示了拥有数百万用户的API中的6个可复制错误：Amadeus、GitHub和OMDb。,计算机系统组织，可靠和容错系统和网络，冗余，嵌入式和网络物理系统，嵌入式系统，机器人，网络，网络属性，网络可靠性，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，计算理论，语义和推理，程序推理，程序验证,,,
N2QZVI59,2022,https://doi.org/10.1145/3540250.3549174,ESEC/FSE 2022,Hierarchical Bayesian multi-kernel learning for integrated classification and summarization of app reviews,"App stores enable users to share their experiences directly with the developers in the form of app reviews. Recent studies have shown that the feedback received from users is a valuable source of information for requirements extraction, which encourages app developers to leverage the reviews for app update and maintenance purposes. Follow-up studies proposed automated techniques to help developers filter the large volume of daily and noisy reviews and/or summarize their content. However, all previous studies approached the app reviews classification and summarization as separate tasks, which complicated the process and introduced unnecessary overhead. Moreover, none of those approaches explored the potential of utilizing the hierarchical relationships that exist between the labels of app reviews for the purpose of building a more accurate model. In this work, we propose Hierarchical Multi-Kernel Relevance Vector Machines (HMK-RVM), a Bayesian multi-kernel technique that integrates app review classification and summarization using a unified model. Moreover, it can provide insights into the learned patterns and underlying data for easier model interpretation. We evaluated our proposed approach on two real-world datasets and showed that in addition to the gained insights, the model produces equal or better results than the state of the art.","Computing methodologies,Machine learning,Machine learning approaches,Information systems,Information systems applications,Data mining,Software and its engineering,Software creation and management,Designing software,Requirements analysis,Software notations and tools,Software configuration management and version control systems",用于应用评论综合分类和摘要的分层贝叶斯多核学习,应用商店允许用户以应用评论的形式直接与开发者分享他们的体验。最近的研究表明，从用户那里收到的反馈是需求提取的宝贵信息来源，这鼓励应用程序开发人员利用评论进行应用程序更新和维护。后续研究提出了自动化技术，以帮助开发人员过滤大量日常和嘈杂的评论和/或总结其内容。然而，之前的所有研究都将应用评论分类和摘要作为单独的任务来处理，这使过程变得复杂，并引入了不必要的开销。此外，这些方法都没有探索利用应用评论标签之间存在的层次关系来建立更准确的模型的潜力。在这项工作中，我们提出了分层多核相关向量机（HMK-RVM），这是一种贝叶斯多核技术，它使用统一的模型集成了应用程序评论分类和摘要。此外，它可以提供对学习模式和基础数据的深入了解，从而更容易地解释模型。我们在两个真实世界的数据集上评估了我们提出的方法，并表明除了获得的见解外，该模型还产生了与现有技术相同或更好的结果。,计算方法学，机器学习，机器学习方法，信息系统，信息系统应用，数据挖掘，软件及其工程，软件创建和管理，设计软件，需求分析，软件符号和工具，软件配置管理和版本控制系统,,,
W3R8QCDY,2022,https://doi.org/10.1145/3540250.3549118,ESEC/FSE 2022,AccessiText: automated detection of text accessibility issues in Android apps,"For 15% of the world population with disabilities, accessibility is arguably the most critical software quality attribute. The growing reliance of users with disability on mobile apps to complete their day-to-day tasks further stresses the need for accessible software. Mobile operating systems, such as iOS and Android, provide various integrated assistive services to help individuals with disabilities perform tasks that could otherwise be difficult or not possible. However, for these assistive services to work correctly, developers have to support them in their app by following a set of best practices and accessibility guidelines. Text Scaling Assistive Service (TSAS) is utilized by people with low vision, to increase the text size and make apps accessible to them. However, the use of TSAS with incompatible apps can result in unexpected behavior introducing accessibility barriers to users. This paper presents approach, an automated testing technique for text accessibility issues arising from incompatibility between apps and TSAS. As a first step, we identify five different types of text accessibility by analyzing more than 600 candidate issues reported by users in (i) app reviews for Android and iOS, and (ii) Twitter data collected from public Twitter accounts.  
To automatically detect such issues, approach utilizes the UI screenshots and various metadata information extracted using dynamic analysis, and then applies various heuristics informed by the different types of text accessibility issues identified earlier.  
Evaluation of approach on 30 real-world Android apps corroborates its effectiveness by achieving 88.27% precision and 95.76% recall on average in detecting text accessibility issues.","Human-centered computing,Accessibility,Accessibility design and evaluation methods,Ubiquitous and mobile computing,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",AccessiveText：自动检测Android应用程序中的文本可访问性问题,对于世界15%的残疾人来说，无障碍可以说是最关键的软件质量属性。残疾用户越来越依赖移动应用程序来完成日常任务，这进一步强调了对无障碍软件的需求。iOS和Android等移动操作系统提供各种集成辅助服务，帮助残疾人执行原本可能很困难或不可能完成的任务。然而，为了让这些辅助服务正常工作，开发人员必须在他们的应用程序中遵循一套最佳实践和无障碍指南来支持它们。视力低下的人使用文本缩放辅助服务（TSAS）来增加文本大小并使他们可以访问应用程序。然而，在不兼容的应用程序中使用TSAS可能会导致意外行为，给用户带来可访问性障碍。本文提出了一种方法，一种针对应用程序和TSAS之间不兼容引起的文本可访问性问题的自动测试技术。作为第一步，我们通过分析用户在（i）Android和iOS的应用程序评论中报告的600多个候选问题，以及（ii）从公共推特账户收集的推特数据，确定了五种不同类型的文本可访问性。,以人为中心的计算，可访问性，可访问性设计和评估方法，无处不在和移动计算，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
DXKFJYZU,2022,https://doi.org/10.1145/3540250.3560879,ESEC/FSE 2022,Reflections on software failure analysis,"Failure studies are important in revealing the root causes, behaviors, and life cycle of defects in software systems. These studies either focus on understanding the characteristics of defects in specific classes of systems, or the characteristics of a specific type of defect in the systems it manifests in. Failure studies have influenced various software engineering research directions, especially in the area of software evolution, defect detection, and program repair.  
","General and reference,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software development process management,Software post-development issues,Software verification and validation,Software defect analysis,Software testing and debugging",关于软件故障分析的几点思考,故障研究对于揭示软件系统中缺陷的根本原因、行为和生命周期非常重要。这些研究要么侧重于理解特定类别系统中缺陷的特征，要么侧重于了解其所体现的系统中特定类型缺陷的特征。故障研究影响了各种软件工程研究方向，尤其是在软件进化、缺陷检测和程序修复领域。,一般和参考，社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件开发过程管理，软件开发后问题，软件验证和确认，软件缺陷分析，软件测试和调试,,,
IHBI4AEG,2022,https://doi.org/10.1145/3540250.3558909,ESEC/FSE 2022,This is your cue! assisting search behaviour with resource style properties,"When learning a software technology, programmers face a large variety of resources in different styles and catering to different requirements. Although search engines are helpful to filter relevant resources, programmers are still required to manually go through a number of resources before they find one pertinent to their needs. Prior work has largely concentrated on helping programmers find the precise location of relevant information within a resource. Our work focuses on helping programmers assess the pertinence of resources to differentiate between resources. We investigated how programmers find learning resources online via a diary and interview study, and observed that programmers use certain cues to determine whether to access a resource. Based on our findings, we investigate the extent to which we can support the cue-following process via a prototype tool. Our research supports programmers’ search behaviour for software technology learning resources to inform resource creators on important factors that programmers look for during their search.","Human-centered computing,Human computer interaction (HCI),Interactive systems and tools,Information systems,Information retrieval,Document representation,Document collection models,Specialized information retrieval,Software and its engineering,Software creation and management,Software organization and properties,Extra-functional properties,Software usability",这是你的提示！使用资源样式属性辅助搜索行为,在学习软件技术时，程序员面临着各种各样的资源，这些资源具有不同的风格并满足不同的需求。尽管搜索引擎有助于过滤相关资源，但程序员仍然需要手动浏览大量资源，然后才能找到与其需求相关的资源。以前的工作主要集中在帮助程序员在资源中找到相关信息的精确位置。我们的工作重点是帮助程序员评估资源的相关性，以区分资源。我们通过日记和访谈研究调查了程序员如何在网上找到学习资源，并观察到程序员使用某些线索来确定是否访问资源。基于我们的发现，我们研究了通过原型工具支持线索跟随过程的程度。我们的研究支持程序员对软件技术学习资源的搜索行为，以告知资源创建者程序员在搜索过程中寻找的重要因素。,以人为中心的计算，人机交互(HCI)，交互系统和工具，信息系统，信息检索，文档表示，文档收集模型，专业信息检索，软件及其工程，软件创建和管理，软件组织和属性，额外功能属性，软件可用性,,,
Q5XM965A,2022,https://doi.org/10.1145/3540250.3549091,ESEC/FSE 2022,Security code smells in apps: are we getting better?,"Users increasingly rely on mobile apps for everyday tasks, including security- and privacy-sensitive tasks such as online banking, e-health, and e-government. Additionally, a wealth of sensors captures the movements and habits of the users for fitness tracking and convenience. Despite legal regulations imposing requirements and limits on the processing of privacy-sensitive data, users must still trust the app developers to apply suffcient protections. In this paper, we investigate the state of security in Android apps and how security-related code smells have evolved since the introduction of the Android operating system.  
","Security and privacy,Software and application security,Systems security,Operating systems security,Social and professional topics,Computing / technology policy,Software and its engineering,Software creation and management,Software verification and validation,Software notations and tools",应用程序中的安全代码气味：我们正在变得更好吗？,用户越来越多地依赖移动应用程序完成日常任务，包括对安全和隐私敏感的任务，如网上银行、电子健康和电子政务。此外，丰富的传感器可以捕捉用户的动作和习惯，以实现健身跟踪和便利性。尽管法律法规对隐私敏感数据的处理提出了要求和限制，但用户仍然必须相信应用程序开发商会提供足够的保护。在本文中，我们调查了安卓应用程序的安全状态，以及自安卓操作系统推出以来，与安全相关的代码气味是如何演变的。,安全和隐私，软件和应用程序安全，系统安全，操作系统安全，社会和专业主题，计算/技术政策，软件及其工程，软件创建和管理，软件验证和确认，软件符号和工具,,,
N8PAMLYM,2022,https://doi.org/10.1145/3540250.3558948,ESEC/FSE 2022,Achievement unlocked: a case study on gamifying DevOps practices in industry,"Gamification is the use of game elements such as points, leaderboards, and badges in a non-game context to encourage a desired behavior from individuals interacting with an environment. Recently, gamification has found its way into software engineering contexts as a means to promote certain activities to practitioners. Previous studies investigated the use of gamification to promote the adoption of a variety of tools and practices, however, these studies were either performed in an educational environment or in small to medium-sized teams of developers in the industry.  
","Human-centered computing,Information systems,Information systems applications,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software development process management,Software notations and tools,Software organization and properties",解锁成果：行业游戏化DevOps实践案例研究,游戏化是指在非游戏环境中使用积分、排行榜和徽章等游戏元素，鼓励与环境互动的个人做出所需的行为。最近，游戏化已经进入软件工程环境，作为向从业者推广某些活动的一种手段。先前的研究调查了使用游戏化来促进各种工具和实践的采用，然而，这些研究要么是在教育环境中进行的，要么是在行业中的中小型开发团队中进行的。,以人为中心的计算，信息系统，信息系统应用，社会和专业主题，专业主题，计算和信息系统的管理，软件及其工程，软件创建和管理，软件开发过程管理，软件符号和工具，软件组织和属性,,,
TJXLT7VI,2022,https://doi.org/10.1145/3540250.3558932,ESEC/FSE 2022,FIM: fault injection and mutation for Simulink,"We introduce FIM, an open-source toolkit for automated fault injection and mutant generation in Simulink models. FIM allows the injection of faults into specific parts, supporting common types of faults and mutation operators whose parameters can be customized to control the time of fault actuation and persistence. Additional flags allow the user to activate the individual fault blocks during testing to observe their effects on the overall system reliability. We provide insights into the design and architecture of FIM, and evaluate its performance on a case study from the avionics domain.","Computing methodologies,Modeling and simulation,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties",FIM：Simulink的故障注入和突变,我们介绍了FIM，这是一个用于Simulink模型中自动故障注入和突变生成的开源工具包。FIM允许将故障注入特定部件，支持常见类型的故障和突变算子，这些算子的参数可以自定义以控制故障激活和持续的时间。附加标志允许用户在测试期间激活单个故障块，以观察其对整个系统可靠性的影响。我们深入了解FIM的设计和架构，并在航空电子领域的案例研究中评估其性能。,计算方法学，建模与仿真，软件及其工程，软件创建与管理，软件验证与确认，软件缺陷分析，软件测试与调试，软件组织与属性,,,
N85VEQNZ,2022,https://doi.org/10.1145/3540250.3549168,ESEC/FSE 2022,Asynchronous technical interviews: reducing the effect of supervised think-aloud on communication ability,"Software engineers often face a critical test before landing a job—passing a technical interview. During these sessions, candidates must write code while thinking aloud as they work toward a solution to a problem under the watchful eye of an interviewer. While thinking aloud during technical interviews gives interviewers a picture of candidates’ problem-solving ability, surprisingly, these types of interviews often prevent candidates from communicating their thought process effectively. To understand if poor performance related to interviewer presence can be reduced while preserving communication and technical skills, we introduce asynchronous technical interviews—where candidates submit recordings of think-aloud and coding. We compare this approach to traditional whiteboard interviews and find that, by eliminating interviewer supervision, asynchronicity significantly improved the clarity of think-aloud via increased informativeness and reduced stress. Moreover, we discovered asynchronous technical interviews preserved, and in some cases even enhanced, technical problem-solving strategies and code quality. This work offers insight into asynchronous technical interviews as a design for supporting communication during interviews, and discusses trade-offs and guidelines for implementing this approach in software engineering hiring practices.","Human-centered computing,Human computer interaction (HCI),Empirical studies in HCI,Social and professional topics,Professional topics,Software and its engineering,Software creation and management,Collaboration in software development,Programming teams",异步技术访谈：降低有监督的大声思考对沟通能力的影响,软件工程师在找到工作之前经常面临一个关键的测试——通过技术面试。在这些环节中，应聘者必须一边写代码，一边大声思考，以便在面试官的监督下找到问题的解决方案。虽然在技术面试中大声思考可以让面试官了解候选人解决问题的能力，但令人惊讶的是，这些类型的面试往往会阻碍候选人有效地交流他们的思维过程。为了了解在保持沟通和技术技能的同时，是否可以减少与面试官在场有关的糟糕表现，我们引入了异步技术面试——候选人提交大声思考和编码的录音。我们将这种方法与传统的白板面试进行了比较，发现通过消除面试官的监督，异步性通过增加信息量和减轻压力，显著提高了大声思考的清晰度。此外，我们发现异步技术访谈保留了技术问题解决策略和代码质量，在某些情况下甚至增强了这些策略和质量。这项工作深入了解了异步技术面试作为面试期间支持沟通的设计，并讨论了在软件工程招聘实践中实施这种方法的权衡和指导方针。,以人为中心的计算，人机交互(HCI)，人机交互的经验研究，社会和专业主题，专业主题，软件及其工程，软件创建和管理，软件开发中的协作，编程团队,,,
4DEVLF3F,2022,https://doi.org/10.1145/3540250.3558968,ESEC/FSE 2022,All you need is logs: improving code completion by learning from anonymous IDE usage logs,"In this work, we propose an approach for collecting completion usage logs from the users in an IDE and using them to train a machine learning based model for ranking completion candidates. We developed a set of features that describe completion candidates and their context, and deployed their anonymized collection in the Early Access Program of IntelliJ-based IDEs. We used the logs to collect a dataset of code completions from users, and employed it to train a ranking CatBoost model. Then, we evaluated it in two settings: on a held-out set of the collected completions and in a separate A/B test on two different groups of users in the IDE. Our evaluation shows that using a simple ranking model trained on the past user behavior logs significantly improved code completion experience. Compared to the default heuristics-based ranking, our model demonstrated a decrease in the number of typing actions necessary to perform the completion in the IDE from 2.073 to 1.832.  
","Computing methodologies,Machine learning,Human-centered computing,Security and privacy,Software and its engineering,Software creation and management,Software development techniques,Automatic programming,Software notations and tools",您所需要的只是日志：通过学习匿名IDE使用日志来改进代码完成,在这项工作中，我们提出了一种在IDE中从用户那里收集完成使用日志的方法，并使用它们来训练一个基于机器学习的模型来对完成候选者进行排名。我们开发了一组描述候选完成及其上下文的功能，并在基于IntelliJ的IDE的早期访问程序中部署了他们的匿名集合。我们使用日志从用户那里收集代码完成的数据集，并使用它来训练排名CatBoost模型。然后，我们在两个设置中对其进行了评估：一个是在收集的完成集上，另一个是对IDE中的两组不同用户进行单独的a/B测试。我们的评估表明，使用在过去的用户行为日志上训练的简单排名模型显著改善了代码完成体验。与默认的基于启发式的排名相比，我们的模型表明，在IDE中执行完成操作所需的键入操作数量从2.073减少到1.832。,计算方法论，机器学习，以人为中心的计算，安全和隐私，软件及其工程，软件创建和管理，软件开发技术，自动编程，软件符号和工具,,,
K5XFDFT7,2022,https://doi.org/10.1145/3540250.3549108,ESEC/FSE 2022,Classifying edits to variability in source code,"For highly configurable software systems, such as the Linux kernel, maintaining and evolving variability information along changes to source code poses a major challenge. While source code itself may be edited, also feature-to-code mappings may be introduced, removed, or changed. In practice, such edits are often conducted ad-hoc and without proper documentation. To support the maintenance and evolution of variability, it is desirable to understand the impact of each edit on the variability. We propose the first complete and unambiguous classification of edits to variability in source code by means of a catalog of edit classes. This catalog is based on a scheme that can be used to build classifications that are complete and unambiguous by construction. To this end, we introduce a complete and sound model for edits to variability. In about 21.5ms per commit, we validate the correctness and suitability of our classification by classifying each edit in 1.7 million commits in the change histories of 44 open-source software systems automatically. We are able to classify all edits with syntactically correct feature-to-code mappings and find that all our edit classes occur in practice.","Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software maintenance,Software and its engineering,Software creation and management,Software development techniques,Software post-development issues,Software evolution,Software version control,Software notations and tools,Software configuration management and version control systems",对源代码可变性的编辑进行分类,对于高度可配置的软件系统（如Linux内核）来说，维护和发展可变性信息以及对源代码的更改是一个重大挑战。虽然可以编辑源代码本身，但也可以引入、删除或更改功能到代码的映射。在实践中，这种编辑通常是临时进行的，没有适当的文档。为了支持可变性的维护和发展，需要了解每次编辑对可变性的影响。我们提出了第一个通过编辑类目录对源代码可变性的编辑进行完整而明确的分类。该目录基于一个方案，该方案可用于构建完整且明确的分类。为此，我们引入了一个完整而合理的模型来编辑可变性。在每次提交约21.5ms的时间内，我们通过将44个开源软件系统的更改历史中的170万次提交中的每一次编辑自动分类，来验证我们分类的正确性和适用性。我们能够用语法正确的特征到代码映射对所有编辑进行分类，并发现我们所有的编辑类都发生在实践中。,社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件维护，软件及其工程，软件创建和管理，软件开发技术，软件开发后问题，软件演化，软件版本控制，软件符号和工具，软件配置管理和版本控制系统,,,
P3WX7YMJ,2022,https://doi.org/10.1145/3540250.3558914,ESEC/FSE 2022,eGEN: an energy-saving modeling language and code generator for location-sensing of mobile apps,"Given the limited tool support for energy-saving strategies during the design phase of android applications, developing battery-aware, location-based android applications is a non-trivial task for developers. To this end, we propose eGEN, consisting of (1) a Domain-Specific Modeling Language (DSML) and (2) a code generator to specify and create native battery-aware, location-based mobile applications. We evaluated eGEN by instrumenting the generated battery-aware code in five location-based, open-source android applications and compared the energy consumption with non-eGEN versions. The experimental results show 188 mA (8.34% of battery per hour) of average reduction in battery consumption while showing only 97 meters of degradation in location accuracy over three kilometers of a cycling path. Hence, we see this tool as a first step in helping developers write battery-aware code in location-based android applications. The GitHub repository with source code and all artifacts is available at https://github.com/Kowndinya2000/egen, and the tool demo video at https://youtu.be/Iadfh4cCw8I.","Software and its engineering,Software creation and management,Software development techniques,Software notations and tools,Context specific languages,Domain specific languages,Development frameworks and environments,Software organization and properties",eGEN：一种用于移动应用程序位置感知的节能建模语言和代码生成器,考虑到在安卓应用程序的设计阶段，对节能策略的工具支持有限，开发电池感知的、基于位置的安卓应用软件对开发人员来说是一项不平凡的任务。为此，我们提出了eGEN，它由（1）领域特定建模语言（DSML）和（2）代码生成器组成，用于指定和创建本地电池感知、基于位置的移动应用程序。我们通过在五个基于位置的开源安卓应用程序中检测生成的电池感知代码来评估eGEN，并将能耗与非eGEN版本进行了比较。实验结果显示，在三公里的骑行路径上，电池消耗平均减少了188毫安（每小时8.34%的电池），而定位精度仅下降了97米。因此，我们将此工具视为帮助开发人员在基于位置的android应用程序中编写电池感知代码的第一步。包含源代码和所有工件的GitHub存储库可在https://github.com/Kowndinya2000/egen，以及上的工具演示视频https://youtu.be/Iadfh4cCw8I.,软件及其工程，软件创建和管理，软件开发技术，软件符号和工具，上下文特定语言，领域特定语言，开发框架和环境，软件组织和属性,,,
4HK74U24,2022,https://doi.org/10.1145/3540250.3549135,ESEC/FSE 2022,Software security during modern code review: the developer's perspective,"To avoid software vulnerabilities, organizations are shifting security to earlier stages of the software development, such as at code review time. In this paper, we aim to understand the developers’  
perspective on assessing software security during code review, the challenges they encounter, and the support that companies and projects provide. To this end, we conduct a two-step investigation: we interview 10 professional developers and survey 182 practitioners about software security assessment during code review. The outcome is an overview of how developers perceive software security during code review and a set of identified challenges. Our study revealed that most developers do not immediately report to focus on security issues during code review. Only after being asked about software security, developers state to always consider it during review and acknowledge its importance. Most companies do not provide security training, yet expect developers to still ensure  
security during reviews. Accordingly, developers report the lack of training and security knowledge as the main challenges they face when checking for security issues. In addition, they have challenges  
with third-party libraries and to identify interactions between parts of code that could have security implications. Moreover, security may be disregarded during reviews due to developers’ assumptions  
about the security dynamic of the application they develop.  
Preprint: https://arxiv.org/abs/2208.04261  
Data and materials: https://doi.org/10.5281/zenodo.6969369","Human-centered computing,Security and privacy,Software and application security,Software security engineering,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software verification and validation,Software notations and tools,Software configuration management and version control systems",现代代码审查中的软件安全：开发者的视角,为了避免软件漏洞，组织正在将安全性转移到软件开发的早期阶段，例如在代码审查时。在本文中，我们旨在了解开发人员,以人为中心的计算，安全和隐私，软件和应用程序安全，软件安全工程，社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统,,,
NILW4RKE,2022,https://doi.org/10.1145/3540250.3549110,ESEC/FSE 2022,Peahen: fast and precise static deadlock detection via context reduction,"Deadlocks still severely inflict reliability and security issues upon software systems of the modern age. Worse still, as we note, in prior static deadlock detectors, good precision does not go hand-in-hand with high scalability --- their approaches are either context-insensitive, thereby engendering many false positives, or suffer from the calling context explosion to reach context-sensitive, thus compromising good efficiency. In this paper, we advocate Peahen, geared towards precise yet also scalable static deadlock detection. At its crux, Peahen decomposes the computational effort for embracing high precision into two cooperative analysis stages: (i) context-insensitive lock-graph construction, which selectively encodes the essential lock-acquisition information on each edge, and (ii) three precise yet lazy refinements, which incorporate such edge information into progressively refining the deadlock cycles in the lock graph only for  
a few interesting calling contexts.  
","Software and its engineering,Software creation and management,Software verification and validation,Software organization and properties,Contextual software domains,Operating systems,Process management,Software functional properties,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",Peahen：通过上下文减少实现快速精确的静态死锁检测,死锁仍然严重地给现代软件系统带来可靠性和安全性问题。更糟糕的是，正如我们所注意到的，在现有的静态死锁检测器中，良好的精度与高可扩展性并不齐头并进——它们的方法要么对上下文不敏感，从而产生许多误报，要么遭受调用上下文爆炸以达到上下文敏感，从而影响良好的效率。在本文中，我们提倡Peahen，致力于精确但可扩展的静态死锁检测。关键是，Peahen将实现高精度的计算工作分解为两个协同分析阶段：（i）上下文不敏感的锁图构建，它选择性地对每条边上的基本锁获取信息进行编码，以及（ii）三个精确但懒惰的细化，其将这种边缘信息合并到逐步细化锁图中的死锁循环中，仅用于,软件及其工程，软件创建和管理，软件验证和确认，软件组织和属性，上下文软件领域，操作系统，过程管理，软件功能属性，计算理论，语义和推理，程序推理，程序验证,,,
48FJFUX7,2022,https://doi.org/10.1145/3540250.3549123,ESEC/FSE 2022,Understanding performance problems in deep learning systems,"Deep learning (DL) has been widely applied to many domains. Unique challenges in engineering DL systems are posed by the programming paradigm shift from traditional systems to DL systems, and performance is one of the challenges. Performance problems (PPs) in DL systems can cause severe consequences such as excessive resource consumption and financial loss. While bugs in DL systems have been extensively investigated, PPs in DL systems have hardly been explored. To bridge this gap, we present the first comprehensive study to i) characterize symptoms, root causes, and introducing and exposing stages of PPs in DL systems developed in TensorFLow and Keras, with 224 PPs collected from 210 StackOverflow posts, and to ii) assess the capability of existing performance analysis approaches in tackling PPs, with a constructed benchmark of 58 PPs in DL systems. Our findings shed light on the implications on developing high-performance DL systems, and detecting and localizing PPs in DL systems. To demonstrate the usefulness of our findings, we develop a static checker DeepPerf to detect three types of PPs. It has detected 488 new PPs in 130 GitHub projects. 105 and 27 PPs have been confirmed and fixed.","General and reference,Cross-computing tools and techniques,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software performance",理解深度学习系统中的性能问题,深度学习（DL）已被广泛应用于许多领域。从传统系统到DL系统的编程范式转变给DL系统的工程设计带来了独特的挑战，而性能是挑战之一。DL系统中的性能问题（PP）可能会导致严重的后果，例如过度的资源消耗和财务损失。虽然DL系统中的缺陷已经被广泛研究，但DL系统中PP几乎没有被探索。为了弥补这一差距，我们提出了第一项全面的研究，以i）表征TensorFLow和Keras开发的DL系统中PP的症状、根本原因以及引入和暴露阶段，从210个StackOverflow帖子中收集了224个PP，以及ii）评估现有性能分析方法在解决PP方面的能力，在DL系统中构建了58个PP的基准。我们的发现揭示了开发高性能DL系统以及检测和定位DL系统中的PP的意义。为了证明我们的发现的有用性，我们开发了一个静态检查器DeepPerf来检测三种类型的PP。它在130个GitHub项目中检测到488个新的PP。105和27个PP已经被确认和固定。,通则和参考，交叉计算工具和技术，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，额外功能属性，软件性能,,,
PB7Y3E3S,2022,https://doi.org/10.1145/3540250.3558908,ESEC/FSE 2022,Sentiment in software engineering: detection and application,"In software engineering the role of human aspects is an important one, especially as developers indicate that they experience a wide range of emotions while developing software. Within software engineering researchers have sought to understand the role emotions and sentiment play in the  
development of software by studying issues, pull-requests and commit messages.  
To detect sentiment, automated tools are used, and in this doctoral thesis we plan to study the use of these sentiment analysis tools, their applications, best practices for their usage and the effect of non-natural language on their performance. In addition to studying the application of sentiment analysis tools, we also aim to study self-admitted technical debt and bots in software engineering, to understand why developers express sentiment and what they signal when they express sentiment. Through studying both the application of sentiment analysis tools and the role of sentiment in software engineering, we hope to provide practical recommendations for both researchers and developers.","Human-centered computing,Collaborative and social computing,Empirical studies in collaborative and social computing,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Software verification and validation,Software notations and tools,Software configuration management and version control systems",软件工程中的情感：检测与应用,在软件工程中，人的方面的作用是重要的，尤其是当开发人员表示他们在开发软件时会经历各种各样的情绪时。在软件工程领域，研究人员试图理解情绪和情感在,以人为中心的计算，协作和社会计算，协作和社会计算的经验研究，社会和专业主题，专业主题，计算和信息系统的管理，软件管理，软件及其工程，软件创建和管理，软件开发中的协作，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统,,,
2WN9WK74,2022,https://doi.org/10.1145/3540250.3549162,ESEC/FSE 2022,"NatGen: generative pre-training by ""naturalizing"" source code","Pre-trained Generative Language models (e.g., PLBART, CodeT5, SPT-Code) for source code yielded strong results on several tasks in the past few years, including code generation and translation. These models have adopted varying pre-training objectives to learn statistics of code construction from very large-scale corpora in a self-supervised fashion; the success of pre-trained models largely hinges on these pre-training objectives. This paper proposes a new pre-training objective, “Naturalizing” of source code, exploiting code’s bimodal, dual-channel (formal & natural channels) nature. Unlike natural language, code’s bimodal, dual-channel nature allows us to generate semantically equivalent code at scale. We introduce six classes of semantic preserving transformations to introduce unnatural forms of code, and then force our model to produce more natural original programs written by developers. Learning to generate equivalent, but more natural code, at scale, over large corpora of open-source code, without explicit manual supervision, helps the model learn to both ingest & generate code. We fine-tune our model in three generative Software Engineering tasks: code generation, code translation, and code refinement with limited human-curated labeled data and achieve state-of-the-art performance rivaling CodeT5. We show that our pre-trained model is especially competitive at zero-shot and few-shot learning, and better at learning code properties (e.g., syntax, data flow)","Computing methodologies,Artificial intelligence,Knowledge representation and reasoning,Natural language processing,Machine learning,Software and its engineering,Software creation and management,Software development techniques,Software notations and tools,General programming languages,Language features",NatGen：通过“自然化”源代码进行生成性预训练,在过去几年中，为源代码预先训练的生成语言模型（例如，PLBART、CodeT5、SPT代码）在包括代码生成和翻译在内的多项任务中取得了丰硕成果。这些模型采用了不同的预训练目标，以自我监督的方式从非常大规模的语料库中学习代码构建的统计信息；预训练模型的成功很大程度上取决于这些预训练目标。本文提出了一个新的预训练目标，即源代码的“自然化”，利用代码的双峰、双通道（形式和自然通道）特性。与自然语言不同，代码的双峰、双通道特性允许我们在一定范围内生成语义等效的代码。我们引入了六类语义保持转换，以引入非自然形式的代码，然后迫使我们的模型生成由开发人员编写的更自然的原始程序。在没有明确的手动监督的情况下，在大型开源代码库上大规模学习生成等效但更自然的代码，有助于模型学习吸收和生成代码。我们在三个生成软件工程任务中微调我们的模型：代码生成、代码翻译和代码精化，使用有限的人工策划的标记数据，并实现与CodeT5相媲美的最先进性能。我们表明，我们的预训练模型在零样本和少热点学习方面尤其具有竞争力，并且在学习代码属性（例如，语法、数据流）方面更好,计算方法学，人工智能，知识表示和推理，自然语言处理，机器学习，软件及其工程，软件创建和管理，软件开发技术，软件符号和工具，通用编程语言，语言特征,,,
3EDD5C4E,2022,https://doi.org/10.1145/3540250.3549089,ESEC/FSE 2022,Putting them under microscope: a fine-grained approach for detecting redundant test cases in natural language,"Natural language (NL) documentation is the bridge between software managers and testers, and NL test cases are prevalent in system-level testing and other quality assurance activities. Due to reasons such as requirements redundancy, parallel testing, tester turn-over within long evolving history, there are inevitably lots of redundant test cases, which significantly increase the cost. Previous redundancy detection approaches typically treat the textual descriptions as a whole to compare their similarity and suffer from low precision. Our observation reveals that a test case can have explicit test-oriented entities, such as tested function Components, Constraints, etc; and there are also specific relations between these entities. This inspires us with a potential opportunity for accurate redundancy detection. In this paper, we first define five test-oriented entity categories and four associated relation categories, and re-formulate the NL test case redundancy detection problem as the comparison of detailed testing content guided by the test-oriented entities and relations. Following that, we propose Tscope, a fine-grained approach for redundant NL test case detection by dissecting test cases into atomic test tuple(s) with the entities restricted by associated relations. To serve as the test case dissection, Tscope designs a context-aware model for the automatic entity and relation extraction. Evaluation on 3,467 test cases from ten projects shows Tscope could achieve 91.8% precision, 74.8% recall and 82.4% F1, significantly outperforming state-of-the-art approaches and commonly-used classifiers. This new formulation of the NL test case redundant detection problem can motivate the follow-up studies in further improving this task and other related tasks involving NL descriptions.","Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation,Process validation,Acceptance testing,Software defect analysis,Software testing and debugging,Theory of computation,Semantics and reasoning,Program reasoning",把它们放在显微镜下：一种在自然语言中检测冗余测试用例的细粒度方法,自然语言（NL）文档是软件经理和测试人员之间的桥梁，NL测试用例在系统级测试和其他质量保证活动中很普遍。在漫长的发展历史中，由于需求冗余、并行测试、测试人员周转等原因，不可避免地会出现大量冗余的测试用例，这大大增加了成本。先前的冗余检测方法通常将文本描述作为一个整体来比较它们的相似性，并且精度较低。我们的观察结果表明，一个测试用例可以有明确的面向测试的实体，例如被测试的功能组件、约束等；这些实体之间也存在特定的关系。这为我们提供了一个精确冗余检测的潜在机会。在本文中，我们首先定义了五个面向测试的实体类别和四个相关的关系类别，并将NL测试用例冗余检测问题重新表述为在面向测试实体和关系的指导下对详细测试内容的比较。然后，我们提出了Tscope，这是一种用于冗余NL测试用例检测的细粒度方法，通过将测试用例分解为原子测试元组，其中实体受关联关系的限制。作为测试用例的解剖，Tscope设计了一个上下文感知模型用于自动实体和关系提取。对来自10个项目的3467个测试用例的评估显示，Tscope可以实现91.8%的准确率、74.8%的召回率和82.4%的F1，显著优于最先进的方法和常用的分类器。NL测试用例冗余检测问题的这种新公式可以激励后续研究进一步改进该任务和其他涉及NL描述的相关任务。,软件及其工程，软件创建和管理，软件后开发问题，软件验证和确认，过程验证，验收测试，软件缺陷分析，软件测试和调试，计算理论，语义和推理，程序推理,,,
8LJZCFUW,2022,https://doi.org/10.1145/3540250.3558943,ESEC/FSE 2022,Testing of machine learning models with limited samples: an industrial vacuum pumping application,"There is often a scarcity of training data for machine learning (ML) classification and regression models in industrial production, especially for time-consuming or sparsely run manufacturing processes. Traditionally, a majority of the limited ground-truth data is used for training, while a handful of samples are left for testing. In that case, the number of test samples is inadequate to properly evaluate the robustness of the ML models under test (i.e., the system under test) for classification and regression. Furthermore, the output of these ML models may be inaccurate or even fail if the input data differ from the expected. This is the case for ML models used in the Electroslag Remelting (ESR) process in the refined steel industry to predict the pressure in a vacuum chamber. A vacuum pumping event that occurs once a workday generates a few hundred samples in a year of pumping for training and testing. In the absence of adequate training and test samples, this paper first presents a method to generate a fresh set of augmented samples based on vacuum pumping principles. Based on the generated augmented samples, three test scenarios and one test oracle are presented to assess the robustness of an ML model used for production on an industrial scale. Experiments are conducted with real industrial production data obtained from Uddeholms AB steel company. The evaluations indicate that Ensemble and Neural Network are the most robust when trained on augmented data using the proposed testing strategy. The evaluation also demonstrates the proposed method's effectiveness in checking and improving ML algorithms' robustness in such situations. The work improves software testing's state-of-the-art robustness testing in similar settings. Finally, the paper presents an MLOps implementation of the proposed approach for real-time ML model prediction and action on the edge node and automated continuous delivery of ML software from the cloud.","Applied computing,Computing methodologies,Machine learning,Learning paradigms,Supervised learning,Supervised learning by classification,Machine learning approaches,Classification and regression trees,Neural networks,Modeling and simulation,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",有限样本的机器学习模型测试：工业真空泵的应用,工业生产中的机器学习（ML）分类和回归模型通常缺乏训练数据，尤其是对于耗时或运行稀少的制造过程。传统上，大多数有限的地面实况数据用于训练，而少数样本用于测试。在这种情况下，测试样本的数量不足以正确评估被测ML模型（即被测系统）对分类和回归的稳健性。此外，如果输入数据与预期不同，这些ML模型的输出可能不准确，甚至失败。精炼钢行业电渣重熔（ESR）过程中使用的ML模型就是这样，用于预测真空室中的压力。一个工作日发生一次的真空泵送事件在一年的泵送过程中产生数百个样本用于培训和测试。在缺乏足够的训练和测试样本的情况下，本文首先提出了一种基于真空泵原理生成新的增强样本集的方法。基于生成的增强样本，提出了三个测试场景和一个测试预言机，以评估用于工业规模生产的ML模型的稳健性。实验使用从Uddeholms AB钢铁公司获得的实际工业生产数据进行。评估表明，当使用所提出的测试策略在增强数据上进行训练时，集成和神经网络是最稳健的。评估还证明了所提出的方法在检查和提高ML算法在这种情况下的鲁棒性方面的有效性。这项工作改进了软件测试在类似环境中最先进的稳健性测试。最后，本文介绍了所提出的方法的MLOps实现，用于边缘节点上的实时ML模型预测和操作，以及从云中自动连续交付ML软件。,应用计算，计算方法论，机器学习，学习范式，监督学习，分类监督学习，机器学习方法，分类和回归树，神经网络，建模和模拟，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
58EUMCRC,2022,https://doi.org/10.1145/3540250.3569443,ESEC/FSE 2022,"On safety, assurance, and reliability: a software engineering perspective (keynote)","From financial services platforms to social networks to vehicle control, software has come to mediate many activities of daily life. Governing bodies and standards organizations have responded to this trend by creating regulations and standards to address issues such as safety, security and privacy. In this environment, the compliance of software development to standards and regulations has emerged as a key requirement. Compliance claims and arguments are often captured in assurance cases, with linked evidence of compliance. Evidence can come from test cases, verification proofs, human judgement, or a combination of these. That is, we try to build (safety-critical) systems carefully according to well justified methods and articulate these justifications in an assurance case that is ultimately judged by a human.  
","General and reference,Cross-computing tools and techniques,Reliability,Social and professional topics,Software and its engineering,Software creation and management,Software verification and validation,Software notations and tools,Software maintenance tools,Software organization and properties,Extra-functional properties,Software reliability,Software functional properties",关于安全、保证和可靠性：软件工程的观点（主题）,从金融服务平台到社交网络再到车辆控制，软件已经开始调解日常生活中的许多活动。管理机构和标准组织对这一趋势作出了回应，制定了法规和标准来解决安全、安保和隐私等问题。在这种环境下，软件开发对标准和法规的遵从性已成为一项关键要求。合规性索赔和论点通常在保证案例中被捕获，并附有合规性的相关证据。证据可以来自测试案例、验证证据、人类判断或这些的组合。也就是说，我们试图根据合理的方法仔细构建（安全关键）系统，并在最终由人类判断的保证案例中阐明这些理由。,通用和参考，交叉计算工具和技术，可靠性，社会和专业主题，软件及其工程，软件创建和管理，软件验证和确认，软件符号和工具，软件维护工具，软件组织和属性，功能外属性，软件可靠性，软件功能属性,,,
82RHVM9J,2022,https://doi.org/10.1145/3540250.3558952,ESEC/FSE 2022,Leveraging test plan quality to improve code review efficacy,"In modern code reviews, many artifacts play roles in knowledge- sharing and documentation: summaries, test plans, and comments, etc. Improving developer tools and facilitating better code reviews require an understanding of the quality of pull requests and their artifacts. This is difficult to measure, however, because they are often free-form natural language and unstructured text data. In this paper, we focus on measuring the quality of test plans at Meta. Test plans are used as a communication mechanism between the author of a pull request and its reviewers, serving as walkthroughs to help confirm that the changed code is behaving as expected. We collected developer opinions on over 650 test plans from more than 500 Meta developers, then introduced a transformer-based model to leverage the success of natural language processing (NLP) tech- niques in the code review domain. In our study, we show that the learned model is able to capture the sentiment of developers and reflect a correlation of test plan quality with review engagement and reversions: compared to a decision tree model, our proposed transformer-based model achieves a 7% higher F1-score. Finally, we present a case study of how such a metric may be useful in experiments to inform improvements in developer tools and experiences.","Human-centered computing,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation,Process validation,Acceptance testing,Walkthroughs,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems",利用测试计划质量提高代码审查效率,在现代代码评审中，许多工件在知识共享和文档中发挥作用：摘要、测试计划和注释等。改进开发人员工具和促进更好的代码评审需要了解拉取请求及其工件的质量。然而，这很难衡量，因为它们通常是自由形式的自然语言和非结构化的文本数据。在这篇论文中，我们专注于衡量Meta测试计划的质量。测试计划被用作拉取请求的作者和审查者之间的沟通机制，用作演练，以帮助确认更改后的代码是否按预期运行。我们从500多名Meta开发人员那里收集了开发人员对650多个测试计划的意见，然后引入了一个基于转换器的模型，以利用自然语言处理（NLP）技术在代码审查领域的成功。在我们的研究中，我们表明，学习模型能够捕捉开发人员的情绪，并反映测试计划质量与审查参与和逆转的相关性：与决策树模型相比，我们提出的基于变压器的模型获得了7%的F1分数。最后，我们提供了一个案例研究，说明这种度量在实验中如何有用，以告知开发人员工具和经验的改进。,以人为中心的计算，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发后问题，软件验证和确认，过程验证，验收测试，演练，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统,,,
F5RYMG8U,2022,https://doi.org/10.1145/3540250.3549102,ESEC/FSE 2022,NMTSloth: understanding and testing efficiency degradation of neural machine translation systems,"Neural Machine Translation (NMT) systems have received much recent attention due to their human-level accuracy. While existing works mostly focus on either improving accuracy or testing accuracy robustness, the computation efficiency of NMT systems, which is of paramount importance due to often vast translation demands and real-time requirements, has surprisingly received little attention. In this paper, we make the first attempt to understand and test potential computation efficiency robustness in state-of-the-art NMT systems. By analyzing the working mechanism and implementation of 1455 public-accessible NMT systems, we observe a fundamental property in NMT systems that could be manipulated in an adversarial manner to reduce computation efficiency significantly. Our interesting observation is that the output length determines the computation efficiency of NMT systems instead of the input, where the output length depends on two factors: an often sufficiently large yet pessimistic pre-configured threshold controlling the max number of iterations and a runtime generated end of sentence (EOS) token. Our key motivation is to generate test inputs that could sufficiently delay the generation of EOS such that NMT systems would have to go through enough iterations to satisfy the pre-configured threshold. We present NMTSloth, which develops a gradient-guided technique that searches for a minimal and unnoticeable perturbation at character-level, token-level, and structure-level, which sufficiently delays the appearance of EOS and forces these inputs to reach the naturally-unreachable threshold. To demonstrate the effectiveness of NMTSloth, we conduct a systematic evaluation on three public-available NMT systems: Google T5, AllenAI WMT14, and Helsinki-NLP translators. Experimental results show that NMTSloth can increase NMT systems' response latency and energy consumption by 85% to 3153% and 86% to 3052%, respectively, by perturbing just one character or token in the input sentence. Our case study shows that inputs generated by NMTSloth significantly affect the battery power in real-world mobile devices (i.e., drain more than 30 times battery power than normal inputs).","Computing methodologies,Artificial intelligence,Natural language processing,Machine translation,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software notations and tools",NMTSloth：对神经机器翻译系统效率退化的理解与检验,神经机器翻译（NMT）系统由于其在人的层面上的准确性而在最近受到了广泛的关注。虽然现有的工作主要集中在提高精度或测试精度稳健性上，但NMT系统的计算效率却出人意料地很少受到关注，因为它通常具有巨大的翻译需求和实时性要求，因此至关重要。在本文中，我们首次尝试理解和测试最先进的NMT系统中潜在的计算效率鲁棒性。通过分析1455个公共可访问NMT系统的工作机制和实现，我们观察到NMT系统中的一个基本特性，即可以以对抗性的方式进行操作，从而显著降低计算效率。我们有趣的观察结果是，输出长度决定了NMT系统的计算效率，而不是输入，其中输出长度取决于两个因素：一个通常足够大但悲观的预先配置阈值，控制最大迭代次数，以及一个运行时生成的句末（EOS）令牌。我们的主要动机是生成可以充分延迟EOS生成的测试输入，使得NMT系统必须经过足够的迭代才能满足预先配置的阈值。我们提出了NMTSloth，它开发了一种梯度引导技术，在字符级、令牌级和结构级搜索最小且不明显的扰动，这充分延迟了EOS的出现，并迫使这些输入达到自然不可达的阈值。为了证明NMTSloth的有效性，我们对三个公共可用的NMT系统进行了系统评估：Google T5、AllenAI WMT14和Helsinki NLP翻译器。实验结果表明，NMTSloth只干扰输入句子中的一个字符或标记，就可以使NMT系统的响应延迟和能耗分别增加85%至3153%和86%至3052%。我们的案例研究表明，NMTSloth产生的输入会显著影响现实世界移动设备中的电池电量（即，消耗的电池电量是正常输入的30倍以上）。,计算方法论，人工智能，自然语言处理，机器翻译，机器学习，机器学习方法，神经网络，软件及其工程，软件符号和工具,,,
E9VU2HGZ,2022,https://doi.org/10.1145/3540250.3558961,ESEC/FSE 2022,Workgraph: personal focus vs. interruption for engineers at Meta,"All engineers dislike interruptions because it takes away from the deep focus time needed to write complex code. Our goal is to reduce unnecessary interruptions at . We first describe our Workgraph platform that logs how engineers use our internal work tools at . Using these anonymized logs, we create sessions. sessions are defined in opposition to interruption and are the amount of time until the engineer is interrupted by, for example, a work chat message. ","Human-centered computing,Social and professional topics,Professional topics,Computing and business,Management of computing and information systems,Project and people management,Software and its engineering,Software creation and management,Collaboration in software development,Software development process management,Software notations and tools,Software configuration management and version control systems",工作图：Meta工程师的个人关注与中断,所有的工程师都不喜欢中断，因为它占用了编写复杂代码所需的深度关注时间。我们的目标是减少对的不必要干扰。我们首先描述了我们的Workgraph平台，该平台记录了工程师如何在中使用我们的内部工作工具。使用这些匿名日志，我们创建会话。会话的定义与中断相反，会话是指工程师被工作聊天消息打断之前的时间。,以人为中心的计算，社会和专业主题，专业主题，计算和商业，计算和信息系统的管理，项目和人员管理，软件及其工程，软件创建和管理，软件开发中的协作，软件开发过程管理，软件符号和工具，软件配置管理和版本控制系统,,,
8NWC3BK8,2022,https://doi.org/10.1145/3540250.3558945,ESEC/FSE 2022,Understanding why we cannot model how long a code review will take: an industrial case study,"Code review is an effective practice for finding defects, but because it is manually intensive it can slow down the continuous integration of changes. Our goal was to understand the factors that influenced the time a change, ie a diff at Meta, would spend in review. A developer survey showed that diff reviews start to feel slow after they have been waiting for around 24 hour review. We built a review time predictor model to identify potential factors that may be causing reviews to take longer, which we could use to predict when would be the best time to nudge reviewers or to identify diff-related factors that we may need to address.  
","Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software development process management,Software post-development issues,Software verification and validation,Software defect analysis,Software notations and tools,Software configuration management and version control systems",理解为什么我们不能对代码审查需要多长时间进行建模：一个行业案例研究,代码审查是发现缺陷的有效实践，但由于它是手动密集型的，它可能会减缓更改的持续集成。我们的目标是了解影响改变（即Meta的差异）在审查中花费的时间的因素。一项开发者调查显示，在等待了大约24小时的评审后，差异评审开始感觉很慢。我们建立了一个评审时间预测模型，以确定可能导致评审耗时更长的潜在因素，我们可以用它来预测何时是推动评审的最佳时间，或者确定我们可能需要解决的差异相关因素。,社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发过程管理，软件开发后问题，软件验证和确认，软件缺陷分析，软件符号和工具，软件配置管理和版本控制系统,,,
ECYW7PMF,2022,https://doi.org/10.1145/3540250.3549121,ESEC/FSE 2022,Declarative smart contracts,"This paper presents DeCon, a declarative programming language for implementing smart contracts and specifying contract-level properties. Driven by the observation that smart contract operations and contract-level properties can be naturally expressed as relational constraints, DeCon models each smart contract as a set of relational tables that store transaction records. This relational representation of smart contracts enables convenient specification of contract properties, facilitates run-time monitoring of potential property violations, and brings clarity to contract debugging via data provenance. Specifically, a DeCon program consists of a set of declarative rules and violation query rules over the relational representation, describing the smart contract implementation and contract-level properties, respectively. We have developed a tool that can compile DeCon programs into executable Solidity programs, with instrumentation for run-time property monitoring. Our case studies demonstrate that DeCon can implement realistic smart contracts such as ERC20 and ERC721 digital tokens. Our evaluation results reveal the marginal overhead of DeCon compared to the open-source reference implementation, incurring 14% median gas overhead for execution, and another 16% median gas overhead for run-time verification.","Information systems,Data management systems,Database management system engines,Software and its engineering,Software creation and management,Software notations and tools,Context specific languages,Domain specific languages,General programming languages,Language types,Theory of computation,Theory and algorithms for application domains",声明式智能合约,本文介绍了DeCon，一种用于实现智能合约和指定合约级属性的声明性编程语言。由于观察到智能合约操作和合约级属性可以自然地表示为关系约束，DeCon将每个智能合约建模为一组存储事务记录的关系表。智能合约的这种关系表示能够方便地指定合约属性，便于在运行时监控潜在的属性违规行为，并通过数据来源为合约调试带来清晰性。具体来说，DeCon程序由关系表示上的一组声明性规则和违规查询规则组成，分别描述智能合约实现和合约级别的属性。我们开发了一种工具，可以将DeCon程序编译为可执行的Solidity程序，并提供运行时属性监控工具。我们的案例研究表明，DeCon可以实现现实的智能合约，如ERC20和ERC721数字代币。我们的评估结果显示，与开源参考实现相比，DeCon的边际开销为14%的执行气体开销中值，以及16%的运行时验证气体开销中值。,信息系统，数据管理系统，数据库管理系统引擎，软件及其工程，软件创建和管理，软件符号和工具，上下文特定语言，领域特定语言，通用编程语言，语言类型，计算理论，应用领域的理论和算法,,,
AY3MPGJV,2022,https://doi.org/10.1145/3540250.3549093,ESEC/FSE 2022,MAAT: a novel ensemble approach to addressing fairness and performance bugs for machine learning software,"Machine Learning (ML) software can lead to unfair and unethical decisions, making software fairness bugs an increasingly significant concern for software engineers. However, addressing fairness bugs often comes at the cost of introducing more ML performance (e.g., accuracy) bugs. In this paper, we propose MAAT, a novel ensemble approach to improving fairness-performance trade-off for ML software. Conventional ensemble methods combine different models with identical learning objectives. MAAT, instead, combines models optimized for different objectives: fairness and ML performance. We conduct an extensive evaluation of MAAT with 5 state-of-the-art methods, 9 software decision tasks, and 15 fairness-performance measurements. The results show that MAAT significantly outperforms the state-of-the-art. In particular, MAAT beats the trade-off baseline constructed by a recent benchmarking tool in 92.2% of the overall cases evaluated, 12.2 percentage points more than the best technique currently available. Moreover, the superiority of MAAT over the state-of-the-art holds on all the tasks and measurements that we study. We have made publicly available the code and data of this work to allow for future replication and extension.","Computing methodologies,Machine learning,Machine learning approaches,General and reference,Cross-computing tools and techniques,Hardware,Power and energy,Power estimation and optimization,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",MAAT：一种解决机器学习软件公平性和性能缺陷的新集成方法,机器学习（ML）软件可能导致不公平和不道德的决策，使软件公平漏洞成为软件工程师日益关注的问题。然而，解决公平性错误往往是以引入更多ML性能（例如准确性）错误为代价的。在本文中，我们提出了MAAT，这是一种改进ML软件公平性能权衡的新集成方法。传统的集成方法将不同的模型与相同的学习目标相结合。相反，MAAT结合了针对不同目标优化的模型：公平性和ML性能。我们用5种最先进的方法、9个软件决策任务和15个公平性性能度量对MAAT进行了广泛的评估。结果表明，MAAT的性能明显优于现有技术。特别是，在92.2%的总体评估案例中，MAAT超过了最近基准测试工具构建的权衡基线，比目前可用的最佳技术高12.2个百分点。此外，MAAT相对于最先进技术的优势适用于我们研究的所有任务和测量。我们已经公开了这项工作的代码和数据，以便将来进行复制和扩展。,计算方法，机器学习，机器学习方法，概论和参考，交叉计算工具和技术，硬件，功率和能量，功率估计和优化，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
9I82RQHX,2022,https://doi.org/10.1145/3540250.3558920,ESEC/FSE 2022,KVS: a tool for knowledge-driven vulnerability searching,"It is difficult to quickly locate and search for specific vulnerabilities and their solutions because vulnerability information is scattered in the existing vulnerability management library. To alleviate this problem, we extract knowledge from vulnerability reports and organize the vulnerability information into the form of a knowledge graph. Then, we implement a tool for knowledge-driven vulnerability searching, KVS. This tool mainly uses the BERT model to realize the vulnerability named entity recognition and construct the vulnerability knowledge graph (VulKG). Finally, we can search vulnerabilities of interest-based on VulKG. The URL of this tool is https://cinnqi.github.io/Neo4j-D3-VKG/. Video of our demo is available at https://youtu.be/FT1BaLUGPk0.","Computing methodologies,Artificial intelligence,Natural language processing,Information extraction,Information systems,Security and privacy,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Theory of computation",KVS：一种用于知识驱动漏洞搜索的工具,由于漏洞信息分散在现有的漏洞管理库中，因此很难快速定位和搜索特定的漏洞及其解决方案。为了缓解这个问题，我们从漏洞报告中提取知识，并将漏洞信息组织成知识图的形式。然后，我们实现了一个用于知识驱动漏洞搜索的工具KVS。该工具主要利用BERT模型实现漏洞命名实体识别，构建漏洞知识图（VulKG）。最后，我们可以基于VulKG搜索感兴趣的漏洞。此工具的URL为https://cinnqi.github.io/Neo4j-D3-VKG/.我们的演示视频可在https://youtu.be/FT1BaLUGPk0.,计算方法论，人工智能，自然语言处理，信息提取，信息系统，安全和隐私，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，计算理论,,,
EEV34E8Z,2022,https://doi.org/10.1145/3540250.3558940,ESEC/FSE 2022,What improves developer productivity at google? code quality,"Understanding what affects software developer productivity can help organizations choose wise investments in their technical and social environment. But the research literature either focuses on what correlates with developer productivity in ecologically valid settings or focuses on what causes developer productivity in highly constrained settings. In this paper, we bridge the gap by studying software developers at Google through two analyses. In the first analysis, we use panel data with 39 productivity factors, finding that code quality, technical debt, infrastructure tools and support, team communication, goals and priorities, and organizational change and process are all causally linked to self-reported developer productivity. In the second analysis, we use a lagged panel analysis to strengthen our causal claims. We find that increases in perceived code quality tend to be followed by increased perceived developer productivity, but not vice versa, providing the strongest evidence to date that code quality affects individual developer productivity.","General and reference,Cross-computing tools and techniques,Empirical studies,Human-centered computing,Social and professional topics,Professional topics,Management of computing and information systems,Project and people management,Software and its engineering,Software creation and management,Software development process management",是什么提高了谷歌的开发者生产力？代码质量,了解影响软件开发人员生产力的因素可以帮助组织在其技术和社会环境中选择明智的投资。但研究文献要么关注生态有效环境中与开发者生产力相关的因素，要么关注高度受限环境中导致开发者生产力的因素。在本文中，我们通过两个分析来研究谷歌的软件开发人员，从而弥合这一差距。在第一次分析中，我们使用了39个生产力因素的面板数据，发现代码质量、技术债务、基础设施工具和支持、团队沟通、目标和优先级以及组织变革和流程都与自我报告的开发人员生产力有因果关系。在第二个分析中，我们使用滞后面板分析来加强我们的因果主张。我们发现，感知代码质量的提高往往伴随着感知开发人员生产力的提高，但反之亦然，这为迄今为止代码质量影响单个开发人员生产力提供了最有力的证据。,概述和参考，交叉计算工具和技术，实证研究，以人为中心的计算，社会和专业主题，专业主题，计算和信息系统管理，项目和人员管理，软件及其工程，软件创建和管理，软件开发过程管理,,,
GEGAB3YZ,2022,https://doi.org/10.1145/3540250.3558944,ESEC/FSE 2022,Input splitting for cloud-based static application security testing platforms,"As software development teams adopt DevSecOps practices, application security is increasingly the responsibility of development teams, who are required to set up their own Static Application Security Testing (SAST) infrastructure. ","General and reference,Cross-computing tools and techniques,Security and privacy,Software and application security,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Formal methods",基于云的静态应用程序安全测试平台的输入拆分,随着软件开发团队采用DevSecOps实践，应用程序安全越来越成为开发团队的责任，他们需要建立自己的静态应用程序安全测试（SAST）基础设施。,一般和参考资料，交叉计算工具和技术，安全和隐私，软件和应用程序安全，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，形式方法,,,
VFEE5B95,2022,https://doi.org/10.1145/3540250.3549085,ESEC/FSE 2022,Fuzzing deep-learning libraries via automated relational API inference,"Deep Learning (DL) has gained wide attention in recent years. Meanwhile, bugs in DL systems can lead to serious consequences, and may even threaten human lives. As a result, a growing body of research has been dedicated to DL model testing. However, there is still limited work on testing DL libraries, e.g., PyTorch and TensorFlow, which serve as the foundations for building, training, and running DL models. Prior work on fuzzing DL libraries can only generate tests for APIs which have been invoked by documentation examples, developer tests, or DL models, leaving a large number of APIs untested. In this paper, we propose DeepREL, the first approach to automatically inferring relational APIs for more effective DL library fuzzing. Our basic hypothesis is that for a DL library under test, there may exist a number of APIs sharing similar input parameters and outputs; in this way, we can easily “borrow” test inputs from invoked APIs to test other relational APIs. Furthermore, we formalize the notion of value equivalence and status equivalence for relational APIs to serve as the oracle for effective bug finding. We have implemented DeepREL as a fully automated end-to-end relational API inference and fuzzing technique for DL libraries, which 1) automatically infers potential API relations based on API syntactic/semantic information, 2) synthesizes concrete test programs for invoking relational APIs, 3) validates the inferred relational APIs via representative test inputs, and finally 4) performs fuzzing on the verified relational APIs to find potential inconsistencies. Our evaluation on two of the most popular DL libraries, PyTorch and TensorFlow, demonstrates that DeepREL can cover 157% more APIs than state-of-the-art FreeFuzz. To date, DeepREL has detected 162 bugs in total, with 106 already confirmed by the developers as previously unknown bugs. Surprisingly, DeepREL has detected 13.5% of the high-priority bugs for the entire PyTorch issue-tracking system in a three-month period. Also, besides the 162 code bugs, we have also detected 14 documentation bugs (all confirmed).","General and reference,Cross-computing tools and techniques,Verification,Human-centered computing,Ubiquitous and mobile computing,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software reliability,Software functional properties,Theory of computation,Logic,Logic and verification,Semantics and reasoning,Program reasoning,Program verification",通过自动关系API推理实现深度学习库的模糊化,深度学习（DL）近年来得到了广泛的关注。同时，DL系统中的错误可能会导致严重后果，甚至可能威胁到人类的生命。因此，越来越多的研究致力于DL模型测试。然而，测试DL库的工作仍然有限，例如PyTorch和TensorFlow，它们是构建、训练和运行DL模型的基础。以前对DL库进行模糊化的工作只能为文档示例、开发人员测试或DL模型调用的API生成测试，从而使大量API未经测试。在本文中，我们提出了DeepREL，这是第一种自动推断关系API以实现更有效的DL库模糊化的方法。我们的基本假设是，对于正在测试的DL库，可能存在许多共享相似输入参数和输出的API；通过这种方式，我们可以很容易地从调用的API中“借用”测试输入来测试其他关系API。此外，我们对关系API的值等价和状态等价的概念进行了形式化，以作为有效查找错误的预言机。我们已经将DeepREL实现为DL库的一种完全自动化的端到端关系API推理和模糊技术，它1）基于API句法/语义信息自动推断潜在的API关系，2）合成用于调用关系API的具体测试程序，3）通过代表性测试输入验证推断的关系API，最后4）对已验证的关系API执行模糊处理，以发现潜在的不一致性。我们对两个最流行的DL库PyTorch和TensorFlow的评估表明，DeepREL可以比最先进的FreeFuzz多覆盖157%的API。到目前为止，DeepREL总共检测到162个错误，其中106个已经被开发人员确认为以前未知的错误。令人惊讶的是，DeepREL在三个月内检测到整个PyTorch问题跟踪系统13.5%的高优先级错误。此外，除了162个代码错误，我们还检测到14个文档错误（均已确认）。,一般与参考，交叉计算工具与技术，验证，以人为中心的计算，泛在与移动计算，软件及其工程，软件创建与管理，软件验证与确认，软件缺陷分析，软件测试与调试，软件组织与属性，额外功能属性，软件可靠性，软件功能属性，计算理论，逻辑，逻辑与验证，语义与推理，程序推理，程序验证,,,
7SCAMXWF,2022,https://doi.org/10.1145/3540250.3549152,ESEC/FSE 2022,Scenario-based test reduction and prioritization for multi-module autonomous driving systems,"When developing autonomous driving systems (ADS), developers often need to replay previously collected driving recordings to check the correctness of newly introduced changes to the system. However, simply replaying the entire recording is not necessary given the high redundancy of driving scenes in a recording (e.g., keeping the same lane for 10 minutes on a highway). In this pa- per, we propose a novel test reduction and prioritization approach for multi-module ADS. First, our approach automatically encodes frames in a driving recording to feature vectors based on a driving scene schema. Then, the given recording is sliced into segments based on the similarity of consecutive vectors. Lengthy segments are truncated to reduce the length of a recording and redundant segments with the same vector are removed. The remaining seg- ments are prioritized based on both the coverage and the rarity of driving scenes. We implemented this approach on an industry- level, multi-module ADS called Apollo and evaluated it on three road maps in various regression settings. The results show that our approach significantly reduced the original recordings by over 34% while keeping comparable test effectiveness, identifying almost all injected faults. Furthermore, our test prioritization method achieves about 22% to 39% and 41% to 53% improvements over three baselines in terms of both the average percentage of faults detected (APFD) and TOP-K.","Computer systems organization,Embedded and cyber-physical systems,Human-centered computing,Information systems,Information systems applications,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",基于场景的多模块自动驾驶系统测试缩减和优先级排序,在开发自动驾驶系统（ADS）时，开发人员通常需要回放以前收集的驾驶记录，以检查新引入的系统更改的正确性。然而，考虑到记录中驾驶场景的高冗余度（例如，在高速公路上保持同一车道10分钟），不需要简单地回放整个记录。在本文中，我们提出了一种新的多模块ADS测试缩减和优先级排序方法。首先，我们的方法基于驾驶场景模式将驾驶记录中的帧自动编码为特征向量。然后，基于连续向量的相似性，将给定的记录切片为片段。长度段被截断以减少记录的长度，并且具有相同矢量的冗余段被去除。其余部分根据驾驶场景的覆盖率和稀有性进行优先排序。我们在一个名为Apollo的行业级多模块ADS上实现了这种方法，并在各种回归设置中的三个路线图上对其进行了评估。结果表明，我们的方法显著减少了34%以上的原始记录，同时保持了可比的测试有效性，几乎识别了所有注入的故障。此外，就检测到的故障的平均百分比（APFD）和TOP-K而言，我们的测试优先级方法在三个基线上分别实现了约22%至39%和41%至53%的改进。,计算机系统组织，嵌入式和网络物理系统，以人为中心的计算，信息系统，信息系统应用，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
KDY9ND4A,2022,https://doi.org/10.1145/3540250.3558969,ESEC/FSE 2022,What did you pack in my app? a systematic analysis of commercial Android packers,"Commercial Android packers have been widely used by developers as a way to protect their apps from being tampered with. However, app packer is usually provided as an online service developed by security vendors, and the packed apps are well protected. It is thus hard to know what exactly is packed in the app, and few existing studies in the community have systematically analyzed the behaviors of commercial app packers. In this paper, we propose PackDiff, a dynamic analysis system to inspect the fine-grained behaviors of commercial packers. By instrumenting the Android system, PackDiff records the runtime behaviors of Android apps (e.g., Linux system call invocations, Java API calls, Binder interactions, etc.), which are further processed to pinpoint the additional sensitive behaviors introduced by packers. By applying PackDiff to roughly 200 apps protected by seven commercial packers, we observe the disappointing facts of existing commercial packers. Most app packers have introduced unnecessary behaviors (e.g., accessing sensitive data), serious performance and compatibility issues, and they can even be abused to create evasive malware and repackaged apps, which contradicts with their design purposes.","General and reference,Security and privacy,Intrusion/anomaly detection and malware mitigation,Software and application security,Software security engineering,Systems security,Operating systems security,Social and professional topics,Computing / technology policy,Software and its engineering,Software creation and management,Software verification and validation",你在我的应用程序中打包了什么？商业Android打包器的系统分析,商业安卓打包器已被开发人员广泛使用，作为保护其应用程序不被篡改的一种方式。然而，应用程序打包器通常是作为安全供应商开发的在线服务提供的，打包的应用程序受到了很好的保护。因此，很难知道应用程序中到底装了什么，社区中现有的研究也很少系统地分析商业应用程序包装商的行为。在本文中，我们提出了PackDiff，这是一个动态分析系统，用于检查商业包装商的细粒度行为。通过检测Android系统，PackDiff记录Android应用程序的运行时行为（例如，Linux系统调用调用、Java API调用、Binder交互等），并对其进行进一步处理，以确定打包器引入的其他敏感行为。通过将PackDiff应用于受七个商业打包商保护的大约200个应用程序，我们观察到了现有商业打包商令人失望的事实。大多数应用程序打包程序都引入了不必要的行为（例如访问敏感数据）、严重的性能和兼容性问题，它们甚至可能被滥用来创建规避的恶意软件和重新打包应用程序，这与它们的设计目的相矛盾。,一般和参考，安全和隐私，入侵/异常检测和恶意软件缓解，软件和应用程序安全，软件安全工程，系统安全，操作系统安全，社会和专业主题，计算/技术政策，软件及其工程，软件创建和管理，软件验证和确认,,,
9D3C2QJZ,2022,https://doi.org/10.1145/3540250.3558933,ESEC/FSE 2022,SemCluster: a semi-supervised clustering tool for crowdsourced test reports with deep image understanding,"Due to the openness of crowdsourced testing, mobile app crowdsourced testing has been subject to duplicate reports. The previous research methods extract the textual features of the crowdsourced test reports, combine with shallow image analysis, and perform unsupervised clustering on the crowdsourced test reports to clarify the duplication of crowdsourced test reports and solve the problem. However, these methods ignore the semantic connection between textual descriptions and screenshots, making the clustering results unsatisfactory and the deduplication effect less accurate.  
","Human-centered computing,Information systems,Information systems applications,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",SemCluster：一个用于众包测试报告的半监督聚类工具，具有深入的图像理解,由于众包测试的开放性，手机应用众包测试一直存在重复报告的情况。以往的研究方法提取众包测试报告的文本特征，结合浅层图像分析，对众包测试结果进行无监督聚类，以澄清众包测试的重复性，解决问题。然而，这些方法忽略了文本描述和屏幕截图之间的语义联系，使得聚类结果不令人满意，重复数据消除效果也不那么准确。,以人为中心的计算，信息系统，信息系统应用，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
6ZGME3DC,2022,https://doi.org/10.1145/3540250.3558910,ESEC/FSE 2022,Explaining and debugging pathological program behavior,"Programs fail. But which part of the input is responsible for the failure? To resolve the issue, developers must first understand how and why the program behaves as it does, notably when it deviates from the expected outcome. A program’s behavior is essentially the set of all its executions. This set is usually diverse, unpredictable, and generally unbounded. A pathological program behavior occurs once the actual outcome does not match the expected behavior. Consequently, developers must fix these issues to ensure the built system is the desired software. In our upcoming research, we want to focus on providing developers with a detailed description of the root causes that resulted in the program’s unwanted behavior. Thus, we aim to automatically produce explanations that capture the circumstances of arbitrary program behavior by correlating individual input elements (features) and their corresponding execution outcome. To this end, we use the scientific method and combine generative and predictive models, allowing us (i) to learn the statistical relations between the features of the inputs and the program behavior and (ii) to generate new inputs to refine or refute our current explanatory prediction model.","Computing methodologies,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation,Computational complexity and cryptography,Oracles and decision trees,Formal languages and automata theory,Grammars and context-free languages,Theory and algorithms for application domains,Machine learning theory,Active learning",解释和调试病态程序行为,程序失败。但是，输入的哪一部分对失败负责？为了解决这个问题，开发人员必须首先了解程序的行为方式和原因，尤其是当它偏离预期结果时。程序的行为本质上是它所有执行的集合。这个集合通常是多样的，不可预测的，并且通常是无限制的。一旦实际结果与预期行为不匹配，就会发生病理程序行为。因此，开发人员必须解决这些问题，以确保构建的系统是所需的软件。在我们即将进行的研究中，我们希望重点为开发人员提供导致程序不必要行为的根本原因的详细描述。因此，我们的目标是通过关联各个输入元素（特征）及其相应的执行结果，自动生成捕捉任意程序行为环境的解释。为此，我们使用科学的方法，将生成和预测模型相结合，使我们能够（i）学习输入特征和程序行为之间的统计关系，以及（ii）生成新的输入，以改进或反驳我们当前的解释性预测模型。,计算方法学，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论，计算复杂性和密码学，神谕和决策树，形式语言和自动机理论，文法和上下文无关语言，应用领域的理论和算法，机器学习理论，主动学习,,,
S6ZJVJAN,2022,https://doi.org/10.1145/3540250.3549126,ESEC/FSE 2022,DynaPyt: a dynamic analysis framework for Python,"Python is a widely used programming language that powers important application domains such as machine learning, data analysis, and web applications. For many programs in these domains it is consequential to analyze aspects like security and performance, and with Python’s dynamic nature, it is crucial to be able to dynamically analyze Python programs. However, existing tools and frameworks  
do not provide the means to implement dynamic analyses easily and practitioners resort to implementing an ad-hoc dynamic analysis for their own use case. This work presents DynaPyt, the first general-purpose framework for heavy-weight dynamic analysis of Python programs. Compared to existing tools for other programming languages, our framework provides a wider range of analysis hooks arranged in a hierarchical structure, which allows developers to concisely implement analyses. DynaPyt features selective instrumentation and execution modification as well. We evaluate our framework on test suites of 9 popular open-source Python projects, 1,268,545 lines of code in total, and show that it, by and large, preserves the semantics of the original execution. The running time of DynaPyt is between 1.2x and 16x times the original execution time, which is in line with similar frameworks designed for other languages, and 5.6%–88.6% faster than analyses using a built-in tracing API offered by Python. We also implement multiple analyses, show the simplicity of implementing them and some potential use cases of DynaPyt. Among the analyses implemented are: an analysis to detect a memory blow up in Pytorch programs, a taint analysis to detect SQL injections, and an analysis to warn about a runtime performance anti-pattern.","General and reference,Cross-computing tools and techniques,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software notations and tools,General programming languages,Software maintenance tools,Theory of computation,Semantics and reasoning,Program reasoning,Program analysis",DynaPyt:Python的动态分析框架,Python是一种广泛使用的编程语言，为机器学习、数据分析和web应用程序等重要应用领域提供支持。对于这些领域中的许多程序来说，分析安全性和性能等方面是至关重要的，而鉴于Python的动态特性，能够动态分析Python程序是至关重要的。然而，现有的工具和框架,通则和参考文献，交叉计算工具和技术，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件符号和工具，通用编程语言，软件维护工具，计算理论，语义和推理，程序推理，程序分析,,,
J3KTQKRB,2022,https://doi.org/10.1145/3540250.3558928,ESEC/FSE 2022,TAPHSIR: towards AnaPHoric ambiguity detection and ReSolution in requirements,"We introduce TAPHSIR – a tool for anaphoric ambiguity detection and anaphora resolution in requirements. TAPHSIR facilities reviewing the use of pronouns in a requirements specification and revising those pronouns that can lead to misunderstandings during the development process. To this end, TAPHSIR detects the requirements which have potential anaphoric ambiguity and further attempts interpreting anaphora occurrences automatically. TAPHSIR employs a hybrid solution composed of an ambiguity detection solution based on machine learning and an anaphora resolution solution based on a variant of the BERT language model. Given a requirements specification, TAPHSIR decides for each pronoun occurrence in the specification whether the pronoun is ambiguous or unambiguous, and further provides an automatic interpretation for the pronoun. The output generated by TAPHSIR can be easily reviewed and validated by requirements engineers. TAPHSIR is publicly available on Zenodo (https://doi.org/10.5281/zenodo.5902117).","Computing methodologies,Artificial intelligence,Natural language processing,Machine learning,Software and its engineering,Software creation and management,Designing software,Requirements analysis,Software development process management,Software development methods,Software verification and validation,Software notations and tools,System description languages,Specification languages",TAPHSIR：面向需求中的AnaPHoric模糊检测和ReSolution,我们在需求中介绍了TAPHSIR——一种用于回指歧义检测和回指解析的工具。TAPHSIR设施审查需求规范中代词的使用，并修改那些在开发过程中可能导致误解的代词。为此，TAPHSIR检测到潜在的回指歧义需求，并进一步尝试自动解释回指现象。TAPHSIR采用了一种混合解决方案，包括基于机器学习的模糊检测解决方案和基于BERT语言模型变体的回指解析解决方案。给定一个需求规范，TAPHSIR为规范中的每个代词的出现决定代词是模糊的还是毫不含糊的，并进一步为代词提供自动解释。需求工程师可以很容易地审查和验证TAPHSIR生成的输出。TAPHSIR在Zenodo上公开(https://doi.org/10.5281/zenodo.5902117)。,计算方法论，人工智能，自然语言处理，机器学习，软件及其工程，软件创建和管理，设计软件，需求分析，软件开发过程管理，软件开发方法，软件验证和确认，软件符号和工具，系统描述语言，规范语言,,,
6VI6H9CN,2022,https://doi.org/10.1145/3540250.3558916,ESEC/FSE 2022,WikiDoMiner: wikipedia domain-specific miner,"We introduce WikiDoMiner – a tool for automatically generating domain-specific corpora by crawling Wikipedia. WikiDoMiner helps requirements engineers create an external knowledge resource that is specific to the underlying domain of a given requirements specification (RS). Being able to build such a resource is important since domain-specific datasets are scarce. WikiDoMiner generates a corpus by first extracting a set of domain-specific keywords from a given RS, and then querying Wikipedia for these keywords. The output of WikiDoMiner is a set of Wikipedia articles relevant to the domain of the input RS. Mining Wikipedia for domain-specific knowledge can be beneficial for multiple requirements engineering tasks, e.g., ambiguity handling, requirements classification, and question answering. WikiDoMiner is publicly available on Zenodo under an open-source license (https: //doi.org/10.5281/zenodo.6672682)","Computing methodologies,Artificial intelligence,Natural language processing,Language resources,Information systems,Software and its engineering,Software creation and management,Designing software,Requirements analysis,Software notations and tools",WikiDoMiner:维基百科领域特定的矿工,我们介绍了WikiDoMiner——一种通过抓取维基百科自动生成特定领域语料库的工具。WikiDoMiner帮助需求工程师创建特定于给定需求规范（RS）底层领域的外部知识资源。能够构建这样的资源很重要，因为特定领域的数据集很少。WikiDoMiner通过首先从给定的RS中提取一组特定于领域的关键词，然后在维基百科中查询这些关键词来生成语料库。WikiDoMiner的输出是一组与输入RS的领域相关的维基百科文章。为领域特定知识挖掘维基百科可能有利于多个需求工程任务，例如模糊处理、需求分类和问题回答。WikiDoMiner在Zenodo上以开源许可证公开提供（https://doi.org/10.5281/Zenodo.66672682）,计算方法论，人工智能，自然语言处理，语言资源，信息系统，软件及其工程，软件创建和管理，设计软件，需求分析，软件符号和工具,,,
ATS7EA4B,2022,https://doi.org/10.1145/3540250.3549167,ESEC/FSE 2022,"A case study of implicit mentoring, its prevalence, and impact in Apache","Mentoring is traditionally viewed as a dyadic, top-down apprenticeship. This perspective, however, overlooks other forms of informal mentoring taking place in everyday activities in which developers invest time and effort. Here, we investigate informal mentoring taking place in Open Source Software (OSS). We define a specific type of informal mentoring—implicit mentoring—situations where contributors guide others through instructions and suggestions embedded in everyday (OSS) activities. We defined implicit mentoring by first performing a review of related work on mentoring, and then through formative interviews with OSS contributors and member-checking. Next, through an empirical investigation of Pull Requests (PRs) in 37 Apache Projects, we built a classifier to extract implicit mentoring. Our analysis of 107,895 PRs shows that implicit mentoring does occur through code reviews (27.41% of all PRs included implicit mentoring) and is beneficial for both mentors and mentees. We analyzed the impact of implicit mentoring on OSS contributors by investigating their contributions and learning trajectories in their projects. Through an online survey (N=231), we then triangulated these results and identified the potential benefits of implicit mentoring from OSS contributors’ perspectives.","Human-centered computing,Collaborative and social computing,Collaborative and social computing systems and tools,Open source software,Social and professional topics,Professional topics,Management of computing and information systems,Project and people management,Software and its engineering,Software creation and management,Collaboration in software development,Programming teams,Software notations and tools,Software configuration management and version control systems",内隐辅导的案例研究、普遍性及其在Apache中的影响,传统上，辅导被视为一种自上而下的二元学徒制。然而，这种观点忽略了开发人员在日常活动中投入时间和精力的其他形式的非正式指导。在这里，我们调查了在开放源码软件（OSS）中进行的非正式辅导。我们定义了一种特定类型的非正式辅导——隐性辅导——参与者通过嵌入日常（OSS）活动中的指示和建议来指导他人的情况。我们通过首先对辅导的相关工作进行回顾，然后通过对OSS贡献者的形成性访谈和成员检查来定义隐性辅导。接下来，通过对37个Apache项目中的Pull Request（PR）的实证调查，我们构建了一个分类器来提取隐式指导。我们对107895个PR的分析表明，隐式辅导确实通过代码审查发生（27.41%的PR包括隐式辅导），对导师和学员都有利。我们通过调查OSS贡献者在项目中的贡献和学习轨迹，分析了隐性辅导对他们的影响。通过一项在线调查（N=231），我们对这些结果进行了三角分析，并从OSS贡献者的角度确定了隐性辅导的潜在好处。,以人为中心的计算，协作和社会计算，协作和社会计算系统和工具，开源软件，社会和专业主题，专业主题，计算和信息系统管理，项目和人员管理，软件及其工程，软件创作和管理，软件开发协作，编程团队，软件符号和工具，软件配置管理和版本控制系统,,,
G9A927YM,2022,https://doi.org/10.1145/3540250.3549177,ESEC/FSE 2022,First come first served: the impact of file position on code review,"The most popular code review tools (e.g., Gerrit and GitHub) present the files to review sorted in alphabetical order. Could this choice or, more generally, the relative position in which a file is presented bias the outcome of code reviews? We investigate this hypothesis by triangulating complementary evidence in a two-step study.  
First, we observe developers’ code review activity. We analyze the review comments pertaining to 219,476 Pull Requests (PRs) from 138 popular Java projects on GitHub. We found files shown earlier in a PR to receive more comments than files shown later, also when controlling for possible confounding factors: e.g., the presence of discussion threads or the lines added in a file. Second, we measure the impact of file position on defect finding in code review. Recruit- ing 106 participants, we conduct an online controlled experiment in which we measure participants’ performance in detecting two unrelated defects seeded into two different files. Participants are assigned to one of two treatments in which the position of the defective files is switched. For one type of defect, participants are not affected by its file’s position; for the other, they have 64% lower odds to identify it when its file is last as opposed to first. Overall, our findings provide evidence that the relative position in which files are presented has an impact on code reviews’ outcome; we discuss these results and implications for tool design and code review.","Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Software verification and validation,Empirical software validation,Software notations and tools,Software configuration management and version control systems",先到先得：文件位置对代码评审的影响,最流行的代码审查工具（例如Gerrit和GitHub）按照字母顺序显示要审查的文件。这种选择，或者更普遍地说，文件呈现的相对位置，会对代码评审的结果产生偏见吗？我们通过在两步研究中对互补证据进行三角分析来研究这一假设。,社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发中的合作，软件验证和确认，经验软件验证，软件符号和工具，软件配置管理和版本控制系统,,,
W3ZMW4GR,2022,https://doi.org/10.1145/3540250.3558905,ESEC/FSE 2022,Automated capacity analysis of limitation-aware microservices architectures,"Over the last years, the concept of API economy has fostered the creation of an ecosystem of public APIs used as business elements. These APIs include various pricing plans, which allow developers to consume an API for a specific price and under certain conditions. These conditions include capacity limits, a.k.a. limitations, that limit the usage of the API. Additionally, modern web applications are usually based on a microservices architecture (MSA), in which multiple services communicate with each other through public APIs using a standardized paradigm, commonly RESTful. When an MSA consumes external APIs with limitations, it is necessary to analyse the impact of these limitations in its capacity. These MSAs are known as Limitation-Aware Microservices Architecture (LAMA). This PhD dissertation aims to provide an automated framework to analyse the capacity of a LAMA given the formal description of its internal topology and the external pricing plans. This framework would be used to solve analysis operations, which deal with the extraction of useful information that helps developers build their LAMAs.","Applied computing,Enterprise computing,Service-oriented architectures,General and reference,Information systems,World Wide Web,Web services,RESTful web services,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software organization and properties,Software system structures,Software architectures",限制感知微服务架构的自动化容量分析,在过去的几年里，API经济的概念促进了作为商业元素的公共API生态系统的创建。这些API包括各种定价计划，允许开发人员在特定价格和特定条件下使用API。这些条件包括容量限制，也就是限制API的使用。此外，现代web应用程序通常基于微服务架构（MSA），其中多个服务通过公共API使用标准化范式（通常为RESTful）相互通信。当MSA使用有限制的外部API时，有必要分析这些限制对其容量的影响。这些MSA被称为限制感知微服务体系结构（LAMA）。这篇博士论文旨在提供一个自动化的框架来分析LAMA的容量，给出其内部拓扑结构和外部定价计划的正式描述。该框架将用于解决分析操作，分析操作处理有用信息的提取，帮助开发人员构建LAMA。,应用计算，企业计算，面向服务的架构，通用和参考，信息系统，万维网，Web服务，RESTful Web服务，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件组织和属性，软件系统结构，软件架构,,,
XYDIHGMG,2022,https://doi.org/10.1145/3540250.3549098,ESEC/FSE 2022,VulRepair: a T5-based automated software vulnerability repair,"As software vulnerabilities grow in volume and complexity, researchers proposed various Artificial Intelligence (AI)-based approaches to help under-resourced security analysts to find, detect, and localize vulnerabilities. However, security analysts still have to spend a huge amount of effort to manually fix or repair such vulnerable functions. Recent work proposed an NMT-based Automated Vulnerability Repair, but it is still far from perfect due to various limitations. In this paper, we propose VulRepair, a T5-based automated software vulnerability repair approach that leverages the pre-training and BPE components to address various technical limitations of prior work. Through an extensive experiment with over 8,482 vulnerability fixes from 1,754 real-world software projects, we find that our VulRepair achieves a Perfect Prediction of 44%, which is 13%-21% more accurate than competitive baseline approaches. These results lead us to conclude that our VulRepair is considerably more accurate than two baseline approaches, highlighting the substantial advancement of NMT-based Automated Vulnerability Repairs. Our additional investigation also shows that our VulRepair can accurately repair as many as 745 out of 1,706 real-world well-known vulnerabilities (e.g., Use After Free, Improper Input Validation, OS Command Injection), demonstrating the practicality and significance of our VulRepair for generating vulnerability repairs, helping under-resourced security analysts on fixing vulnerabilities.","Security and privacy,Systems security,Vulnerability management,Social and professional topics,Computing / technology policy,Computer crime,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",VulRepair：一种基于T5的自动化软件漏洞修复,随着软件漏洞的数量和复杂性不断增加，研究人员提出了各种基于人工智能的方法，以帮助资源不足的安全分析师发现、检测和定位漏洞。然而，安全分析师仍然需要花费大量精力来手动修复或修复这些易受攻击的功能。最近的工作提出了一种基于NMT的漏洞自动修复，但由于各种限制，它还远远不够完善。在本文中，我们提出了VulRepair，这是一种基于T5的自动化软件漏洞修复方法，它利用预训练和BPE组件来解决先前工作的各种技术限制。通过对1754个真实世界软件项目中超过#8482漏洞修复的广泛实验，我们发现我们的VulRepair实现了44%的完美预测，比竞争对手的基线方法准确13%-21%。这些结果使我们得出结论，我们的VulRepair比两种基线方法准确得多，突出了基于NMT的自动漏洞修复的实质性进步。我们的额外调查还表明，我们的VulRepair可以准确修复1706个现实世界中众所周知的漏洞中的745个（例如，释放后使用、不当输入验证、操作系统命令注入），证明了我们的Vul repair在生成漏洞修复方面的实用性和重要性，帮助资源不足的安全分析师修复漏洞。,安全和隐私，系统安全，漏洞管理，社会和专业话题，计算/技术政策，计算机犯罪，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
W7UIDZ68,2022,https://doi.org/10.1145/3540250.3558947,ESEC/FSE 2022,Investigating and improving log parsing in practice,"Logs are widely used for system behavior diagnosis by automatic log mining. Log parsing is an important data preprocessing step that converts semi-structured log messages into structured data as the feature input for log mining. Currently, many studies are devoted to proposing new log parsers. However, to the best of our knowledge, no previous study comprehensively investigates the effectiveness of log parsers in industrial practice. To investigate the effectiveness of the log parsers in industrial practice, in this paper, we conduct an empirical study on the effectiveness of six state-of-the-art log parsers on 10 microservice applications of Ant Group. Our empirical results highlight two challenges for log parsing in practice: 1) various separators. There are various separators in a log message, and the separators in different event templates or different applications are also various. Current log parsers cannot perform well because they do not consider various separators. 2) Various lengths due to nested objects. The log messages belonging to the same event template may also have various lengths due to nested objects. The log messages of 6 out of 10 microservice applications at Ant Group with various lengths due to nested objects. 4 out of 6 state-of-the-art log parsers cannot deal with various lengths due to nested objects. In this paper, we propose an improved log parser named Drain+ based on a state-of-the-art log parser Drain. Drain+ includes two innovative components to address the above two challenges: a statistical-based separators generation component, which generates separators automatically for log message splitting, and a candidate event template merging component, which merges the candidate event templates by a template similarity method. We evaluate the effectiveness of Drain+ on 10 microservice applications of Ant Group and 16 public datasets. The results show that Drain+ outperforms the six state-of-the-art log parsers on industrial applications and public datasets. Finally, we conclude the observations in the road ahead for log parsing to inspire other researchers and practitioners.","General and reference,Information systems,Information systems applications,Data mining,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software organization and properties",日志解析在实践中的探索与改进,日志通过自动日志挖掘被广泛用于系统行为诊断。日志解析是一个重要的数据预处理步骤，它将半结构化的日志消息转换为结构化数据，作为日志挖掘的特征输入。目前，许多研究致力于提出新的日志解析器。然而，据我们所知，以前没有任何研究全面调查日志解析器在工业实践中的有效性。为了研究日志解析器在工业实践中的有效性，本文对蚂蚁集团10个微服务应用程序中6个最先进的日志解析器的有效性进行了实证研究。我们的实证结果突出了日志解析在实践中面临的两个挑战：1）各种分离器。日志消息中有各种分隔符，不同事件模板或不同应用程序中的分隔符也各不相同。当前的日志解析器不能很好地执行，因为它们没有考虑各种分隔符。2） 由于嵌套对象而导致的各种长度。由于嵌套的对象，属于同一事件模板的日志消息也可能具有不同的长度。蚂蚁集团10个微服务应用程序中有6个的日志消息由于嵌套对象而具有不同的长度。6个最先进的日志解析器中有4个由于嵌套对象而无法处理各种长度。在本文中，我们在最先进的日志解析器Drain的基础上提出了一个改进的日志解析器，名为Drain+。Drain+包括两个创新组件来解决上述两个挑战：一个是基于统计的分隔符生成组件，它自动生成用于日志消息拆分的分隔符；另一个是候选事件模板合并组件，它通过模板相似性方法合并候选事件模板。我们在蚂蚁集团的10个微服务应用和16个公共数据集上评估了Drain+的有效性。结果表明，Drain+在工业应用程序和公共数据集上的性能优于六个最先进的日志解析器。最后，我们总结了日志解析未来的观察结果，以激励其他研究人员和从业者。,概述和参考，信息系统，信息系统应用，数据挖掘，软件及其工程，软件创建和管理，软件后开发问题，维护软件，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件组织和属性,,,
YXTIN74J,2022,https://doi.org/10.1145/3540250.3549090,ESEC/FSE 2022,Demystifying the underground ecosystem of account registration bots,"Member services are a core part of most online systems. For example, member services in online social networks and video platforms make it possible to serve users customized content or track their footprint for a recommendation. However, there is a dark side to membership that lurks behind influencer marketing, coupon harvesting, and spreading fake news. All these activities rely heavily on owning masses of fake accounts, and to create new accounts efficiently, malicious registrants use automated registration bots with anti-human verification services that can easily bypass a website’s security strategies. ","Security and privacy,Intrusion/anomaly detection and malware mitigation,Software and application security,Systems security,Social and professional topics,Computing / technology policy,Computer crime",破解账户注册机器人的地下生态系统,会员服务是大多数在线系统的核心部分。例如，在线社交网络和视频平台中的会员服务可以为用户提供定制内容或跟踪他们的推荐足迹。然而，在影响者营销、优惠券获取和传播假新闻的背后，隐藏着会员制的黑暗面。所有这些活动都严重依赖于拥有大量虚假账户，为了高效创建新账户，恶意注册者使用带有反人类验证服务的自动注册机器人，可以轻松绕过网站的安全策略。,安全和隐私，入侵/异常检测和恶意软件缓解，软件和应用程序安全，系统安全，社会和专业主题，计算/技术政策，计算机犯罪,,,
E2FGFX9X,2022,https://doi.org/10.1145/3540250.3549096,ESEC/FSE 2022,DeepDev-PERF: a deep learning-based approach for improving software performance,"Improving software performance is an important yet challenging part of the software development cycle. Today, the majority of performance inefficiencies are identified and patched by performance experts. Recent advancements in deep learning approaches and the wide-spread availability of open-source data creates a great opportunity to automate the identification and patching of performance problems. In this paper, we present DeepDev-PERF, a transformer-based approach to suggest performance improvements for C# applications. We pretrain DeepDev-PERF on English and Source code corpora, followed by finetuning for the task of generating performance improvement patches for C# applications. Our evaluation shows that our model can generate the same performance improvement suggestion as the developer fix in  ‍53","Computing methodologies,Artificial intelligence,Machine learning,Machine learning approaches,General and reference,Cross-computing tools and techniques,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software notations and tools,Software organization and properties,Extra-functional properties,Software performance",DeepDev PERF：一种基于深度学习的提高软件性能的方法,提高软件性能是软件开发周期中一个重要但具有挑战性的部分。如今，大多数性能低下都是由性能专家发现并修补的。深度学习方法的最新进展和开源数据的广泛可用性为自动识别和修补性能问题创造了一个绝佳的机会。在本文中，我们介绍了DeepDev PERF，这是一种基于转换器的方法，旨在为C#应用程序提出性能改进建议。我们在英语和源代码语料库上预训练DeepDev PERF，然后对生成C#应用程序性能改进补丁的任务进行微调。我们的评估表明，我们的模型可以生成与开发人员在中修复的相同的性能改进建议 ‍53,计算方法，人工智能，机器学习，机器学习方法，概论和参考，交叉计算工具和技术，社会和专业主题，专业主题，计算和信息系统的管理，软件及其工程，软件创建和管理，软件符号和工具，软件组织和属性，额外功能属性，软件性能,,,
HSV3RH3A,2022,https://doi.org/10.1145/3540250.3549120,ESEC/FSE 2022,Neural termination analysis,"We introduce a novel approach to the automated termination analysis of  
computer programs: we use neural networks to represent ranking functions.  
Ranking functions map program states to values that are bounded from below  
and decrease as a program runs; the existence of a ranking function proves  
that the program terminates. We train a neural network from sampled  
execution traces of a program so that the network's output decreases along  
the traces; then, we use symbolic reasoning to formally verify that it  
generalises to all possible executions. Upon the affirmative answer we obtain a  
formal certificate of termination for the program, which we call a neural  
ranking function. We demonstrate that, thanks to the ability of neural  
networks to represent nonlinear functions, our method succeeds over programs  
that are beyond the reach of state-of-the-art tools. This includes programs  
that use disjunctions in their loop conditions and programs that include  
nonlinear expressions.","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software organization and properties,Software functional properties,Theory of computation,Logic,Semantics and reasoning,Program reasoning,Program analysis,Program verification",神经终止分析,我们介绍了一种新的自动终止分析方法,计算方法，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件验证和确认，形式软件验证，软件组织和性质，软件功能性质，计算理论，逻辑，语义和推理，程序推理，程序分析，程序验证,,,
N77XLEAM,2022,https://doi.org/10.1145/3540250.3560878,ESEC/FSE 2022,A study on identifying code author from real development,"Identifying code authors is important in many research topics, and various approaches have been proposed. Although these approaches achieve promising results on their datasets, their true effectiveness is still in question. To the best of our knowledge, only one large-scale study was conducted to explore the impacts of related factors (e.g., the temporal effect and the distribution of files per author). This study selected Google Code Jam programs as their subjects, but such programs are quite different from the source files that programmers write in daily development. To understand their effectiveness and challenges, we replicate their study and use their approach to analyze source files that are retrieved from real projects. The prior study claims that the temporal effect and the distribution of files per author have only minor impacts on their trained models. In the contrast, we find that in 85.48% pairs of training and testing sets, the accuracy of a trained model is less effective when the temporal effect is considered, and in total, the average accuracy decreases by 0.4298. In addition, when we use the real distribution of files as inputs, their approach can accurately identify only one or two core code authors, although a project can have more than ten authors. By revealing the limitations of the prior approach, our study sheds lights on where to make future improvements.","Computing methodologies,Artificial intelligence,Natural language processing,Information systems,Information systems applications,Data mining,Security and privacy,Intrusion/anomaly detection and malware mitigation,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software notations and tools,Software configuration management and version control systems,Software libraries and repositories,Software maintenance tools",从实际开发中识别代码作者的研究,识别代码作者在许多研究主题中都很重要，已经提出了各种方法。尽管这些方法在其数据集上取得了有希望的结果，但其真正有效性仍存在疑问。据我们所知，只进行了一项大规模研究来探索相关因素的影响（例如，时间效应和每个作者的文件分布）。这项研究选择了谷歌代码果酱程序作为研究对象，但这些程序与程序员在日常开发中编写的源文件截然不同。为了了解他们的有效性和挑战，我们复制了他们的研究，并使用他们的方法来分析从真实项目中检索的源文件。先前的研究声称，时间效应和每个作者的文件分布对他们训练的模型只有很小的影响。相反，我们发现，在85.48%的训练集和测试集对中，当考虑时间效应时，训练模型的准确性较差，总的来说，平均准确性降低了0.4298。此外，当我们使用文件的真实分布作为输入时，他们的方法只能准确地识别一两个核心代码作者，尽管一个项目可能有十多个作者。通过揭示先前方法的局限性，我们的研究揭示了未来改进的方向。,计算方法，人工智能，自然语言处理，信息系统，信息系统应用，数据挖掘，安全和隐私，入侵/异常检测和恶意软件缓解，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件符号和工具，软件配置管理和版本控制系统，软件库和存储库，软件维护工具,,,
UXHPI6D9,2022,https://doi.org/10.1145/3540250.3549114,ESEC/FSE 2022,The evolution of type annotations in python: an empirical study,"Type annotations and gradual type checkers attempt to reveal errors and facilitate maintenance in dynamically typed programming languages. Despite the availability of these features and tools, it is currently unclear how quickly developers are adopting them, what strategies they follow when doing so, and whether adding type annotations reveals more type errors. This paper presents the first large-scale empirical study of the evolution of type annotations and type errors in Python. The study is based on an analysis of 1,414,936 type annotation changes, which we extract from 1,123,393 commits among 9,655 projects. Our results show that (i) type annotations are getting more popular, and once added, often remain unchanged in the projects for a long time, (ii) projects follow three evolution patterns for type annotation usage -- regular annotation, type sprints, and occasional uses -- and that the used pattern correlates with the number of contributors, (iii) more type annotations help find more type errors (0.704 correlation), but nevertheless, many commits (78.3%) are committed despite having such errors. Our findings show that better developer training and automated techniques for adding type annotations are needed, as most code still remains unannotated, and they call for a better integration of gradual type checking into the development process.","General and reference,Cross-computing tools and techniques,Empirical studies,Software and its engineering,Software creation and management,Software post-development issues,Software evolution,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,General programming languages,Language features,Software organization and properties,Software functional properties,Formal methods,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",python中类型注释的演变：一项实证研究,类型注释和渐进式类型检查器试图在动态类型编程语言中揭示错误并便于维护。尽管有这些功能和工具可用，但目前尚不清楚开发人员采用它们的速度有多快，他们在采用这些功能时遵循了什么策略，以及添加类型注释是否会显示更多的类型错误。本文首次对Python中类型注释和类型错误的演变进行了大规模的实证研究。该研究基于对1414936个类型注释更改的分析，我们从9655个项目中的1123393个提交中提取了这些更改。我们的结果表明，（i）类型注释越来越受欢迎，一旦添加，通常在项目中长期保持不变；（ii）项目遵循类型注释使用的三种演变模式——常规注释、类型冲刺和偶尔使用——并且使用的模式与贡献者的数量相关，（iii）更多的类型注释有助于发现更多的类型错误（0.704相关性），但尽管如此，仍有许多提交（78.3%）被提交，尽管存在此类错误。我们的研究结果表明，需要更好的开发人员培训和添加类型注释的自动化技术，因为大多数代码仍然没有注释，它们要求将渐进的类型检查更好地集成到开发过程中。,概论和参考文献，交叉计算工具和技术，经验研究，软件及其工程，软件创建和管理，软件后开发问题，软件演化，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，通用编程语言，语言特征，软件组织和属性，软件功能特性，形式方法，计算理论，语义和推理，程序推理，程序验证,,,
HD2F65SR,2022,https://doi.org/10.1145/3540250.3560885,ESEC/FSE 2022,Group-based corpus scheduling for parallel fuzzing,"Parallel fuzzing relies on hardware resources to guarantee test throughput and efficiency. In industrial practice, it is well known that parallel fuzzing faces the challenge of task division, but most works neglect the important process of corpus allocation. In this paper, we proposed a group-based corpus scheduling strategy to address these two issues, which has been accepted by the LLVM community. And we implement a parallel fuzzer based on this strategy called glibFuzzer. glibFuzzer first groups the global corpus into different subsets and then assigns different energy scores and different scores to them. The energy scores were mainly determined by the seed size and the length of coverage information, and the difference score can describe the degree of difference in the code covered by different subsets of seeds. In each round of key local corpus construction, the master node selects high-quality seeds by combining the two scores to improve test efficiency and avoid task conflict. To prove the effectiveness of the strategy, we conducted an extensive evaluation on the real-world programs and FuzzBench. After 4×24 CPU-hours, glibFuzzer covered 22.02% more branches and executed 19.42 times more test cases than libFuzzer in 18 real-world programs. glibFuzzer showed an average branch coverage increase of 73.02%, 55.02%, 55.86% over AFL, PAFL, UniFuzz, respectively. More importantly, glibFuzzer found over 100 unique vulnerabilities.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Theory of computation",基于组的并行模糊语料库调度,并行模糊化依靠硬件资源来保证测试吞吐量和效率。在工业实践中，众所周知，并行模糊处理面临着任务划分的挑战，但大多数工作忽略了语料库分配的重要过程。在本文中，我们提出了一种基于组的语料库调度策略来解决这两个问题，该策略已被LLVM社区所接受。并在此策略的基础上实现了一个并行模糊器glibFuzzer。glibFuzzer首先将全局语料库分组为不同的子集，然后为它们分配不同的能量分数和不同的分数。能量得分主要由种子大小和覆盖信息的长度决定，差异得分可以描述不同种子子集覆盖的代码的差异程度。在每一轮关键的局部语料库构建中，主节点通过结合两个分数来选择高质量的种子，以提高测试效率，避免任务冲突。为了证明该策略的有效性，我们对真实世界的程序和FuzzBench进行了广泛的评估。在4×24个CPU小时后，glibFuzzer在18个真实世界的程序中覆盖了22.02%的分支，执行了19.42倍于libFuzzer的测试用例。glibFuzzer的平均分支覆盖率分别比AFL、PAFL和UniFuzz增加了73.02%、55.02%和55.86%。更重要的是，glibFuzzer发现了100多个独特的漏洞。,安全与隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，计算理论,,,
FZRT9UE9,2022,https://doi.org/10.1145/3540250.3569444,ESEC/FSE 2022,"AI-assisted programming: applications, user experiences, and neuro-symbolic techniques (keynote)","AI can enhance programming experiences for a diverse set of programmers: from professional developers and data scientists (proficient programmers) who need help in software engineering and data wrangling, all the way to spreadsheet users (low-code programmers) who need help in authoring formulas, and students (novice programmers) who seek hints when stuck with their programming homework. To communicate their need to AI, users can express their intent explicitly—as input-output examples or natural-language specification—or implicitly—where they encounter a bug (and expect AI to suggest a fix), or simply allow AI to observe their last few lines of code or edits (to have it suggest the next steps).  
","Computing methodologies,Artificial intelligence,Knowledge representation and reasoning,Machine learning,Human-centered computing,Human computer interaction (HCI),Software and its engineering,Software creation and management,Software development techniques,Automatic programming",人工智能辅助编程：应用程序、用户体验和神经符号技术（主题演讲）,人工智能可以增强不同程序员的编程体验：从在软件工程和数据争论中需要帮助的专业开发人员和数据科学家（熟练的程序员），到在编写公式时需要帮助的电子表格用户（低代码程序员），以及在做编程作业时寻求提示的学生（新手程序员）。为了向人工智能传达他们的需求，用户可以明确地表达他们的意图——作为输入输出示例或自然语言规范——或者隐含地表达他们遇到错误的地方（并期望人工智能建议修复），或者简单地允许人工智能观察他们最后几行代码或编辑（让它建议下一步行动）。,计算方法论，人工智能，知识表示和推理，机器学习，以人为中心的计算，人机交互(HCI)，软件及其工程，软件创建和管理，软件开发技术，自动编程,,,
9D9UCKCJ,2022,https://doi.org/10.1145/3540250.3549159,ESEC/FSE 2022,Detecting Simulink compiler bugs via controllable zombie blocks mutation,"As a popular Cyber-Physical System (CPS) development tool chain, MathWorks Simulink is widely used to prototype CPS models in safety-critical applications, e.g., aerospace and healthcare. It is crucial to ensure the correctness and reliability of Simulink compiler (i.e., the compiler module of Simulink) in practice since all CPS models depend on compilation. However, Simulink compiler testing is challenging due to millions of lines of source code and the lack of the complete formal language specification. Although several methods have been proposed to automatically test Simulink compiler, there still remains two challenges to be tackled, namely the limited variant space and the insufficient mutation diversity. To address these challenges, we propose COMBAT, a new differential testing method for Simulink compiler testing. COMBAT includes an EMI (Equivalence Modulo Input) mutation component and a diverse variant generation component. The EMI mutation component inserts assertion statements (e.g., If /While blocks) at arbitrary points of the seed CPS model. These statements break each insertion point into true and false branches. Then, COMBAT feeds all the data passed through the insertion point into the true branch to preserve the equivalence of CPS variants. In such a way, the body of the false branch could be viewed as a new variant space, thus addressing the first challenge. The diverse variant generation component uses Markov chain Monte Carlo optimization to sample the seed CPS model and generate complex mutations of long sequences of blocks in the variant space, thus addressing the second challenge. Experiments demonstrate that COMBAT significantly outperforms the state-of-the-art approaches in Simulink compiler testing. Within five months, COMBAT has reported 16 valid bugs for Simulink R2021b, of which 11 bugs have been confirmed as new bugs by MathWorks Support.","Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software defect analysis,Software testing and debugging,Software notations and tools,Compilers,Software organization and properties,Extra-functional properties,Software reliability,Software functional properties,Formal methods,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",通过可控僵尸块突变检测Simulink编译器错误,作为一种流行的网络物理系统（CPS）开发工具链，MathWorks-Simulink被广泛用于航空航天和医疗保健等安全关键应用中的CPS模型原型。由于所有CPS模型都依赖于编译，因此在实践中确保Simulink编译器（即Simulink的编译器模块）的正确性和可靠性至关重要。然而，由于数百万行的源代码和缺乏完整的正式语言规范，Simulink编译器测试具有挑战性。尽管已经提出了几种方法来自动测试Simulink编译器，但仍有两个挑战需要解决，即变量空间有限和变异多样性不足。为了应对这些挑战，我们提出了COMBAT，这是一种用于Simulink编译器测试的新的差分测试方法。COMBAT包括一个EMI（等效模输入）突变组件和一个多样化的变体生成组件。EMI突变组件在种子CPS模型的任意点插入断言语句（例如，If/While块）。这些语句将每个插入点分解为true和false分支。然后，COMBAT将通过插入点传递的所有数据馈送到真正的分支中，以保持CPS变体的等效性。通过这种方式，假分支的主体可以被视为一个新的变体空间，从而解决了第一个挑战。多样性变体生成组件使用马尔可夫链蒙特卡罗优化对种子CPS模型进行采样，并在变体空间中生成长序列块的复杂突变，从而解决第二个挑战。实验表明，COMBAT在Simulink编译器测试中明显优于最先进的方法。在五个月内，COMBAT报告了16个Simulink R2021b的有效错误，其中11个错误已被MathWorks支持部门确认为新错误。,软件及其工程，软件创建和管理，软件验证和确认，形式软件验证，软件缺陷分析，软件测试和调试，软件符号和工具，编译器，软件组织和属性，额外功能属性，软件可靠性，软件功能属性，形式化方法，计算理论，语义和推理，程序推理，程序验证,,,
BGDFVWKU,2022,https://doi.org/10.1145/3540250.3558918,ESEC/FSE 2022,CLIFuzzer: mining grammars for command-line invocations,"The behavior of command-line utilities can be very much influenced by passing command-line options and arguments—configuration 
settings that enable, disable, or otherwise influence parts of the code to be executed. Hence, systematic testing of command-line utilities requires testing them with diverse configurations of supported command-line options. 
","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation",CLIFuzzer:为命令行调用挖掘语法,命令行实用程序的行为可能会受到传递命令行选项和参数的很大影响——配置,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论,,,
54VAHNAL,2022,https://doi.org/10.1145/3540250.3558955,ESEC/FSE 2022,Are elevator software robust against uncertainties? results and experiences from an industrial case study,"Industrial elevator systems are complex Cyber-Physical Systems operating in uncertain environments and experiencing uncertain passenger behaviors, hardware delays, and software errors. Identifying, understanding, and classifying such uncertainties are essential to enable system designers to reason about uncertainties and subsequently develop solutions for empowering elevator systems to deal with uncertainties systematically. To this end, we present a method, called RuCynefin, based on the Cynefin framework to classify uncertainties in industrial elevator systems from our industrial partner (Orona, Spain), results of which can then be used for assessing their robustness. RuCynefin is equipped with a novel classification algorithm to identify the Cynefin contexts for a variety of uncertainties in industrial elevator systems, and a novel metric for measuring the robustness using the uncertainty classification. We evaluated RuCynefin with an industrial case study of 90 dispatchers from Orona to assess their robustness against uncertainties. Results show that RuCynefin could effectively identify several situations for which certain dispatchers were not robust. Specifically, 93% of such versions showed some degree of low robustness against uncertainties. We also provide insights on the potential practical usages of RuCynefin, which are useful for practitioners in this field.","Computer systems organization,Embedded and cyber-physical systems,Embedded systems,Computing methodologies,General and reference,Cross-computing tools and techniques,Information systems,Information systems applications,Software and its engineering,Software creation and management,Software verification and validation,Empirical software validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software performance",电梯软件是否对不确定性具有鲁棒性？一个工业案例研究的结果与经验,工业电梯系统是一种复杂的网络物理系统，在不确定的环境中运行，并经历不确定的乘客行为、硬件延迟和软件错误。识别、理解和分类这些不确定性对于使系统设计者能够对不确定性进行推理并随后开发解决方案以使电梯系统能够系统地处理不确定性至关重要。为此，我们提出了一种称为RuCynefin的方法，该方法基于Cyneffin框架，对我们的工业合作伙伴（西班牙奥罗纳）的工业电梯系统中的不确定性进行分类，其结果可用于评估其稳健性。RuCynefin配备了一种新的分类算法，用于识别工业电梯系统中各种不确定性的Cyneffin上下文，以及一种使用不确定性分类测量稳健性的新度量。我们对RuCynefin进行了评估，并对奥罗纳的90名调度员进行了工业案例研究，以评估其对不确定性的稳健性。结果表明，RuCynefin可以有效地识别某些调度员不稳健的几种情况。具体而言，93%的此类版本对不确定性表现出一定程度的低稳健性。我们还提供了RuCynefin潜在实际用途的见解，这对该领域的从业者很有用。,计算机系统组织，嵌入式和信息物理系统，嵌入式系统，计算方法，通用和参考，交叉计算工具和技术，信息系统，信息系统应用，软件及其工程，软件创建和管理，软件验证和确认，经验软件确认，软件缺陷分析，软件测试和调试，软件组织和属性，功能外属性，软件性能,,,
PA3ITC6B,2022,https://doi.org/10.1145/3540250.3558922,ESEC/FSE 2022,GFI-bot: automated good first issue recommendation on GitHub,"To facilitate newcomer onboarding, GitHub recommends the use of ""good first issue"" (GFI) labels to signal issues suitable for newcomers to resolve. However, previous research shows that manually labeled GFIs are scarce and inappropriate, showing a need for automated recommendations. In this paper, we present GFI-Bot (accessible at https://gfibot.io), a proof-of-concept machine learning powered bot for automated GFI recommendation in practice. Project maintainers can configure GFI-Bot to discover and label possible GFIs so that newcomers can easily locate issues for making their first contributions. GFI-Bot also provides a high-quality, up-to-date dataset for advancing GFI recommendation research.","Human-centered computing,Collaborative and social computing,Collaborative and social computing theory, concepts and paradigms,Information systems,Information retrieval,Retrieval tasks and goals,Recommender systems,Information systems applications,Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Software notations and tools,Software configuration management and version control systems",GFI机器人：GitHub上的自动好的第一期推荐,为了方便新人入职，GitHub建议使用“良好的第一个问题”（GFI）标签来表示适合新人解决的问题。然而，先前的研究表明，手动标记的GFI很少且不合适，这表明需要自动推荐。在本文中，我们介绍了GFI Bot（可访问https://gfibot.io)，一个在实践中用于自动GFI推荐的概念验证机器学习机器人。项目维护人员可以配置GFI Bot来发现和标记可能的GFI，这样新来者就可以很容易地找到他们第一次贡献的问题。GFI Bot还为推进GFI推荐研究提供了一个高质量、最新的数据集。,以人为中心的计算，协作和社会计算，协作和社会计算理论，概念和范式，信息系统，信息检索，检索任务和目标，推荐系统，信息系统应用，软件及其工程，软件创建和管理，软件开发中的协作，开放源码模型，软件符号和工具，软件配置管理和版本控制系统,,,
XGRY62IH,2022,https://doi.org/10.1145/3540250.3558963,ESEC/FSE 2022,An empirical study of log analysis at Microsoft,"Logs are crucial to the management and maintenance of software systems. In recent years, log analysis research has achieved notable progress on various topics such as log parsing and log-based anomaly detection. However, the real voices from front-line practitioners are seldom heard. For example, what are the pain points of log analysis in practice? In this work, we conduct a comprehensive survey study on log analysis at Microsoft. We collected feedback from 105 employees through a questionnaire of 13 questions and individual interviews with 12 employees. We summarize the format, scenario, method, tool, and pain points of log analysis. Additionally, by comparing the industrial practices with academic research, we discuss the gaps between academia and industry, and future opportunities on log analysis with four inspiring findings. Particularly, we observe a huge gap exists between log anomaly detection research and failure alerting practices regarding the goal, technique, efficiency, etc. Moreover, data-driven log parsing, which has been widely studied in recent research, can be alternatively achieved by simply logging template IDs during software development. We hope this paper could uncover the real needs of industrial practitioners and the unnoticed yet significant gap between industry and academia, and inspire interesting future directions that converge efforts from both sides.","Social and professional topics,Professional topics,Management of computing and information systems,Project and people management,Software management,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software verification and validation,Software notations and tools,Software configuration management and version control systems",微软对数分析的实证研究,日志对于软件系统的管理和维护至关重要。近年来，日志分析研究在日志解析和基于日志的异常检测等方面取得了显著进展。然而，来自一线从业者的真实声音却很少被听到。例如，日志分析在实践中有哪些痛点？在这项工作中，我们对微软的日志分析进行了全面的调查研究。我们通过13个问题的问卷调查和12名员工的个人访谈收集了105名员工的反馈。我们总结了日志分析的格式、场景、方法、工具和痛点。此外，通过将工业实践与学术研究进行比较，我们讨论了学术界和工业界之间的差距，以及日志分析的未来机遇，并得出了四个鼓舞人心的发现。特别是，我们观察到日志异常检测研究与故障警报实践在目标、技术、效率等方面存在巨大差距。此外，最近研究中广泛研究的数据驱动日志解析也可以通过在软件开发过程中简单地记录模板ID来实现。我们希望这篇论文能够揭示行业从业者的真实需求，以及行业和学术界之间未被注意但显著的差距，并启发双方共同努力的有趣的未来方向。,社会和专业主题，专业主题，计算和信息系统管理，项目和人员管理，软件管理，软件及其工程，软件创建和管理，软件开发后问题，维护软件，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统,,,
3D6H4523,2022,https://doi.org/10.1145/3540250.3549119,ESEC/FSE 2022,"CommentFinder: a simpler, faster, more accurate code review comments recommendation","Code review is an effective quality assurance practice, but can be labor-intensive since developers have to manually review the code and provide written feedback. Recently, a Deep Learning (DL)-based approach was introduced to automatically recommend code review comments based on changed methods. While the approach showed promising results, it requires expensive computational resource and time which limits its use in practice. To address this limitation, we propose CommentFinder – a retrieval-based approach to recommend code review comments. Through an empirical evaluation of 151,019 changed methods, we evaluate the effectiveness and efficiency of CommentFinder against the state-of-the-art approach. We find that when recommending the best-1 review comment candidate, our CommentFinder is 32% better than prior work in recommending the correct code review comment. In addition, CommentFinder is 49 times faster than the prior work. These findings highlight that our CommentFinder could help reviewers to reduce the manual efforts by recommending code review comments, while requiring less computational time.","Computing methodologies,Machine learning,Human-centered computing,Information systems,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software development techniques,Software verification and validation,Software notations and tools,Software configuration management and version control systems",CommentFinder：一个更简单、更快、更准确的代码审查评论推荐,代码审查是一种有效的质量保证实践，但由于开发人员必须手动审查代码并提供书面反馈，因此可能会耗费大量人力。最近，引入了一种基于深度学习（DL）的方法来根据更改的方法自动推荐代码评审注释。虽然该方法显示出了有希望的结果，但它需要昂贵的计算资源和时间，这限制了它在实践中的使用。为了解决这一限制，我们提出了CommentFinder——一种基于检索的方法来推荐代码评审注释。通过对151019种更改方法的实证评估，我们评估了CommentFinder相对于最先进方法的有效性和效率。我们发现，在推荐最佳-1评审评论候选者时，我们的CommentFinder在推荐正确的代码评审评论方面比以前的工作好32%。此外，CommentFinder比之前的工作快49倍。这些发现突出表明，我们的CommentFinder可以通过推荐代码评审注释来帮助评审人员减少手动工作量，同时需要更少的计算时间。,计算方法学，机器学习，以人为中心的计算，信息系统，社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件开发技术，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统,,,
KSWHPE3A,2022,https://doi.org/10.1145/3540250.3549141,ESEC/FSE 2022,Lighting up supervised learning in user review-based code localization: dataset and benchmark,"As User Reviews (URs) of mobile Apps are proven to provide valuable feedback for maintaining and evolving applications, how to make full use of URs more efficiently in the release cycle of mobile Apps has become a widely concerned and researched topic in the Software Engineering (SE) community. In order to speed up the completion of coding work related to URs to shorten the release cycle as much as possible, the task of User Review-based code localization is proposed and studied in depth. However, due to the lack of large-scale ground truth dataset (i.e., truly related <UR, Code> pairs), existing methods are all unsupervised learning-based. In order to light up supervised learning approaches, which are driven by large labeled datasets, for Review2Code, and to compare their performances with unsupervised learning-based methods, we first introduce a large-scale human-labeled <UR, Code> ground truth dataset, including the annotation process and statistical analysis. Then, a benchmark consisting of two SOTA unsupervised learning-based and four supervised learning-based Review2Code methods is constructed based on this dataset. We believe that this paper can provide a basis for in-depth exploration of the supervised learning-based Review2Code solutions.","Computing methodologies,Artificial intelligence,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software development techniques,Software post-development issues,Software verification and validation,Software defect analysis,Software notations and tools,Software configuration management and version control systems,Software maintenance tools",在基于用户评审的代码本地化中点亮监督学习：数据集和基准,随着移动应用程序的用户评价（URs）被证明为维护和发展应用程序提供了有价值的反馈，如何在移动应用程序发布周期中更有效地充分利用URs已成为软件工程界广泛关注和研究的话题。为了加快完成与URs相关的编码工作，尽可能缩短发布周期，提出并深入研究了基于用户评审的代码本地化任务。然而，由于缺乏大规模的地面实况数据集（即真正相关的<UR，Code>对），现有的方法都是基于无监督学习的。为了为Review2Code提供由大型标记数据集驱动的监督学习方法，并将其与基于无监督学习的方法的性能进行比较，我们首先介绍了一个大规模的人类标记<UR，Code>地面实况数据集，包括注释过程和统计分析。然后，基于该数据集构建了一个由两种基于SOTA的无监督学习和四种基于监督学习的Review2Code方法组成的基准。我们相信，本文可以为深入探索基于监督学习的Review2Code解决方案提供基础。,计算方法学，人工智能，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发技术，软件开发后问题，软件验证和确认，软件缺陷分析，软件符号和工具，软件配置管理和版本控制系统，软件维护工具,,,
29B2IGQQ,2022,https://doi.org/10.1145/3540250.3549086,ESEC/FSE 2022,Perfect is the enemy of test oracle,"Automation of test oracles is one of the most challenging facets of software testing, but remains comparatively less addressed compared to automated test input generation. Test oracles rely on a ground-truth that can distinguish between the correct and buggy behavior to determine whether a test fails (detects a bug) or passes. What makes the oracle problem challenging and undecidable is the assumption that the ground-truth should know the exact expected, correct, or buggy behavior. However, we argue that one can still build an accurate oracle without knowing the exact correct or buggy behavior, but how these two might differ. This paper presents , a learning-based approach that in the absence of test assertions or other types of oracle, can determine whether a unit test passes or fails on a given method under test (MUT). To build the ground-truth, jointly embeds unit tests and the implementation of MUTs into a unified vector space, in such a way that the neural representation of tests are similar to that of MUTs they pass on them, but dissimilar to MUTs they fail on them. The classifier built on top of this vector representation serves as the oracle to generate “fail” labels, when test inputs detect a bug in MUT or “pass” labels, otherwise. Our extensive experiments on applying to more than 5K unit tests from a diverse set of open-source Java projects show that the produced oracle is (1) effective in predicting the fail or pass labels, achieving an overall accuracy, precision, recall, and F1 measure of 93%, 86%, 94%, and 90%, (2) generalizable, predicting the labels for the unit test of projects that were not in training or validation set with negligible performance drop, and (3) efficient, detecting the existence of bugs in only 6.5 milliseconds on average. Moreover, by interpreting the neural model and looking at it beyond a closed-box solution, we confirm that the oracle is valid, i.e., it predicts the labels through learning relevant features.","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation",完美是测试预言的敌人,测试预言机的自动化是软件测试中最具挑战性的方面之一，但与自动化测试输入生成相比，它仍然相对较少得到解决。测试预言器依赖于一个基本事实，该事实可以区分正确和错误的行为，以确定测试是失败（检测到错误）还是通过。使预言机问题具有挑战性和不可判定性的是，假设基本事实应该知道确切的预期、正确或错误行为。然而，我们认为，在不知道确切的正确或错误行为的情况下，仍然可以构建一个准确的预言机，但这两者可能会有什么不同。本文提出了一种基于学习的方法，在没有测试断言或其他类型的预言机的情况下，可以确定单元测试在给定的被测方法（MUT）上是通过还是失败。为了构建基本事实，将单元测试和MUT的实现联合嵌入到一个统一的向量空间中，这样测试的神经表示与它们传递的MUT相似，但与它们失败的MUT不同。当测试输入检测到MUT或“通过”标签中的错误时，建立在该向量表示之上的分类器充当生成“失败”标签的预言机。我们在一组不同的开源Java项目中应用于超过5K个单元测试的广泛实验表明，所生产的oracle（1）在预测失败或通过标签方面是有效的，实现了93%、86%、94%和90%的总体准确度、准确度、召回率和F1指标，（2）可推广，预测不在训练或验证集中的项目的单元测试的标签，性能下降可以忽略不计，以及（3）有效，平均仅在6.5毫秒内检测到错误的存在。此外，通过解释神经模型并超越闭盒解决方案来看待它，我们确认了预言机是有效的，即它通过学习相关特征来预测标签。,计算方法论，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论,,,
3PBY5DT7,2022,https://doi.org/10.1145/3540250.3558954,ESEC/FSE 2022,Industry experiences with large-scale refactoring,"Software refactoring plays an important role in software engineering. Developers often turn to refactoring when they want to restructure software to improve its quality without changing its external behavior. Small-scale (floss) refactoring is common in industry and is often performed by a single developer in short sessions, even though developers do much of this work manually instead of using refactoring tools. However, some refactoring efforts are much larger in scale, requiring entire teams and months or years of effort, and the role of tools in these efforts is not as well studied. In this paper, we report on a survey we conducted with developers to understand large-scale refactoring and its tool support needs. Our results from 107 industry developers demonstrate that projects commonly go through multiple large-scale refactorings, each of which requires considerable effort. Our study finds that developers use several categories of tools to support large-scale refactoring and rely more heavily on general-purpose tools like IDEs than on tools designed specifically to support refactoring. Tool support varies across the different activities, with some particularly challenging activities seeing little use of tools in practice. Furthermore, our analysis suggests significant impact is possible through advances in tool support for comprehension and testing, as well as through support for the needs of business stakeholders.","Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software development process management,Software post-development issues,Maintaining software,Software evolution,Software notations and tools,Development frameworks and environments,Software configuration management and version control systems,Software maintenance tools",大规模重构的行业经验,软件重构在软件工程中发挥着重要作用。当开发人员想要在不改变外部行为的情况下重组软件以提高其质量时，他们通常会转向重构。小规模（floss）重构在行业中很常见，通常由单个开发人员在短时间内执行，尽管开发人员大部分工作都是手动完成的，而不是使用重构工具。然而，一些重构工作的规模要大得多，需要整个团队、数月或数年的努力，而工具在这些工作中的作用并没有得到很好的研究。在本文中，我们报告了我们对开发人员进行的一项调查，以了解大规模重构及其工具支持需求。我们来自107个行业开发人员的结果表明，项目通常要经过多次大规模重构，每一次都需要付出相当大的努力。我们的研究发现，开发人员使用几种类型的工具来支持大规模重构，并且更依赖于IDE等通用工具，而不是专门为支持重构而设计的工具。不同活动的工具支持各不相同，一些特别具有挑战性的活动在实践中很少使用工具。此外，我们的分析表明，通过对理解和测试的工具支持的进步，以及对业务利益相关者需求的支持，可能会产生重大影响。,社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发过程管理，软件开发后问题，维护软件，软件进化，软件符号和工具，开发框架和环境，软件配置管理和版本控制系统，软件维护工具,,,
ZKXE9EUY,2022,https://doi.org/10.1145/3540250.3549149,ESEC/FSE 2022,Do bugs lead to unnaturalness of source code?,"Texts in natural languages are highly repetitive and predictable because of the naturalness of natural languages. Recent research validated that source code in programming languages is also repetitive and predictable, and naturalness is an inherent property of source code. It was also reported that buggy code is significantly less natural than bug-free one, and bug fixing substantially improves the naturalness of the involved source code. In this paper, we revisit the naturalness of buggy code and investigate the effect of bug-fixing on the naturalness of source code. Different from the existing investigation, we leverage two large-scale and high-quality bug repositories where bug-irrelevant changes in bug-fixing commits have been explicitly excluded. Our evaluation results confirm that buggy lines are often less natural than bug-free ones. However, fixing bugs could not significantly improve the naturalness of involved code lines. Fixed lines on average are as unnatural as buggy ones. Consequently, bugs are not the root cause of the unnaturalness of source code, and it could be inaccurate to identify buggy code lines solely by the naturalness of source code. Our evaluation results suggest that the naturalness-based buggy line detection results in extremely low precision (less than one percentage).","Computing methodologies,Artificial intelligence,Natural language processing,Human-centered computing,Software and its engineering,Software creation and management,Software post-development issues,Software notations and tools",错误会导致源代码不自然吗？,由于自然语言的自然性，自然语言中的文本具有高度的重复性和可预测性。最近的研究证实，编程语言中的源代码也是重复的和可预测的，自然性是源代码的固有特性。据报道，有缺陷的代码明显不如无缺陷的代码自然，修复缺陷大大提高了相关源代码的自然性。在本文中，我们重新审视了错误代码的自然性，并研究了错误修复对源代码自然性的影响。与现有的调查不同，我们利用了两个大规模且高质量的bug存储库，其中明确排除了bug修复提交中与bug无关的更改。我们的评估结果证实，有缺陷的线路通常不如没有缺陷的线路自然。然而，修复错误并不能显著提高相关代码行的自然度。固定线路平均来说和bug线路一样不自然。因此，错误并不是源代码不自然的根本原因，仅仅通过源代码的自然性来识别错误代码行可能是不准确的。我们的评估结果表明，基于自然度的错误线检测导致极低的精度（低于一个百分比）。,计算方法论，人工智能，自然语言处理，以人为中心的计算，软件及其工程，软件创建和管理，软件开发后问题，软件符号和工具,,,
YB2HIZYK,2022,https://doi.org/10.1145/3540250.3569446,ESEC/FSE 2022,Multi-perspective representation learning for source code analytics (invited tutorial),"Programming languages are artificial and highly restricted languages. But source code is there to tell computers as well as programmers what to do, as an act of communication. Despite its weird syntax and is riddled with different delimiters, the good news is that the very large corpus of open-source code is available. That makes it reasonable to apply machine learning techniques to source code to enable the source code analytics.  
","Computing methodologies,Artificial intelligence,Natural language processing,Machine learning,Software and its engineering,Software creation and management,Software notations and tools",用于源代码分析的多视角表示学习（受邀教程）,程序设计语言是人工的、高度受限的语言。但源代码是用来告诉计算机和程序员该做什么的，这是一种交流行为。尽管它的语法很奇怪，而且有很多不同的分隔符，但好消息是，可以使用大量的开源代码。这使得将机器学习技术应用于源代码以实现源代码分析是合理的。,计算方法论，人工智能，自然语言处理，机器学习，软件及其工程，软件创建和管理，软件符号和工具,,,
N958LUSP,2022,https://doi.org/10.1145/3540250.3549079,ESEC/FSE 2022,Accurate method and variable tracking in commit history,"Tracking program elements in the commit history of a project is essential for supporting various software maintenance, comprehension and evolution tasks. Accuracy is of paramount importance for the adoption of program element tracking tools by developers and researchers. To this end, we propose CodeTracker, a refactoring-aware tool that can generate the commit change history for method and variable declarations with a very high accuracy. More specifically, CodeTracker has 99.9% precision and recall in method tracking, surpassing the previous state-of-the-art tool, CodeShovel, with a comparable execution time. CodeTracker is the first tool of its kind that can track the change history of variables with 99.7% precision and 99.8% recall. To evaluate its accuracy in variable tracking, we extended the oracle created by Grund et al. for the evaluation of CodeShovel, with the complete change history of all 1345 variables and parameters declared in the 200 methods comprising the Grund et al. oracle. We make our tool and extended oracle publicly available to enable the replication of our experiments and facilitate future research on program element tracking techniques.","Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software maintenance,Software and its engineering,Software creation and management,Software post-development issues,Software notations and tools,Software configuration management and version control systems,Software maintenance tools",提交历史中的精确方法和变量跟踪,跟踪项目提交历史中的程序元素对于支持各种软件维护、理解和进化任务至关重要。准确性对于开发人员和研究人员采用程序元素跟踪工具至关重要。为此，我们提出了CodeTracker，这是一个具有重构意识的工具，可以非常准确地生成方法和变量声明的提交更改历史。更具体地说，CodeTracker在方法跟踪方面具有99.9%的精度和召回率，超过了以前最先进的工具CodeShovel，其执行时间相当。CodeTracker是同类工具中第一个可以以99.7%的精度和99.8%的召回率跟踪变量的变化历史的工具。为了评估其在变量跟踪中的准确性，我们扩展了Grund等人创建的用于评估CodeShovel的预言机，包括Grund等人预言机的200种方法中声明的所有1345个变量和参数的完整变化历史。我们公开了我们的工具和扩展的oracle，以实现我们实验的复制，并促进未来对程序元素跟踪技术的研究。,社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件维护，软件及其工程，软件创建和管理，软件开发后问题，软件符号和工具，软件配置管理和版本控制系统，软件维护工具,,,
5DJLXERU,2022,https://doi.org/10.1145/3540250.3558938,ESEC/FSE 2022,TSA: a tool to detect and quantify network side-channels,"Mobile applications, Internet of Things devices and web services are pervasive and they all encrypt the communications between servers and clients to not have information leakages. While the network traffic is encrypted, packet sizes and timings are still visible to an eavesdropper and these properties can leak information and sacrifice user privacy. We present TSA, a black box network side-channel analysis tool which detects and quantifies side-channel information leakages. TSA provides the users with the means to automate trace gathering by providing a framework in which the users can write mutators for the inputs to the system under analysis. TSA can also take as input traces directly for analysis if the user prefers to gather them separately. TSA is open-source and available as a Python package and a command-line tool. TSA demo, tool and benchmarks are available at https://github.com/kadron/tsa-tool.","General and reference,Cross-computing tools and techniques,Networks,Network services,Security and privacy,Network security,Software and application security,Web application security,Systems security,Operating systems security,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",TSA：检测和量化网络侧信道的工具,移动应用程序、物联网设备和网络服务无处不在，它们都对服务器和客户端之间的通信进行加密，以避免信息泄露。虽然网络流量是加密的，但窃听者仍然可以看到数据包的大小和时间，这些属性可能会泄露信息并牺牲用户隐私。我们提出TSA，一个黑匣子网络侧信道分析工具，它可以检测和量化侧信道信息泄漏。TSA通过提供一个框架，为用户提供了自动化跟踪收集的方法，在该框架中，用户可以为被分析系统的输入编写变体。如果用户喜欢单独收集，TSA也可以直接将跟踪作为输入进行分析。TSA是开源的，可以作为Python包和命令行工具使用。TSA演示、工具和基准测试可在https://github.com/kadron/tsa-tool.,一般和参考，交叉计算工具和技术，网络，网络服务，安全和隐私，网络安全，软件和应用程序安全，Web应用程序安全，系统安全，操作系统安全，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
2C55GG87,2022,https://doi.org/10.1145/3540250.3558907,ESEC/FSE 2022,Effective and scalable fault injection using bug reports and generative language models,"Previous research has shown that artificial faults can be useful in many software engineering tasks such as testing, fault-tolerance assessment, debugging, dependability evaluation, risk analysis, etc. However, such artificial-fault-based applications can be questioned or inaccurate when the considered faults misrepresent real bugs. Since typically, fault injection techniques (i.e. mutation testing) produce a large number of faults by altering ”blindly” the code in arbitrary locations, they are unlikely capable to produce few but relevant real-like faults. In our work, we tackle this challenge by guiding the injection towards resembling bugs that have been previously introduced by developers. For this purpose, we propose iBiR, the first fault injection approach that leverages information from bug reports to inject ”realistic” faults. iBiR injects faults on the locations that are more likely to be related to a given bug-report by applying appropriate inverted fix-patterns, which are manually or automatically crafted by automated-program-repair researchers. We assess our approach using bugs from the Defects4J dataset and show that iBiR outperforms significantly conventional mutation testing in terms of injecting faults that semantically resemble and couple with real ones, in the vast majority of the cases. Similarly, the faults produced by iBiR give significantly better fault-tolerance estimates than conventional mutation testing in around 80% of the cases.","Human-centered computing,Visualization,Visualization techniques,Software and its engineering,Software creation and management,Software development techniques,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems,Software organization and properties,Extra-functional properties,Software fault tolerance,Software reliability",使用错误报告和生成语言模型进行有效且可扩展的故障注入,先前的研究表明，人工故障可以用于许多软件工程任务，如测试、容错评估、调试、可靠性评估、风险分析等。然而，当所考虑的故障歪曲了真实的错误时，这种基于人工故障的应用程序可能会受到质疑或不准确。由于故障注入技术（即突变测试）通常通过在任意位置“盲目”更改代码来产生大量故障，因此它们不太可能产生少量但相关的类真实故障。在我们的工作中，我们通过引导注入类似于开发人员以前引入的错误来应对这一挑战。为此，我们提出了iBiR，这是第一种故障注入方法，它利用错误报告中的信息来注入“真实”的故障。iBiR通过应用适当的反向修复模式，在更可能与给定错误报告相关的位置注入故障，这些模式由自动程序修复研究人员手动或自动制定。我们使用Defects4J数据集的错误评估了我们的方法，并表明在绝大多数情况下，iBiR在注入语义上类似于真实错误并与真实错误耦合的错误方面显著优于传统的突变测试。类似地，在大约80%的情况下，iBiR产生的故障比传统的突变测试给出了更好的容错估计。,以人为中心的计算，可视化，可视化技术，软件及其工程，软件创建和管理，软件开发技术，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统，软件组织和属性，功能外属性，软件容错，软件可靠性,,,
3XI7Q6UL,2022,https://doi.org/10.1145/3540250.3558950,ESEC/FSE 2022,Understanding automated code review process and developer experience in industry,"Code Review Automation can reduce human efforts during code review by automatically providing valuable information to reviewers. Nevertheless, it is a challenge to automate the process for large-scale companies, such as Samsung Electronics, due to their complexity: various development environments, frequent review requests, huge size of software, and diverse process among the teams. ","General and reference,Cross-computing tools and techniques,Empirical studies,Social and professional topics,Professional topics,Management of computing and information systems,Project and people management,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Software verification and validation,Empirical software validation,Software defect analysis,Software notations and tools,Development frameworks and environments,Application specific development environments,Software configuration management and version control systems",了解自动化代码审查过程和开发人员在行业中的经验,代码审查自动化可以通过自动向审查人员提供有价值的信息来减少代码审查过程中的人力工作。尽管如此，对于像三星电子这样的大型公司来说，实现流程自动化是一个挑战，因为它们的复杂性：各种开发环境、频繁的审查请求、庞大的软件规模以及团队之间的不同流程。,一般和参考，交叉计算工具和技术，经验研究，社会和专业主题，专业主题，计算和信息系统管理，项目和人员管理，软件管理，软件及其工程，软件创建和管理，软件开发中的协作，软件验证和确认，经验软件验证，软件缺陷分析，软件符号和工具，开发框架和环境，特定应用开发环境，软件配置管理和版本控制系统,,,
N9QXRNUE,2022,https://doi.org/10.1145/3540250.3549164,ESEC/FSE 2022,RoboFuzz: fuzzing robotic systems over robot operating system (ROS) for finding correctness bugs,"Robotic systems are becoming an integral part of human lives.  
Responding to the increased demands for robot productions, Robot  
Operating System (ROS), an open-source middleware suite for robotic  
development, is gaining traction by providing practical tools and  
libraries for quickly developing robots. In this paper, we are  
concerned with a relatively less-tested class of bugs in ROS and  
ROS-based robotic systems, called semantic correctness bugs, including  
the violation of specification, violation of physical laws, and  
cyber-physical discrepancy. These bugs often stem from the  
cyber-physical nature of robotic systems, in which noisy hardware  
components are intertwined with software components, and thus cannot be  
detected by existing fuzzing approaches that mostly focus on finding  
memory-safety bugs.  
","Computer systems organization,Embedded and cyber-physical systems,Robotics,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",RoboFuzz：在机器人操作系统（ROS）上模糊机器人系统，以发现正确性错误,机器人系统正在成为人类生活不可或缺的一部分。,计算机系统组织，嵌入式和网络物理系统，机器人技术，软件及其工程，软件创建和管理，软件验证和确认，形式化软件验证，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，计算理论，语义和推理，程序推理，程序验证,,,
8SGWZQ45,2022,https://doi.org/10.1145/3540250.3558967,ESEC/FSE 2022,An empirical study of deep transfer learning-based program repair for Kotlin projects,"Deep learning-based automated program repair (DL-APR) can automatically fix software bugs and has received significant attention in the industry because of its potential to significantly reduce software development and maintenance costs. The Samsung mobile experience (MX) team is currently switching from Java to Kotlin projects. This study reviews the application of DL-APR, which automatically fixes defects that arise during this switching process; however, the shortage of Kotlin defect-fixing datasets in Samsung MX team precludes us from fully utilizing the power of deep learning. Therefore, strategies are needed to effectively reuse the pretrained DL-APR model. This demand can be met using the Kotlin defect-fixing datasets constructed from industrial and open-source repositories, and transfer learning.  
This study aims to validate the performance of the pretrained DL-APR model in fixing defects in the Samsung Kotlin projects, then improve its performance by applying transfer learning. We show that transfer learning with open source and industrial Kotlin defect-fixing datasets can improve the defect-fixing performance of the existing DL-APR by 307%. Furthermore, we confirmed that the performance was improved by 532% compared with the baseline DL-APR model as a result of transferring the knowledge of an industrial (non-defect) bug-fixing dataset. We also discovered that the embedded vectors and overlapping code tokens of the code-change pairs are valuable features for selecting useful knowledge transfer instances by improving the performance of APR models by up to 696%. Our study demonstrates the possibility of applying transfer learning to practitioners who review the application of DL-APR to industrial software.","Computing methodologies,Machine learning,Human-centered computing,Ubiquitous and mobile computing,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems",基于深度迁移学习的Kotlin项目程序修复的实证研究,基于深度学习的自动程序修复（DL-APR）可以自动修复软件错误，并因其显著降低软件开发和维护成本的潜力而受到业界的极大关注。三星移动体验（MX）团队目前正从Java项目转向Kotlin项目。本研究回顾了DL-APR的应用，它可以自动修复在切换过程中出现的缺陷；然而，Samsung MX团队中Kotlin缺陷修复数据集的短缺阻碍了我们充分利用深度学习的力量。因此，需要策略来有效地重用预训练的DL-APR模型。这一需求可以通过使用由工业和开源存储库构建的Kotlin缺陷修复数据集以及迁移学习来满足。,计算方法论，机器学习，以人为中心的计算，无处不在的和移动计算，软件及其工程，软件创建和管理，软件开发后问题，维护软件，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统,,,
FC4ZPU73,2022,https://doi.org/10.1145/3540250.3560882,ESEC/FSE 2022,In war and peace: the impact of world politics on software ecosystems,"Reliance on third-party libraries is now commonplace in contemporary software engineering.  
Being open source in nature, these libraries should advocate for a world where the freedoms and opportunities of open source software can be enjoyed by all.  
Yet, there is a growing concern related to maintainers using their influence to make political stances (i.e., referred to as protestware).  
In this paper, we reflect on the impact of world politics on software ecosystems, especially in the context of the ongoing War in Ukraine.  
We show three cases where world politics has had an impact on a software ecosystem, and how these incidents may result in either benign or malignant consequences.  
We further point to specific opportunities for research, and conclude with a research agenda with ten research questions to guide future research directions.","Security and privacy,Human and societal aspects of security and privacy,Social aspects of security and privacy,Social and professional topics,Computing / technology policy,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software post-development issues,Software notations and tools,Software configuration management and version control systems,Software libraries and repositories",在战争与和平中：世界政治对软件生态系统的影响,对第三方库的依赖现在在当代软件工程中很常见。,安全和隐私，安全和隐私的人和社会方面，安全和隐私的社会方面，社会和专业主题，计算/技术政策，专业主题，计算和信息系统的管理，软件及其工程，软件创建和管理，软件开发后问题，软件符号和工具，软件配置管理和版本控制系统，软件库和存储库,,,
BWMB5XVN,2022,https://doi.org/10.1145/3540250.3558964,ESEC/FSE 2022,Metadata-based retrieval for resolution recommendation in AIOps,"For a cloud service provider, the goal is to proactively identify  
signals that can help reduce outages and/or reduce the mean-time-to-detect and mean-time-to-resolve. After an incident is reported, the Site Reliability Engineers diagnose the fault and search for a resolution by formulating a textual query to find similar historical incidents - this approach is called text-based retrieval. However, it has been observed that the formulated queries are inadequate and short. An alternate approach, presented in this paper, integrates information spread across heterogeneous and siloed datasets, as a ready-to-use knowledge base for metadata-based resolution retrieval. Additionally, it exploits historical problem context for building metadata prediction models which are used at run-time for automatically formulating queries from log anomalies detected by the Log Anomaly Detection module. The query, thus formed, is run against the metadata-based index, unlike the text-based index in text retrieval, resulting in superior performance, in terms of relevancy of the resolution documents retrieved. Through experiments on web application server applications deployed on the cloud, we show the efficacy of metadata-based retrieval, which not only returns targeted results as compared to text-based retrieval but also the relevant resolution document appear amongst the top 3 positions for 60% of the queries.","Information systems,Data management systems,Database management system engines,Information integration,Information retrieval,Information retrieval query processing,Query reformulation,Information systems applications,Software and its engineering,Software organization and properties,Software system structures,Distributed systems organizing principles,Cloud computing",基于元数据的AIOps解决方案推荐检索,对于云服务提供商来说，目标是主动识别,信息系统，数据管理系统，数据库管理系统引擎，信息集成，信息检索，信息检索查询处理，查询重构，信息系统应用，软件及其工程，软件组织和属性，软件体系结构，分布式系统组织原则，云计算,,,
AQCCUG94,2022,https://doi.org/10.1145/3540250.3549175,ESEC/FSE 2022,AutoPruner: transformer-based call graph pruning,"Constructing a static call graph requires trade-offs between soundness and precision.  
Program analysis techniques for constructing call graphs are unfortunately usually imprecise.  
To address this problem, researchers have recently proposed call graph pruning empowered by machine learning to post-process call graphs constructed by static analysis. A machine learning model is built to capture information from the call graph by extracting structural features for use in a random forest classifier. It then removes edges that are predicted to be false positives. Despite the improvements shown by machine learning models, they are still limited as they do not consider the source code semantics and thus often are not able to effectively distinguish true and false positives.  
","Computing methodologies,Machine learning,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software organization and properties,Software functional properties,Formal methods,Automated static analysis,Theory of computation,Semantics and reasoning",AutoPruner:基于转换器的调用图修剪,构造静态调用图需要在健全性和精确性之间进行权衡。,计算方法论，机器学习，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件组织和属性，软件功能属性，形式化方法，自动静态分析，计算理论，语义和推理,,,
XPST969V,2022,https://doi.org/10.1145/3540250.3559081,ESEC/FSE 2022,Improving IDE code inspections with tree automata,"Integrated development environments (IDEs) are equipped with code inspections to warn developers of malformed or incorrect code by analyzing the code's data flow. However, the data flow analysis performed by the IDE code inspections may issue warnings in code where warnings are irrelevant. Existing methods to prevent any bogus warnings are either too conservative --- suppressing the same type of warnings in the entire codebase --- or are not sustainable as they require adding a set of heuristics to the data flow analysis. We propose a programming-by-example (PBE) framework that synthesizes tree automata (TAs) to suppress bogus warnings in all the code containing the same patterns as the user-provided examples. Experiments with a prototype of the framework demonstrate that several real-world patterns heuristically suppressed in production IDE can be mechanically translated to a TA in a systematic way. Furthermore, we briefly discuss how TA-based solution can be useful in other IDE features such as code refactoring.","Software and its engineering,Software creation and management,Software development process management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Development frameworks and environments,General programming languages,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",用树自动机改进IDE代码检查,集成开发环境（IDE）配备了代码检查，通过分析代码的数据流来警告开发人员代码格式错误或不正确。但是，IDE代码检查执行的数据流分析可能会在与警告无关的代码中发出警告。现有的防止任何虚假警告的方法要么过于保守——在整个代码库中抑制相同类型的警告——要么不可持续，因为它们需要在数据流分析中添加一组启发式方法。我们提出了一个示例编程（PBE）框架，该框架综合了树自动机（TA），以在所有包含与用户提供的示例相同模式的代码中抑制虚假警告。对该框架原型的实验表明，在生产IDE中启发式抑制的几个真实世界模式可以以系统的方式机械地转换为TA。此外，我们还简要讨论了基于TA的解决方案如何在其他IDE功能（如代码重构）中发挥作用。,软件及其工程，软件创建和管理，软件开发过程管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，开发框架和环境，通用编程语言，计算理论，语义和推理，程序推理，程序验证,,,
ZDFY56AI,2022,https://doi.org/10.1145/3540250.3558956,ESEC/FSE 2022,FlakeRepro: automated and efficient reproduction of concurrency-related flaky tests,"Flaky tests, which can non-deterministically pass or fail on the same code, impose significant burden on developers by providing misleading signals during regression testing. Microsoft developers consider flaky tests as one of the top two reasons for slowing down software development. In order to debug the root-cause of a flaky behavior, a developer often needs to first reliably reproduce a failed execution. Unfortunately, this is non-trivial. For example, most of the flakiness in unit tests are caused by concurrency, and reproducing their failures requires specific thread interleaving. To address this challenge, we introduce FlakeRepro that helps developers reproduce a failed execution of a flaky test caused by concurrency. FlakeRepro combines static and dynamic analysis to quickly identify an interleaving that makes a flaky test fail with the same original error message. FlakeRepro is efficient: it can reproduce a failed execution after exploring few tens of interleavings. FlakeRepro integrates well with existing systems: it automatically instruments test binaries that can run on existing and unmodified test pipelines. ","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools",FlakeRepro：与并发相关的片状测试的自动高效复制,缺陷测试可以在同一代码上非决定性地通过或失败，通过在回归测试中提供误导性信号，给开发人员带来了巨大负担。微软开发人员认为，不稳定的测试是减缓软件开发的两大原因之一。为了调试不稳定行为的根本原因，开发人员通常需要首先可靠地再现失败的执行。不幸的是，这并非小事。例如，单元测试中的大多数片状是由并发性引起的，并且再现它们的失败需要特定的线程交错。为了应对这一挑战，我们引入了FlakeRepro，它可以帮助开发人员重现由并发引起的薄片测试的失败执行。FlakeRepro将静态和动态分析相结合，可以快速识别出一种交错，这种交错会使片状测试在相同的原始错误消息下失败。FlakeRepro是高效的：它可以在探索几十次插入后重现失败的执行。FlakeRepro与现有系统集成良好：它自动检测可以在现有和未修改的测试管道上运行的测试二进制文件。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具,,,
3SFMBKTV,2022,https://doi.org/10.1145/3540250.3549083,ESEC/FSE 2022,Cross-device record and replay for Android apps,"Cross-device replay for Android apps is challenging because apps have to adapt or even restructure their GUIs responsively upon screen-size or orientation change across devices. As a first exploratory work, this paper demonstrates that cross-device record and replay can be made simple and practical by a one-pass, greedy algorithm by the Rx framework leveraging the least surprise principle in the GUI design. The experimental results of over 1,000 replay settings encouragingly show that our implemented Rx prototype tool effectively solved non-trivial cross-device replay cases beyond any known non-search-based work's scope, and had still competitive capabilities on same-device replay with start-of-the-art techniques.","Human-centered computing,Ubiquitous and mobile computing,Software and its engineering,Software creation and management,Software notations and tools,Development frameworks and environments,Application specific development environments,Software maintenance tools",Android应用程序的跨设备记录和回放,Android应用程序的跨设备回放具有挑战性，因为应用程序必须根据屏幕大小或设备方向的变化来调整甚至重组其GUI。作为第一项探索性工作，本文证明了通过Rx框架利用GUI设计中的最小惊喜原则，使用一次性贪婪算法可以使跨设备记录和回放变得简单实用。对1000多个回放设置的实验结果令人鼓舞地表明，我们实现的Rx原型工具有效地解决了超出任何已知的非基于搜索的工作范围的非琐碎的跨设备回放情况，并且在同一设备回放上仍然具有最新技术的竞争能力。,以人为中心的计算，无处不在和移动计算，软件及其工程，软件创建和管理，软件符号和工具，开发框架和环境，特定于应用的开发环境，软件维护工具,,,
IZ3JWRYM,2022,https://doi.org/10.1145/3540250.3549173,ESEC/FSE 2022,On the vulnerability proneness of multilingual code,"Software construction using multiple languages has long been a norm, yet it is still unclear if multilingual code construction has significant security implications and real security consequences. This paper aims to address this question with a large-scale study of popular multi-language projects on GitHub and their evolution histories, enabled by our novel techniques for multilingual code characterization. We found statistically significant associations between the proneness of multilingual code to vulnerabilities (in general and of specific categories) and its language selection. We also found this association is correlated with that of the language interfacing mechanism, not that of individual languages. We validated our statistical findings with in-depth case studies on actual vulnerabilities, explained via the mechanism and language selection. Our results call for immediate actions to assess and defend against multilingual vulnerabilities, for which we provide practical recommendations.","Human-centered computing,Visualization,Visualization techniques,Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation,Software notations and tools",论多语言代码的脆弱性,使用多种语言的软件构建长期以来一直是一种规范，但目前尚不清楚多语言代码构建是否具有重大的安全影响和真正的安全后果。本文旨在通过对GitHub上流行的多语言项目及其演变历史的大规模研究来解决这个问题，这得益于我们新的多语言代码表征技术。我们发现，多语言代码对漏洞的倾向性（一般和特定类别）与其语言选择之间存在统计学上的显著关联。我们还发现，这种联系与语言接口机制的联系有关，而不是与个别语言的联系有关。我们通过对实际漏洞的深入案例研究验证了我们的统计发现，并通过机制和语言选择进行了解释。我们的结果要求立即采取行动，评估和防范多语言漏洞，为此我们提出了切实可行的建议。,以人为中心的计算，可视化，可视化技术，安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和确认，软件符号和工具,,,
PQDEXW8U,2022,https://doi.org/10.1145/3540250.3558925,ESEC/FSE 2022,PolyFax: a toolkit for characterizing multi-language software,"Today’s software systems are mostly developed in multiple languages (i.e., multi-language software), yet tool support for understanding and assuring these systems is rare. To facilitate future research on multi-language software engineering, this paper presents PolyFax, a toolkit that offers automated means for dataset collection from GitHub and two analysis utilities--a vulnerability-fixing commit categorization tool (VCC) and a language interfacing mechanism identification/categorization tool (LIC). The VCC tool immediately assists with assessing the vulnerability proneness of a given multi-language project based on its version histories, while the LIC tool enables dissection of the most important aspect of the construction of multi-language systems. Application of PolyFax to 7,113 multi-language projects with 12.6 million commits showed its practical usefulness in terms of promising efficiency and accuracy for studying multi-language software.","General and reference,Cross-computing tools and techniques,Security and privacy,Software and application security,Software security engineering,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software development process management,Software post-development issues,Software verification and validation,Software notations and tools,Software configuration management and version control systems",PolyFax：一个用于表征多语言软件的工具包,今天的软件系统大多是用多种语言开发的（即多语言软件），但很少有工具支持来理解和确保这些系统。为了促进未来对多语言软件工程的研究，本文介绍了PolyFax，这是一个从GitHub提供数据集自动收集方法的工具包，以及两个分析实用程序——漏洞修复提交分类工具（VCC）和语言接口机制识别/分类工具（LIC）。VCC工具可根据给定多语言项目的版本历史立即帮助评估其易受攻击性，而LIC工具则可对多语言系统构建的最重要方面进行剖析。PolyFax在7113个多语言项目中的应用表明，它在研究多语言软件方面具有良好的效率和准确性。,概述和参考，交叉计算工具和技术，安全和隐私，软件和应用程序安全，软件安全工程，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发过程管理，软件开发后问题，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统,,,
VWY2PTUX,2022,https://doi.org/10.1145/3540250.3549171,ESEC/FSE 2022,"UTANGO: untangling commits with context-aware, graph-based, code change clustering learning model","During software evolution, developers make several changes and commit them into the repositories. Unfortunately, many of them tangle different purposes, both hampering program comprehension and reducing separation of concerns. Automated approaches with deterministic solutions have been proposed to untangle commits. However, specifying an effective clustering criteria on the changes in a commit for untangling is challenging for those approaches. In this work, we present UTango, a machine learning (ML)-based approach that learns to untangle the changes in a commit. We develop a novel code change clustering learning model that learns to cluster the code changes, represented by the embeddings, into different groups with different concerns. We adapt the agglomerative clustering algorithm into a supervised-learning clustering model operating on the learned code change embeddings via trainable parameters and a loss function in comparing the predicted clusters and the correct ones during training. To facilitate our clustering learning model, we develop a context-aware, graph-based, code change representation learning model, leveraging Label, Graph-based Convolution Network to produce the contextualized embeddings for code changes, that integrates program dependencies and the surrounding contexts of the changes. The contexts and cloned code are also explicitly represented, helping UTango distinguish the concerns. Our empirical evaluation on C# and Java datasets with 1,612 and 14k tangled commits show that it achieves the accuracy of 28.6%– 462.5% and 13.3%–100.0% relatively higher than the state-of-the-art commit-untangling approaches for C# and Java, respectively.","Computing methodologies,Machine learning,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software maintenance,Software and its engineering,Software creation and management,Software development techniques,Software post-development issues,Software evolution,Software notations and tools,Software configuration management and version control systems",UTANGO:使用上下文感知、基于图、代码更改的集群学习模型来解开提交,在软件进化过程中，开发人员会进行一些更改并将其提交到存储库中。不幸的是，它们中的许多都混淆了不同的目的，既阻碍了程序的理解，又减少了关注点的分离。已经提出了具有确定性解决方案的自动化方法来解开提交。然而，对于这些方法来说，对提交中的更改指定一个有效的聚类标准以进行解聚类是具有挑战性的。在这项工作中，我们介绍了UTango，这是一种基于机器学习（ML）的方法，可以学习如何解开提交中的更改。我们开发了一种新的代码变化聚类学习模型，该模型学习将以嵌入为代表的代码变化分组到具有不同关注点的不同组中。我们通过可训练参数和在训练期间比较预测聚类和正确聚类的损失函数，将聚集聚类算法调整为对学习的代码变化嵌入进行操作的监督学习聚类模型。为了促进我们的聚类学习模型，我们开发了一个上下文感知、基于图的代码更改表示学习模型，利用基于图的卷积网络标签来生成代码更改的上下文嵌入，该模型集成了程序依赖性和更改的周围上下文。上下文和克隆的代码也被显式表示，帮助UTango区分关注点。我们对具有1612和14k纠缠提交的C#和Java数据集的经验评估表明，它分别实现了28.6%–462.5%和13.3%–100.0%的准确率，相对高于C#和Java的最先进的提交解开方法。,计算方法，机器学习，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件维护，软件及其工程，软件创建和管理，软件开发技术，软件开发后问题，软件进化，软件符号和工具，软件配置管理和版本控制系统,,,
2Y7H4EZG,2022,https://doi.org/10.1145/3540250.3549137,ESEC/FSE 2022,Fault localization to detect co-change fixing locations,"Fault Localization (FL) is a precursor step to most Automated Program Repair (APR) approaches, which fix the faulty statements identified by the FL tools. We present FixLocator, a Deep Learning (DL)-based fault localization approach supporting the detection of faulty statements in one or multiple methods that need to be modified accordingly in the same fix. Let us call them co-change (CC) fixing locations for a fault. We treat this FL problem as dual-task learning with two models. The method-level FL model, MethFL, learns the methods to be fixed together. The statement-level FL model, StmtFL, learns the statements to be co-fixed. Correct learning in one model can benefit the other and vice versa. Thus, we simultaneously train them with soft-sharing the models' parameters via cross-stitch units to enable the propagation of the impact of MethFL and StmtFL onto each other. Moreover, we explore a novel feature for FL: the co-changed statements. We also use Graph-based Convolution Network to integrate different types of program dependencies.  
","Computing methodologies,Machine learning,Machine learning approaches,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation,Logic,Logic and verification",故障定位以检测共变修复位置,故障定位（FL）是大多数自动程序修复（APR）方法的先导步骤，这些方法修复由FL工具识别的故障语句。我们提出了FixLocator，这是一种基于深度学习（DL）的故障定位方法，支持在一个或多个方法中检测故障语句，这些方法需要在同一修复中进行相应的修改。让我们将它们称为故障的共同更改（CC）修复位置。我们将这个FL问题视为具有两个模型的双重任务学习。方法级FL模型MethFL学习要固定在一起的方法。语句级FL模型StmtFL学习要共同固定的语句。一种模式下的正确学习可以使另一种模式受益，反之亦然。因此，我们通过十字绣单元软共享模型参数来同时训练它们，以使MethFL和StmtFL的影响能够相互传播。此外，我们还探讨了外语的一个新特点：共变语句。我们还使用基于图的卷积网络来集成不同类型的程序依赖关系。,计算方法论，机器学习，机器学习方法，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论，逻辑，逻辑和验证,,,
IWJZNMY5,2022,https://doi.org/10.1145/3540250.3549142,ESEC/FSE 2022,Cross-language Android permission specification,"The Android system manages access to sensitive APIs by permission enforcement. An application (app) must declare proper permissions before invoking specific Android APIs. However, there is no official documentation providing the complete list of permission-protected APIs and the corresponding permissions to date. Researchers have spent significant efforts extracting such API protection mapping from the Android API framework, which leverages static code analysis to determine if specific permissions are required before accessing an API. Nevertheless, none of them has attempted to analyze the protection mapping in the native library (i.e., code written in C and C++), an essential component of the Android framework that handles communication with the lower-level hardware, such as cameras and sensors. While the protection mapping can be utilized to detect various security vulnerabilities in Android apps, such as permission over-privilege, imprecise mapping will lead to false results in detecting such security vulnerabilities. To fill this gap, we thereby propose to construct the protection mapping involved in the native libraries of the Android framework to present a complete and accurate specification of Android API protection. We develop a prototype system, named NatiDroid, to facilitate the cross-language static analysis and compare its performance with two state-of-the-practice tools, termed Axplorer and Arcade. We evaluate NatiDroid on more than 11,000 Android apps, including system apps from custom Android ROMs and third-party apps from the Google Play. Our NatiDroid can identify up to 464 new API-permission mappings, in contrast to the worst-case results derived from both Axplorer and Arcade, where approximately 71% apps have at least one false positive in permission over-privilege. We have disclosed all the potential vulnerabilities detected to the stakeholders.","Security and privacy,Software and application security,Systems security,Operating systems security,Mobile platform security,Social and professional topics,Computing / technology policy,Software and its engineering,Software notations and tools,Software organization and properties,Software functional properties,Theory of computation,Semantics and reasoning",跨语言Android权限规范,安卓系统通过权限强制管理对敏感API的访问。在调用特定的Android API之前，应用程序必须声明适当的权限。然而，到目前为止，还没有官方文件提供受权限保护的API的完整列表和相应的权限。研究人员花费了大量精力从Android API框架中提取此类API保护映射，该框架利用静态代码分析来确定在访问API之前是否需要特定权限。尽管如此，他们都没有试图分析原生库（即用C和C++编写的代码）中的保护映射，原生库是安卓框架的一个重要组件，用于处理与较低级别硬件（如相机和传感器）的通信。虽然保护映射可以用来检测安卓应用程序中的各种安全漏洞，例如权限超过权限，但不精确的映射会导致检测此类安全漏洞的错误结果。为了填补这一空白，我们建议在Android框架的原生库中构建保护映射，以提供完整准确的Android API保护规范。我们开发了一个名为NatiDroid的原型系统，以促进跨语言静态分析，并将其性能与两种实践工具Axplorer和Arcade进行比较。我们在11000多个Android应用程序上评估了NatiDroid，包括来自自定义Android ROM的系统应用程序和来自Google Play的第三方应用程序。我们的NatiDroid可以识别多达464个新的API权限映射，这与Axplorer和Arcade得出的最坏情况结果形成了鲜明对比，其中大约71%的应用程序在权限超过特权方面至少有一个假阳性。我们已经向利益相关者披露了检测到的所有潜在漏洞。,安全和隐私，软件和应用程序安全，系统安全，操作系统安全，移动平台安全，社会和专业主题，计算/技术政策，软件及其工程，软件符号和工具，软件组织和属性，软件功能属性，计算理论，语义和推理,,,
9H2JAYZB,2022,https://doi.org/10.1145/3540250.3549081,ESEC/FSE 2022,Automating code review activities by large-scale pre-training,"Code review is an essential part to software development lifecycle since it aims at guaranteeing the quality of codes. Modern code review activities necessitate developers viewing, understanding and even running the programs to assess logic, functionality, latency, style and other factors. It turns out that developers have to spend far too much time reviewing the code of their peers. Accordingly, it is in significant demand to automate the code review process. In this research, we focus on utilizing pre-training techniques for the tasks in the code review scenario. We collect a large-scale dataset of real-world code changes and code reviews from open-source projects in nine of the most popular programming languages. To better understand code diffs and reviews, we propose CodeReviewer, a pre-trained model that utilizes four pre-training tasks tailored specifically for the code review scenario. To evaluate our model, we focus on three key tasks related to code review activities, including code change quality estimation, review comment generation and code refinement. Furthermore, we establish a high-quality benchmark dataset based on our collected data for these three tasks and conduct comprehensive experiments on it. The experimental results demonstrate that our model outperforms the previous state-of-the-art pre-training approaches in all tasks. Further analysis show that our proposed pre-training tasks and the multilingual pre-training dataset benefit the model on the understanding of code changes and reviews.","Computing methodologies,Machine learning,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software development techniques,Automatic programming,Software post-development issues,Software notations and tools,Software configuration management and version control systems",通过大规模预培训实现代码审查活动的自动化,代码评审是软件开发生命周期的重要组成部分，因为它旨在保证代码的质量。现代的代码审查活动要求开发人员查看、理解甚至运行程序，以评估逻辑、功能、延迟、风格和其他因素。事实证明，开发人员不得不花太多时间来审查同行的代码。因此，对代码审查过程的自动化需求很大。在这项研究中，我们专注于将预训练技术用于代码审查场景中的任务。我们用九种最流行的编程语言从开源项目中收集了一个大规模的真实世界代码更改和代码评论数据集。为了更好地理解代码差异和评审，我们提出了CodeReviewer，这是一个预先训练的模型，它利用了四个专门针对代码评审场景定制的预先训练任务。为了评估我们的模型，我们专注于与代码评审活动相关的三项关键任务，包括代码更改质量估计、评审注释生成和代码精化。此外，我们根据这三项任务收集的数据建立了一个高质量的基准数据集，并对其进行了全面的实验。实验结果表明，我们的模型在所有任务中都优于以前最先进的预训练方法。进一步的分析表明，我们提出的预训练任务和多语言预训练数据集有利于模型理解代码更改和审查。,计算方法，机器学习，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发技术，自动编程，软件后开发问题，软件符号和工具，软件配置管理和版本控制系统,,,
EP3JAWGU,2022,https://doi.org/10.1145/3540250.3549122,ESEC/FSE 2022,Generic sensitivity: customizing context-sensitive pointer analysis for generics,"Generic programming has been extensively used in object-oriented programs such as Java. However, existing context-sensitive pointer analyses perform poorly in analyzing generics. This paper introduces generic sensitivity, a new context customization scheme targeting generics. We design our context customization scheme in such a way that generic instantiation sites, i.e., locations instantiating generic classes/methods with concrete types, are always preserved as key context elements. This is realized by augmenting contexts with a type variable lookup map, which is efficiently updated during the analysis in a context-sensitive manner. ","Software and its engineering,Software notations and tools,Compilers,General programming languages,Language features,Language types,Object oriented languages,Theory of computation,Semantics and reasoning,Program constructs,Program reasoning,Program analysis",泛型敏感度：自定义泛型的上下文敏感指针分析,泛型编程已被广泛用于Java等面向对象程序中。然而，现有的上下文敏感指针分析在分析泛型方面表现不佳。本文介绍了一种针对泛型的上下文定制方案——泛型敏感度。我们设计上下文定制方案的方式是，通用实例化站点，即用具体类型实例化通用类/方法的位置，始终保留为关键上下文元素。这是通过使用类型变量查找映射来扩充上下文来实现的，该映射在分析过程中以上下文敏感的方式进行有效更新。,软件及其工程，软件符号和工具，编译器，通用编程语言，语言特征，语言类型，面向对象语言，计算理论，语义和推理，程序构造，程序推理，程序分析,,,
23S4PQCN,2022,https://doi.org/10.1145/3540250.3549099,ESEC/FSE 2022,AUGER: automatically generating review comments with pre-training models,"Code review is one of the best practices as a powerful safeguard for software quality. In practice, senior or highly skilled reviewers inspect source code and provide constructive comments, consider- ing what authors may ignore, for example, some special cases. The collaborative validation between contributors results in code being highly qualified and less chance of bugs. However, since personal knowledge is limited and varies, the efficiency and effectiveness of code review practice are worthy of further improvement. In fact, it still takes a colossal and time-consuming effort to deliver useful review comments.  
This paper explores a synergy of multiple practical review comments to enhance code review and proposes AUGER (AUtomatically GEnerating Review comments): a review comments generator with pre-training models. We first collect empirical review data from 11 notable Java projects and construct a dataset of 10,882 code changes. By leveraging Text-to-Text Transfer Transformer (T5) models, the framework synthesizes valuable knowledge in the training stage and effectively outperforms baselines by 37.38% in ROUGE-L. 29% of our automatic review comments are considered useful according to prior studies. The inference generates just in 20 seconds and is also open to training further. Moreover, the performance also gets improved when thoroughly analyzed in case study.","Computing methodologies,Artificial intelligence,Natural language processing,Machine learning,Information systems,Information systems applications,Data mining,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software post-development issues,Software notations and tools,Software configuration management and version control systems,Software maintenance tools",AUGER：使用预训练模型自动生成评论,代码审查是软件质量的有力保障，是最佳实践之一。在实践中，资深或高技能的评审人员会检查源代码并提供建设性的意见，考虑作者可能忽略的内容，例如，一些特殊情况。贡献者之间的协作验证可以使代码具有很高的合格性，并减少出现错误的机会。然而，由于个人知识有限且各不相同，代码审查实践的效率和有效性值得进一步提高。事实上，提供有用的审查意见仍然需要付出巨大而耗时的努力。,计算方法学，人工智能，自然语言处理，机器学习，信息系统，信息系统应用，数据挖掘，社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件后开发问题，软件符号和工具，软件配置管理和版本控制系统，软件维护工具,,,
6C7APSP4,2022,https://doi.org/10.1145/3540250.3549092,ESEC/FSE 2022,Actionable and interpretable fault localization for recurring failures in online service systems,"Fault localization is challenging in an online service system due to its monitoring data's large volume and variety and complex dependencies across/within its components (e.g., services or databases). Furthermore, engineers require fault localization solutions to be actionable and interpretable, which existing research approaches cannot satisfy. Therefore, the common industry practice is that, for a specific online service system, its experienced engineers focus on localization for recurring failures based on the knowledge accumulated about the system and historical failures. More specifically, 1) they can identify the underlying root causes and take mitigation actions when pinpointing a group of indicative metrics on the faulty component; 2) their diagnosis knowledge is roughly based on how one failure might affect the components in the whole system.  
","General and reference,Cross-computing tools and techniques,Performance,Reliability,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software reliability,Software system structures,Distributed systems organizing principles,Cloud computing",在线服务系统中重复故障的可操作和可解释故障定位,在线服务系统中的故障定位具有挑战性，因为它的监控数据量大、种类繁多，并且在其组件（例如服务或数据库）之间/内部具有复杂的依赖性。此外，工程师们要求故障定位解决方案具有可操作性和可解释性，而现有的研究方法无法满足这一要求。因此，常见的行业实践是，对于特定的在线服务系统，其经验丰富的工程师根据积累的系统和历史故障知识，专注于重复故障的本地化。更具体地说，1）他们可以识别潜在的根本原因，并在精确定位故障部件上的一组指示性指标时采取缓解措施；2） 他们的诊断知识大致基于一个故障可能如何影响整个系统中的组件。,通用和参考，交叉计算工具和技术，性能，可靠性，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，功能外属性，软件可靠性，软件系统结构，分布式系统组织原则，云计算,,,
6XLTBURB,2022,https://doi.org/10.1145/3540250.3549082,ESEC/FSE 2022,Understanding skills for OSS communities on GitHub,"The development of open source software (OSS) is a broad field which requires diverse skill sets. For example, maintainers help lead the project and promote its longevity, technical writers assist with documentation, bug reporters identify defects in software, and developers program the software.  
However, it is unknown which skills are used in OSS development as well as OSS contributors' general attitudes towards skills in OSS. In this paper, we address this gap by administering a survey to a diverse set of 455 OSS contributors. Guided by these responses as well as prior literature on software development expertise and social factors of OSS, we develop a model of skills in OSS that considers the many contexts OSS contributors work in. This model has 45 skills in the following 9 categories: technical skills, working styles, problem solving, contribution types, project-specific skills, interpersonal skills, external relations, management, and characteristics. Through a mix of qualitative and quantitative analyses, we find that OSS contributors are actively motivated to improve skills and perceive many benefits in sharing their skills with others. We then use this analysis to derive a set of design implications and best practices for those who incorporate skills into OSS tools and platforms, such as GitHub.","Human-centered computing,Collaborative and social computing,Collaborative and social computing systems and tools,Open source software,Collaborative and social computing theory, concepts and paradigms,Social and professional topics,Professional topics,Computing and business,Computer supported cooperative work,Management of computing and information systems,Software and its engineering,Software creation and management,Collaboration in software development",了解GitHub上OSS社区的技能,开放源码软件（OSS）的开发是一个需要多种技能的广阔领域。例如，维护人员帮助领导项目并延长其使用寿命，技术编写人员协助编写文档，错误报告员识别软件中的缺陷，开发人员对软件进行编程。,以人为中心的计算，协作和社会计算，协作和社会计算系统和工具，开放源码软件，协作和社会计算理论，概念和范式，社会和专业主题，专业主题，计算和商业，计算机支持的协作工作，计算和信息系统管理，软件及其工程，软件创建和管理，软件开发中的协作,,,
YXC8277F,2022,https://doi.org/10.1145/3540250.3549160,ESEC/FSE 2022,How to formulate specific how-to questions in software development?,"Developers often ask how-to questions using search engines, technical Q&A communities, and interactive Q&A systems to seek help for specific programming tasks. However, they often do not formulate the questions in a specific way, making it hard for the systems to return the best answers. We propose an approach (TaskKG4Q) that interactively helps developers formulate a programming related how-to question. TaskKG4Q is using a programming task knowledge graph (task KG in short) mined from Stack Overflow questions, which provides a hierarchical conceptual structure for tasks in terms of [actions], [objects], and [constraints]. An empirical evaluation of the intrinsic quality of the task KG revealed that 75.0% of the annotated questions in the task KG are correct. The comparison between TaskKG4Q and two baselines revealed that TaskKG4Q can help developers formulate more specific how-to questions. More so, an empirical study with novice programmers revealed that they write more effective questions for finding answers to their programming tasks on Stack Overflow.","Human-centered computing,Information systems,Software and its engineering,Software creation and management,Software post-development issues,Documentation,Software notations and tools,Software maintenance tools",如何制定软件开发中的具体操作问题？,开发人员经常使用搜索引擎、技术问答社区和交互式问答系统来询问如何操作的问题，以寻求特定编程任务的帮助。然而，他们往往没有以特定的方式提出问题，这使得系统很难返回最佳答案。我们提出了一种方法（TaskKG4Q），以交互方式帮助开发人员制定与编程相关的操作问题。TaskKG4Q使用的是从Stack Overflow问题中挖掘的编程任务知识图（简称task KG），它为任务提供了一个按[动作]、[对象]和[约束]划分的层次概念结构。对任务KG内在质量的实证评估显示，任务KG中75.0%的注释问题是正确的。TaskKG4Q和两个基线之间的比较表明，TaskKG4Q可以帮助开发人员制定更具体的操作问题。更重要的是，一项针对新手程序员的实证研究表明，他们在Stack Overflow上编写更有效的问题来寻找编程任务的答案。,以人为中心的计算，信息系统，软件及其工程，软件创建和管理，软件开发后问题，文档，软件符号和工具，软件维护工具,,,
UZBSXB3Y,2022,https://doi.org/10.1145/3540250.3559078,ESEC/FSE 2022,RESTInfer: automated inferring parameter constraints from natural language RESTful API descriptions,"RESTful APIs have been applied to provide cloud services by various notable companies. The quality assurance of RESTful API is critical. Several automatic RESTful API testing techniques have been proposed to tame this issue. By analyzing crashes caused by each test case, developers can find potential bugs in cloud services. However, it is difficult for automated tools to generate feasible parameters under complicating constraints randomly. Fortunately, RESTful API descriptions can be used to infer possible parameter constraints. Given parameter constraints, automated tools can further improve the efficiency of testing.  
","Human-centered computing,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software maintenance tools,Software organization and properties,Software functional properties,Theory of computation,Semantics and reasoning,Program reasoning",RESTInfer：从自然语言RESTful API描述中自动推断参数约束,RESTful API已被多家知名公司应用于提供云服务。RESTful API的质量保证至关重要。已经提出了几种自动RESTful API测试技术来解决这个问题。通过分析每个测试用例引起的崩溃，开发人员可以发现云服务中的潜在漏洞。然而，自动化工具很难在复杂的约束条件下随机生成可行的参数。幸运的是，RESTful API描述可以用来推断可能的参数约束。在给定参数约束的情况下，自动化工具可以进一步提高测试效率。,以人为中心的计算，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件维护工具，软件组织和属性，软件功能属性，计算理论，语义和推理，程序推理,,,
4SBHWNDD,2022,https://doi.org/10.1145/3540250.3558935,ESEC/FSE 2022,CodeMatcher: a tool for large-scale code search based on query semantics matching,"Due to the emergence of large-scale codebases, such as GitHub and Gitee, searching and reusing existing code can help developers substantially improve software development productivity. Over the years, many code search tools have been developed. Early tools leveraged the information retrieval (IR) technique to perform an efficient code search for a frequently changed large-scale codebase. However, the search accuracy was low due to the semantic mismatch between query and code. In the recent years, many tools leveraged Deep Learning (DL) technique to address this issue. But the DL-based tools are slow and the search accuracy is unstable.  
","Information systems,Information retrieval,Information retrieval query processing,Software and its engineering,Software creation and management,Software notations and tools,Software maintenance tools",CodeMatcher：一种基于查询语义匹配的大规模代码搜索工具,由于GitHub和Gitee等大规模代码库的出现，搜索和重用现有代码可以帮助开发人员大幅提高软件开发生产力。多年来，已经开发了许多代码搜索工具。早期的工具利用信息检索（IR）技术对频繁更改的大规模代码库执行高效的代码搜索。然而，由于查询和代码之间的语义不匹配，搜索准确率很低。近年来，许多工具利用深度学习（DL）技术来解决这个问题。但是基于DL的工具速度较慢，并且搜索精度不稳定。,信息系统，信息检索，信息检索查询处理，软件及其工程，软件创建和管理，软件符号和工具，软件维护工具,,,
9ZZVY8V5,2022,https://doi.org/10.1145/3540250.3549111,ESEC/FSE 2022,Testing of autonomous driving systems: where are we and where should we go?,"Autonomous driving has shown great potential to reform modern transportation. Yet its reliability and safety have drawn a lot of attention and concerns. Compared with traditional software systems, autonomous driving systems (ADSs) often use deep neural networks in tandem with logic-based modules. This new paradigm poses unique challenges for software testing. Despite the recent development of new ADS testing techniques, it is not clear to what extent those techniques have addressed the needs of ADS practitioners. To fill this gap, we present the first comprehensive study to identify the current practices and needs of ADS testing. We conducted semi-structured interviews with developers from 10 autonomous driving companies and surveyed 100 developers who have worked on autonomous driving systems. A systematic analysis of the interview and survey data revealed 7 common practices and 4 emerging needs of autonomous driving testing. Through a comprehensive literature review, we developed a taxonomy of existing ADS testing techniques and analyzed the gap between ADS research and practitioners’ needs. Finally, we proposed several future directions for SE researchers, such as developing test reduction techniques to accelerate simulation-based ADS testing.","Human-centered computing,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Formal methods",自动驾驶系统测试：我们在哪里，我们应该去哪里？,自动驾驶在改革现代交通方面显示出巨大潜力。然而，它的可靠性和安全性引起了人们的广泛关注。与传统的软件系统相比，自动驾驶系统通常使用深度神经网络和基于逻辑的模块。这种新的范式对软件测试提出了独特的挑战。尽管最近开发了新的ADS测试技术，但尚不清楚这些技术在多大程度上满足了ADS从业者的需求。为了填补这一空白，我们提出了第一项全面的研究，以确定ADS测试的当前实践和需求。我们对来自10家自动驾驶公司的开发人员进行了半结构化采访，并调查了100名从事自动驾驶系统开发的开发人员。对访谈和调查数据的系统分析揭示了自动驾驶测试的7种常见做法和4种新需求。通过全面的文献综述，我们对现有的ADS测试技术进行了分类，并分析了ADS研究与从业者需求之间的差距。最后，我们为SE研究人员提出了几个未来的方向，例如开发测试简化技术来加速基于模拟的ADS测试。,以人为中心的计算，软件及其工程，软件创建和管理，软件验证和确认，形式化软件验证，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，形式化方法,,,
UDWPPB57,2022,https://doi.org/10.1145/3540250.3549155,ESEC/FSE 2022,SamplingCA: effective and efficient sampling-based pairwise testing for highly configurable software systems,"Combinatorial interaction testing (CIT) is an effective paradigm for testing highly configurable systems, and its goal is to generate a t-wise covering array (CA) as a test suite, where t is the strength of testing. It is recognized that pairwise testing (i.e., CIT with t=2) is the most common CIT technique, and has high fault detection capability in practice. The problem of pairwise CA generation (PCAG), which is a core problem in pairwise testing, aims at generating a pairwise CA (i.e., 2-wise CA) of minimum size, subject to hard constraints. The PCAG problem is a hard combinatorial optimization problem, which urgently requires practical methods for generating pairwise CAs (PCAs) of small sizes. However, existing PCAG algorithms suffer from the severe scalability issue; that is, when solving large-scale PCAG instances, existing state-of-the-art PCAG algorithms usually cost a fairly long time to generate large PCAs, which would make the testing of highly configurable systems both ineffective and inefficient. In this paper, we propose a novel and effective sampling-based approach dubbed SamplingCA for solving the PCAG problem. SamplingCA first utilizes sampling techniques to obtain a small test suite that covers valid pairwise tuples as many as possible, and then adds a few more test cases into the test suite to ensure that all valid pairwise tuples are covered. Extensive experiments on 125 public PCAG instances show that our approach can generate much smaller PCAs than its state-of-the-art competitors, indicating the effectiveness of SamplingCA. Also, our experiments show that SamplingCA runs one to two orders of magnitude faster than its competitors, demonstrating the efficiency of SamplingCA. Our results confirm that SamplingCA is able to address the scalability issue and considerably pushes forward the state of the art in PCAG solving.","Software and its engineering,Software creation and management,Search-based software engineering,Software development process management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",SamplingCA：针对高度可配置软件系统的有效且高效的基于采样的成对测试,组合交互测试（CIT）是测试高度可配置系统的有效范例，其目标是生成一个t向覆盖数组（CA）作为测试套件，其中t是测试的强度。众所周知，成对测试（即t=2的CIT）是最常见的CIT技术，并且在实践中具有很高的故障检测能力。成对CA生成问题（PCAG）是成对测试中的核心问题，旨在生成一个受硬约束的最小大小的成对CA（即双向CA）。PCAG问题是一个困难的组合优化问题，迫切需要实用的方法来生成小尺寸的成对CA。然而，现有的PCAG算法存在严重的可扩展性问题；也就是说，在解决大规模PCAG实例时，现有最先进的PCAG算法通常需要相当长的时间来生成大型PCA，这将使对高度可配置系统的测试既无效又低效。在本文中，我们提出了一种新的、有效的基于采样的方法，称为SamplingCA，用于解决PCAG问题。SamplingCA首先利用采样技术获得一个小的测试套件，该套件覆盖尽可能多的有效成对元组，然后在测试套件中再添加几个测试用例，以确保覆盖所有有效成对元组。在125个公共PCAG实例上进行的大量实验表明，我们的方法可以产生比其最先进的竞争对手小得多的PCA，这表明SamplingCA的有效性。此外，我们的实验表明，SamplingCA的运行速度比竞争对手快一到两个数量级，证明了SamplingCA的效率。我们的结果证实，SamplingCA能够解决可扩展性问题，并在很大程度上推动了PCAG解决的最新技术。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件开发过程管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论，语义和推理，程序推理，程序验证,,,
2UDV35II,2022,https://doi.org/10.1145/3540250.3558946,ESEC/FSE 2022,An empirical investigation of missing data handling in cloud node failure prediction,"Cloud computing systems have become increasingly popular in recent years.  
A typical cloud system utilizes millions of computing nodes as the basic infrastructure.  
Node failure has been identified as one of the most prevalent causes of cloud system downtime.  
To improve the reliability of cloud systems, many previous studies collected monitoring metrics from nodes and built models to predict node failures before the failures happen.  
However, based on our experience with large-scale real-world cloud systems in Microsoft, we find that the task of predicting node failure is severely hampered by missing data.  
There is a large amount of missing data, and the online latest data utilized for prediction is even worse.  
As a result, the real-time performance of the node prediction model is limited.  
In this paper, we first characterize the missing data problem for node failure prediction.  
Then, we evaluate several existing data interpolation approaches, and find that node dimension interpolation approaches outperform time dimension ones and deep learning based interpolation is the best for early prediction.  
Our findings can help academics and engineers address the missing data problem in cloud node failure prediction and other data-driven software engineering scenarios.","Computer systems organization,Architectures,Distributed architectures,Cloud computing,General and reference,Cross-computing tools and techniques,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software verification and validation,Software organization and properties,Extra-functional properties,Software system structures,Distributed systems organizing principles",云节点故障预测中缺失数据处理的实证研究,近年来，云计算系统变得越来越流行。,计算机系统组织，体系结构，分布式体系结构，云计算，通则和参考文献，交叉计算工具和技术，软件及其工程，软件创建和管理，软件后开发问题，维护软件，软件验证和确认，软件组织和属性，额外功能属性，软件系统结构，分布式系统组织原则,,,
36D8HXLQ,2022,https://doi.org/10.1145/3540250.3558949,ESEC/FSE 2022,Nalanda: a socio-technical graph platform for building software analytics tools at enterprise scale,"Software development is information-dense knowledge work that requires collaboration with other developers and awareness of artifacts such as work items, pull requests, and file changes. With the speed of development increasing, information overload and information discovery are challenges for people developing and maintaining these systems. Finding information about similar code changes and experts is difficult for software engineers, especially when they work in large software systems or have just recently joined a project. In this paper, we build a large scale data platform named Nalanda platform to address the challenges of information overload and discovery. Nalanda contains two subsystems: (1) a large scale socio-technical graph system, named Nalanda graph system, and (2) a large scale index system, named Nalanda index system that aims at satisfying the information needs of software developers. To show the versatility of the Nalanda platform, we built two applications: (1) a software analytics application with a news feed named MyNalanda that has Daily Active Users (DAU) of 290 and Monthly Active Users (MAU) of 590, and (2) a recommendation system for related work items and pull requests that accomplished similar tasks (artifact recommendation) and a recommendation system for subject matter experts (expert recommendation), augmented by the Nalanda socio-technical graph. Initial studies of the two applications found that developers and engineering managers are favorable toward continued use of the news feed application for information discovery. The studies also found that developers agreed that a system like Nalanda artifact and expert recommendation application could reduce the time spent and the number of places needed to visit to find information.","Human-centered computing,Collaborative and social computing,Information systems,Information systems applications,Software and its engineering,Software creation and management,Collaboration in software development,Programming teams",Nalanda：一个用于构建企业级软件分析工具的社会技术图形平台,软件开发是信息密集的知识工作，需要与其他开发人员合作，并了解工件，如工作项、拉取请求和文件更改。随着发展速度的加快，信息过载和信息发现对开发和维护这些系统的人来说是一个挑战。软件工程师很难找到类似代码更改和专家的信息，尤其是当他们在大型软件系统中工作或刚刚加入项目时。在本文中，我们构建了一个名为Nalanda平台的大型数据平台，以应对信息过载和发现的挑战。Nalanda包含两个子系统：（1）一个大规模的社会技术图形系统，名为Nalanda图形系统；（2）一个旨在满足软件开发人员信息需求的大规模指标系统，名为由Nalanda指标系统。为了展示Nalanda平台的多功能性，我们构建了两个应用程序：（1）一个名为MyNalanda的软件分析应用程序，其每日活跃用户（DAU）为290，每月活跃用户（MAU）为590，以及（2）用于完成类似任务的相关工作项目和拉取请求的推荐系统（工件推荐）和用于主题专家的建议系统（专家推荐），通过Nalanda社会技术图来增强。对这两个应用程序的初步研究发现，开发人员和工程经理有利于继续使用新闻提要应用程序进行信息发现。研究还发现，开发人员一致认为，像Nalanda工件和专家推荐应用程序这样的系统可以减少查找信息所需的时间和地点数量。,以人为中心的计算，协作和社会计算，信息系统，信息系统应用，软件及其工程，软件创建和管理，软件开发协作，编程团队,,,
CC2T3HX3,2022,https://doi.org/10.1145/3540250.3549144,ESEC/FSE 2022,Online testing of RESTful APIs: promises and challenges,"Online testing of web APIs—testing APIs in production—is gaining traction in industry. Platforms such as RapidAPI and Sauce Labs provide online testing and monitoring services of web APIs 24/7, typically by re-executing manually designed test cases on the target APIs on a regular basis. In parallel, research on the automated generation of test cases for RESTful APIs has seen significant advances in recent years. However, despite their promising results in the lab, it is unclear whether research tools would scale to industrial-size settings and, more importantly, how they would perform in an online testing setup, increasingly common in practice. In this paper, we report the results of an empirical study on the use of automated test case generation methods for online testing of RESTful APIs. Specifically, we used the RESTest framework to automatically generate and execute test cases in 13 industrial APIs for 15 days non-stop, resulting in over one million test cases. To scale at this level, we had to transition from a monolithic tool approach to a multi-bot architecture with over 200 bots working cooperatively in tasks like test generation and reporting. As a result, we uncovered about 390K failures, which we conservatively triaged into 254 bugs, 65 of which have been acknowledged or fixed by developers to date. Among others, we identified confirmed faults in the APIs of Amadeus, Foursquare, Yelp, and YouTube, accessed by millions of applications worldwide. More importantly, our reports have guided developers on improving their APIs, including bug fixes and documentation updates in the APIs of Amadeus and YouTube. Our results show the potential of online testing of RESTful APIs as the next must-have feature in industry, but also some of the key challenges to overcome for its full adoption in practice.","General and reference,Cross-computing tools and techniques,Information systems,World Wide Web,Web services,RESTful web services,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",RESTful API的在线测试：承诺与挑战,web API的在线测试——在生产中测试API——在行业中越来越受欢迎。RapidAPI和Sauce Labs等平台提供web API的全天候在线测试和监控服务，通常通过定期在目标API上重新执行手动设计的测试用例。与此同时，近年来，对RESTful API测试用例自动生成的研究取得了重大进展。然而，尽管它们在实验室中取得了有希望的结果，但尚不清楚研究工具是否会扩展到工业规模的环境中，更重要的是，它们在实践中越来越常见的在线测试环境中会如何表现。在本文中，我们报告了一项关于在RESTful API的在线测试中使用自动测试用例生成方法的实证研究结果。具体来说，我们使用RESTest框架在13个工业API中自动生成和执行测试用例，持续15天，产生了超过100万个测试用例。为了在这个级别上进行扩展，我们必须从单一的工具方法过渡到多机器人架构，200多个机器人在测试生成和报告等任务中协同工作。因此，我们发现了大约390K个故障，我们保守地将其分为254个错误，其中65个已被开发人员确认或修复。除其他外，我们在Amadeus、Foursquare、Yelp和YouTube的API中发现了已确认的故障，全球数百万应用程序都在访问这些API。更重要的是，我们的报告指导开发人员改进他们的API，包括Amadeus和YouTube的API中的错误修复和文档更新。我们的研究结果显示了在线测试RESTful API作为行业中下一个必备功能的潜力，但也显示了在实践中充分采用RESTful API需要克服的一些关键挑战。,一般和参考文献，交叉计算工具和技术，信息系统，万维网，Web服务，REST风格的Web服务，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，计算理论，语义和推理，程序推理，程序验证,,,
XMCR7Z4Z,2022,https://doi.org/10.1145/3540250.3560881,ESEC/FSE 2022,Discrepancies among pre-trained deep neural networks: a new threat to model zoo reliability,"Training deep neural networks (DNNs) takes significant time and resources. A practice for expedited deployment is to use pre-trained deep neural networks (PTNNs), often from model zoos--collections of PTNNs; yet, the reliability of model zoos remains unexamined. In the absence of an industry standard for the implementation and performance of PTNNs, engineers cannot confidently incorporate them into production systems. As a first step, discovering potential discrepancies between PTNNs across model zoos would reveal a threat to model zoo reliability. Prior works indicated existing variances in deep learning systems in terms of accuracy. However, broader measures of reliability for PTNNs from model zoos are unexplored. This work measures notable discrepancies between accuracy, latency, and architecture of 36 PTNNs across four model zoos. Among the top 10 discrepancies, we find differences of 1.23%-2.62% in accuracy and 9%-131% in latency. We also find mismatches in architecture for well-known DNN architectures (e.g., ResNet and AlexNet). Our findings call for future works on empirical validation, automated tools for measurement, and best practices for implementation.","Computer systems organization,Architectures,Computing methodologies,Machine learning,Machine learning approaches,Neural networks,General and reference,Cross-computing tools and techniques,Networks,Software and its engineering,Software creation and management,Software development techniques,Reusability",预先训练的深度神经网络之间的差异：对模型动物园可靠性的新威胁,训练深度神经网络（DNN）需要大量的时间和资源。快速部署的一种做法是使用预先训练的深度神经网络（PTNN），通常来自模型动物园——PTNN的集合；然而，模型动物园的可靠性仍未得到检验。在缺乏PTNN实施和性能的行业标准的情况下，工程师无法自信地将其纳入生产系统。作为第一步，发现模型动物园之间PTNN之间的潜在差异将揭示对模型动物园可靠性的威胁。先前的工作表明，深度学习系统在准确性方面存在差异。然而，模型动物园的PTNN的更广泛的可靠性指标尚未探索。这项工作测量了四个模型动物园中36个PTNN的准确性、延迟和架构之间的显著差异。在前10个差异中，我们发现准确性差异为1.23%-2.62%，潜伏期差异为9%-131%。我们还发现众所周知的DNN架构（例如，ResNet和AlexNet）的架构不匹配。我们的研究结果呼吁未来在实证验证、自动化测量工具和实施最佳实践方面开展工作。,计算机系统组织，体系结构，计算方法，机器学习，机器学习方法，神经网络，通用和参考，交叉计算工具和技术，网络，软件及其工程，软件创建和管理，软件开发技术，可重用性,,,
VMXISJR6,2022,https://doi.org/10.1145/3540250.3569445,ESEC/FSE 2022,Task modularity and the emergence of software value streams (impact award paper keynote),"The creation of techniques, languages and tools to support building software from modular parts has enabled the development and evolution of large complex software systems. For many years, the focus of modularity was on the structure of the software system. The thinking was that the right modularity would enable software teams involved in different pieces of the system to work as independently as possible. As a community, we learned over the years that a system has no one optimal modularity, and in fact, the work that is undertaken to add new features or fix defects often crosscuts the modularity of the software. The research we conducted on task contexts and Mylyn over fifteen years ago recognized that capturing the activity performed on development tasks provided a means to make explicit emergent modularity for a system. With Mylyn, we explored how the activity of one developer could enable the surfacing of emergent modularity to help a developer perform tasks on a software system. Through stewardship of the Mylyn open source project and the creation of Tasktop Technologies, we brought these ideas into use in industry. Over the past decade, through interactions with practicing developers and their organizations, we have learned more about how modularity emerges at the individual developer, team, and organizational levels. Tasktop’s products moved accordingly from supporting the activity of individual developers to supporting the streams of value teams within an organization produce. In this talk, we will discuss how following the flow of tasks within software development supports value stream management and outline open research questions about the socio-technical aspects of developing complex software systems.","Human-centered computing,Social and professional topics,Professional topics,Management of computing and information systems,Project and people management,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Programming teams,Software development process management,Software development methods,Software notations and tools,Software configuration management and version control systems",任务模块化与软件价值流的出现（影响力奖论文主题）,通过创建技术、语言和工具来支持从模块化部件构建软件，实现了大型复杂软件系统的开发和进化。多年来，模块化的焦点都集中在软件系统的结构上。当时的想法是，正确的模块化将使参与系统不同部分的软件团队能够尽可能独立地工作。作为一个社区，我们多年来了解到，一个系统没有一个最佳的模块性，事实上，添加新功能或修复缺陷的工作往往会横切软件的模块性。15年前，我们对任务上下文和Mylyn进行的研究认识到，捕捉开发任务中执行的活动为系统提供了一种明确的应急模块化的方法。通过Mylyn，我们探索了一个开发人员的活动如何能够实现紧急模块化，以帮助开发人员在软件系统上执行任务。通过管理Mylyn开源项目和创建Tasktop Technologies，我们将这些想法应用于行业。在过去的十年里，通过与实践开发人员及其组织的互动，我们了解了更多关于模块化如何在个人开发人员、团队和组织层面出现的信息。Tasktop的产品相应地从支持单个开发人员的活动转变为支持组织内价值团队的生产流。在本次演讲中，我们将讨论遵循软件开发中的任务流如何支持价值流管理，并概述关于开发复杂软件系统的社会技术方面的开放研究问题。,以人为中心的计算，社会和专业主题，专业主题，计算和信息系统管理，项目和人员管理，软件管理，软件及其工程，软件创建和管理，软件开发协作，编程团队，软件开发过程管理，软件开发方法，软件符号和工具，软件配置管理和版本控制系统,,,
3MPPZ8UT,2022,https://doi.org/10.1145/3540250.3558936,ESEC/FSE 2022,VulCurator: a vulnerability-fixing commit detector,"Open-source software (OSS) vulnerability management process is important nowadays, as the number of discovered OSS vulnerabilities is increasing over time. Monitoring vulnerability-fixing commits is a part of the standard process to prevent vulnerability exploitation. Manually detecting vulnerability-fixing commits is, however, time-consuming due to the possibly large number of commits to review. Recently, many techniques have been proposed to automatically detect vulnerability-fixing commits using machine learning. These solutions either: (1) did not use deep learning, or (2) use deep learning on only limited sources of information. This paper proposes VulCurator, a tool that leverages deep learning on richer sources of information, including commit messages, code changes and issue reports for vulnerability-fixing commit classification. Our experimental results show that VulCurator outperforms the state-of-the-art baselines up to 16.1% in terms of F1-score.  
","Computing methodologies,Machine learning,Learning paradigms,Supervised learning,Supervised learning by classification,Security and privacy,Systems security,Vulnerability management,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software verification and validation,Software notations and tools,Software configuration management and version control systems,Software organization and properties",VulCurator：一个修复提交检测器的漏洞,开放源码软件（OSS）漏洞管理过程如今很重要，因为发现的OSS漏洞数量随着时间的推移而增加。监控漏洞修复提交是防止漏洞利用的标准流程的一部分。然而，手动检测修复漏洞的提交非常耗时，因为可能需要审查大量的提交。最近，已经提出了许多使用机器学习来自动检测漏洞修复提交的技术。这些解决方案要么是：（1）不使用深度学习，要么是（2）仅在有限的信息来源上使用深度学习。本文提出了VulCurator，这是一种利用深度学习获取更丰富信息源的工具，包括提交消息、代码更改和漏洞修复提交分类的问题报告。我们的实验结果表明，VulCurator在F1得分方面优于最先进的基线，高达16.1%。,计算方法，机器学习，学习范例，监督学习，分类监督学习，安全和隐私，系统安全，漏洞管理，社会和专业主题，专业主题，计算和信息系统的管理，软件及其工程，软件创建和管理，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统，软件组织和属性,,,
LGHHE29S,2022,https://doi.org/10.1145/3540250.3558927,ESEC/FSE 2022,MANDO-GURU: vulnerability detection for smart contract source code by heterogeneous graph embeddings,"Smart contracts are increasingly used with blockchain systems for high-value applications. It is highly desired to ensure the quality of smart contract source code before they are deployed. This paper proposes a new deep learning-based tool, MANDO-GURU, that aims to accurately detect vulnerabilities in smart contracts at both coarse-grained contract-level and fine-grained line-level. Using a combination of control-flow graphs and call graphs of Solidity code, we design new heterogeneous graph attention neural networks to encode more structural and potentially semantic relations among different types of nodes and edges of such graphs and use the encoded embeddings of the graphs and nodes to detect vulnerabilities. Our validation of real-world smart contract datasets shows that MANDO-GURU can significantly improve many other vulnerability detection techniques by up to 24% in terms of the F1-score at the contract level, depending on vulnerability types. It is the first learning-based tool for Ethereum smart contracts that identify vulnerabilities at the line level and significantly improves the traditional code analysis-based techniques by up to 63.4%. Our tool is publicly available at https://github.com/MANDO-Project/ge-sc-machine. A test version is currently deployed at http://mandoguru.com, and a demo video of our tool is available at http://mandoguru.com/demo-video.","Computing methodologies,Machine learning,Machine learning approaches,Security and privacy,Intrusion/anomaly detection and malware mitigation,Network security,Software and application security,Software security engineering,Social and professional topics,Computing / technology policy,Computer crime,Software and its engineering,Software creation and management,Software verification and validation,Software organization and properties",MANDO-GURU：基于异构图嵌入的智能合约源代码漏洞检测,智能合约越来越多地与区块链系统一起用于高价值应用。在部署智能合约源代码之前，我们非常希望确保它们的质量。本文提出了一种新的基于深度学习的工具MANDO-GURU，旨在在粗粒度合约级别和细粒度行级别准确检测智能合约中的漏洞。使用Solidity代码的控制流图和调用图的组合，我们设计了新的异构图注意力神经网络，以编码这些图的不同类型的节点和边之间更具结构和潜在语义的关系，并使用图和节点的编码嵌入来检测漏洞。我们对真实世界智能合约数据集的验证表明，根据漏洞类型的不同，MANDO-GURU可以显著提高许多其他漏洞检测技术，在合约级别的F1分数方面提高高达24%。它是第一个用于以太坊智能合约的基于学习的工具，可以在线路级别识别漏洞，并将传统的基于代码分析的技术显著提高了63.4%。我们的工具可在https://github.com/MANDO-Project/ge-sc-machine.测试版本当前部署在http://mandoguru.com，我们的工具演示视频可在http://mandoguru.com/demo-video.,计算方法，机器学习，机器学习方法，安全和隐私，入侵/异常检测和恶意软件缓解，网络安全，软件和应用程序安全，软件安全工程，社会和专业主题，计算/技术政策，计算机犯罪，软件及其工程，软件创建和管理，软件验证和确认，软件组织和属性,,,
8YZ4NTAI,2022,https://doi.org/10.1145/3540250.3549165,ESEC/FSE 2022,The best of both worlds: integrating semantic features with expert features for defect prediction and localization,"To improve software quality, just-in-time defect prediction (JIT-DP) (identifying defect-inducing commits) and just-in-time defect localization (JIT-DL) (identifying defect-inducing code lines in commits) have been widely studied by learning semantic features or expert features respectively, and indeed achieved promising performance. Semantic features and expert features describe code change commits from different aspects, however, the best of the two features have not been fully explored together to boost the just-in-time  
defect prediction and localization in the literature yet. Additional, JIT-DP identifies defects at the coarse commit level, while as the  
consequent task of JIT-DP, JIT-DL cannot achieve the accurate localization of defect-inducing code lines in a commit without JIT-DP.  
We hypothesize that the two JIT tasks can be combined together to boost the accurate prediction and localization of defect-inducing  
commits by integrating semantic features with expert features. Therefore, we propose to build a unified model, JIT-Fine, for the  
just-in-time defect prediction and localization by leveraging the best of semantic features and expert features. To assess the feasibility  
of JIT-Fine, we first build a large-scale line-level manually labeled dataset, JIT-Defects4J. Then, we make a comprehensive comparison  
with six state-of-the-art baselines under various settings using ten performance measures grouped into two types: effort-agnostic  
and effort-aware. The experimental results indicate that JIT-Fine can outperform all state-of-the-art baselines on both JIT-DP and JITDL  
tasks in terms of ten performance measures with a substantial improvement (i.e., 10%-629% in terms of effort-agnostic measures on JIT-DP, 5%-54% in terms of effort-aware measures on JIT-DP, and 4%-117% in terms of effort-aware measures on JIT-DL).","Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems",两全其美：将语义特征与专家特征相结合，用于缺陷预测和定位,为了提高软件质量，实时缺陷预测（JIT-DP）（识别引起缺陷的提交）和实时缺陷定位（JIT-DL）（识别提交中的引起缺陷的代码行）已经分别通过学习语义特征或专家特征进行了广泛的研究，并实现了良好的性能。语义特征和专家特征从不同的方面描述代码更改提交，然而，这两个特征中最好的一个还没有被充分地结合起来以提高即时性,社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件开发后问题，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统,,,
E2T4ET9V,2022,https://doi.org/10.1145/3540250.3549128,ESEC/FSE 2022,Generating realistic vulnerabilities via neural code editing: an empirical study,"The availability of large-scale, realistic vulnerability datasets is essential both for benchmarking existing techniques and for developing effective new data-driven approaches for software security. Yet such datasets are critically lacking. A promising solution is to generate such datasets by injecting vulnerabilities into real-world programs, which are richly available. Thus, in this paper, we explore the feasibility of vulnerability injection through neural code editing. With a synthetic dataset and a real-world one, we investigate the potential and gaps of three state-of-the-art neural code editors for vulnerability injection. We find that the studied editors have critical limitations on the real-world dataset, where the best accuracy is only 10.03%, versus 79.40% on the synthetic dataset. While the graph-based editors are more effective (successfully injecting vulnerabilities in up to 34.93% of real-world testing samples) than the sequence-based one (0 success), they still suffer from complex code structures and fall short for long edits due to their insufficient designs of the preprocessing and deep learning (DL) models. We reveal the promise of neural code editing for generating realistic vulnerable samples, as they help boost the effectiveness of DL-based vulnerability detectors by up to 49.51% in terms of F1 score. We also provide insights into the gaps in current editors (e.g., they are good at deleting but not at replacing code) and actionable suggestions for addressing them (e.g., designing effective editing primitives).","Security and privacy,Intrusion/anomaly detection and malware mitigation,Software and application security,Software security engineering,Systems security,Vulnerability management,Social and professional topics,Computing / technology policy,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis",通过神经代码编辑生成现实漏洞：一项实证研究,大规模、现实的漏洞数据集的可用性对于基准测试现有技术和开发有效的新的数据驱动软件安全方法都至关重要。然而，这样的数据集严重缺乏。一个有前景的解决方案是通过将漏洞注入现实世界中的程序来生成这样的数据集，这些程序是丰富可用的。因此，在本文中，我们通过神经代码编辑来探索漏洞注入的可行性。通过一个合成数据集和一个真实世界的数据集，我们研究了三种最先进的神经代码编辑器用于漏洞注入的潜力和差距。我们发现，所研究的编辑器在真实世界的数据集上有严重的局限性，其中最佳准确率仅为10.03%，而在合成数据集上为79.40%。虽然基于图的编辑器比基于序列的编辑器更有效（在高达34.93%的真实世界测试样本中成功注入漏洞）（0成功），但由于预处理和深度学习（DL）模型设计不足，它们仍然存在复杂的代码结构，无法进行长时间编辑。我们揭示了神经代码编辑生成真实易受攻击样本的前景，因为它们有助于将基于DL的漏洞检测器的F1分数提高49.51%。我们还提供了对当前编辑器中差距的见解（例如，他们擅长删除但不擅长替换代码），以及解决这些差距的可行建议（例如，设计有效的编辑原语）。,安全和隐私，入侵/异常检测和恶意软件缓解，软件和应用程序安全，软件安全工程，系统安全，漏洞管理，社会和专业主题，计算/技术政策，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析,,,
3XLUEQ2R,2022,https://doi.org/10.1145/3540250.3549088,ESEC/FSE 2022,23 shades of self-admitted technical debt: an empirical study on machine learning software,"In software development, the term “technical debt” (TD) is used to characterize short-term solutions and workarounds implemented in source code which may incur a long-term cost. Technical debt has a variety of forms and can thus affect multiple qualities of software including but not limited to its legibility, performance, and structure. In this paper, we have conducted a comprehensive study on the technical debts in machine learning (ML) based software. TD can appear differently in ML software by infecting the data that ML models are trained on, thus affecting the functional behavior of ML systems. The growing inclusion of ML components in modern software systems have introduced a new set of TDs. Does ML software have similar TDs to traditional software? If not, what are the new types of ML specific TDs? Which ML pipeline stages do these debts appear? Do these debts differ in ML tools and applications and when they get removed? Currently, we do not know the state of the ML TDs in the wild. To address these questions, we mined 68,820 self-admitted technical debts (SATD) from all the revisions of a curated dataset consisting of 2,641 popular ML repositories from GitHub, along with their introduction and removal. By applying an open-coding scheme and following upon prior works, we provide a comprehensive taxonomy of ML SATDs. Our study analyzes ML SATD type organizations, their frequencies within stages of ML software, the differences between ML SATDs in applications and tools, and quantifies the removal of ML SATDs. The findings discovered suggest implications for ML developers and researchers to create maintainable ML systems.","Computing methodologies,Machine learning,Human-centered computing,Visualization,Visualization techniques,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software post-development issues,Software notations and tools,Software configuration management and version control systems",23种自我承认的技术债务：机器学习软件的实证研究,在软件开发中，术语“技术债务”（TD）用于描述在源代码中实现的短期解决方案和变通方法，这些解决方案和方法可能会产生长期成本。技术债务有多种形式，因此可以影响软件的多种质量，包括但不限于其易读性、性能和结构。在本文中，我们对基于机器学习（ML）的软件中的技术债务进行了全面的研究。TD在ML软件中的表现可能会有所不同，因为它会感染ML模型所训练的数据，从而影响ML系统的功能行为。ML组件在现代软件系统中的日益包含引入了一组新的TD。ML软件是否具有与传统软件类似的TD？如果不是，有哪些新类型的ML特定TD？这些债务出现在哪些ML管道阶段？这些债务在ML工具和应用程序中以及何时删除时是否有所不同？目前，我们还不知道ML TDs在野外的状态。为了解决这些问题，我们从一个由GitHub的2641个流行ML存储库组成的精心策划的数据集的所有修订中挖掘了68820个自我承认的技术债务（SATD），以及它们的引入和删除。通过应用开放编码方案并遵循先前的工作，我们提供了ML SATD的全面分类。我们的研究分析了ML SATD类型的组织，它们在ML软件阶段内的频率，ML SATD在应用程序和工具中的差异，并量化了ML SATD。这些发现为ML开发人员和研究人员创建可维护的ML系统提供了启示。,计算方法学，机器学习，以人为中心的计算，可视化，可视化技术，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件后开发问题，软件符号和工具，软件配置管理和版本控制系统,,,
8ANFAA2Z,2022,https://doi.org/10.1145/3540250.3549130,ESEC/FSE 2022,PyTER: effective program repair for Python type errors,"We present PyTER, an automated program repair (APR) technique for Python type errors. Python developers struggle with type error exceptions that are prevalent and difficult to fix. Despite the importance, however, automatically repairing type errors in dynamically typed languages such as Python has received little attention in the APR community and no existing techniques are readily available for practical use. PyTER is the first technique that is carefully designed to fix diverse type errors in real-world Python applications. To this end, we present a novel APR approach that uses dynamic and static analyses to infer correct and incorrect types of program variables, and leverage their difference to effectively identify faulty locations and patch candidates. We evaluated PyTER on 93 type errors collected from open-source projects. The result shows that PyTER is able to fix 48.4% of them with a precision of 77.6%.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software organization and properties,Software functional properties,Formal methods,Automated static analysis,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",PyTER：Python类型错误的有效程序修复,我们介绍了PyTER，一种针对Python类型错误的自动程序修复（APR）技术。Python开发人员很难处理常见且难以修复的类型错误异常。然而，尽管如此，在动态类型语言（如Python）中自动修复类型错误在APR社区中几乎没有受到关注，也没有现成的技术可供实际使用。PyTER是第一种经过精心设计的技术，用于修复现实世界中Python应用程序中的各种类型错误。为此，我们提出了一种新的APR方法，该方法使用动态和静态分析来推断正确和不正确类型的程序变量，并利用它们的差异来有效地识别故障位置和候选补丁。我们对从开源项目中收集的93个类型错误进行了PyTER评估。结果表明，PyTER能够固定48.4%的样本，准确率为77.6%。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件组织和属性，软件功能属性，形式化方法，自动静态分析，计算理论，语义和推理，程序推理，程序验证,,,
UQ9JTNGJ,2022,https://doi.org/10.1145/3540250.3558911,ESEC/FSE 2022,Change-aware mutation testing for evolving systems,"Although the strongest test criteria, traditional mutation testing has shown to not scale with modern incremental development practices. In this work, we describe our proposal of commit-aware mutation testing and introduce the concept of commit-relevant mutants suitable to evaluate the system's behaviour after being affected by regression changes. We show that commit-relevant mutants represent a small but effective set that assesses the delta of behaviours between two consecutive software versions. Commit-aware mutation testing provides the guidance for developers to quantify to which extent they have tested error-prone locations impacted by program changes. In this paper, we portray our efforts to make mutation criteria change-aware as we study characteristics of commit-relevant mutants striving to bring mutation testing closer to being worthwhile for evolving systems.","Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software post-development issues,Software evolution,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools",进化系统的变化感知突变测试,尽管传统的突变测试是最强的测试标准，但它已被证明无法与现代增量开发实践相适应。在这项工作中，我们描述了我们提出的承诺感知突变测试，并引入了承诺相关突变体的概念，适用于评估系统在受到回归变化影响后的行为。我们表明，提交相关突变体代表了一个小但有效的集合，用于评估两个连续软件版本之间的行为增量。Commit-aware突变测试为开发人员提供了指导，以量化他们在多大程度上测试了受程序更改影响的易出错位置。在这篇论文中，我们描述了我们在研究承诺相关突变体的特征时，努力使突变标准发生变化，努力使变异测试更接近进化系统的价值。,社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件开发后问题，软件演化，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具,,,
QR55DEE9,2022,https://doi.org/10.1145/3540250.3558931,ESEC/FSE 2022,MultIPAs: applying program transformations to introductory programming assignments for data augmentation,"There has been a growing interest, over the last few years, in the topic of automated program repair applied to fixing introductory programming assignments (IPAs).  
However, the datasets of IPAs publicly available tend to be small and with no valuable annotations about the defects of each program. Small datasets are not very useful for program repair tools that rely on machine learning models. Furthermore, a large diversity of correct implementations allows computing a smaller set of repairs to fix a given incorrect program rather than always using the same set of correct implementations for a given IPA. For these reasons, there has been an increasing demand for the task of augmenting IPAs benchmarks.  
","Applied computing,Education,Computer-assisted instruction,Computing methodologies,Machine learning,Social and professional topics,Professional topics,Computing education,Computing education programs,Computer science education,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation,Semantics and reasoning,Program reasoning,Program analysis,Program semantics",MultIPA：将程序转换应用于数据扩充的入门编程任务,在过去的几年里，人们对应用于修复介绍性编程任务（IPA）的自动程序修复这一主题越来越感兴趣。,应用计算，教育，计算机辅助教学，计算方法论，机器学习，社会和专业主题，专业主题，计算教育，计算教育程序，计算机科学教育，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论，语义和推理，程序推理，程序分析，程序语义学,,,
2BNPIPFV,2022,https://doi.org/10.1145/3540250.3549116,ESEC/FSE 2022,Static executes-before analysis for event driven programs,"The executes-before relation between tasks is fundamental in the analysis of Event Driven Programs with several downstream applications like race detection and identifying redundant synchronizations. We present a sound, efficient, and effective static analysis technique to compute executes-before pairs of tasks for a general class of event driven programs. The analysis is based on a small but comprehensive set of rules evaluated on a novel structure called the task post graph of a program. We show how to use the executes-before information to identify disjoint-blocks in event driven programs and further use them to improve the precision of data race detection for these programs. We have implemented our analysis in the Flowdroid framework in a tool called AndRacer and evaluated it on several Android apps, bringing out the scalability, recall, and improved precision of the analyses","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Formal methods,Automated static analysis,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",在分析事件驱动程序之前执行静态,任务之间的先执行后执行关系是分析具有几个下游应用程序（如竞争检测和识别冗余同步）的事件驱动程序的基础。我们提出了一种合理、高效和有效的静态分析技术，用于在一类事件驱动程序的任务对之前计算执行情况。该分析基于一组小而全面的规则，这些规则是在一种称为程序任务后图的新结构上评估的。我们展示了如何使用执行前信息来识别事件驱动程序中的不相交块，并进一步使用它们来提高这些程序的数据竞赛检测精度。我们已经在名为AndRacer的工具中的Flowdroid框架中实现了我们的分析，并在几个Android应用程序上对其进行了评估，从而提高了分析的可扩展性、召回率和准确性,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，形式化方法，自动静态分析，计算理论，语义和推理，程序推理，程序验证,,,
8TEBR3PK,2022,https://doi.org/10.1145/3540250.3549109,ESEC/FSE 2022,On-the-fly syntax highlighting using neural networks,"With the presence of online collaborative tools for software developers, source code is shared and consulted frequently, from code viewers to merge requests and code snippets. Typically, code highlighting quality in such scenarios is sacrificed in favor of system responsiveness. In these on-the-fly settings, performing a formal grammatical analysis of the source code is not only expensive, but also intractable for the many times the input is an invalid derivation of the language. Indeed, current popular highlighters heavily rely on a system of regular expressions, typically far from the specification of the language's lexer. Due to their complexity, regular expressions need to be periodically updated as more feedback is collected from the users and their design unwelcome the detection of more complex language formations. This paper delivers a deep learning-based approach suitable for on-the-fly grammatical code highlighting of correct and incorrect language derivations, such as code viewers and snippets. It focuses on alleviating the burden on the developers, who can reuse the language's parsing strategy to produce the desired highlighting specification. Moreover, this approach is compared to nowadays online syntax highlighting tools and formal methods in terms of accuracy and execution time, across different levels of grammatical coverage, for three mainstream programming languages. The results obtained show how the proposed approach can consistently achieve near-perfect accuracy in its predictions, thereby outperforming regular expression-based strategies.","Computing methodologies,Artificial intelligence,Machine learning,Learning paradigms,Machine learning approaches,Neural networks,Human-centered computing,Software and its engineering,Software creation and management,Software notations and tools,Software organization and properties,Software functional properties,Formal methods,Automated static analysis",使用神经网络的动态语法高亮显示,随着软件开发人员在线协作工具的出现，从代码查看器到合并请求和代码片段，源代码都会被频繁共享和查阅。通常，在这样的场景中，为了系统响应性，会牺牲代码突出显示的质量。在这些动态设置中，对源代码进行形式语法分析不仅成本高昂，而且在许多情况下，输入是语言的无效派生，因此也很难处理。事实上，当前流行的highlighter在很大程度上依赖于正则表达式系统，通常与语言的lexer规范相去甚远。由于正则表达式的复杂性，需要定期更新正则表达式，因为从用户那里收集了更多的反馈，并且它们的设计不受检测到更复杂的语言形式的影响。本文提供了一种基于深度学习的方法，适用于动态语法代码，突出显示正确和不正确的语言派生，如代码查看器和代码片段。它专注于减轻开发人员的负担，他们可以重用语言的解析策略来生成所需的突出显示规范。此外，在三种主流编程语言的不同语法覆盖级别上，这种方法在准确性和执行时间方面与当今的在线语法突出显示工具和形式化方法进行了比较。所获得的结果表明，所提出的方法可以始终如一地实现近乎完美的预测精度，从而优于基于正则表达式的策略。,计算方法论，人工智能，机器学习，学习范例，机器学习方法，神经网络，以人为中心的计算，软件及其工程，软件创建和管理，软件符号和工具，软件组织和属性，软件功能属性，形式方法，自动静态分析,,,
H647Y4V9,2022,https://doi.org/10.1145/3540250.3549156,ESEC/FSE 2022,Automated unearthing of dangerous issue reports,"The coordinated vulnerability disclosure (CVD) process is commonly adopted for open source software (OSS) vulnerability management, which suggests to privately report the discovered vulnerabilities and keep relevant information secret until the official disclosure. However, in practice, due to various reasons (e.g., lacking security domain expertise or the sense of security management), many vulnerabilities are first reported via public issue reports (IRs) before its official disclosure. Such IRs are dangerous IRs, since attackers can take advantages of the leaked vulnerability information to launch zero-day attacks. It is crucial to identify such dangerous IRs at an early stage, such that OSS users can start the vulnerability remediation process earlier and OSS maintainers can timely manage the dangerous IRs. In this paper, we propose and evaluate a deep learning based approach, namely MemVul, to automatically identify dangerous IRs at the time they are reported. MemVul augments the neural networks with a memory component, which stores the external vulnerability knowledge from Common Weakness Enumeration (CWE). We rely on publicly accessible CVE-referred IRs (CIRs) to operationalize the concept of dangerous IR. We mine 3,937 CIRs distributed across 1,390 OSS projects hosted on GitHub. Evaluated under a practical scenario of high data imbalance, MemVul achieves the best trade-off between precision and recall among all baselines. In particular, the F1-score of MemVul (i.e., 0.49) improves the best performing baseline by 44%. For IRs that are predicted as CIRs but not reported to CVE, we conduct a user study to investigate their usefulness to OSS stakeholders. We observe that 82% (41 out of 50) of these IRs are security-related and 28 of them are suggested by security experts to be publicly disclosed, indicating MemVul is capable of identifying undisclosed dangerous IRs.","Computing methodologies,Machine learning,Information systems,Information systems applications,Security and privacy,Software and application security,Software security engineering,Social and professional topics,Computing / technology policy,Software and its engineering,Software creation and management",自动挖掘危险问题报告,开放源码软件（OSS）漏洞管理通常采用协调漏洞披露（CVD）流程，建议对发现的漏洞进行私下报告，并对相关信息保密，直到正式披露。然而，在实践中，由于各种原因（例如，缺乏安全领域专业知识或安全管理意识），许多漏洞在正式披露之前首先通过公共问题报告（IRs）进行报告。这种IR是危险的IR，因为攻击者可以利用泄露的漏洞信息发动零日攻击。在早期识别此类危险的IR至关重要，这样OSS用户就可以更早地开始漏洞修复过程，OSS维护人员也可以及时管理这些危险的IR。在本文中，我们提出并评估了一种基于深度学习的方法，即MemVul，以在危险的IR被报告时自动识别它们。MemVul通过内存组件增强了神经网络，该组件存储来自常见弱点枚举（CWE）的外部漏洞知识。我们依靠可公开访问的CVE参考IR（CIR）来操作危险IR的概念。我们挖掘了3937个CIR，分布在GitHub上托管的1390个OSS项目中。在高数据不平衡的实际场景下进行评估，MemVul在所有基线中实现了精度和召回率之间的最佳权衡。特别地，MemVul的F1分数（即0.49）将表现最好的基线提高了44%。对于预测为CIR但未报告给CVE的IR，我们进行了一项用户研究，以调查其对OSS利益相关者的有用性。我们观察到，这些IR中有82%（50个中有41个）与安全相关，其中28个由安全专家建议公开披露，这表明MemVul能够识别未公开的危险IR。,计算方法，机器学习，信息系统，信息系统应用，安全和隐私，软件和应用程序安全，软件安全工程，社会和专业主题，计算/技术政策，软件及其工程，软件创建和管理,,,
TZ6YXLRC,2022,https://doi.org/10.1145/3540250.3549115,ESEC/FSE 2022,CORMS: a GitHub and Gerrit based hybrid code reviewer recommendation approach for modern code review,"Modern Code review (MCR) techniques are widely adopted in both open-source software platforms and organizations to ensure the quality of their software products. However, the selection of reviewers for code review is cumbersome with the increasing size of development teams. The recommendation of inappropriate reviewers for code review can take more time and effort to complete the task effectively. In this paper, we extended the baseline of reviewers' recommendation framework - RevFinder - to handle issues with newly created files, retired reviewers, the external validity of results, and the accuracies of the state-of-the-art RevFinder. Our proposed hybrid approach, CORMS, works on similarity analysis to compute similarities among file paths, projects/sub-projects, author information, and prediction models to recommend reviewers based on the subject of the change. We conducted a detailed analysis on the widely used 20 projects of both Gerrit and GitHub to compare our results with RevFinder. Our results reveal that on average, CORMS, can achieve top-1, top-3, top-5, and top-10 accuracies, and Mean Reciprocal Rank (MRR) of 45.1%, 67.5%, 74.6%, 79.9% and 0.58 for the 20 projects, consequently improves the RevFinder approach by 44.9%, 34.4%, 20.8%, 12.3% and 18.4%, respectively.","Information systems,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Software post-development issues,Software verification and validation,Software notations and tools,Software configuration management and version control systems,Software libraries and repositories",CORMS：一种基于GitHub和Gerrit的混合代码评审推荐方法，用于现代代码评审,现代代码评审（MCR）技术在开源软件平台和组织中都被广泛采用，以确保其软件产品的质量。然而，随着开发团队规模的不断扩大，代码审查的审查人员的选择非常繁琐。推荐不合适的评审人员进行代码评审可能会花费更多的时间和精力来有效地完成任务。在本文中，我们扩展了评审员推荐框架RevFinder的基线，以处理新创建的文件、退休评审员、结果的外部有效性以及最先进的RevFinder准确性等问题。我们提出的混合方法CORMS致力于相似性分析，以计算文件路径、项目/子项目、作者信息和预测模型之间的相似性，从而根据更改主题推荐审阅者。我们对Gerrit和GitHub广泛使用的20个项目进行了详细分析，将我们的结果与RevFinder进行了比较。我们的结果表明，平均而言，CORMS可以实现前1、前3、前5和前10的准确率，20个项目的平均倒数排名（MRR）分别为45.1%、67.5%、74.6%、79.9%和0.58，从而使RevFinder方法分别提高了44.9%、34.4%、20.8%、12.3%和18.4%。,信息系统，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发中的合作，软件开发后问题，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统，软件库和存储库,,,
J5F7Q2RR,2022,https://doi.org/10.1145/3540250.3549097,ESEC/FSE 2022,Automatically deriving JavaScript static analyzers from specifications using Meta-level static analysis,"JavaScript is one of the most dominant programming languages. However, despite its popularity, it is a challenging task to correctly understand the behaviors of JavaScript programs because of their highly dynamic nature. Researchers have developed various static analyzers that strive to conform to ECMA-262, the standard specification of JavaScript. Unfortunately, all the existing JavaScript static analyzers require manual updates for new language features. This problem has become more critical since 2015 because the JavaScript language itself rapidly evolves with a yearly release cadence and open development process.  
","Software and its engineering,Software creation and management,Software verification and validation,Software notations and tools,Compilers,Software maintenance tools,Software organization and properties,Software functional properties,Formal methods,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",使用元级静态分析从规范中自动派生JavaScript静态分析器,JavaScript是最主要的编程语言之一。然而，尽管JavaScript程序很受欢迎，但由于其高度动态性，正确理解其行为是一项具有挑战性的任务。研究人员开发了各种静态分析器，努力符合ECMA-262，JavaScript的标准规范。不幸的是，所有现有的JavaScript静态分析器都需要手动更新新的语言功能。自2015年以来，这个问题变得更加关键，因为JavaScript语言本身以每年发布一次的节奏和开放的开发过程快速发展。,软件及其工程，软件创建和管理，软件验证和确认，软件符号和工具，编译器，软件维护工具，软件组织和属性，软件功能属性，形式化方法，计算理论，语义和推理，程序推理，程序验证,,,
4NGEUYY6,2022,https://doi.org/10.1145/3540250.3549147,ESEC/FSE 2022,NeuDep: neural binary memory dependence analysis,"Determining whether multiple instructions can access the same memory location is a critical task in binary analysis. It is challenging as statically computing precise alias information is undecidable in theory. The problem aggravates at the binary level due to the presence of compiler optimizations and the absence of symbols and types. Existing approaches either produce significant spurious dependencies due to conservative analysis or scale poorly to complex binaries.  
","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Security and privacy,Software and application security,Software reverse engineering,Software and its engineering,Software notations and tools,Theory of computation,Theory and algorithms for application domains,Machine learning theory",NeuDep：神经二元记忆依赖性分析,确定多个指令是否可以访问相同的内存位置是二进制分析中的一项关键任务。静态计算精确的别名信息在理论上是不可确定的，这是具有挑战性的。由于存在编译器优化以及缺少符号和类型，该问题在二进制级别上更加严重。现有的方法要么由于保守的分析而产生显著的虚假依赖关系，要么难以扩展到复杂的二进制文件。,计算方法，机器学习，机器学习方法，神经网络，安全和隐私，软件和应用程序安全，软件逆向工程，软件及其工程，软件符号和工具，计算理论，应用领域的理论和算法，机器学习理论,,,
PSWMTRSZ,2022,https://doi.org/10.1145/3540250.3549084,ESEC/FSE 2022,Correlates of programmer efficacy and their link to experience: a combined EEG and eye-tracking study,"Background: Despite similar education and background, programmers can exhibit vast differences in efficacy. While research has identified some potential factors, such as programming experience and domain knowledge, the effect of these factors on programmers' efficacy is not well understood.  
  
Aims: We aim at unraveling the relationship between efficacy (speed and correctness) and measures of programming experience. We further investigate the correlates of programmer efficacy in terms of reading behavior and cognitive load.  
  
Method: For this purpose, we conducted a controlled experiment with 37 participants using electroencephalography (EEG) and eye tracking. We asked participants to comprehend up to 32 Java source-code snippets and observed their eye gaze and neural correlates of cognitive load. We analyzed the correlation of participants' efficacy with popular programming experience measures.  
  
Results: We found that programmers with high efficacy read source code more targeted and with lower cognitive load. Commonly used experience levels do not predict programmer efficacy well, but self-estimation and indicators of learning eagerness are fairly accurate.  
  
Implications: The identified correlates of programmer efficacy can be used for future research and practice (e.g., hiring). Future research should also consider efficacy as a group sampling method, rather than using simple experience measures.","Applied computing,Computing methodologies,Artificial intelligence,Philosophical/theoretical foundations of artificial intelligence,Cognitive science,Human-centered computing,Human computer interaction (HCI),Empirical studies in HCI,HCI design and evaluation methods,Social and professional topics,Professional topics,Software and its engineering",程序员效能的相关性及其与经验的联系：脑电图和眼动追踪的联合研究,背景：尽管受教育程度和背景相似，但程序员在效能方面可能表现出巨大差异。虽然研究已经确定了一些潜在因素，如编程经验和领域知识，但这些因素对程序员效能的影响还没有得到很好的理解。,应用计算，计算方法论，人工智能，人工智能的哲学/理论基础，认知科学，以人为中心的计算，人机交互(HCI)，人机交互的经验研究，人机交互设计和评估方法，社会和专业主题，专业主题，软件及其工程,,,
RPRNFNEU,2022,https://doi.org/10.1145/3540250.3569451,ESEC/FSE 2022,Machine learning and natural language processing for automating software testing (tutorial),"In this tutorial, we see how natural language processing and machine learning can help us address the open challenges of software testing. We overview the open challenges of testing autonomous and self-adaptive software systems, discuss the leading-edge technologies that can address the core issues, and see the latest progresses and future prospective of natural language processing and machine learning to cope with core problems.  
","Computing methodologies,Artificial intelligence,Natural language processing,Machine learning,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation",用于自动化软件测试的机器学习和自然语言处理（教程）,在本教程中，我们将了解自然语言处理和机器学习如何帮助我们应对软件测试的开放挑战。我们概述了测试自主和自适应软件系统的开放挑战，讨论了能够解决核心问题的前沿技术，并了解了自然语言处理和机器学习的最新进展和未来前景，以应对核心问题。,计算方法论，人工智能，自然语言处理，机器学习，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论,,,
YCZG8U6P,2022,https://doi.org/10.1145/3540250.3549150,ESEC/FSE 2022,DeJITLeak: eliminating JIT-induced timing side-channel leaks,"Timing side-channels can be exploited to infer secret information when the execution time of a program is correlated with secrets. Recent work has shown that Just-In-Time (JIT) compilation can introduce new timing side-channels in programs even if they are time-balanced at the source code level. In this paper, we propose a novel approach to eliminate JIT-induced leaks. We first formalise timing side-channel security under JIT compilation via the notion of time-balancing, laying the foundation for reasoning about programs with JIT compilation. We then propose to eliminate JIT-induced leaks via a fine-grained JIT compilation. To this end, we provide an automated approach to generate compilation policies and a novel type system to guarantee its soundness. We develop a tool DeJITLeak for real-world Java and implement the fine-grained JIT compilation in HotSpot JVM. Experimental results show that DeJITLeak can effectively and efficiently eliminate JIT-induced leaks on three widely adopted benchmarks in the setting of side-channel detection.","Security and privacy,Formal methods and theory of security,Formal security models,Logic and verification,Software and application security,Systems security,Operating systems security,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software notations and tools,Software organization and properties,Software functional properties,Theory of computation,Semantics and reasoning,Program reasoning,Program analysis,Program verification",DeJITLeak：消除JIT引起的定时侧通道泄漏,当程序的执行时间与秘密相关时，可以利用定时侧信道来推断秘密信息。最近的工作表明，实时（JIT）编译可以在程序中引入新的定时侧通道，即使它们在源代码级别是时间平衡的。在本文中，我们提出了一种新的方法来消除JIT引起的泄漏。我们首先通过时间平衡的概念正式化了JIT编译下的定时侧通道安全性，为JIT编译程序的推理奠定了基础。然后，我们建议通过细粒度的JIT编译来消除JIT引发的泄漏。为此，我们提供了一种自动生成编译策略的方法，并提供了一个新颖的类型系统来保证其可靠性。我们为真实世界的Java开发了一个工具DeJITLeak，并在HotSpot JVM中实现了细粒度JIT编译。实验结果表明，在侧通道检测设置中，在三个广泛采用的基准上，DeJITLeak可以有效地消除JIT引起的泄漏。,安全和隐私，安全的形式方法和理论，形式安全模型，逻辑和验证，软件和应用安全，系统安全，操作系统安全，软件及其工程，软件创建和管理，软件验证和确认，正式软件验证，软件符号和工具，软件组织和属性，软件功能属性，计算理论，语义和推理，程序推理，程序分析，程序验证,,,
42KHD6CP,2022,https://doi.org/10.1145/3540250.3560877,ESEC/FSE 2022,Paving the way for mature secondary research: the seven types of literature review,"Confusion over different kinds of secondary research, and their divergent purposes, is undermining the effectiveness and usefulness of secondary studies in software engineering. This short paper therefore explains the differences between ad hoc review, case survey, critical review, meta-analysis (aka systematic literature review), meta-synthesis (aka thematic analysis), rapid review and scoping review (aka systematic mapping study). These definitions and associated guidelines help researchers better select and describe their literature reviews, while helping reviewers select more appropriate evaluation criteria.","General and reference,Document types,Surveys and overviews,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Designing software,Software development process management,Software notations and tools",为成熟的二次研究铺平道路：七类文献综述,对不同类型的二次研究及其不同目的的混淆，正在削弱软件工程中二次研究的有效性和实用性。因此，这篇简短的论文解释了特设综述、案例调查、批判性综述、荟萃分析（又名系统文献综述）、元综合（又名主题分析）、快速综述和范围界定综述（又名系统制图研究）之间的差异。这些定义和相关指南有助于研究人员更好地选择和描述他们的文献综述，同时帮助审查人员选择更合适的评估标准。,一般和参考，文件类型，调查和概述，社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，设计软件，软件开发过程管理，软件符号和工具,,,
AEILZBWX,2022,https://doi.org/10.1145/3540250.3549112,ESEC/FSE 2022,PaReco: patched clones and missed patches among the divergent variants of a software family,"Re-using whole repositories as a starting point for new projects is often done by maintaining a variant fork parallel to the original. However, the common artifacts between both are not always kept up to date. As a result, patches are not optimally integrated across the two repositories, which may lead to sub-optimal maintenance between the variant and the original project. A bug existing in both repositories can be patched in one but not the other (we see this as a missed opportunity) or it can be manually patched in both probably by different developers (we see this as effort duplication). In this paper we present a tool (named PaReCo) which relies on clone detection to mine cases of missed opportunity and effort duplication from a pool of patches. We analyzed 364 (source to target) variant pairs with 8,323 patches resulting in a curated dataset containing 1,116 cases of effort duplication and 1,008 cases of missed opportunities. We achieve a precision of 91%, recall of 80%, accuracy of 88%, and F1-score of 85%. Furthermore, we investigated the time interval between patches and found out that, on average, missed patches in the target variants have been introduced in the source variants 52 weeks earlier. Consequently, PaReCo can be used to manage variability in “time” by automatically identifying interesting patches in later project releases to be backported to supported earlier releases.","Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software maintenance,Software and its engineering,Software creation and management,Software post-development issues,Software version control,Software verification and validation,Software defect analysis,Software notations and tools,Software configuration management and version control systems,Software maintenance tools",PaReco：一个软件家族的不同变体中的补丁克隆和遗漏补丁,重新使用整个存储库作为新项目的起点通常是通过维护与原始存储库平行的变体分叉来完成的。然而，两者之间的共同工件并不总是最新的。因此，补丁没有在两个存储库中进行最佳集成，这可能导致变体和原始项目之间的维护不理想。两个存储库中存在的错误可以在其中一个中进行修补，但不能在另一个中修补（我们认为这是一个错失的机会），也可以由不同的开发人员手动在两者中进行修补（我们将此视为重复工作）。在本文中，我们提出了一种工具（名为PaReCo），它依赖于克隆检测来从补丁库中挖掘错过机会和重复工作的情况。我们分析了364个（源到目标）变体对和8323个补丁，得到了一个包含1116例工作重复和1008例错失机会的精心策划的数据集。我们的准确率为91%，召回率为80%，准确率为88%，F1得分为85%。此外，我们调查了补丁之间的时间间隔，发现平均而言，目标变体中的遗漏补丁已在52周前引入源变体中。因此，可以使用PaReCo来管理“时间”的可变性，方法是在稍后的项目版本中自动识别感兴趣的补丁，以便将其后移植到受支持的早期版本。,社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件维护，软件及其工程，软件创建和管理，软件开发后问题，软件版本控制，软件验证和确认，软件缺陷分析，软件符号和工具，软件配置管理和版本控制系统，软件维护工具,,,
TM5QGIKB,2022,https://doi.org/10.1145/3540250.3549166,ESEC/FSE 2022,Multi-phase invariant synthesis,"Loops with multiple phases are challenging to verify because they require disjunctive invariants.  
Invariants could also have the form of implication between a precondition for the phase and a lemma that is valid throughout the phase.  
Such invariant structure is however not widely supported in state-of-the-art verification.  
We present a novel SMT-based approach to synthesize implication invariants for multi-phase loops.  
Our technique computes Model Based Projections to discover the program's phases and leverages data learning to get relationships among loop variables at an arbitrary place in the loop.  
It is effective in the challenging cases of mutually-dependent periodic phases, where many implication invariants need to be discovered simultaneously.  
Our approach has shown promising results in its ability to verify programs with complex phase structures.  
We have implemented and evaluated our algorithm against several state-of-the-art solvers.","Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software organization and properties,Software functional properties,Formal methods,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",多相位不变合成,具有多个阶段的循环很难验证，因为它们需要析取不变量。,软件及其工程，软件创建和管理，软件验证和确认，形式化软件验证，软件组织和属性，软件功能属性，形式化方法，计算理论，语义和推理，程序推理，程序验证,,,
8V6UNA7R,2022,https://doi.org/10.1145/3540250.3549127,ESEC/FSE 2022,Pair programming conversations with agents vs. developers: challenges and opportunities for SE community,"Recent research has shown feasibility of an interactive pair-programming conversational agent, but implementing such an agent poses three challenges: a lack of benchmark datasets, absence of software engineering specific labels, and the need to understand developer conversations. To address these challenges, we conducted a Wizard of Oz study with 14 participants pair programming with a simulated agent and collected 4,443 developer-agent utterances. Based on this dataset, we created 26 software engineering labels using an open coding process to develop a hierarchical classification scheme. To understand labeled developer-agent conversations, we compared the accuracy of three state-of-the-art transformer-based language models, BERT, GPT-2, and XLNet, which performed interchangeably. In order to begin creating a developer-agent dataset, researchers and practitioners need to conduct resource intensive Wizard of Oz studies. Presently, there exists vast amounts of developer-developer conversations on video hosting websites. To investigate the feasibility of using developer-developer conversations, we labeled a publicly available developer-developer dataset (3,436 utterances) with our hierarchical classification scheme and found that a BERT model trained on developer-developer data performed ~10% worse than the BERT trained on developer-agent data, but when using transfer-learning, accuracy improved. Finally, our qualitative analysis revealed that developer-developer conversations are more implicit, neutral, and opinionated than developer-agent conversations. Our results have implications for software engineering researchers and practitioners developing conversational agents.","Computing methodologies,Artificial intelligence,Human-centered computing,Human computer interaction (HCI),Empirical studies in HCI,Software and its engineering,Software creation and management,Collaboration in software development,Software notations and tools",将编程对话与代理和开发人员配对：SE社区面临的挑战和机遇,最近的研究表明，交互式配对编程会话代理是可行的，但实现这样的代理带来了三个挑战：缺乏基准数据集，缺乏软件工程特定的标签，以及需要理解开发人员的对话。为了应对这些挑战，我们进行了一项绿野仙踪研究，14名参与者用模拟代理配对编程，并收集了4443条开发者代理话语。基于该数据集，我们使用开放编码过程创建了26个软件工程标签，以开发分层分类方案。为了理解标记的开发人员-代理对话，我们比较了三种最先进的基于转换器的语言模型BERT、GPT-2和XLNet的准确性，这三种语言模型可以互换执行。为了开始创建开发人员代理数据集，研究人员和从业者需要进行资源密集型的绿野仙踪研究。目前，在视频托管网站上存在大量的开发者-开发者对话。为了研究使用开发者-开发者对话的可行性，我们用分层分类方案标记了一个公开的开发者-开发者数据集（3436个话语），发现在开发者-开发者的数据上训练的BERT模型比在开发者-代理的数据上训练的BERT表现差约10%，但当使用迁移学习时，准确性提高了。最后，我们的定性分析表明，与开发人员-代理的对话相比，开发人员-开发人员的对话更为含蓄、中立和固执己见。我们的研究结果对开发会话代理的软件工程研究人员和从业者具有启示意义。,计算方法学，人工智能，以人为中心的计算，人机交互（HCI），HCI的实证研究，软件及其工程，软件创建和管理，软件开发协作，软件符号和工具,,,
DQK8JF43,2022,https://doi.org/10.1145/3540250.3569449,ESEC/FSE 2022,Program analysis using WALA (tutorial),"Static analysis is widely used in research and practice for multiple purposes such as fault localization, vulnerability detection, code clone identification, code refactoring, optimization, etc. Since implementing static analyzers is a non-trivial task, engineers often rely on existing frameworks to implement their techniques. The IBM T.J. Watson Libraries for Analysis (WALA) is one of such frameworks that allows the analysis of multiple environments, such as Java bytecode (and related languages), JavaScript, Android, Python, etc. In this tutorial, we walk through the process of using WALA for program analysis. First, the tutorial will cover all the required background knowledge that is necessary to understand the technical implementation details of the explained algorithms and techniques. Subsequently, we provide a technical overview of the WALA framework and its support for analysis of multiple programming languages and frameworks code. Then, we will do several live demonstration of using WALA to implement client analyses. We will focus on two common uses of analysis: a form of security analysis, taint analysis, and on using analysis graphs for machine learning of code.","General and reference,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Formal methods,Automated static analysis,Theory of computation,Semantics and reasoning,Program reasoning,Program analysis",使用WALA的程序分析（教程）,静态分析在研究和实践中被广泛用于多种目的，如故障定位、漏洞检测、代码克隆识别、代码重构、优化等。由于实现静态分析器是一项不平凡的任务，工程师通常依赖现有的框架来实现他们的技术。IBM T.J.Watson Libraries for Analysis（WALA）就是这样一个框架，它允许分析多种环境，如Java字节码（及相关语言）、JavaScript、Android、Python等。在本教程中，我们将介绍使用WALA进行程序分析的过程。首先，本教程将涵盖理解所解释的算法和技术的技术实现细节所需的所有背景知识。随后，我们对WALA框架及其对分析多种编程语言和框架代码的支持进行了技术概述。然后，我们将进行几个使用WALA实现客户端分析的现场演示。我们将重点讨论分析的两种常见用途：一种是安全分析形式，污染分析，以及使用分析图进行代码的机器学习。,一般和参考资料，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，形式化方法，自动静态分析，计算理论，语义和推理，程序推理，程序分析,,,
7DWK8ZGK,2022,https://doi.org/10.1145/3540250.3558942,ESEC/FSE 2022,Discovering feature flag interdependencies in Microsoft office,"Feature flags are a popular method to control functionality in released code. They enable rapid development and deployment, but can also quickly accumulate technical debt. Complex interactions between feature flags can go unnoticed, especially if interdependent flags are located far apart in the code, and these unknown dependencies could become a source of serious bugs. Testing all possible combinations of feature flags is infeasible in large systems like Microsoft Office, which has about 12000 active flags. The goal of our research is to aid product teams in improving system reliability by providing an approach to automatically discover feature flag interdependencies. We use probabilistic reasoning to infer causal relationships from feature flag query logs. Our approach is language-agnostic, scales easily to large heterogeneous codebases, and is robust against noise such as code drift or imperfect log data. We evaluated our approach on real-world query logs from Microsoft Office and are able to achieve over 90% precision while recalling non-trivial indirect feature flag relationships across different source files. We also investigated re-occurring patterns of relationships and describe applications for targeted testing, determining deployment velocity, error mitigation, and diagnostics.","Information systems,Mathematics of computing,Probability and statistics,Probabilistic inference problems,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems",在Microsoft office中发现功能标志的相互依赖性,功能标志是控制已发布代码中功能的常用方法。它们可以实现快速开发和部署，但也可以快速积累技术债务。功能标志之间的复杂交互可能会被忽视，尤其是如果相互依赖的标志在代码中相距甚远，并且这些未知的依赖关系可能会成为严重错误的来源。在像Microsoft Office这样的大型系统中，测试所有可能的功能标志组合是不可行的，因为它有大约12000个活动标志。我们研究的目标是通过提供一种自动发现功能标志相互依赖性的方法来帮助产品团队提高系统可靠性。我们使用概率推理从特征标志查询日志中推断因果关系。我们的方法与语言无关，易于扩展到大型异构代码库，并且对代码漂移或不完美的日志数据等噪声具有鲁棒性。我们在Microsoft Office的真实世界查询日志上评估了我们的方法，能够实现90%以上的精度，同时回忆不同源文件之间的非琐碎间接功能标志关系。我们还研究了重复出现的关系模式，并描述了目标测试、确定部署速度、错误缓解和诊断的应用程序。,信息系统，计算数学，概率和统计，概率推理问题，社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件后开发问题，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统,,,
HJCL4EM4,2022,https://doi.org/10.1145/3540250.3549104,ESEC/FSE 2022,Using nudges to accelerate code reviews at scale,"We describe a large-scale study to reduce the amount of time code review takes. Each quarter at Meta we survey developers. Combining sentiment data from a developer experience survey and telemetry data from our diff review tool, we address, “When does a diff review feel too slow?” From the sentiment data alone, we learn that 84.7% of developers are satisfied with the time their diffs spend in review. By enriching the survey results with telemetry for each respondent, we determined that sentiment is closely associated with the 75th percentile time in review for that respondent’s diffs, ie those that take more than 24 hours. ","Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Software verification and validation,Software notations and tools,Software configuration management and version control systems",使用推送加速大规模的代码审查,我们描述了一项大规模的研究，以减少代码审查所花费的时间。Meta每季度都会对开发者进行调查。结合来自开发人员体验调查的情绪数据和来自diff审查工具的遥测数据，我们解决了“什么时候diff审查感觉太慢？”仅从情绪数据中，我们就了解到84.7%的开发人员对diff在审查中花费的时间感到满意。通过对每个受访者的遥测数据丰富调查结果，我们确定情绪与该受访者差异的第75百分位审查时间密切相关，即需要超过24小时的差异。,社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发中的合作，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统,,,
ICQ8EXHM,2022,https://doi.org/10.1145/3540250.3560883,ESEC/FSE 2022,Exploring the under-explored terrain of non-open source data for software engineering through the lens of federated learning,"The availability of open source projects on platforms like GitHub has led to the wide use of the artifacts from these projects in software engineering research. These publicly available artifacts have been used to train artificial intelligence models used in various empirical studies and the development of tools. However, these advancements have missed out on the artifacts from non-open source projects due to the unavailability of the data. A major cause for the unavailability of the data from non-open source repositories is the issue concerning data privacy. In this paper, we propose using federated learning to address the issue of data privacy to enable the use of data from non-open source to train AI models used in software engineering research. We believe that this can potentially enable industries to collaborate with software engineering researchers without concerns about privacy. We present the preliminary evaluation of the use of federated learning to train a classifier to label bug-fix commits from an existing study to demonstrate its feasibility. The federated approach achieved an F1 score of 0.83 compared to a score of 0.84 using the centralized approach. We also present our vision of the potential implications of the use of federated learning in software engineering research.","Computing methodologies,Machine learning,Security and privacy,Social and professional topics,Software and its engineering,Software creation and management,Collaboration in software development,Software notations and tools",通过联合学习的视角探索软件工程中未被充分探索的非开源数据领域,GitHub等平台上开源项目的可用性导致了这些项目的工件在软件工程研究中的广泛使用。这些公开可用的人工制品已被用于训练各种实证研究和工具开发中使用的人工智能模型。然而，由于数据不可用，这些进步错过了非开源项目的工件。非开源存储库中数据不可用的一个主要原因是数据隐私问题。在本文中，我们建议使用联合学习来解决数据隐私问题，从而能够使用非开源数据来训练软件工程研究中使用的人工智能模型。我们相信，这可能使行业能够与软件工程研究人员合作，而不必担心隐私问题。我们对使用联合学习来训练分类器来标记现有研究中的错误修复提交进行了初步评估，以证明其可行性。联合方法的F1得分为0.83，而使用集中式方法的F1分数为0.84。我们还介绍了在软件工程研究中使用联合学习的潜在含义。,计算方法，机器学习，安全和隐私，社会和专业主题，软件及其工程，软件创建和管理，软件开发中的协作，软件符号和工具,,,
ETJ6V4R4,2022,https://doi.org/10.1145/3540250.3558965,ESEC/FSE 2022,Incorporating domain knowledge through task augmentation for front-end JavaScript code generation,"Code generation aims to generate a code snippet automatically from natural language descriptions. Generally, the mainstream code generation methods rely on a large amount of paired training data, including both the natural language description and the code. However, in some domain-specific scenarios, building such a large paired corpus for code generation is difficult because there is no directly available pairing data, and a lot of effort is required to manually write the code descriptions to construct a high-quality training dataset. Due to the limited training data, the generation model cannot be well trained and is likely to be overfitting, making the model's performance unsatisfactory for real-world use. To this end, in this paper, we propose a task augmentation method that incorporates domain knowledge into code generation models through auxiliary tasks and a Subtoken-TranX model by extending the original TranX model to support subtoken-level code generation. To verify our proposed approach, we collect a real-world code generation dataset and conduct experiments on it. Our experimental results demonstrate that the subtoken-level TranX model outperforms the original TranX model and the Transformer model on our dataset, and the exact match accuracy of Subtoken-TranX improves significantly by 12.75% with the help of our task augmentation method. The model performance on several code categories has satisfied the requirements for application in industrial systems. Our proposed approach has been adopted by Alibaba's BizCook platform. To the best of our knowledge, this is the first domain code generation system adopted in industrial development environments.","Computing methodologies,Artificial intelligence,Natural language processing,Machine learning,Software and its engineering,Software creation and management,Software development techniques,Software notations and tools,Compilers,Source code generation",通过任务扩充整合领域知识，用于前端JavaScript代码生成,代码生成旨在根据自然语言描述自动生成代码片段。通常，主流的代码生成方法依赖于大量的成对训练数据，包括自然语言描述和代码。然而，在一些特定领域的场景中，构建这样一个用于代码生成的大型配对语料库是困难的，因为没有直接可用的配对数据，并且需要大量的精力手动编写代码描述来构建高质量的训练数据集。由于训练数据有限，生成模型无法很好地训练，并且很可能过拟合，这使得模型的性能无法满足现实世界的使用。为此，在本文中，我们提出了一种任务扩充方法，该方法通过辅助任务将领域知识纳入代码生成模型，并通过扩展原始TranX模型来支持子标记级别的代码生成，从而提出了一个子标记TranX模型。为了验证我们提出的方法，我们收集了一个真实世界的代码生成数据集并对其进行了实验。我们的实验结果表明，在我们的数据集上，子标记级别的TranX模型优于原始TranX和Transformer模型，并且在我们的任务增强方法的帮助下，子标记TranX的精确匹配精度显著提高了12.75%。模型在几个代码类别上的性能已经满足了在工业系统中应用的要求。我们提出的方法已被阿里巴巴的BizCook平台采用。据我们所知，这是第一个在工业开发环境中采用的领域代码生成系统。,计算方法论，人工智能，自然语言处理，机器学习，软件及其工程，软件创建和管理，软件开发技术，软件符号和工具，编译器，源代码生成,,,
3A3EWM7N,2022,https://doi.org/10.1145/3540250.3558958,ESEC/FSE 2022,AutoTSG: learning and synthesis for incident troubleshooting,"Incident management is a key aspect of operating large-scale cloud services. To aid with faster and efficient resolution of incidents, engineering teams document frequent troubleshooting steps in the form of Troubleshooting Guides (TSGs), to be used by on-call engineers (OCEs). However, TSGs are siloed, unstructured, and often incomplete, requiring developers to manually understand and execute necessary steps. This results in a plethora of issues such as on-call fatigue, reduced productivity, and human errors. In this work, we conduct a large-scale empirical study of over 4K+ TSGs mapped to incidents and find that TSGs are widely used and help significantly reduce mitigation efforts. We then analyze feedback on TSGs provided by 400+ OCEs and propose a taxonomy of issues that highlights significant gaps in TSG quality. To alleviate these gaps, we investigate the automation of TSGs and propose AutoTSG -- a novel framework for automation of TSGs to executable workflows by combining machine learning and program synthesis. Our evaluation of AutoTSG on 50 TSGs shows the effectiveness in both identifying TSG statements (accuracy 0.89) and parsing them for execution (precision 0.94 and recall 0.91). Lastly, we survey ten Microsoft engineers and show the importance of TSG automation and the usefulness of AutoTSG.","General and reference,Cross-computing tools and techniques,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems,Software maintenance tools,Software organization and properties",AutoTSG：事件故障排除的学习和综合,事件管理是运营大规模云服务的一个关键方面。为了帮助更快、高效地解决事故，工程团队以故障排除指南（TSG）的形式记录了频繁的故障排除步骤，供随叫随到的工程师（OCE）使用。然而，TSG是孤立的、非结构化的，并且通常是不完整的，需要开发人员手动理解和执行必要的步骤。这导致了大量的问题，如随叫随到的疲劳、生产力下降和人为错误。在这项工作中，我们对映射到事件的4K以上TSG进行了大规模的实证研究，发现TSG被广泛使用，有助于显著减少缓解工作。然后，我们分析了400多个OCE提供的TSG反馈，并提出了一个问题分类法，突出了TSG质量方面的重大差距。为了缓解这些差距，我们研究了TSG的自动化，并提出了AutoTSG——一种通过结合机器学习和程序合成将TSG自动化到可执行工作流的新框架。我们在50个TSG上对AutoTSG的评估表明，在识别TSG语句（准确度0.89）和解析它们以供执行（准确度0.94和召回率0.91）方面都是有效的。最后，我们调查了10名微软工程师，展示了TSG自动化的重要性和AutoTSG的有用性。,通则和参考，交叉计算工具和技术，社会和专业主题，专业主题，计算和信息系统的管理，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统，软件维护工具，软件组织和属性,,,
ILPAH272,2022,https://doi.org/10.1145/3540250.3549129,ESEC/FSE 2022,Large-scale analysis of non-termination bugs in real-world OSS projects,"Termination is a crucial program property. Non-termination bugs can be subtle to detect and may remain hidden for long before they take effect. Many real-world programs still suffer from vast consequences (e.g., no response) caused by non-termination bugs. As a classic problem, termination proving has been studied for many years. Many termination checking tools and techniques have been developed and demonstrated effectiveness on existing well-established benchmarks. However, the capability of these tools in finding practical non-termination bugs has yet to be tested on real-world projects. To fill in this gap, in this paper, we conducted the first large-scale empirical study of non-termination bugs in real-world OSS projects. Specifically, we first devoted substantial manual efforts in collecting and analyzing 445 non-termination bugs from 3,142 GitHub commits and provided a systematic classifi-cation of the bugs based on their root causes. We constructed a new benchmark set characterizing the real-world bugs with simplified programs, including a non-termination dataset with 56 real and reproducible non-termination bugs and a termination dataset with 58 fixed programs. With the constructed benchmark, we evaluated five state-of-the-art termination analysis tools. The results show that the capabilities of the tested tools to make correct verdicts have obviously dropped compared with the existing benchmarks. Meanwhile, we identified the challenges and limitations that these tools face by analyzing the root causes of their unhandled bugs. Fi-nally, we summarized the challenges and future research directions for detecting non-termination bugs in real-world projects.","General and reference,Cross-computing tools and techniques,Human-centered computing,Visualization,Visualization techniques,Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems,Software organization and properties,Software functional properties,Theory of computation,Semantics and reasoning,Program reasoning,Program analysis",真实世界OSS项目中非终止漏洞的大规模分析,终止是一个重要的程序属性。非终止错误可能很难检测到，并且在生效之前可能会隐藏很长时间。许多现实世界中的程序仍然遭受由非终止错误引起的巨大后果（例如，没有响应）。终止证明作为一个经典问题，研究了很多年。已经开发了许多终止检查工具和技术，并在现有的既定基准上证明了其有效性。然而，这些工具在发现实际的非终止错误方面的能力尚未在现实世界的项目中进行测试。为了填补这一空白，在本文中，我们对现实世界OSS项目中的非终止漏洞进行了首次大规模的实证研究。具体来说，我们首先投入了大量的手动工作，从3142次GitHub提交中收集和分析了445个非终止错误，并根据其根本原因对这些错误进行了系统的分类。我们构建了一个新的基准集，用简化的程序来表征真实世界的错误，包括一个包含56个真实和可复制的非终止错误的非终止数据集和一个包含58个固定程序的终止数据集。利用构建的基准，我们评估了五种最先进的终止分析工具。结果表明，与现有基准相比，测试工具做出正确判断的能力明显下降。同时，我们通过分析未处理错误的根本原因，确定了这些工具面临的挑战和局限性。最后，我们总结了在现实世界项目中检测非终止错误的挑战和未来的研究方向。,通用和参考，交叉计算工具和技术，以人为中心的计算，可视化，可视化技术，软件及其工程，软件创建和管理，软件开发后问题，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统，软件组织和属性，软件功能属性，计算理论，语义与推理，程序推理，程序分析,,,
JJ8LA867,2022,https://doi.org/10.1145/3540250.3549087,ESEC/FSE 2022,How to better utilize code graphs in semantic code search?,"Semantic code search greatly facilitates software reuse, which enables users to find code snippets highly matching user-specified natural language queries. Due to the rich expressive power of code graphs (e.g., control-flow graph and program dependency graph), both of the two mainstream research works (i.e., multi-modal models and pre-trained models) have attempted to incorporate code graphs for code modelling. However, they still have some limitations: First, there is still much room for improvement in terms of search effectiveness. Second, they have not fully considered the unique features of code graphs. ","Computing methodologies,Information systems,Software and its engineering,Software creation and management,Software development techniques,Reusability,Software notations and tools",如何在语义代码搜索中更好地利用代码图？,语义代码搜索极大地促进了软件重用，使用户能够找到与用户指定的自然语言查询高度匹配的代码片段。由于代码图（如控制流图和程序依赖图）具有丰富的表达能力，两种主流研究工作（即多模态模型和预训练模型）都试图将代码图纳入代码建模。然而，它们仍然有一些局限性：首先，在搜索效率方面还有很大的改进空间。其次，他们没有充分考虑代码图的独特性。,计算方法，信息系统，软件及其工程，软件创建和管理，软件开发技术，可重用性，软件符号和工具,,,
3XFNJPKP,2022,https://doi.org/10.1145/3540250.3549154,ESEC/FSE 2022,Quantitative relational modelling with QAlloy,"Alloy is a popular language and tool for formal software design. A key factor to this popularity is its relational logic, an elegant specification language with a minimal syntax and semantics. However, many software problems nowadays involve both structural and quantitative requirements, and Alloy's relational logic is not well suited to reason about the latter. This paper introduces QAlloy, an extension of Alloy with quantitative relations that add integer quantities to associations between domain elements. Having integers internalised in relations, instead of being explicit domain elements like in standard Alloy, allows quantitative requirements to be specified in QAlloy with a similar elegance to structural requirements, with the side-effect of providing basic dimensional analysis support via the type system. The QAlloy Analyzer also implements an SMT-based engine that enables quantities to be unbounded, thus avoiding many problems that may arise with the current bounded integer semantics of Alloy.","Software and its engineering,Software creation and management,Designing software,Requirements analysis,Software development process management,Software notations and tools,System description languages,Specification languages,Software organization and properties,Software functional properties,Formal methods,Software system structures,Software system models,Theory of computation,Semantics and reasoning",用QAlloy进行定量关系建模,Alloy是一种流行的正式软件设计语言和工具。这种流行的一个关键因素是它的关系逻辑，这是一种优雅的规范语言，具有最少的语法和语义。然而，现在许多软件问题同时涉及结构和数量需求，Alloy的关系逻辑不太适合对后者进行推理。本文介绍了QAlloy，它是Alloy的一个扩展，具有向域元素之间的关联添加整数的数量关系。将整数内部化为关系，而不是像标准Alloy中那样的显式域元素，可以在QAlloy中以类似于结构要求的优雅方式指定定量要求，其副作用是通过类型系统提供基本的维度分析支持。QAlloy Analyzer还实现了一个基于SMT的引擎，该引擎可以使数量无界，从而避免了Alloy当前有界整数语义可能出现的许多问题。,软件及其工程，软件创建和管理，设计软件，需求分析，软件开发过程管理，软件符号和工具，系统描述语言，规范语言，软件组织和属性，软件功能属性，形式化方法，软件系统结构，软件系统模型，计算理论，语义和推理,,,
HQAMCWSA,2022,https://doi.org/10.1145/3540250.3558915,ESEC/FSE 2022,SFLKit: a workbench for statistical fault localization,"Statistical fault localization aims at detecting execution features that correlate with failures, such as whether individual lines are part of the execution. We introduce SFLKit, an out-of-the-box workbench for statistical fault localization. The framework provides straightforward access to the fundamental concepts of statistical fault localization. It supports five predicate types, four coverage-inspired spectra, like lines, and 44 similarity coefficients, e.g., TARANTULA or OCHIAI, for statistical program analysis.  
","General and reference,Cross-computing tools and techniques,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software libraries and repositories,Software organization and properties,Software functional properties,Formal methods,Software verification,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",SFLKit：用于统计故障定位的工作台,统计故障定位旨在检测与故障相关的执行特征，例如单个行是否是执行的一部分。我们介绍了SFLKit，一个开箱即用的统计故障定位工作台。该框架提供了对统计故障定位基本概念的直接访问。它支持五种谓词类型、四种受覆盖启发的谱（如线）和44个相似系数，例如TARANTULA或OCHIAI，用于统计程序分析。,一般和参考资料，交叉计算工具和技术，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件库和存储库，软件组织和属性，软件功能属性，形式化方法，软件验证，计算理论，语义和推理，程序推理，程序验证,,,
J72MS4MS,2022,https://doi.org/10.1145/3540250.3558912,ESEC/FSE 2022,Infrastructure as code for dynamic deployments,"Modern DevOps organizations require a high degree of automation to achieve software stability at frequent changes. Further, there is a need for flexible, timely reconfiguration of the infrastructure, e.g., to use pay-per-use infrastructure efficiently based on application load. Infrastructure as Code (IaC) is the DevOps tool to automate infrastructure. However, modern static IaC solutions only support infrastructures that are deployed and do not change afterward. To implement infrastructures that change dynamically over time, static IaC programs have to be (updated and) re-run, e.g., in a CI/CD pipeline, or configure an external orchestrator that implements the dynamic behavior, e.g., an autoscaler or Kubernetes operator. Both do not capture the dynamic behavior in the IaC program and prevent analyzing and testing the infrastructure configuration jointly with its dynamic behavior. ","Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software verification and validation,Software notations and tools,Software configuration management and version control systems,System description languages,Architecture description languages,Orchestration languages,Software organization and properties,Software system structures,Distributed systems organizing principles,Cloud computing",作为动态部署代码的基础结构,现代DevOps组织需要高度自动化，以在频繁更改时实现软件稳定性。此外，需要灵活、及时地重新配置基础设施，例如，根据应用程序负载高效地使用按次付费基础设施。基础设施即代码（IaC）是实现基础设施自动化的DevOps工具。然而，现代静态IaC解决方案只支持已部署的基础设施，之后不会更改。为了实现随时间动态变化的基础设施，静态IaC程序必须（更新和）重新运行，例如在CI/CD管道中，或者配置实现动态行为的外部协调器，例如自动缩放器或Kubernetes运算符。两者都不捕获IaC程序中的动态行为，并阻止将基础结构配置与其动态行为一起进行分析和测试。,社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统，系统描述语言，体系结构描述语言，协调语言，软件组织和属性，软件系统结构，分布式系统组织原理，云计算,,,
DZD68ZLY,2022,https://doi.org/10.1145/3540250.3558929,ESEC/FSE 2022,RegMiner: mining replicable regression dataset from code repositories,"In this work, we introduce a tool, RegMiner, to automate the process of collecting replicable regression bugs from a set of Git repositories. In the code commit history, RegMiner searches for regressions where a test can pass a regression-fixing commit, fail a regressioninducing commit, and pass a previous working commit again. Technically, RegMiner (1) identifies potential regression-fixing commits from the code evolution history, (2) migrates the test and its code dependencies in the commit over the history, and (3) minimizes the compilation overhead during the regression search. Our experients show that RegMiner can successfully collect 1035 regressions over 147 projects in 8 weeks, creating the largest replicable regression dataset within the shortest period, to the best of our knowledge. In addition, our experiments further show that (1) RegMiner can construct the regression dataset with very high precision and acceptable recall, and (2) the constructed regression dataset is of high authenticity and diversity. The source code of RegMiner is available at https://github.com/SongXueZhi/RegMiner, the mined regression dataset is available at https://regminer.github.io/, and the demonstration video is available at https://youtu.be/yzcM9Y4unok.","Information systems,Information systems applications,Data mining,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software evolution,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems",RegMiner：从代码库中挖掘可复制的回归数据集,在这项工作中，我们介绍了一个工具RegMiner，它可以自动从一组Git存储库中收集可复制的回归错误。在代码提交历史记录中，RegMiner搜索回归，其中测试可以通过回归修复提交，在回归过程中提交失败，并再次通过上一次工作提交。从技术上讲，RegMiner（1）从代码进化历史中识别潜在的回归修复提交，（2）在历史上迁移提交中的测试及其代码依赖关系，以及（3）最大限度地减少回归搜索期间的编译开销。我们的经验表明，据我们所知，RegMiner可以在8周内成功收集147个项目的1035个回归数据，在最短的时间内创建最大的可复制回归数据集。此外，我们的实验进一步表明：（1）RegMiner可以构建具有非常高精度和可接受召回率的回归数据集；（2）构建的回归数据具有高度的真实性和多样性。RegMiner的源代码可在https://github.com/SongXueZhi/RegMiner，挖掘的回归数据集可在https://regminer.github.io/，演示视频可在https://youtu.be/yzcM9Y4unok.,信息系统，信息系统应用，数据挖掘，软件及其工程，软件创建和管理，软件开发后问题，维护软件，软件演化，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统,,,
DDKN2TU7,2022,https://doi.org/10.1145/3540250.3549131,ESEC/FSE 2022,Toward interactive bug reporting for (android app) end-users,"Many software bugs are reported manually, particularly bugs that manifest themselves visually in the user interface. End-users typically report these bugs via app reviewing websites, issue trackers, or in-app built-in bug reporting tools, if available. While these systems have various features that facilitate bug reporting (e.g., textual templates or forms), they often provide limited guidance, concrete feedback, or quality verification to end-users, who are often inexperienced at reporting bugs and submit low-quality bug reports that lead to excessive developer effort in bug report management tasks.  
We propose an interactive bug reporting system for end-users (Burt), implemented as a task-oriented chatbot. Unlike existing bug reporting systems, Burt provides guided reporting of essential bug report elements (i.e., the observed behavior, expected behavior, and steps to reproduce the bug), instant quality verification, and graphical suggestions for these elements. We implemented a version of Burt for Android and conducted an empirical evaluation study with end-users, who reported 12 bugs from six Android apps studied in prior work. The reporters found that Burt’s guidance and automated suggestions/clarifications are useful and Burt is easy to use. We found that Burt reports contain higher-quality information than reports collected via a template-based bug reporting system. Improvements to Burt, informed by the reporters, include support for various wordings to describe bug report elements and improved quality verification. Our work marks an important paradigm shift from static to interactive bug reporting for end-users.","Human-centered computing,Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software maintenance tools",面向（安卓应用程序）最终用户的交互式错误报告,许多软件错误都是手动报告的，尤其是在用户界面中直观显示的错误。最终用户通常通过应用程序审查网站、问题跟踪器或应用程序内置的错误报告工具（如果可用）报告这些错误。虽然这些系统具有方便错误报告的各种功能（例如，文本模板或表格），但它们通常为最终用户提供有限的指导、具体反馈或质量验证，最终用户通常在报告错误方面缺乏经验，并提交低质量的错误报告，这导致开发人员在错误报告管理任务中付出了过多的努力。,以人为本的计算，软件及其工程，软件创建和管理，软件开发后问题，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件维护工具,,,
FYFPJ3WP,2022,https://doi.org/10.1145/3540250.3549132,ESEC/FSE 2022,"Code, quality, and process metrics in graduated and retired ASFI projects","Recent work on open source sustainability shows that successful trajectories of projects in the Apache Software Foundation Incubator (ASFI) can be predicted early on, using a set of socio-technical measures. Because OSS projects are socio-technical systems centered around code artifacts, we hypothesize that sustainable projects may exhibit different code and process patterns than unsustainable ones, and that those patterns can grow more apparent as projects evolve over time. Here we studied the code and coding processes of over 200 ASFI projects, and found that ASFI graduated projects have different patterns of code quality and complexity than retired ones. Likewise for the coding processes – e.g., feature commits or bug-fixing commits are correlated with project graduation success. We find that minor contributors and major contributors (who contribute <5%, respectively >=95% commits) associate with graduation outcomes, implying that having also developers who contribute fewer commits are important for a project’s success. This study provides evidence that OSS projects, especially nascent ones, can benefit from introspection and instrumentation using multidimensional modeling of the whole system, including code, processes, and code quality measures, and how they are interconnected over time.","General and reference,Social and professional topics,Professional topics,Management of computing and information systems,Project and people management,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Software notations and tools,Software configuration management and version control systems",已毕业和已退休的ASFI项目中的代码、质量和流程指标,最近关于开源可持续性的工作表明，Apache软件基金会孵化器（ASFI）项目的成功轨迹可以通过一系列社会技术措施尽早预测。因为OSS项目是以代码工件为中心的社会技术系统，我们假设可持续项目可能会表现出与不可持续项目不同的代码和流程模式，并且随着项目的发展，这些模式可能会变得更加明显。在这里，我们研究了200多个ASFI项目的代码和编码过程，发现ASFI毕业的项目与退休的项目在代码质量和复杂性方面有不同的模式。同样，对于编码过程——例如，功能提交或错误修复提交与项目毕业成功相关。我们发现，次要贡献者和主要贡献者（分别贡献<5%和>=95%的承诺）与毕业结果有关，这意味着同样有贡献较少承诺的开发人员对项目的成功很重要。这项研究提供了证据，证明OSS项目，尤其是新生项目，可以从使用整个系统的多维建模的内省和工具中受益，包括代码、流程和代码质量度量，以及它们如何随着时间的推移相互关联。,一般和参考，社会和专业主题，专业主题，计算和信息系统管理，项目和人员管理，软件管理，软件及其工程，软件创建和管理，软件开发中的协作，开放源码模型，软件符号和工具，软件配置管理和版本控制系统,,,
VCPT6JX7,2022,https://doi.org/10.1145/3540250.3549139,ESEC/FSE 2022,Input invariants,"How can we generate valid system inputs? Grammar-based fuzzers are highly efficient in producing syntactically valid system inputs. However, programs will often reject inputs that are semantically invalid. We introduce ISLa, a declarative specification language for context-sensitive properties of structured system inputs based on context-free grammars. With ISLa, it is possible to specify input constraints like ""a variable has to be defined before it is used,"" ""the 'file name' block must be 100 bytes long,"" or ""the number of columns in all CSV rows must be identical.""","Software and its engineering,Software creation and management,Software post-development issues,Documentation,Software reverse engineering,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Compilers,Parsers,Formal language definitions,Semantics,Syntax,General programming languages,Language types,Constraint and logic languages,System description languages,Specification languages,Theory of computation,Formal languages and automata theory,Formalisms,Grammars and context-free languages,Semantics and reasoning,Program reasoning",输入不变量,我们如何生成有效的系统输入？基于语法的模糊器在生成语法有效的系统输入方面是高效的。然而，程序通常会拒绝语义无效的输入。我们介绍了ISLa，这是一种基于上下文无关语法的结构化系统输入上下文敏感属性的声明性规范语言。使用ISLa，可以指定输入约束，如“在使用变量之前必须定义变量”、“文件名”块必须为100字节长”或“所有CSV行中的列数必须相同”,软件及其工程，软件创建和管理，软件后开发问题，文档，软件逆向工程，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，编译器，解析器，形式语言定义，语义，语法，通用编程语言，语言类型，约束和逻辑语言，系统描述语言，规范语言，计算理论，形式语言和自动机理论，形式主义，语法和上下文无关语言，语义和推理，程序推理,,,
ZGQEZ6M9,2022,https://doi.org/10.1145/3540250.3549157,ESEC/FSE 2022,Parasol: efficient parallel synthesis of large model spaces,"Formal analysis is an invaluable tool for software engineers, yet state-of-the-art formal analysis techniques suffer from well-known limitations in terms of scalability. In particular, some software design domains—such as tradeoff analysis and security analysis—require systematic exploration of potentially huge model spaces, which further exacerbates the problem. Despite this present and urgent challenge, few techniques exist to support the systematic exploration of large model spaces. This paper introduces Parasol, an approach and accompanying tool suite, to improve the scalability of large-scale formal model space exploration. Parasol presents a novel parallel model space synthesis approach, backed with unsupervised learning to automatically derive domain knowledge, guiding a balanced partitioning of the model space. This allows Parasol to synthesize the models in each partition in parallel, significantly reducing synthesis time and making large-scale systematic model space exploration for real-world systems more tractable. Our empirical results corroborate that Parasol substantially reduces (by 460% on average) the time required for model space synthesis, compared to state-of-the-art model space synthesis techniques relying on both incremental and parallel constraint solving technologies as well as competing, non-learning-based partitioning methods.","Security and privacy,Formal methods and theory of security,Logic and verification,Software and its engineering,Software creation and management,Designing software,Software implementation planning,Software development process management,Software verification and validation,Formal software verification,Software notations and tools,Software organization and properties,Software functional properties,Formal methods,Theory of computation",Parasol：大模型空间的高效并行合成,形式分析对于软件工程师来说是一种宝贵的工具，但最先进的形式分析技术在可扩展性方面存在众所周知的局限性。特别是，一些软件设计领域——如权衡分析和安全分析——需要系统地探索潜在的巨大模型空间，这进一步加剧了问题。尽管存在这一紧迫的挑战，但很少有技术支持对大型模型空间的系统探索。本文介绍了Parasol，一种提高大规模形式化模型空间探索可扩展性的方法和配套工具套件。Parasol提出了一种新的并行模型空间合成方法，以无监督学习为支持，自动推导领域知识，指导模型空间的平衡划分。这使得Parasol能够并行地合成每个分区中的模型，显著减少了合成时间，并使对真实世界系统的大规模系统模型空间探索更容易处理。我们的经验结果证实，与依赖增量和并行约束求解技术以及竞争性的、基于非学习的划分方法的最先进的模型空间合成技术相比，Parasol大大减少了（平均减少460%）模型空间合成所需的时间。,安全和隐私，安全的形式化方法和理论，逻辑和验证，软件及其工程，软件创建和管理，软件设计，软件实施规划，软件开发过程管理，软件验证和确认，正式软件验证，软件符号和工具，软件组织和属性，软件功能属性，形式化方法，计算理论,,,
JJGI8WUD,2022,https://doi.org/10.1145/3540250.3549163,ESEC/FSE 2022,Program merge conflict resolution via neural transformers,"Collaborative software development is an integral part of the modern software development life cycle, essential to the success of large-scale software projects. When multiple developers make concurrent changes around the same lines of code, a merge conflict may occur. Such conflicts stall pull requests and continuous integration pipelines for hours to several days, seriously hurting developer productivity. To address this problem, we introduce MergeBERT, a novel neural program merge framework based on token-level three-way differencing and a transformer encoder model. By exploiting the restricted nature of merge conflict resolutions, we reformulate the task of generating the resolution sequence as a classification task over a set of primitive merge patterns extracted from real-world merge commit data. Our model achieves 63–68% accuracy for merge resolution synthesis, yielding nearly a 3× performance improvement over existing semi-structured, and 2× improvement over neural program merge tools. Finally, we demonstrate that MergeBERT is sufficiently flexible to work with source code files in Java, JavaScript, TypeScript, and C# programming languages. To measure the practical use of MergeBERT, we conduct a user study to evaluate MergeBERT suggestions with 25 developers from large OSS projects on 122 real-world conflicts they encountered. Results suggest that in practice, MergeBERT resolutions would be accepted at a higher rate than estimated by automatic metrics for precision and accuracy. Additionally, we use participant feedback to identify future avenues for improvement of MergeBERT.","Computing methodologies,Machine learning,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Collaboration in software development,Software development techniques,Automatic programming,Software post-development issues,Software version control,Software notations and tools,General programming languages,Software configuration management and version control systems",通过神经变换器解决程序合并冲突,协同软件开发是现代软件开发生命周期中不可或缺的一部分，对大型软件项目的成功至关重要。当多个开发人员围绕同一行代码进行并发更改时，可能会发生合并冲突。这样的冲突会使pull请求和连续集成管道停滞数小时到数天，严重影响开发人员的生产力。为了解决这个问题，我们引入了MergeBERT，这是一种基于令牌级三元差分和转换器编码器模型的新型神经程序合并框架。通过利用合并冲突解决方案的受限性质，我们将生成解决序列的任务重新表述为对从真实世界的合并提交数据中提取的一组原始合并模式的分类任务。我们的模型在合并分辨率合成方面实现了63–68%的准确率，与现有的半结构化相比，性能提高了近3倍，与神经程序合并工具相比，性能提升了2倍。最后，我们展示了MergeBERT足够灵活，可以使用Java、JavaScript、TypeScript和C#编程语言中的源代码文件。为了衡量MergeBERT的实际使用，我们进行了一项用户研究，与来自大型OSS项目的25名开发人员就他们遇到的122个现实世界冲突评估MergeBERT建议。结果表明，在实践中，MergeBERT分辨率的接受率将高于精度和准确性的自动度量所估计的接受率。此外，我们使用参与者的反馈来确定未来改进MergeBERT的途径。,计算方法论，机器学习，社会和专业主题，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件开发中的协作，软件开发技术，自动编程，软件后开发问题，软件版本控制，软件符号和工具，通用编程语言，软件配置管理和版本控制系统,,,
MQLN52I9,2022,https://doi.org/10.1145/3540250.3549169,ESEC/FSE 2022,RULER: discriminative and iterative adversarial training for deep neural network fairness,"Deep Neural Networks (DNNs) are becoming an integral part of many real-world applications, such as autonomous driving and financial management. While these models enable autonomy, there are however concerns regarding their ethics in decision making. For instance, fairness is an aspect that requires particular attention. A number of fairness testing techniques have been proposed to address this issue, e.g., by generating test cases called individual discriminatory instances for repairing DNNs. Although they have demonstrated great potential, they tend to generate many test cases that are not directly effective in improving fairness and incur substantial computation overhead. We propose a new model repair technique, RULER, by discriminating sensitive and non-sensitive attributes during test case generation for model repair. The generated cases are then used in training to improve DNN fairness. RULER balances the trade-off between accuracy and fairness by decomposing the training procedure into two phases and introducing a novel iterative adversarial training method for fairness. Compared to the state-of-the-art techniques on four datasets, RULER has 7-28 times more effective repair test cases generated, is 10-15 times faster in test generation, and has 26-43% more fairness improvement on ‍average.","Computing methodologies,Machine learning,Learning paradigms,Supervised learning,Machine learning approaches,Neural networks,Information systems,Information systems applications,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation,Theory and algorithms for application domains,Machine learning theory",RULER：用于深度神经网络公平性的判别和迭代对抗性训练,深度神经网络（DNN）正成为许多现实世界应用的组成部分，如自动驾驶和财务管理。尽管这些模式能够实现自主性，但人们对其决策中的道德操守表示担忧。例如，公平是一个需要特别关注的方面。已经提出了许多公平性测试技术来解决这个问题，例如，通过生成被称为个体歧视性实例的测试用例来修复DNN。尽管它们已经证明了巨大的潜力，但它们往往会生成许多测试用例，这些用例在提高公平性方面并不直接有效，并且会产生大量的计算开销。我们提出了一种新的模型修复技术RULER，通过在模型修复的测试用例生成过程中区分敏感属性和非敏感属性。然后在训练中使用生成的案例来提高DNN的公平性。RULER通过将训练过程分解为两个阶段并引入一种新的迭代对抗性训练方法来平衡准确性和公平性之间的权衡。与四个数据集上的最先进技术相比，RULER生成的修复测试用例的有效性提高了7-28倍，测试生成速度提高了10-15倍，公平性提高了26-43% ‍平均的,计算方法，机器学习，学习范例，监督学习，机器学习方法，神经网络，信息系统，信息系统应用，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论，应用领域的理论和算法，机器学习理论,,,
LPFLAU29,2022,https://doi.org/10.1145/3540250.3549100,ESEC/FSE 2022,MOSAT: finding safety violations of autonomous driving systems using multi-objective genetic algorithm,"Autonomous Driving Systems (ADSs) are safety-critical systems, and safety violations of Autonomous Vehicles (AVs) in real traffic will cause huge losses. Therefore, ADSs must be fully tested before deployed on real world roads. Simulation testing is essential to find safety violations of ADS. This paper proposes MOSAT, a multi-objective search-based testing framework, which constructs diverse and adversarial driving environment to expose safety violations of ADSs. Specifically, based on atomic driving maneuvers, MOSAT introduces motif pattern, which describes a sequence of maneuvers that can challenge ADS effectively. MOSAT constructs test scenarios by atomic maneuvers and motif patterns, and uses multi-objective genetic algorithm to search for adversarial and diverse test scenarios. Moreover, in order to test the performance of ADS comprehensively during long-mile driving, we design a novel continuous simulation testing technique, which runs the scenarios generated by multiple parallel search processes alternately in the simulator and can continuously create different perturbations to ADS. We demonstrate MOSAT on an industrial-grade platform, Baidu Apollo, and the experimental results show that MOSAT can effectively generate safety-critical scenarios to crash ADSs and it exposes 11 distinct types of safety violations in a short period of time. It also outperforms state-of-the-art techniques by finding more 6 distinct safety violations on the same road.","Computing methodologies,Artificial intelligence,Control methods,Search methodologies,Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation,Software defect analysis,Software testing and debugging",MOSAT：使用多目标遗传算法发现自动驾驶系统的安全违规行为,自动驾驶系统（ADS）是安全关键系统，自动驾驶汽车在实际交通中的安全违规行为将造成巨大损失。因此，ADSs在部署到现实世界的道路上之前必须经过充分的测试。模拟测试是发现ADS安全违规行为的关键。本文提出了一种基于多目标搜索的测试框架MOSAT，该框架构建了多样化的对抗性驾驶环境，以揭露ADS的安全违规行为。具体来说，在原子驾驶机动的基础上，MOSAT引入了motif模式，描述了一系列可以有效挑战ADS的机动。MOSAT通过原子机动和基序模式构建测试场景，并使用多目标遗传算法搜索对抗性和多样性的测试场景。此外，为了全面测试ADS在长距离驾驶中的性能，我们设计了一种新的连续模拟测试技术，该技术在模拟器中交替运行多个并行搜索过程产生的场景，并可以连续地对ADS产生不同的扰动，实验结果表明，MOSAT可以有效地生成ADS崩溃的安全关键场景，并在短时间内暴露出11种不同类型的安全违规行为。它还优于最先进的技术，在同一条道路上发现了6种不同的安全违规行为。,计算方法，人工智能，控制方法，搜索方法，软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认，软件缺陷分析，软件测试和调试,,,
BQMNSPSR,2022,https://doi.org/10.1145/3540250.3558960,ESEC/FSE 2022,Sometimes you have to treat the symptoms: tackling model drift in an industrial clone-and-own software product line,"Many industrial software product lines use a clone-and-own approach for reuse among software products. As a result, the different products in the product line may drift apart, which implies increased efforts for tasks such as change propagation, domain analysis, and quality assurance. While many solutions have been proposed in the literature, these are often difficult to apply in a real-world setting. We study this drift of products in a concrete large-scale industrial model-driven clone-and-own software product line in the railway domain at our industry partner. For this purpose, we conducted interviews and a survey, and we investigated the models in the model history of this project. We found that increased efforts are mainly caused by large model differences and increased communication efforts. We argue that, in the short-term, treating the symptoms (i.e., handling large model differences) can help to keep efforts for software product-line engineering acceptable — instead of employing sophisticated variability management. To treat the symptoms, we employ a solution based on semantic-lifting to simplify model differences. Using the interviews and the survey, we evaluate the feasibility of variability management approaches and the semantic-lifting approach in the context of this project.","Software and its engineering,Software creation and management,Software development process management,Software development methods,Software development techniques,Reusability,Software product lines,Software notations and tools,Software configuration management and version control systems",有时你必须治疗症状：解决工业克隆和自己的软件产品线中的模型漂移,许多工业软件产品线使用克隆和自己的方法在软件产品之间重用。因此，产品线中的不同产品可能会分散开来，这意味着要加大对变更传播、领域分析和质量保证等任务的努力。虽然文献中已经提出了许多解决方案，但这些解决方案通常很难在现实世界中应用。我们在一个具体的大规模工业模型中研究产品的漂移，该模型驱动我们的行业合作伙伴在铁路领域的克隆和自有软件产品线。为此，我们进行了采访和调查，并调查了该项目模型历史中的模型。我们发现，增加的努力主要是由大的模型差异和增加的沟通努力造成的。我们认为，在短期内，治疗症状（即处理大的模型差异）有助于保持软件产品线工程的努力是可接受的，而不是采用复杂的可变性管理。为了治疗症状，我们采用了一种基于语义提升的解决方案来简化模型差异。通过访谈和调查，我们评估了可变性管理方法和语义提升方法在本项目中的可行性。,软件及其工程，软件创建和管理，软件开发过程管理，软件开发方法，软件开发技术，可重用性，软件产品线，软件符号和工具，软件配置管理和版本控制系统,,,
EWPACGKQ,2022,https://doi.org/10.1145/3540250.3549133,ESEC/FSE 2022,Modus: a Datalog dialect for building container images,"Containers help share and deploy software by packaging it with all its dependencies. Tools, like Docker or Kubernetes, spawn containers from images as specified by a build system’s language, such as Dockerfile. A build system takes many parameters to build an image, including OS and application versions. These build parameters can interact: setting one can restrict another. Dockerfile lacks support for reifying and constraining these interactions, thus forcing developers to write a build script per workflow. As a result, developers have resorted to creating ad-hoc solutions such as templates or domain-specific frameworks that harm performance and complicate maintenance because they are verbose and mix languages.  
","Software and its engineering,Software creation and management,Software notations and tools,Context specific languages,Domain specific languages,General programming languages,Software organization and properties,Contextual software domains,Software infrastructure,Virtual machines,Theory of computation,Logic,Constraint and logic programming",Modus：一种用于构建容器映像的Datalog方言,容器通过将软件与其所有依赖项打包来帮助共享和部署软件。Docker或Kubernetes等工具根据构建系统语言（如Dockerfile）指定的映像生成容器。构建系统需要许多参数来构建映像，包括操作系统和应用程序版本。这些构建参数可以相互作用：设置一个可以限制另一个。Dockerfile缺乏对这些交互的具体化和约束的支持，因此迫使开发人员为每个工作流编写构建脚本。因此，开发人员不得不创建特殊的解决方案，如模板或特定于域的框架，这些解决方案会损害性能并使维护复杂化，因为它们冗长且混合了语言。,软件及其工程，软件创建和管理，软件符号和工具，上下文特定语言，领域特定语言，通用编程语言，软件组织和属性，上下文软件领域，软件基础设施，虚拟机，计算理论，逻辑，约束和逻辑编程,,,
7ACMGYAN,2022,https://doi.org/10.1145/3540250.3558919,ESEC/FSE 2022,FastKLEE: faster symbolic execution via reducing redundant bound checking of type-safe pointers,"Symbolic execution (SE) has been widely adopted for automatic program analysis and software testing. Many SE engines (e.g., KLEE or Angr) need to interpret certain Intermediate Representations (IR) of code during execution, which may be slow and costly. Although a plurality of studies proposed to accelerate SE, few of them consider optimizing the internal interpretation operations. In this paper, we propose FastKLEE, a faster SE engine that aims to speed up execution via reducing redundant bound checking of type-safe pointers during IR code interpretation. Specifically, in FastKLEE, a type inference system is first leveraged to classify pointer types (i.e., safe or unsafe) for the most frequently interpreted read/write instructions. Then, a customized memory operation is designed to perform bound checking for only the unsafe pointers and omit redundant checking on safe pointers. We implement FastKLEE on top of the well-known SE engine KLEE and combined it with the notable type inference system CCured. Evaluation results demonstrate that FastKLEE is able to reduce by up to 9.1% (5.6% on average) as the state-of-the-art approach KLEE in terms of the time to explore the same number (i.e., 10k) of execution paths. FastKLEE is opensourced at https://github.com/haoxintu/FastKLEE. A video demo of FastKLEE is available at https://youtu.be/fjV_a3kt-mo.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Theory of computation,Semantics and reasoning,Program reasoning,Program verification",FastKLEE：通过减少类型安全指针的冗余绑定检查来加快符号执行,符号执行（SE）已被广泛用于自动程序分析和软件测试。许多SE引擎（例如KLEE或Angr）在执行过程中需要解释代码的某些中间表示（IR），这可能是缓慢且昂贵的。尽管许多研究提出了加速SE，但很少有研究考虑优化内部解释操作。在本文中，我们提出了FastKLEE，这是一种更快的SE引擎，旨在通过减少IR代码解释过程中类型安全指针的冗余边界检查来加快执行。具体来说，在FastKLEE中，首先利用类型推理系统对最频繁解释的读/写指令的指针类型（即安全或不安全）进行分类。然后，设计了一个自定义的内存操作，只对不安全的指针执行绑定检查，而忽略对安全指针的冗余检查。我们在著名的SE引擎KLEE的基础上实现了FastKLEE，并将其与著名的类型推理系统CCured相结合。评估结果表明，在探索相同数量（即10k）的执行路径的时间方面，作为最先进的KLEE方法，FastKLEE能够减少高达9.1%（平均5.6%）。FastKLEE开源于https://github.com/haoxintu/FastKLEE.FastKLEE的视频演示可在https://youtu.be/fjV_a3kt-mo.,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，计算理论，语义和推理，程序推理，程序验证,,,
MSHMN3SK,2022,https://doi.org/10.1145/3540250.3558939,ESEC/FSE 2022,Unite: an adapter for transforming analysis tools to web services via OSLC,"This paper describes Unite, a new tool intended as an adapter for transforming non-interactive command-line analysis tools to OSLC-compliant web services. Unite aims to make such tools easier to adopt and more convenient to use by allowing them to be accessible, both locally and remotely, in a unified way and to be easily integrated into various development environments. Open Services for Lifecycle Collaboration (OSLC) is an open standard for tool integration and was chosen for this task due to its robustness, extensibility, support of data from various domains, and its growing popularity. The work is motivated by allowing existing analysis tools to be more widely used with a strong emphasis on widening their industrial usage. We have implemented Unite and used it with multiple existing static as well as dynamic analysis and verification tools, and then successfully deployed it internationally in the industry to automate verification tasks for development teams in Honeywell. We discuss Honeywell's experience with using Unite and with OSLC in general. Moreover, we also provide the Unite Client (UniC) for Eclipse to allow users to easily run various analysis tools directly from the Eclipse IDE.","Applied computing,Enterprise computing,Service-oriented architectures,Software and its engineering,Software creation and management,Software development process management,Software verification and validation,Formal software verification,Software notations and tools,Development frameworks and environments,Software organization and properties,Software functional properties,Formal methods,Theory of computation,Semantics and reasoning,Program reasoning",Unite：通过OSLC将分析工具转换为web服务的适配器,本文介绍了Unite，这是一种新的工具，旨在将非交互式命令行分析工具转换为符合OSLC的web服务。Unite旨在通过允许这些工具以统一的方式在本地和远程访问，并轻松集成到各种开发环境中，使这些工具更易于采用和使用。Open Services for Lifecycle Collaboration（OSLC）是一种用于工具集成的开放标准，之所以选择它来执行这项任务，是因为它的健壮性、可扩展性、对来自各个领域的数据的支持以及它越来越受欢迎。这项工作的动机是允许现有的分析工具得到更广泛的使用，并强调扩大其工业用途。我们已经实施了Unite，并将其与多个现有的静态和动态分析和验证工具一起使用，然后在国际上成功地将其部署在行业中，为霍尼韦尔的开发团队自动化验证任务。我们讨论了霍尼韦尔在使用Unite和OSLC方面的经验。此外，我们还为Eclipse提供了Unite Client（UniC），允许用户直接从Eclipse IDE轻松运行各种分析工具。,应用计算，企业计算，面向服务的体系结构，软件及其工程，软件创建和管理，软件开发过程管理，软件验证和确认，形式软件验证，软件符号和工具，开发框架和环境，软件组织和属性，软件功能属性，形式方法，计算理论，语义和推理，程序推理,,,
CFVIK2K4,2022,https://doi.org/10.1145/3540250.3549148,ESEC/FSE 2022,What motivates software practitioners to contribute to inner source?,"Software development organizations have adopted open source development practices to support or augment their software development processes, a phenomenon referred to as inner source. Given the rapid adoption of inner source, we wonder what motivates software practitioners to contribute to inner source projects. We followed a mixed-methods approach--a qualitative phase of interviews with 20 interviewees, followed by a quantitative phase of an exploratory survey with 124 respondents from 13 countries across four continents. Our study uncovers practitioners' motivation to contribute to inner source projects, as well as how the motivation differs from what motivates practitioners to participate in open source projects. We also investigate how software practitioners' motivation impacts their contribution level and continuance intention in inner source projects. Based on our findings, we outline directions for future research and provide recommendations for organizations and software practitioners.","Human-centered computing,Collaborative and social computing,Empirical studies in collaborative and social computing,Social and professional topics,Professional topics,Management of computing and information systems,Project and people management,Software and its engineering,Software creation and management,Collaboration in software development",是什么激励软件从业者为内部资源做出贡献？,软件开发组织已经采用开源开发实践来支持或增强他们的软件开发过程，这种现象被称为内部源代码。考虑到内部源代码的快速采用，我们想知道是什么激励软件从业者为内部源代码项目做出贡献。我们采用了混合方法——对20名受访者进行定性采访，然后对来自四大洲13个国家的124名受访者进行定量探索性调查。我们的研究揭示了从业者为内部源代码项目做出贡献的动机，以及这种动机与激励从业者参与开源项目的动机有何不同。我们还研究了软件从业者的动机如何影响他们在内部源项目中的贡献水平和持续意图。基于我们的发现，我们概述了未来研究的方向，并为组织和软件从业者提供了建议。,以人为中心的计算，协作和社会计算，协作和社会计算的经验研究，社会和专业主题，专业主题，计算和信息系统管理，项目和人员管理，软件及其工程，软件创建和管理，软件开发中的协作,,,
HQAGLI4Y,2022,https://doi.org/10.1145/3540250.3549153,ESEC/FSE 2022,You see what I want you to see: poisoning vulnerabilities in neural code search,"Searching and reusing code snippets from open-source software repositories based on natural-language queries can greatly improve programming productivity.Recently, deep-learning-based approaches have become increasingly popular for code search. Despite substantial progress in training accurate models of code search, the robustness of these models has received little attention so far.  
","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Information systems,Security and privacy,Software and application security,Software security engineering,Theory of computation,Theory and algorithms for application domains",你看到了我想让你看到的：毒害神经代码搜索中的漏洞,基于自然语言查询搜索和重用开源软件存储库中的代码片段可以极大地提高编程效率。最近，基于深度学习的方法在代码搜索中越来越流行。尽管在训练精确的代码搜索模型方面取得了实质性进展，但到目前为止，这些模型的稳健性几乎没有受到关注。,计算方法，机器学习，机器学习方法，神经网络，信息系统，安全和隐私，软件和应用安全，软件安全工程，计算理论，应用领域的理论和算法,,,
PKFSE73F,2022,https://doi.org/10.1145/3540250.3549170,ESEC/FSE 2022,Detecting non-crashing functional bugs in Android apps via deep-state differential analysis,"Non-crashing functional bugs of Android apps can seriously affect user experience.  
Often buried in rare program paths, such bugs are difficult to detect but lead to severe consequences.  
Unfortunately, very few automatic functional bug oracles for Android apps exist, and they are all specific to limited types of bugs.  
In this paper, we introduce a novel technique named deep-state differential analysis, which brings the classical ""bugs as  
deviant behaviors"" oracle to Android apps as a generic automatic test oracle.  
Our oracle utilizes the observations on the execution of automatically generated test inputs that  
(1) there can be a large number of traces reaching internal app states with similar GUI layouts, and only a small portion of them would reach an erroneous app state, and  
(2) when performing the same sequence of actions on similar GUI layouts, the outcomes will be limited.  
Therefore, for each set of test inputs terminating at similar GUI layouts, we manifest comparable app behaviors by appending the same events to these inputs, cluster the manifested behaviors, and identify minorities as possible anomalies.  
We also calibrate the distribution of these test inputs by a novel input calibration procedure, to ensure the distribution of these test inputs is balanced with rare bug occurrences.  
","Human-centered computing,Ubiquitous and mobile computing,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Formal methods,Theory of computation,Semantics and reasoning,Program reasoning",通过深度状态差分分析检测安卓应用程序中的非崩溃功能错误,安卓应用程序的非崩溃功能错误会严重影响用户体验。,以人为中心的计算，普适和移动计算，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，形式化方法，计算理论，语义和推理，程序推理,,,
N2X8NNIY,2022,https://doi.org/10.1145/3540250.3549136,ESEC/FSE 2022,AgileCtrl: a self-adaptive framework for configuration tuning,"Software systems increasingly expose performance-sensitive configuration parameters, or PerfConfs, to users. Unfortunately, the right settings of these PerfConfs are difficult to decide and often change at run time. To address this problem, prior research has proposed self-adaptive frameworks that automatically monitor the software’s behavior and dynamically tune configurations to provide the desired performance despite dynamic changes. However, these frameworks often require configuration themselves; sometimes explicitly in the form of additional parameters, sometimes implicitly in the form of training. ","Computer systems organization,Architectures,Distributed architectures,Cloud computing,General and reference,Cross-computing tools and techniques,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software notations and tools,Software configuration management and version control systems,Software organization and properties,Extra-functional properties,Software performance,Software reliability",AgileCtrl:一个用于配置调整的自适应框架,软件系统越来越多地向用户公开性能敏感的配置参数或PerfConf。不幸的是，这些PerfConf的正确设置很难决定，并且经常在运行时更改。为了解决这个问题，先前的研究提出了自适应框架，该框架可以自动监控软件的行为，并动态调整配置，以在动态变化的情况下提供所需的性能。然而，这些框架通常需要自己进行配置；有时以附加参数的形式显式地，有时以训练的形式隐式地。,计算机系统组织，体系结构，分布式体系结构，云计算，一般和参考，交叉计算工具和技术，社会和专业主题，专业主题，计算和信息系统的管理，软件及其工程，软件创建和管理，软件符号和工具，软件配置管理和版本控制系统，软件组织和属性，额外功能属性，软件性能，软件可靠性,,,
RW6R7KMD,2022,https://doi.org/10.1145/3540250.3549161,ESEC/FSE 2022,SymMC: approximate model enumeration and counting using symmetry information for Alloy specifications,"Specifying and analyzing critical properties of software systems plays an important role in the development of reliable systems. Alloy is a mature tool-set that provides a first-order relational logic for writing specifications, and a fully automatic powerful backend for analyzing the specifications. It has been widely applied in areas including verification, security, and synthesis.  
","Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software organization and properties,Software functional properties,Formal methods,Software verification,Theory of computation,Logic,Automated reasoning,Verification by model checking",SymMC：使用合金规格的对称信息进行近似模型枚举和计数,指定和分析软件系统的关键特性在开发可靠系统中起着重要作用。Alloy是一个成熟的工具集，它为编写规范提供了一阶关系逻辑，并为分析规范提供了全自动的强大后端。它已被广泛应用于核查、安全和综合等领域。,软件及其工程，软件创建和管理，软件验证和确认，形式化软件验证，软件组织和属性，软件功能属性，形式化方法，软件验证，计算理论，逻辑，自动推理，模型检查验证,,,
WC6BR7NA,2022,https://doi.org/10.1145/3540250.3549106,ESEC/FSE 2022,Quantifying community evolution in developer social networks,"Understanding the evolution of communities in developer social networks (DSNs) around open source software (OSS) projects can provide valuable insights about the socio-technical process of OSS development. Existing studies show the evolutionary behaviors of social communities can effectively be described using patterns including split, shrink, merge, expand, emerge, and extinct. However, existing pattern-based approaches are limited in supporting quantitative analysis, and are potentially problematic for using the patterns in a mutually exclusive manner when describing community evolution. In this work, we propose that different patterns can occur simultaneously between every pair of communities during the evolution, just in different degrees. Four entropy-based indices are devised to measure the degree of community split, shrink, merge, and expand, respectively, which can provide a comprehensive and quantitative measure of community evolution in DSNs. The indices have properties desirable to quantify community evolution including monotonicity, and bounded maximum and minimum values that correspond to meaningful cases. They can also be combined to describe more patterns such as community emerge and extinct. We conduct studies with real-world OSS projects to evaluate the validity of the proposed indices. The results suggest the proposed indices can effectively capture community evolution, and are consistent with existing approaches in detecting evolution patterns in DSNs with an accuracy of 94.1%. The results also show that the indices are useful in predicting OSS team productivity with an accuracy of 0.718. In summary, the proposed approach is among the first to quantify the degree of community evolution with respect to different patterns, which is promising in supporting future research and applications about DSNs and OSS development.","General and reference,Cross-computing tools and techniques,Metrics,Human-centered computing,Collaborative and social computing,Collaborative and social computing theory, concepts and paradigms,Information systems,Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Programming teams",量化开发者社交网络中的社区进化,了解开发者社交网络（DSN）中社区围绕开源软件（OSS）项目的演变，可以为OSS开发的社会技术过程提供有价值的见解。现有研究表明，社会群体的进化行为可以有效地用分裂、缩小、合并、扩大、出现和灭绝等模式来描述。然而，现有的基于模式的方法在支持定量分析方面受到限制，并且在描述群落进化时以互斥的方式使用模式可能会产生问题。在这项工作中，我们提出，在进化过程中，每对群落之间可以同时发生不同的模式，只是程度不同。设计了四个基于熵的指数来分别测量群落分裂、收缩、合并和扩展的程度，这可以为DSN中的群落进化提供全面和定量的测量。这些指数具有量化群落进化所需的特性，包括单调性，以及对应于有意义情况的有界最大值和最小值。它们也可以组合起来描述更多的模式，如群落的出现和灭绝。我们对真实世界的OSS项目进行了研究，以评估所提出的指数的有效性。结果表明，所提出的指数能够有效地捕捉社区进化，与现有的检测DSN进化模式的方法一致，准确率为94.1%。结果还表明，该指数在预测OSS团队生产力方面是有用的，准确度为0.718。总之，所提出的方法是首批量化不同模式的社区进化程度的方法之一，这有助于支持未来关于DSN和OSS开发的研究和应用。,一般和参考，交叉计算工具和技术，物联网，以人为中心的计算，协作和社会计算，协作和社会计算理论，概念和范式，信息系统，社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发协作，开源模型，编程团队,,,
RTSKGBAB,2022,https://doi.org/10.1145/3540250.3558962,ESEC/FSE 2022,Industry practice of configuration auto-tuning for cloud applications and services,"Auto-tuning attracts increasing attention in industry practice to optimize the performance of a system with many configurable parameters. It is particularly useful for cloud applications and services since they have complex system hierarchies and intricate knob correlations. However, existing tools and algorithms rarely consider practical problems such as workload pressure control, the support for distributed deployment, and expensive time costs, etc., which are utterly important for enterprise cloud applications and services. In this work, we significantly extend an open source tuning tool – KeenTune to optimize several typical enterprise cloud applications and services. Our practice is in collaboration with enterprise users and tuning tool developers to address the aforementioned problems. Specifically, we highlight five key challenges from our experiences and provide a set of solutions accordingly. Through applying the improved tuning tool to different application scenarios, we achieve 2%-14% improvements for the performance of MySQL, OceanBase, nginx, ingress-nginx, and 5%-70% improvements for the performance of ACK cloud container service.","Computer systems organization,Architectures,Distributed architectures,General and reference,Cross-computing tools and techniques,Performance,Social and professional topics,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software notations and tools,Software maintenance tools,Software organization and properties,Extra-functional properties,Software performance,Software system structures,Distributed systems organizing principles",云应用程序和服务配置自动调整的行业实践,自动调谐在工业实践中越来越受到关注，以优化具有许多可配置参数的系统的性能。它对云应用程序和服务特别有用，因为它们具有复杂的系统层次结构和复杂的旋钮相关性。然而，现有的工具和算法很少考虑实际问题，如工作负载压力控制、对分布式部署的支持以及昂贵的时间成本等，这些问题对企业云应用和服务至关重要。在这项工作中，我们显著扩展了开源调优工具KeenTune，以优化几个典型的企业云应用程序和服务。我们的实践是与企业用户和调优工具开发人员合作解决上述问题。具体而言，我们强调了我们经验中的五个关键挑战，并提供了一套相应的解决方案。通过将改进后的调优工具应用于不同的应用场景，我们实现了MySQL、OceanBase、nginx、ingress nginx性能提升2%-14%，ACK云容器服务性能提升5%-70%。,计算机系统组织，体系结构，分布式体系结构，一般和参考，交叉计算工具和技术，性能，社会和专业主题，专业主题，计算和信息系统的管理，软件及其工程，软件创建和管理，软件符号和工具，软件维护工具，软件组织和属性，额外功能属性，软件性能，软件系统结构，分布式系统组织原则,,,
KCY38DSZ,2022,https://doi.org/10.1145/3540250.3558966,ESEC/FSE 2022,"Demystifying ""removed reviews"" in iOS app store","The app markets enable users to submit feedback for downloaded apps in the form of star ratings and text reviews, which are meant to be helpful and trustworthy for decision making to both developers and other users. App markets have released strict guidelines/policies for user review submissions. However, there has been growing evidence showing the untrustworthy and poor-quality of app reviews, making the app store review environment a shambles. Therefore, review removal is a common practice, and market maintainers have to remove undesired reviews from the market periodically in a reactive manner. Although some reports and news outlets have mentioned removed reviews, our research community still lacks the comprehensive understanding of the landscape of this kind of reviews. To fill the void, in this paper, we present a large-scale and longitudinal study of removed reviews in iOS App Store. We first collaborate with our industry partner to collect over 30 million removed reviews for 33,665 popular apps over the course of a full year in 2020. This comprehensive dataset enables us to characterize the overall landscape of removed reviews. We next investigate the practical reasons leading to the removal of policy-violating reviews, and summarize several interesting reasons, including fake reviews, offensive reviews, etc. More importantly, most of these mis-behaviors can be reflected on reviews’ basic information including the posters, narrative content, and posting time. It motivates us to design an automated approach to flag the policy-violation reviews, and our experiment result on the labelled benchmark can achieve a good performance (F1=97%). We further make an attempt to apply our approach to the large-scale industry setting, and the result suggests the promising industry usage scenario of our approach. Our approach can act as a gatekeeper to pinpoint policy-violation reviews beforehand, which will be quite effective in improving the maintenance process of app reviews in the industrial setting.","General and reference,Cross-computing tools and techniques,Human-centered computing,Ubiquitous and mobile computing,Information systems,Information systems applications,Social and professional topics,Professional topics,Software and its engineering,Software creation and management,Software notations and tools,Software configuration management and version control systems",揭开iOS应用商店中“已删除评论”的神秘面纱,应用程序市场允许用户以星级评定和文本评论的形式提交下载应用程序的反馈，这对开发者和其他用户的决策都是有益和值得信赖的。应用市场发布了严格的用户评论提交指南/政策。然而，越来越多的证据表明，应用评论的不可信和质量差，使应用商店的评论环境变得一团糟。因此，删除评论是一种常见的做法，市场维护者必须以被动的方式定期从市场上删除不想要的评论。尽管一些报道和新闻媒体提到了删除的评论，但我们的研究界仍然缺乏对这类评论的全面了解。为了填补这一空白，在本文中，我们对iOS应用商店中的删除评论进行了大规模的纵向研究。我们首先与行业合作伙伴合作，在2020年全年内为33665款热门应用收集了超过3000万条被删除的评论。这个全面的数据集使我们能够描述被删除评论的整体情况。接下来，我们调查了导致政策违规评论被删除的实际原因，并总结了几个有趣的原因，包括虚假评论、攻击性评论等。更重要的是，这些错误行为大多可以反映在评论的基本信息上，包括海报、叙事内容和发布时间。它激励我们设计一种自动方法来标记政策违反审查，并且我们在标记的基准上的实验结果可以获得良好的性能（F1=97%）。我们进一步尝试将我们的方法应用于大规模的行业环境，结果表明我们的方法具有很好的行业应用前景。我们的方法可以充当看门人，提前查明政策违规审查，这将非常有效地改善行业环境中应用程序审查的维护过程。,通则和参考，交叉计算工具和技术，以人为中心的计算，无处不在的和移动计算，信息系统，信息系统应用，社会和专业主题，专业主题，软件及其工程，软件创建和管理，软件符号和工具，软件配置管理和版本控制系统,,,
F5L7GGPP,2022,https://doi.org/10.1145/3540250.3549113,ESEC/FSE 2022,No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence,"Pre-trained models have been shown effective in many code intelligence tasks. These models are pre-trained on large-scale unlabeled corpus and then fine-tuned in downstream tasks. However, as the inputs to pre-training and downstream tasks are in different forms, it is hard to fully explore the knowledge of pre-trained models. Besides, the performance of fine-tuning strongly relies on the amount of downstream data, while in practice, the scenarios with scarce data are common. Recent studies in the natural language processing (NLP) field show that prompt tuning, a new paradigm for tuning, alleviates the above issues and achieves promising results in various NLP tasks. In prompt tuning, the prompts inserted during tuning provide task-specific knowledge, which is especially beneficial for tasks with relatively scarce data. In this paper, we empirically evaluate the usage and effect of prompt tuning in code intelligence tasks. We conduct prompt tuning on popular pre-trained models CodeBERT and CodeT5 and experiment with three code intelligence tasks including defect prediction, code summarization, and code translation. Our experimental results show that prompt tuning consistently outperforms fine-tuning in all three tasks. In addition, prompt tuning shows great potential in low-resource scenarios, e.g., improving the BLEU scores of fine-tuning by more than 26% on average for code summarization. Our results suggest that instead of fine-tuning, we could adapt prompt tuning for code intelligence tasks to achieve better performance, especially when lacking task-specific data.","Computing methodologies,Artificial intelligence,Natural language processing,Machine learning,Software and its engineering,Software creation and management,Software development techniques,Software notations and tools",不再进行微调？代码智能中即时调优的实验评价,经过预训练的模型已经在许多代码智能任务中被证明是有效的。这些模型在大规模未标记语料库上进行预训练，然后在下游任务中进行微调。然而，由于预训练和下游任务的输入形式不同，很难充分探索预训练模型的知识。此外，微调的性能强烈依赖于下游数据的数量，而在实践中，数据稀缺的场景很常见。最近在自然语言处理（NLP）领域的研究表明，即时调优作为一种新的调优范式，缓解了上述问题，并在各种NLP任务中取得了有希望的结果。在提示调优中，调优期间插入的提示提供了特定于任务的知识，这对于数据相对较少的任务尤其有益。在本文中，我们实证评估了代码智能任务中即时调优的使用和效果。我们对流行的预训练模型CodeBERT和CodeT5进行了快速调整，并对三个代码智能任务进行了实验，包括缺陷预测、代码摘要和代码翻译。我们的实验结果表明，在所有三项任务中，即时调整始终优于微调。此外，提示调优在低资源场景中显示出巨大的潜力，例如，对于代码摘要，微调的BLEU分数平均提高26%以上。我们的结果表明，我们可以对代码智能任务进行即时调整，以获得更好的性能，而不是进行微调，尤其是在缺乏特定于任务的数据时。,计算方法论，人工智能，自然语言处理，机器学习，软件及其工程，软件创建和管理，软件开发技术，软件符号和工具,,,
LKPKDSW5,2022,https://doi.org/10.1145/3540250.3549176,ESEC/FSE 2022,SPINE: a scalable log parser with feedback guidance,"Log parsing, which extracts log templates and parameters, is a critical prerequisite step for automated log analysis techniques. Though existing log parsers have achieved promising accuracy on public log datasets, they still face many challenges when applied in the industry. Through studying the characteristics of real-world log data and analyzing the limitations of existing log parsers, we identify two problems. Firstly, it is non-trivial to scale a log parser to a vast number of logs, especially in real-world scenarios where the log data is extremely imbalanced. Secondly, existing log parsers overlook the importance of user feedback, which is imperative for parser fine-tuning under the continuous evolution of log data. To overcome the challenges, we propose SPINE, which is a highly scalable log parser with user feedback guidance. Based on our log parser equipped with initial grouping and progressive clustering,we propose a novel log data scheduling algorithm to improve the efficiency of parallelization under the large-scale imbalanced log data. Besides, we introduce user feedback to make the parser fast adapt to the evolving logs. We evaluated SPINE on 16 public log datasets. SPINE achieves more than 0.90 parsing accuracy on average with the highest parsing efficiency, which outperforms the state-of-the-art log parsers. We also evaluated SPINE in the production environment of Microsoft, in which SPINE can parse 30million logs in less than 8 minutes under 16 executors, achieving near real-time performance. In addition, our evaluations show that SPINE can consistently achieve good accuracy under log evolution with a moderate number of user feedback.","Computing methodologies,Machine learning,Information systems,Data management systems,Information systems applications,Data mining,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software notations and tools,Theory of computation,Theory and algorithms for application domains",SPINE：具有反馈指导的可扩展日志解析器,日志解析提取日志模板和参数，是自动化日志分析技术的关键前提步骤。尽管现有的日志解析器在公共日志数据集上取得了很好的准确性，但在行业中应用时仍面临许多挑战。通过研究真实世界日志数据的特点和分析现有日志解析器的局限性，我们发现了两个问题。首先，将日志解析器扩展到大量日志是非常重要的，尤其是在日志数据极不平衡的真实场景中。其次，现有的日志解析器忽视了用户反馈的重要性，这对于在日志数据不断进化的情况下进行解析器微调是必不可少的。为了克服这些挑战，我们提出了SPINE，这是一个具有用户反馈指导的高度可扩展的日志解析器。基于我们的具有初始分组和渐进聚类的日志解析器，我们提出了一种新的日志数据调度算法，以提高在大规模不平衡日志数据下的并行化效率。此外，我们引入了用户反馈，使解析器快速适应不断发展的日志。我们在16个公共日志数据集上评估了SPINE。SPINE以最高的解析效率平均达到0.90以上的解析精度，优于最先进的日志解析器。我们还在微软的生产环境中评估了SPINE，在该环境中，SPINE可以在16个执行器下，在不到8分钟的时间内解析3000万条日志，实现近乎实时的性能。此外，我们的评估表明，SPINE在具有适度数量的用户反馈的日志进化下可以始终如一地实现良好的准确性。,计算方法学，机器学习，信息系统，数据管理系统，信息系统应用，数据挖掘，软件及其工程，软件创建和管理，软件后期开发问题，维护软件，软件符号和工具，计算理论，应用领域的理论和算法,,,
XAYE366W,2022,https://doi.org/10.1145/3540250.3549124,ESEC/FSE 2022,API recommendation for machine learning libraries: how far are we?,"Application Programming Interfaces (APIs) are designed to help developers build software more effectively. Recommending the right APIs for specific tasks is gaining increasing attention among researchers and developers.  
However, most of the existing approaches are mainly evaluated for general programming tasks using statically typed programming languages such as Java. Little is known about their practical effectiveness and usefulness for machine learning (ML) programming tasks with dynamically typed programming languages such as Python, whose paradigms are fundamentally different from general programming tasks. This is of great value considering the increasing popularity of ML and the large number of new questions appearing on question answering websites.  
In this work, we set out to investigate the effectiveness of existing API recommendation approaches for Python-based ML programming tasks from Stack Overflow (SO). Specifically, we conducted an empirical study of six widely-used Python-based ML libraries using two state-of-the-art API recommendation approaches, i.e., BIKER and DeepAPI. We found that the existing approaches perform poorly for two main reasons: (1) Python-based ML tasks often require significant long API sequences; and (2) there are common API usage patterns in Python-based ML programming tasks that existing approaches cannot handle.  
Inspired by our findings, we proposed a simple but effective frequent itemset mining-based approach, i.e., FIMAX, to boost API recommendation approaches, i.e., enhance existing API recommendation approaches for Python-based ML programming tasks by leveraging the common API usage information from SO questions. Our evaluation shows that FIMAX improves existing state-of-the-art API recommendation approaches by up to 54.3% and 57.4% in MRR and MAP, respectively. Our user study with 14 developers further demonstrates the practicality of FIMAX for API recommendation.","Computing methodologies,Machine learning,Information systems,Information retrieval,Retrieval tasks and goals,Recommender systems,Information systems applications,Data mining,Software and its engineering,Software creation and management,Software notations and tools",API推荐机器学习库：我们有多远？,应用程序编程接口（API）旨在帮助开发人员更有效地构建软件。为特定任务推荐合适的API越来越受到研究人员和开发人员的关注。,计算方法，机器学习，信息系统，信息检索，检索任务和目标，推荐系统，信息系统应用，数据挖掘，软件及其工程，软件创建和管理，软件符号和工具,,,
LC6N62H2,2022,https://doi.org/10.1145/3540250.3559082,ESEC/FSE 2022,CheapET-3: cost-efficient use of remote DNN models,"On complex problems, state of the art prediction accuracy of Deep Neural Networks (DNN) can be achieved using very large-scale models, consisting of billions of parameters. Such models can only be run on dedicated servers, typically provided by a 3th party service, which leads to a substantial monetary cost for every prediction. We propose a new software architecture for client-side applications, where a small local DNN is used alongside a remote large-scale model, aiming to make easy predictions locally at negligible monetary cost, while still leveraging the benefits of a large model for challenging inputs. In a proof of concept we reduce prediction cost by up to 50% without negatively impacting system accuracy.","Computer systems organization,Architectures,Other architectures,Computing methodologies,Machine learning,Learning paradigms,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Designing software",CheapET-3：远程DNN模型的经济高效使用,在复杂问题上，深度神经网络（DNN）的最新预测精度可以使用由数十亿个参数组成的非常大规模的模型来实现。这样的模型只能在专用服务器上运行，通常由第三方服务提供，这导致每次预测都要付出巨大的金钱成本。我们为客户端应用程序提出了一种新的软件架构，其中小型本地DNN与远程大型模型一起使用，旨在以可忽略的货币成本在本地进行简单的预测，同时仍然利用大型模型的优势进行具有挑战性的输入。在概念验证中，我们将预测成本降低了50%，而不会对系统精度产生负面影响。,计算机系统组织，体系结构，其他体系结构，计算方法，机器学习，学习范例，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件设计,,,
2SWBM6J7,2022,https://doi.org/10.1145/3540250.3558921,ESEC/FSE 2022,Clang __usercall: towards native support for user defined calling conventions,"In reverse engineering interfacing with C/C++ functions is of great interest because it provides much more flexibility for product development and security purpose. However, it has been a great challenge  
when interfacing functions with user defined calling conventions due to the lack of sufficient and user-friendly tooling. In this work, we design and implement Clang __usercall, which aims to provide programmers with an elegant and familiar syntax to specify user defined calling conventions on functions in C/C++ source code. Our key novelties lie in mimicing the most popular syntax and adapting Clang for interfacing purpose. Our preliminary user study shows that our solution outperforms the existing ones in multiple key aspects including user experience and required lines of code. Clang __usercall is already added to the Compiler Explorer website as well.","Software and its engineering,Software creation and management,Software development techniques,Software post-development issues,Software reverse engineering,Software notations and tools,Compilers,General programming languages",Clang __usercall：实现对用户定义的调用约定的本地支持,在逆向工程中，与C/C++函数的接口非常有趣，因为它为产品开发和安全目的提供了更大的灵活性。然而，这是一个巨大的挑战,软件及其工程，软件创建和管理，软件开发技术，软件后开发问题，软件逆向工程，软件符号和工具，编译器，通用编程语言,,,
348AVEIS,2022,https://doi.org/10.1145/3540250.3558953,ESEC/FSE 2022,Towards developer-centered automatic program repair: findings from Bloomberg,"This paper reports on qualitative research into automatic program repair (APR) at Bloomberg. Six focus groups were conducted with a total of seventeen participants (including both developers of the APR tool and developers using the tool) to consider: the development at Bloomberg of a prototype APR tool (Fixie); developers’ early experiences using the tool; and developers’ perspectives on  
how they would like to interact with the tool in future. APR is developing rapidly and it is important to understand in greater detail developers' experiences using this emerging technology. In this paper, we provide in-depth, qualitative data from an industrial setting. We found that the development of APR at Bloomberg had become increasingly user-centered, emphasising how fixes were presented to developers, as well as particular features, such as customisability. From the focus groups with developers who had used Fixie, we found particular concern with the pragmatic aspects of APR, such as how and when fixes were presented to them. Based on our findings, we make a series of recommendations to inform future APR development, highlighting how APR tools should 'start small', be customisable, and fit with developers' workflows. We also suggest that APR tools should capitalise on the promise of repair bots and draw on advances in explainable AI.","Human-centered computing,Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools",走向以开发人员为中心的自动程序修复：彭博社的研究结果,本文在彭博社报道了对自动程序修复（APR）的定性研究。共进行了六个焦点小组，共有17名参与者（包括APR工具的开发者和使用该工具的开发者）考虑：在彭博社开发原型APR工具（Fixie）；开发人员早期使用该工具的经验；以及开发人员对,以人为中心的计算，软件及其工程，软件创建和管理，软件开发后问题，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具,,,
27DJBR2R,2022,https://doi.org/10.1145/3540250.3549140,ESEC/FSE 2022,NL2Viz: natural language to visualization via constrained syntax-guided synthesis,"Recent development in NL2CODE (Natural Language to Code) research allows end-users, especially novice programmers to create a concrete implementation of their ideas such as data visualization by providing natural language (NL) instructions. An NL2CODE system often fails to achieve its goal due to three major challenges: the user's words have contextual semantics, the user may not include all details needed for code generation, and the system results are imperfect and require further refinement. To address the aforementioned three challenges for NL to Visualization, we propose a new approach and its supporting tool named NL2VIZ with three salient features: (1) leveraging not only the user's NL input but also the data and program context that the NL query is upon, (2) using hard/soft constraints to reflect different confidence levels in the constraints retrieved from the user input and data/program context, and (3) providing support for result refinement and reuse.  
","Human-centered computing,Human computer interaction (HCI),Software and its engineering,Software creation and management,Software development techniques,Automatic programming,Software notations and tools,Context specific languages,Visual languages",NL2Viz：通过受限语法引导合成实现可视化的自然语言,NL2CODE（自然语言到代码）研究的最新进展使最终用户，尤其是新手程序员，能够通过提供自然语言（NL）指令来创建他们想法的具体实现，例如数据可视化。NL2CODE系统通常由于三个主要挑战而无法实现其目标：用户的单词具有上下文语义，用户可能不包括代码生成所需的所有细节，以及系统结果不完美，需要进一步细化。为了解决前面提到的NL可视化的三个挑战，我们提出了一种新的方法及其支持工具NL2VIZ，该方法具有三个显著特征：（1）不仅利用用户的NL输入，还利用NL查询所基于的数据和程序上下文，（2）使用硬/软约束来反映从用户输入和数据/程序上下文检索到的约束中的不同置信水平，以及（3）为结果细化和重用提供支持。,以人为中心的计算，人机交互(HCI)，软件及其工程，软件创建和管理，软件开发技术，自动编程，软件符号和工具，上下文特定语言，可视化语言,,,
XW29ZVNW,2022,https://doi.org/10.1145/3540250.3558906,ESEC/FSE 2022,Blackbox adversarial attacks and explanations for automatic speech recognition,"Automatic speech recognition (ASR) models are used widely in applications for voice navigation and voice control of domestic appliances. The computational core of ASRs are Deep Neural Networks (DNNs) that have been shown to be susceptible to adversarial perturbations and exhibit unwanted biases and ethical issues. To assess the security of ASRs, we propose techniques that generate blackbox (agnostic to the DNN) adversarial attacks that are portable across ASRs. This is in contrast to existing work that focuses on whitebox attacks that are time consuming and lack portability. Apart from that, to figure out why ASRs(always blackbox) are easily attacked, we provide explanation methods on ASRs that help increase our understanding of the system and ultimately help build trust in the system.","Computing methodologies,Machine learning,Security and privacy,Network security,Security services,Software and application security,Systems security,Social and professional topics,Computing / technology policy,Computer crime,Software and its engineering,Software organization and properties",用于自动语音识别的Blackbox对抗性攻击和解释,自动语音识别（ASR）模型广泛应用于家用电器的语音导航和语音控制。ASRs的计算核心是深度神经网络（DNN），该网络已被证明易受对抗性扰动的影响，并表现出不必要的偏见和道德问题。为了评估ASR的安全性，我们提出了生成可跨ASR移植的黑盒（与DNN无关）对抗性攻击的技术。这与现有的工作形成了鲜明对比，现有的工作侧重于耗时且缺乏可移植性的白盒攻击。除此之外，为了弄清楚为什么ASR（总是黑盒）很容易受到攻击，我们提供了关于ASR的解释方法，这些方法有助于增加我们对系统的理解，并最终有助于建立对系统的信任。,计算方法，机器学习，安全和隐私，网络安全，安全服务，软件和应用程序安全，系统安全，社会和专业主题，计算/技术政策，计算机犯罪，软件及其工程，软件组织和属性,,,
MNNSUGA3,2022,https://doi.org/10.1145/3540250.3549101,ESEC/FSE 2022,"Less training, more repairing please: revisiting automated program repair via zero-shot learning","Due to the promising future of Automated Program Repair (APR), researchers have proposed various APR techniques, including heuristic-based, template-based, and constraint-based techniques. Among such classic APR techniques, template-based techniques have been widely recognized as state of the art. However, such template-based techniques require predefined templates to perform repair, and their effectiveness is thus limited. To this end, researchers have leveraged the recent advances in Deep Learning to further improve APR. Such learning-based techniques typically view APR as a Neural Machine Translation problem, using the buggy/fixed code snippets as the source/target languages for translation. In this way, such techniques heavily rely on large numbers of high-quality bug-fixing commits, which can be extremely costly/challenging to construct and may limit their edit variety and context representation. ","Computer systems organization,Architectures,Other architectures,Neural networks,Hardware,Power and energy,Power estimation and optimization,Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems",请减少培训，增加修复：通过零样本学习重新访问自动化程序修复,由于自动程序修复（APR）的前景广阔，研究人员提出了各种APR技术，包括基于启发式、基于模板和基于约束的技术。在这些经典的APR技术中，基于模板的技术已被广泛认为是最先进的技术。然而，这种基于模板的方法需要预定义的模板来进行修复，因此其有效性有限。为此，研究人员利用深度学习的最新进展来进一步改进APR。这种基于学习的技术通常将APR视为神经机器翻译问题，使用有缺陷/固定的代码片段作为翻译的源/目标语言。通过这种方式，这些技术在很大程度上依赖于大量高质量的错误修复提交，这可能是极其昂贵/具有挑战性的构建，并且可能限制其编辑多样性和上下文表示。,计算机系统组织，体系结构，其他体系结构，神经网络，硬件，功率和能源，功率估计和优化，软件及其工程，软件创建和管理，软件开发后问题，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统,,,
2N9VI76X,2022,https://doi.org/10.1145/3540250.3549138,ESEC/FSE 2022,"Psychologically-inspired, unsupervised inference of perceptual groups of GUI widgets from GUI images","Graphical User Interface (GUI) is not merely a collection of individual and unrelated widgets, but rather partitions discrete widgets into groups by various visual cues, thus forming higher-order perceptual units such as tab, menu, card or list. The ability to automatically segment a GUI into perceptual groups of widgets constitutes a fundamental component of visual intelligence to automate GUI design, implementation and automation tasks. Although humans can partition a GUI into meaningful perceptual groups of widgets in a highly reliable way, perceptual grouping is still an open challenge for computational approaches. Existing methods rely on ad-hoc heuristics or supervised machine learning that is dependent on specific GUI implementations and runtime information. Research in psychology and biological vision has formulated a set of principles (i.e., Gestalt theory of perception) that describe how humans group elements in visual scenes based on visual cues like connectivity, similarity, proximity and continuity. These principles are domain-independent and have been widely adopted by practitioners to structure content on GUIs to improve aesthetic pleasantness and usability. Inspired by these principles, we present a novel unsupervised image-based method for inferring perceptual groups of GUI widgets. Our method requires only GUI pixel images, is independent of GUI implementation, and does not require any training data. The evaluation on a dataset of 1,091 GUIs collected from 772 mobile apps and 20 UI design mockups shows that our method significantly outperforms the state-of-the-art ad-hoc heuristics-based baseline. Our perceptual grouping method creates opportunities for improving UI-related software engineering tasks.","Computing methodologies,Human-centered computing,Human computer interaction (HCI),Software and its engineering",从GUI图像中对GUI小部件感知组的心理启发、无监督推理,图形用户界面（GUI）不仅仅是单个和不相关的小部件的集合，而是通过各种视觉线索将离散的小部件划分为组，从而形成更高阶的感知单元，如选项卡、菜单、卡片或列表。将GUI自动划分为感知小部件组的能力构成了视觉智能的基本组成部分，以自动化GUI设计、实现和自动化任务。尽管人类可以以高度可靠的方式将GUI划分为有意义的感知小部件组，但感知分组对计算方法来说仍然是一个悬而未决的挑战。现有方法依赖于特定GUI实现和运行时信息的自组织启发式或监督机器学习。心理学和生物视觉的研究已经制定了一套原则（即格式塔感知理论），描述了人类如何根据视觉线索（如连接性、相似性、接近性和连续性）对视觉场景中的元素进行分组。这些原则是独立于领域的，并且已经被从业者广泛采用来在GUI上构建内容，以提高美观性和可用性。受这些原理的启发，我们提出了一种新的基于图像的无监督方法来推断GUI窗口小部件的感知组。我们的方法只需要GUI像素图像，独立于GUI实现，并且不需要任何训练数据。对从772个移动应用程序和20个UI设计模型中收集的1091个GUI数据集的评估表明，我们的方法显著优于最先进的基于自组织启发式的基线。我们的感知分组方法为改进与UI相关的软件工程任务创造了机会。,计算方法论，以人为中心的计算，人机交互，软件及其工程,,,
7QHS5ITJ,2022,https://doi.org/10.1145/3540250.3558957,ESEC/FSE 2022,Uncertainty-aware transfer learning to evolve digital twins for industrial elevators,"Digital twins are increasingly developed to support the development, operation, and maintenance of cyber-physical systems such as industrial elevators. However, industrial elevators continuously evolve due to changes in physical installations, introducing new software features, updating existing ones, and making changes due to regulations (e.g., enforcing restricted elevator capacity due to COVID-19), etc. Thus, digital twin functionalities (often built on neural network-based models) need to evolve themselves constantly to be synchronized with the industrial elevators. Such an evolution is preferred to be automated, as manual evolution is time-consuming and error-prone. Moreover, collecting sufficient data to re-train neural network models of digital twins could be expensive or even infeasible. To this end, we propose unceRtaInty-aware tranSfer lEarning enriched Digital Twins LATTICE, a transfer learning based approach capable of transferring knowledge about the waiting time prediction capability of a digital twin of an industrial elevator across different scenarios. LATTICE also leverages uncertainty quantification to further improve its effectiveness. To evaluate LATTICE, we conducted experiments with 10 versions of an elevator dispatching software from Orona, Spain, which are deployed in a Software in the Loop (SiL) environment. Experiment results show that LATTICE, on average, improves the Mean Squared Error by 13.131% and the utilization of uncertainty quantification further improves it by 2.71%.","Computer systems organization,Computing methodologies,Artificial intelligence,Information systems,Information systems applications,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software evolution,Software verification and validation,Empirical software validation,Software organization and properties,Software system structures,Embedded software",具有不确定性意识的迁移学习为工业电梯发展数字孪生,数字孪生技术正日益发展，以支持工业电梯等网络物理系统的开发、运营和维护。然而，工业电梯由于物理安装的变化、引入新的软件功能、更新现有功能以及由于法规（例如，由于新冠肺炎而强制执行受限的电梯容量）等而不断发展。因此，数字孪生功能（通常建立在基于神经网络的模型上）需要不断发展，才能与工业电梯同步。这样的进化优选是自动化的，因为手动进化耗时且容易出错。此外，收集足够的数据来重新训练数字双胞胎的神经网络模型可能是昂贵的，甚至是不可行的。为此，我们提出了一种基于迁移学习的方法，该方法能够在不同的场景中传递有关工业电梯数字孪生的等待时间预测能力的知识。LATTICE还利用不确定性量化来进一步提高其有效性。为了评估LATTICE，我们使用西班牙奥罗纳的10个版本的电梯调度软件进行了实验，这些软件部署在软件在环（SiL）环境中。实验结果表明，LATTICE平均将均方误差提高了13.131%，不确定度量化的利用率进一步提高了2.71%。,计算机系统组织，计算方法论，人工智能，信息系统，信息系统应用，软件及其工程，软件创建和管理，软件后开发问题，维护软件，软件进化，软件验证和确认，经验软件验证，软件组织和属性，软件系统结构，嵌入式软件,,,
MFMCGJVI,2022,https://doi.org/10.1145/3540250.3549125,ESEC/FSE 2022,Tracking patches for open source software vulnerabilities,"Open source software (OSS) vulnerabilities threaten the security of software systems that use OSS. Vulnerability databases provide valuable information (e.g., vulnerable version and patch) to mitigate OSS vulnerabilities. There arises a growing concern about the information quality of vulnerability databases. However, it is unclear what the quality of patches in existing vulnerability databases is; and existing manual or heuristic-based approaches for patch tracking are either too expensive or too specific to apply to all OSS vulnerabilities.  
","Information systems,Information systems applications,Collaborative and social computing systems and tools,Open source software,Security and privacy,Systems security,Vulnerability management,Social and professional topics,Computing / technology policy,Professional topics,Management of computing and information systems,Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation",跟踪开源软件漏洞的修补程序,开放源码软件（OSS）漏洞威胁到使用OSS的软件系统的安全。漏洞数据库提供了有价值的信息（例如易受攻击的版本和补丁）来缓解OSS的漏洞。人们越来越关注漏洞数据库的信息质量。然而，目前尚不清楚现有漏洞数据库中补丁的质量如何；现有的手动或启发式补丁跟踪方法要么过于昂贵，要么过于具体，无法应用于所有OSS漏洞。,信息系统，信息系统应用，协作和社会计算系统和工具，开放源码软件，安全和隐私，系统安全，漏洞管理，社会和专业主题，计算/技术政策，专业主题，计算和信息系统管理，软件及其工程，软件创建和管理，软件后开发问题，软件验证和确认,,,
LCSSGCDK,2022,https://doi.org/10.1145/3540250.3560880,ESEC/FSE 2022,"Language-agnostic dynamic analysis of multilingual code: promises, pitfalls, and prospects","Analyzing multilingual code holistically is key to systematic quality assurance of real-world software which is mostly developed in multiple computer languages. Toward such analyses, state-of-the-art approaches propose an almost-fully language-agnostic methodology and apply it to dynamic dependence analysis/slicing of multilingual code, showing great promises. We investigated this methodology through a technical analysis followed by a replication study applying it to 10 real-world multilingual projects of diverse language combinations. Our results revealed critical practicality (i.e., having the levels of efficiency/scalability, precision, and extensibility to various language combinations for practical use) challenges to the methodology. Based on the results, we reflect on the underlying pitfalls of the language-agnostic design that leads to such challenges. Finally, looking forward to the prospects of dynamic analysis for multilingual code, we identify a new research direction towards better practicality and precision while not sacrificing extensibility much, as supported by preliminary results. The key takeaway is that pursuing fully language-agnostic analysis may be both impractical and unnecessary, and striving for a better balance between language independence and practicality may be more fruitful.","General and reference,Cross-computing tools and techniques,Human-centered computing,Visualization,Visualization techniques,Social and professional topics,Software and its engineering,Software creation and management,Software development process management,Software verification and validation,Software notations and tools,Software organization and properties,Software functional properties,Formal methods,Dynamic analysis",多语言代码的语言无关动态分析：承诺、陷阱和前景,全面分析多语言代码是保证现实世界软件质量的关键，而现实世界软件大多是用多种计算机语言开发的。对于这种分析，最先进的方法提出了一种几乎完全与语言无关的方法，并将其应用于多语言代码的动态依赖性分析/切片，显示出巨大的前景。我们通过技术分析和复制研究对这种方法进行了研究，并将其应用于10个不同语言组合的真实世界多语言项目。我们的结果揭示了该方法的关键实用性（即具有针对实际使用的各种语言组合的效率/可扩展性、精度和可扩展性）挑战。基于这些结果，我们反思了导致这些挑战的语言不可知设计的潜在陷阱。最后，展望了多语言代码动态分析的前景，我们确定了一个新的研究方向，即在不牺牲太多可扩展性的同时，提高实用性和准确性，这得到了初步结果的支持。关键的结论是，追求完全的语言不可知论分析可能既不切实际又不必要，而在语言独立性和实用性之间寻求更好的平衡可能会更有成效。,通则和参考，交叉计算工具和技术，以人为中心的计算，可视化，可视化技术，社会和专业主题，软件及其工程，软件创建和管理，软件开发过程管理，软件验证和确认，软件符号和工具，软件组织和属性，软件功能属性，形式方法，动态分析,,,
TNVVB25K,2022,https://doi.org/10.1145/3540250.3549105,ESEC/FSE 2022,"An empirical study of blockchain system vulnerabilities: modules, types, and patterns","Blockchain, as a distributed ledger technology, becomes increasingly popular, especially for enabling valuable cryptocurrencies and smart contracts. However, the blockchain software systems inevitably have many bugs. Although bugs in smart contracts have been extensively investigated, security bugs of the underlying blockchain systems are much less explored. In this paper, we conduct an empirical study on blockchain’s system vulnerabilities from four representative blockchains, Bitcoin, Ethereum, Monero, and Stellar. Specifically, we first design a systematic filtering process to effectively identify 1,037 vulnerabilities and their 2,317 patches from 34,245 issues/PRs (pull requests) and 85,164 commits on GitHub. We thus build the first blockchain vulnerability dataset, which is available at https://github.com/VPRLab/BlkVulnDataset. We then perform unique analyses of this dataset at three levels, including (i) file-level vulnerable module categorization by identifying and correlating module paths across projects, (ii) text-level vulnerability type clustering by natural language processing and similarity-based sentence clustering, and (iii) code-level vulnerability pattern analysis by generating and clustering code change signatures that capture both syntactic and semantic information of patch code fragments.  
","Security and privacy,Software and application security,Systems security,Social and professional topics,Computing / technology policy,Software and its engineering,Software creation and management,Software verification and validation,Software organization and properties",区块链系统漏洞的实证研究：模块、类型和模式,区块链作为一种分布式账本技术，越来越受欢迎，尤其是在启用有价值的加密货币和智能合约方面。然而，区块链软件系统不可避免地存在许多漏洞。尽管智能合约中的漏洞已经被广泛调查，但底层区块链系统的安全漏洞却很少被探索。在本文中，我们从比特币、以太坊、Monero和Stellar四个具有代表性的区块链中对区块链的系统漏洞进行了实证研究。具体来说，我们首先设计了一个系统的过滤过程，从GitHub上的34245个问题/PR（拉取请求）和85164个提交中有效识别1037个漏洞及其2317个补丁。因此，我们构建了第一个区块链漏洞数据集，该数据集可在https://github.com/VPRLab/BlkVulnDataset.然后，我们在三个层面上对该数据集进行了独特的分析，包括（i）通过识别和关联项目之间的模块路径进行文件级易受攻击模块分类，（ii）通过自然语言处理和基于相似性的句子聚类进行文本级易受伤害类型聚类，以及（iii）通过生成和聚类捕获补丁代码片段的语法和语义信息的代码更改签名来进行代码级漏洞模式分析。,安全和隐私，软件和应用程序安全，系统安全，社会和专业主题，计算/技术政策，软件及其工程，软件创建和管理，软件验证和确认，软件组织和属性,,,
2I89MLCH,2022,https://doi.org/10.1145/3540250.3558913,ESEC/FSE 2022,RecipeGen++: an automated trigger action programs generator,"Trigger Action Programs (TAPs) are event-driven rules that allow users to automate smart-devices and internet services. Users can write TAPs by specifying triggers and actions from a set of predefined channels and functions. Despite its simplicity, composing TAPs can still be challenging for users due to the enormous search space of available triggers and actions. The growing popularity of TAPs is followed by the increasing number of supported devices and services, resulting in a huge number of possible combinations between triggers and actions. Motivated by such a fact, we improve our prior work and propose RecipeGen++, a deep-learning-based approach that leverages Transformer seq2seq (sequence-to-sequence) architecture to generate TAPs given natural language descriptions. RecipeGen++ can generate TAPs in the Interactive, One-Click, or Functionality Discovery modes. In the Interactive mode, users can provide feedback to guide the prediction of a trigger or action component. In contrast, the One-Click mode allows users to generate all TAP components directly. Additionally, RecipeGen++ also enables users to discover functionalities at the channel level through the Functionality Discovery mode. We have evaluated RecipeGen++ on real-world datasets in all modes. Our results demonstrate that RecipeGen++ can outperform the baseline by 2.2%-16.2% in the gold-standard benchmark and 5%-29.2% in the noisy benchmark.","Computing methodologies,Artificial intelligence,Natural language processing,Machine learning,Machine learning approaches,Human-centered computing,Information systems,Information retrieval,Retrieval models and ranking,Information systems applications,Software and its engineering,Software creation and management,Search-based software engineering",RecipeGen++：一个自动触发动作程序生成器,触发操作程序（TAP）是事件驱动的规则，允许用户自动化智能设备和互联网服务。用户可以通过指定一组预定义通道和函数中的触发器和操作来编写TAP。尽管TAP很简单，但由于可用触发器和操作的巨大搜索空间，编写TAP对用户来说仍然是一项挑战。TAP越来越受欢迎，随之而来的是越来越多的受支持的设备和服务，导致触发器和操作之间存在大量可能的组合。受此启发，我们改进了之前的工作，并提出了RecipeGen++，这是一种基于深度学习的方法，利用Transformer seq2seq（序列到序列）架构生成给定自然语言描述的TAP。RecipeGen++可以在“交互式”、“一键式”或“功能发现”模式下生成TAP。在交互模式中，用户可以提供反馈以指导对触发或动作组件的预测。相比之下，一键模式允许用户直接生成所有TAP组件。此外，RecipeGen++还允许用户通过功能发现模式在渠道级别发现功能。我们已经在所有模式下的真实世界数据集上评估了RecipeGen++。我们的结果表明，RecipeGen++在金标准基准中的表现优于基线2.2%-16.2%，在嘈杂基准中的成绩优于基线5%-29.2%。,计算方法论，人工智能，自然语言处理，机器学习，机器学习方法，以人为中心的计算，信息系统，信息检索，检索模型和排名，信息系统应用，软件及其工程，软件创建和管理，基于搜索的软件工程,,,
RFYWUHVC,2022,https://doi.org/10.1145/3540250.3569447,ESEC/FSE 2022,Academic prototyping (invited tutorial),"Much of our research requires building tools to evaluate and demonstrate new approaches. Yet, tool building can take large amounts of time and resources. And it brings risks: The original idea might not work; rendering all efforts futile. And after the student in charge has left, the tool becomes a maintenance problem.  
","Software and its engineering,Software creation and management,Designing software,Software implementation planning,Software development process management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools",学术原型（受邀教程）,我们的许多研究都需要构建工具来评估和演示新方法。然而，工具构建可能需要大量的时间和资源。它带来了风险：最初的想法可能行不通；使得所有的努力都是徒劳的。负责的学生离开后，该工具就成了一个维护问题。,软件及其工程，软件创建和管理，设计软件，软件实施规划，软件开发过程管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具,,,
8EIZWH83,2022,https://doi.org/10.1145/3540250.3549103,ESEC/FSE 2022,Adaptive fairness improvement based on causality analysis,"Given a discriminating neural network, the problem of fairness improvement is to systematically reduce discrimination without significantly scarifies its performance (i.e., accuracy). Multiple categories of fairness improving methods have been proposed for neural networks, including pre-processing, in-processing and post-processing. Our empirical study however shows that these methods are not always effective (e.g., they may improve fairness by paying the price of huge accuracy drop) or even not helpful (e.g., they may even worsen both fairness and accuracy). In this work, we propose an approach which adaptively chooses the fairness improving method based on causality analysis. That is, we choose the method based on how the neurons and attributes responsible for unfairness are distributed among the input attributes and the hidden neurons. Our experimental evaluation shows that our approach is effective (i.e., always identify the best fairness improving method) and efficient (i.e., with an average time overhead of 5 minutes).","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software organization and properties,Extra-functional properties,Theory of computation,Theory and algorithms for application domains,Machine learning theory",基于因果关系分析的自适应公平性改进,给定一个判别神经网络，公平性改进的问题是系统地减少判别，而不会显著降低其性能（即准确性）。已经为神经网络提出了多种类型的公平性改进方法，包括预处理、处理中和后处理。然而，我们的实证研究表明，这些方法并不总是有效的（例如，它们可能会以巨大的准确性下降为代价来提高公平性），甚至没有帮助（例如，他们甚至可能恶化公平性和准确性）。在这项工作中，我们提出了一种基于因果关系分析的自适应选择公平性改进方法的方法。也就是说，我们根据负责不公平的神经元和属性在输入属性和隐藏神经元之间的分布来选择方法。我们的实验评估表明，我们的方法是有效的（即始终确定最佳的公平性改进方法）和高效的（即平均时间开销为5分钟）。,计算方法，机器学习，机器学习方法，神经网络，软件及其工程，软件组织和性质，额外功能性质，计算理论，应用领域的理论和算法，机器学习理论,,,
L6243UFC,2022,https://doi.org/10.1145/3540250.3549094,ESEC/FSE 2022,Diet code is healthy: simplifying programs for pre-trained models of code,"Pre-trained code representation models such as CodeBERT have demonstrated superior performance in a variety of software engineering tasks, yet they are often heavy in complexity, quadratically with the length of the input sequence. Our empirical analysis of CodeBERT's attention reveals that CodeBERT pays more attention to certain types of tokens and statements such as keywords and data-relevant statements. Based on these findings, we propose DietCode, which aims at lightweight leverage of large pre-trained models for source code. DietCode simplifies the input program of CodeBERT with three strategies, namely, word dropout, frequency filtering, and an attention-based strategy that selects statements and tokens that receive the most attention weights during pre-training. Hence, it gives a substantial reduction in the computational cost without hampering the model performance. Experimental results on two downstream tasks show that DietCode provides comparable results to CodeBERT with 40% less computational cost in fine-tuning and testing.","Computing methodologies,Artificial intelligence,Natural language processing,Machine learning,Software and its engineering,Software creation and management,Software notations and tools",饮食代码是健康的：简化预先训练的代码模型的程序,CodeBERT等经过预训练的代码表示模型在各种软件工程任务中表现出了卓越的性能，但它们的复杂性往往很高，与输入序列的长度呈二次方关系。我们对CodeBERT注意力的实证分析表明，CodeBERT更关注某些类型的令牌和语句，如关键字和数据相关语句。基于这些发现，我们提出了DietCode，旨在轻量级地利用大型预训练模型来编写源代码。DietCode通过三种策略简化了CodeBERT的输入程序，即单词丢弃、频率过滤和基于注意力的策略，该策略选择在预训练期间获得最多注意力权重的语句和标记。因此，它在不妨碍模型性能的情况下大幅降低了计算成本。在两个下游任务上的实验结果表明，DietCode提供了与CodeBERT相当的结果，在微调和测试方面的计算成本降低了40%。,计算方法论，人工智能，自然语言处理，机器学习，软件及其工程，软件创建和管理，软件符号和工具,,,
ULCYVZKY,2022,https://doi.org/10.1145/3540250.3549146,ESEC/FSE 2022,TraceCRL: contrastive representation learning for microservice trace analysis,"Due to the large amount and high complexity of trace data, microservice trace analysis tasks such as anomaly detection, fault diagnosis, and tail-based sampling widely adopt machine learning technology. These trace analysis approaches usually use a preprocessing step to map structured features of traces to vector representations in an ad-hoc way. Therefore, they may lose important information such as topological dependencies between service operations. In this paper, we propose TraceCRL, a trace representation learning approach based on contrastive learning and graph neural network, which can incorporate graph structured information in the downstream trace analysis tasks. Given a trace, TraceCRL constructs an operation invocation graph where nodes represent service operations and edges represent operation invocations together with predefined features for invocation status and related metrics. Based on the operation invocation graphs of traces TraceCRL uses a contrastive learning method to train a graph neural network-based model for trace representation. In particular, TraceCRL employs six trace data augmentation strategies to alleviate the problems of class collision and uniformity of representation in contrastive learning. Our experimental studies show that TraceCRL can significantly improve the performance of trace anomaly detection and offline trace sampling. It also confirms the effectiveness of the trace augmentation strategies and the efficiency of TraceCRL.","Computing methodologies,Machine learning,Learning paradigms,General and reference,Information systems,Information systems applications,Software and its engineering,Software creation and management,Software organization and properties,Software functional properties,Formal methods,Dynamic analysis,Software system structures,Distributed systems organizing principles,Cloud computing",TraceCRL：用于微服务跟踪分析的对比表示学习,由于跟踪数据量大、复杂度高，异常检测、故障诊断、基于尾部采样等微服务跟踪分析任务广泛采用机器学习技术。这些痕迹分析方法通常使用预处理步骤，以特定的方式将痕迹的结构化特征映射到向量表示。因此，它们可能会丢失重要信息，例如服务操作之间的拓扑依赖关系。在本文中，我们提出了TraceCRL，这是一种基于对比学习和图神经网络的痕迹表示学习方法，可以在下游的痕迹分析任务中加入图结构信息。给定跟踪，TraceCRL构建一个操作调用图，其中节点表示服务操作，边表示操作调用，以及调用状态和相关度量的预定义特性。基于跟踪的操作调用图，TraceCRL使用对比学习方法来训练基于图神经网络的跟踪表示模型。特别是，TraceCRL采用了六种跟踪数据增强策略来缓解对比学习中的类冲突和表示一致性问题。我们的实验研究表明，TraceCRL可以显著提高跟踪异常检测和离线跟踪采样的性能。它还证实了跟踪增强策略的有效性和TraceCRL的效率。,计算方法论，机器学习，学习范例，概论和参考，信息系统，信息系统应用，软件及其工程，软件创建和管理，软件组织和属性，软件功能属性，形式化方法，动态分析，软件体系结构，分布式系统组织原理，云计算,,,
SXD6H9H6,2022,https://doi.org/10.1145/3540250.3558934,ESEC/FSE 2022,iTiger: an automatic issue title generation tool,"In both commercial and open-source software, bug reports or issues are used to track bugs or feature requests. However, the quality of issues can differ a lot. Prior research has found that bug reports with good quality tend to gain more attention than the ones with poor quality. As an essential component of an issue, title quality is an important aspect of issue quality. Moreover, issues are usually presented in a list view, where only the issue title and some metadata are present. In this case, a concise and accurate title is crucial for readers to grasp the general concept of the issue and facilitate the issue triaging. Previous work formulated the issue title generation task as a one-sentence summarization task. A sequence-to-sequence model was employed to solve this task. However, it requires a large amount of domain-specific training data to attain good performance in issue title generation. Recently, pre-trained models, which learned knowledge from large-scale general corpora, have shown much success in software engineering tasks. ","Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems,Software maintenance tools",iTiger：一个自动生成问题标题的工具,在商业软件和开源软件中，错误报告或问题都用于跟踪错误或功能请求。然而，问题的质量可能会有很大差异。先前的研究发现，质量好的错误报告往往比质量差的报告获得更多的关注。标题质量作为期刊的重要组成部分，是期刊质量的一个重要方面。此外，问题通常以列表视图显示，其中只显示问题标题和一些元数据。在这种情况下，一个简洁准确的标题对于读者掌握问题的总体概念和方便问题的审理至关重要。以前的工作将问题标题生成任务定义为一句话的摘要任务。采用序列到序列的模型来解决这一任务。然而，它需要大量特定领域的训练数据才能在问题标题生成中获得良好的性能。最近，从大规模通用语料库中学习知识的预训练模型在软件工程任务中取得了很大成功。,社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发后问题，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统，软件维护工具,,,
UK8JZBNZ,2022,https://doi.org/10.1145/3540250.3558917,ESEC/FSE 2022,Python-by-contract dataset,"Design-by-contract as a programming technique is becoming popular in Python community as various tools have been developed for automatically testing the code based on the contracts. However, there  
is no sufficiently large and representative Python code base with contracts to evaluate these different testing tools. We present Python-by-contract dataset containing 514 Python functions annotated with contracts using icontract library. We show that our Python-by-contract dataset can be easily used by existing testing tools that take advantage of contracts. The demo video can be found at https://youtu.be/08wZN-xh6mY.","General and reference,Cross-computing tools and techniques,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software defect analysis,Software testing and debugging,Software notations and tools,Software maintenance tools,Software organization and properties",Python按合约数据集,合约设计作为一种编程技术在Python社区中越来越流行，因为已经开发了各种工具来自动测试基于合约的代码。然而,一般和参考资料，交叉计算工具和技术，软件及其工程，软件创建和管理，软件验证和确认，正式软件验证，软件缺陷分析，软件测试和调试，软件符号和工具，软件维护工具，软件组织和属性,,,
GPHQC3RT,2022,https://doi.org/10.1145/3540250.3549117,ESEC/FSE 2022,Corporate dominance in open source ecosystems: a case study of OpenStack,"Corporate participation plays an increasing role in Open Source Software (OSS) development. Unlike volunteers in OSS projects, companies are driven by business objectives. To pursue corporate interests, companies may try to dominate the development direction of OSS projects. One company's domination in OSS may 'crowd out' other contributors, changing the nature of the project, and jeopardizing the sustainability of the OSS ecosystem. Prior studies of corporate involvement in OSS have primarily focused on predominately positive aspects such as business strategies, contribution models, and collaboration patterns. However, there is a scarcity of research on the potential drawbacks of corporate engagement. In this paper, we investigate corporate dominance in OSS ecosystems. We draw on the field of Economics and quantify company domination using a dominance measure; we investigate the prevalence, patterns, and impact of domination in the evolution of the OpenStack ecosystem. We find evidence of company domination in over 73% of the repositories in OpenStack, and approximately 25% of companies dominate one or more repositories per version. We identify five patterns of corporate dominance: Early incubation, Full-time hosting, Growing domination, Occasional domination, and Last remaining. We find that domination has a significantly negative relationship with the survival probability of OSS projects. This study provides insights for building sustainable relationships between companies and the OSS ecosystems in which they seek to get involved.","Social and professional topics,Professional topics,Management of computing and information systems,Project and people management,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Programming teams,Software notations and tools,Software configuration management and version control systems",企业在开源生态系统中的主导地位：以OpenStack为例,企业参与在开放源码软件（OSS）开发中发挥着越来越大的作用。与OSS项目中的志愿者不同，公司是由商业目标驱动的。为了追求企业利益，公司可能会试图主导OSS项目的发展方向。一家公司在OSS中的主导地位可能会“排挤”其他贡献者，改变项目的性质，并危及OSS生态系统的可持续性。以前对企业参与开放源码软件的研究主要集中在积极的方面，如商业战略、贡献模式和合作模式。然而，关于企业参与的潜在弊端的研究却很少。在本文中，我们研究了企业在OSS生态系统中的主导地位。我们借鉴经济学领域，并使用支配度测度来量化公司支配；我们研究了OpenStack生态系统进化中统治的普遍性、模式和影响。我们在OpenStack中超过73%的存储库中发现了公司主导的证据，每个版本大约有25%的公司主导一个或多个存储库。我们确定了五种企业主导模式：早期孵化、全职托管、成长主导、偶尔主导和最后剩余。我们发现，支配地位与OSS项目的生存概率呈显著负相关。这项研究为公司和他们寻求参与的OSS生态系统之间建立可持续的关系提供了见解。,社会和专业主题，专业主题，计算和信息系统管理，项目和人员管理，软件管理，软件及其工程，软件创建和管理，软件开发中的合作，开放源码模型，编程团队，软件符号和工具，软件配置管理和版本控制系统,,,
T3KLYNKJ,2022,https://doi.org/10.1145/3540250.3549143,ESEC/FSE 2022,Making Python code idiomatic by automatic refactoring non-idiomatic Python code with pythonic idioms,"Compared to other programming languages (e.g., Java), Python has more idioms to make Python code concise and efficient. Although pythonic idioms are well accepted in the Python community, Python programmers are often faced with many challenges in using them, for example, being unaware of certain pythonic idioms or do not know how to use them properly. Based on an analysis of 7,638 Python repositories on GitHub, we find that non-idiomatic Python code that can be implemented with pythonic idioms occurs frequently and widely. Unfortunately, there is no tool for automatically refactoring such non-idiomatic code into idiomatic code. In this paper, we design and implement an automatic refactoring tool to make Python code idiomatic. We identify nine pythonic idioms by systematically contrasting the abstract syntax grammar of Python and Java. Then we define the syntactic patterns for detecting non-idiomatic code for each pythonic idiom. Finally, we devise atomic AST-rewriting operations and refactoring steps to refactor non-idiomatic code into idiomatic code. We test and review over 4,115 refactorings applied to 1,065 Python projects from GitHub, and submit 90 pull requests for the 90 randomly sampled refactorings to 84 projects. These evaluations confirm the high accuracy, practicality and usefulness of our refactoring tool on real-world Python code.","Software and its engineering,Software creation and management,Software post-development issues,Software evolution,Software reverse engineering,Software notations and tools,General programming languages",通过使用Python习语自动重构非惯用Python代码，使Python代码惯用,与其他编程语言（如Java）相比，Python有更多的习语使Python代码简洁高效。尽管Python社区普遍接受Python习惯用法，但Python程序员在使用它们时经常面临许多挑战，例如，不知道某些Python习惯用法或不知道如何正确使用它们。基于对GitHub上7638个Python存储库的分析，我们发现可以用Python习语实现的非惯用Python代码频繁而广泛地出现。不幸的是，没有工具可以自动将这种非惯用代码重构为惯用代码。在本文中，我们设计并实现了一个自动重构工具，以使Python代码惯用。通过系统对比Python和Java的抽象语法，我们确定了九个Python习语。然后，我们定义了用于检测每个Python习语的非惯用代码的语法模式。最后，我们设计了原子AST重写操作和重构步骤，将非惯用代码重构为惯用代码。我们测试和审查了应用于GitHub 1065个Python项目的4115个重构，并向84个项目提交了90个随机抽样重构的拉取请求。这些评估证实了我们的重构工具在现实世界的Python代码上的高准确性、实用性和有用性。,软件及其工程，软件创建和管理，软件开发后问题，软件进化，软件逆向工程，软件符号和工具，通用编程语言,,,
VHT3KQV2,2022,https://doi.org/10.1145/3540250.3549134,ESEC/FSE 2022,Avgust: automating usage-based test generation from videos of app executions,"Writing and maintaining UI tests for mobile apps is a time-consuming and tedious task. While decades of research have produced auto- mated approaches for UI test generation, these approaches typically focus on testing for crashes or maximizing code coverage. By contrast, recent research has shown that developers prefer usage-based tests, which center around specific uses of app features, to help support activities such as regression testing. Very few existing techniques support the generation of such tests, as doing so requires automating the difficult task of understanding the semantics of UI screens and user inputs. In this paper, we introduce Avgust, which automates key steps of generating usage-based tests. Avgust uses neural models for image understanding to process video recordings of app uses to synthesize an app-agnostic state-machine encoding of those uses. Then, Avgust uses this encoding to synthesize test cases for a new target app. We evaluate Avgust on 374 videos of common uses of 18 popular apps and show that 69% of the tests Avgust generates successfully execute the desired usage, and that Avgust’s classifiers outperform the state of the art.","Computing methodologies,Machine learning,Human-centered computing,Ubiquitous and mobile computing,Information systems,Information systems applications,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools",Avgust:从应用程序执行的视频中自动生成基于使用情况的测试,编写和维护移动应用程序的UI测试是一项耗时且乏味的任务。尽管几十年的研究已经产生了用于UI测试生成的自动匹配方法，但这些方法通常侧重于测试崩溃或最大限度地提高代码覆盖率。相比之下，最近的研究表明，开发人员更喜欢以应用程序功能的特定用途为中心的基于使用情况的测试，以帮助支持回归测试等活动。很少有现有技术支持生成这样的测试，因为这样做需要自动化理解UI屏幕和用户输入的语义这一艰巨任务。在本文中，我们介绍了Avgust，它自动化了生成基于使用情况的测试的关键步骤。Avgust使用用于图像理解的神经模型来处理应用程序使用的视频记录，以合成这些使用的与应用程序无关的状态机编码。然后，Avgust使用这种编码来合成新目标应用程序的测试用例。我们对18个流行应用程序的374个常用视频进行了评估，结果表明Avgust生成的测试中有69%成功地执行了所需的用途，Avgust的分类器优于现有技术。,计算方法论，机器学习，以人为中心的计算，无处不在的和移动计算，信息系统，信息系统应用，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具,,,
PKTJT7HG,2022,https://doi.org/10.1145/3540250.3558924,ESEC/FSE 2022,MpBP: verifying robustness of neural networks with multi-path bound propagation,"Robustness of neural networks need be guaranteed in many safety-critical scenarios, such as autonomous driving and cyber-physical controlling. In this paper, we present MpBP, a tool for verifying the robustness of neural networks. MpBP is inspired by classical bound propagation methods for neural network verification, and aims to improve the effectiveness by exploiting the notion of propagation paths. Specifically, MpBP extends classical bound propagation methods, including forward bound propagation, backward bound propagation, and forward+backward bound propagation, with multiple propagation paths.  
MpBP is based on the widely-used PyTorch machine learning framework, hence providing efficient parallel verification on GPUs and user-friendly usage. We evaluate MpBP on neural networks trained on standard datasets MNIST, CIFAR-10 and Tiny ImageNet. The results demonstrate the effectiveness  
advantage of MpBP beyond two state-of-the-art bound propagation tools LiRPA and GPUPoly, with comparable efficiency to LiRPA and significantly higher efficiency than GPUPoly. A video demonstration that showcases the main features of MpBP can be found at https://youtu.be/3KyPMuPpfR8. Source code is available at https://github.com/formes20/MpBP and https://doi.org/10.5281/zenodo.7029261.","Computer systems organization,Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software organization and properties,Software functional properties,Formal methods,Theory of computation",MpBP：用多路径有界传播验证神经网络的鲁棒性,在许多安全关键场景中，如自动驾驶和网络物理控制，需要保证神经网络的鲁棒性。在本文中，我们提出了MpBP，一种用于验证神经网络鲁棒性的工具。MpBP的灵感来自用于神经网络验证的经典有界传播方法，旨在通过利用传播路径的概念来提高有效性。具体而言，MpBP扩展了经典的定界传播方法，包括前向定界传播、后向定界传播和前向+后向绑定传播，具有多个传播路径。,计算机系统组织，计算方法学，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件验证和确认，形式软件验证，软件组织和属性，软件功能属性，形式方法，计算理论,,,
ZBWPP25H,2022,https://doi.org/10.1145/3540250.3559077,ESEC/FSE 2022,A practical call graph construction method for Python,"Python has become one of the most popular programming languages today. Call graph is one of the essential data structures for many applications in software engineering. However, the precision and recall rate of the existing Python call graph construction methods are generally not high enough, which affects their use in practice. This paper proposes PyPt, a static call graph construction method for Python based on flow-insensitive context-insensitve pointer analysis, which can deal with some dynamic features, such as the dynamic resolution of attributes, higher-order functions, etc. This paper compares PyPt with the state-of-the-art call graph tool PyCG on a benchmark containing 99 manually constructed programs and six real-world open-source projects. The results show that PyPt is better than PyCG in both soundness and completeness.","Software and its engineering,Software creation and management,Software verification and validation,Software notations and tools,General programming languages,Software organization and properties,Software functional properties,Formal methods,Automated static analysis,Theory of computation,Semantics and reasoning",一种实用的Python调用图构造方法,Python已经成为当今最流行的编程语言之一。调用图是软件工程中许多应用程序的基本数据结构之一。然而，现有Python调用图构建方法的精度和召回率普遍不够高，影响了它们在实践中的使用。本文提出了一种基于流不敏感上下文不敏感指针分析的Python静态调用图构造方法PyPt，它可以处理一些动态特性，如属性的动态分辨率、高阶函数等。本文将PyPt与最先进的调用图工具PyCG在一个包含99个手动构建程序和6个真实世界开源项目的基准上进行了比较。结果表明，PyPt在稳健性和完备性方面均优于PyCG。,软件及其工程，软件创建和管理，软件验证和确认，软件符号和工具，通用编程语言，软件组织和属性，软件功能属性，形式化方法，自动静态分析，计算理论，语义和推理,,,
PVRA5K6M,2022,https://doi.org/10.1145/3540250.3549107,ESEC/FSE 2022,Minerva: browser API fuzzing with dynamic mod-ref analysis,"Browser APIs are essential to the modern web experience. Due to their large number and complexity, they vastly expand the attack surface of browsers. To detect vulnerabilities in these APIs, fuzzers generate test cases with a large amount of random API invocations. However, the massive search space formed by arbitrary API combinations hinders their effectiveness: since randomly-picked API invocations unlikely interfere with each other (i.e., compute on partially shared data), few interesting API interactions are explored. Consequently, reducing the search space by revealing inter-API relations is a major challenge in browser fuzzing. ","Security and privacy,Software and application security,Software security engineering,Systems security,Browser security,Operating systems security,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties",Minerva:浏览器API模糊与动态mod-ref分析,浏览器API对现代网络体验至关重要。由于它们的数量和复杂性，它们极大地扩展了浏览器的攻击面。为了检测这些API中的漏洞，模糊器通过大量随机调用API来生成测试用例。然而，任意API组合形成的巨大搜索空间阻碍了它们的有效性：由于随机选择的API调用不太可能相互干扰（即在部分共享数据上进行计算），因此很少探索有趣的API交互。因此，通过揭示API之间的关系来减少搜索空间是浏览器模糊化的主要挑战。,安全和隐私，软件和应用程序安全，软件安全工程，系统安全，浏览器安全，操作系统安全，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性,,,
JA98Y9WI,2022,https://doi.org/10.1145/3540250.3558941,ESEC/FSE 2022,Improving ML-based information retrieval software with user-driven functional testing and defect class analysis,"Machine Learning (ML) has become the cornerstone of information retrieval (IR) software, as it can drive better user experience by leveraging information-rich data and complex models. However, evaluating the emergent behavior of ML-based IR software can be challenging with traditional software testing approaches: when developers modify the software, they cannot often extract useful information from individual test instances; rather, they seek to holistically verify whether—and where—their modifications caused significant regressions or improvements at scale. In this paper, we introduce not only such a holistic approach to evaluate the system-level behavior of the software, but also the concept of a defect class, which represents a partition of the input space on which the ML-based software does measurably worse for an existing feature or on which the ML task is more challenging for a new feature. We leverage large volumes of functional test cases, automatically obtained, to derive these defect classes, and propose new ways to improve the IR software from an end-user’s perspective. Applying our approach on a real production Search-AutoComplete system that contains a query interpretation ML component, we demonstrate that (1) our holistic metrics successfully identified two regressions and one improvement, where all 3 were independently verified with retrospective A/B experiments, (2) the automatically obtained defect classes provided actionable insights during early-stage ML development, and (3) we also detected defect classes at the finer sub-component level for which there were significant regressions, which we blocked prior to different releases.","Information systems,Information retrieval,Evaluation of retrieval results,Relevance assessment,Test collections,Software and its engineering,Software creation and management,Software post-development issues,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems",利用用户驱动的功能测试和缺陷类分析改进基于ML的信息检索软件,机器学习（ML）已经成为信息检索（IR）软件的基石，因为它可以通过利用信息丰富的数据和复杂的模型来推动更好的用户体验。然而，用传统的软件测试方法评估基于ML的IR软件的突发行为可能具有挑战性：当开发人员修改软件时，他们通常无法从单个测试实例中提取有用的信息；相反，他们试图全面验证他们的修改是否以及在哪里导致了大规模的显著倒退或改进。在本文中，我们不仅介绍了一种评估软件系统级行为的整体方法，还介绍了缺陷类的概念，缺陷类表示输入空间的一个分区，在该分区上，基于ML的软件对现有特征的表现明显较差，或者对新特征的ML任务更具挑战性。我们利用自动获得的大量功能测试用例来导出这些缺陷类别，并从最终用户的角度提出改进IR软件的新方法。将我们的方法应用于包含查询解释ML组件的实际生产搜索自动完成系统，我们证明：（1）我们的整体指标成功地确定了两个回归和一个改进，其中所有三个都通过回顾性a/B实验进行了独立验证，（2）自动获得的缺陷类在早期ML开发过程中提供了可操作的见解，以及（3）我们还检测到了更精细的子组件级别的缺陷类，这些缺陷类存在显著的回归，我们在不同的发布之前阻止了这些回归。,信息系统，信息检索，检索结果评估，相关性评估，测试集合，软件及其工程，软件创建和管理，软件开发后问题，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统,,,
NFRJRM4I,2022,https://doi.org/10.1145/3540250.3558959,ESEC/FSE 2022,Exploring and evaluating personalized models for code generation,"Large Transformer models achieved the state-of-the-art status for Natural Language Understanding tasks and are increasingly becoming the baseline model architecture for modeling source code. Transformers are usually pre-trained on large unsupervised corpora, learning token representations and transformations relevant to modeling generally available text, and are then fine-tuned on a particular downstream task of interest. While fine-tuning is a tried-and-true method for adapting a model to a new domain -- for example, question-answering on a given topic -- generalization remains an on-going challenge. In this paper, we explore and evaluate transformer model fine-tuning for personalization. In the context of generating unit tests for Java methods, we evaluate learning to personalize to a specific software project using several personalization techniques. We consider three key approaches: (i) custom fine-tuning, which allows all the model parameters to be tuned; (ii) lightweight fine-tuning, which freezes most of the model's parameters, allowing tuning of the token embeddings and softmax layer only or the final layer alone; (iii) prefix tuning, which keeps model parameters frozen, but optimizes a small project-specific prefix vector. Each of these techniques offers a trade-off in total compute cost and predictive performance, which we evaluate by code and task-specific metrics, training time, and total computational operations. We compare these fine-tuning strategies for code generation and discuss the potential generalization and cost benefits of each in various deployment scenarios.","Computing methodologies,Artificial intelligence,Natural language processing,Machine learning,Learning paradigms,Machine learning approaches,Information systems,Information retrieval,Retrieval tasks and goals,Recommender systems,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools",探索和评估用于代码生成的个性化模型,大型Transformer模型在自然语言理解任务中达到了最先进的地位，并越来越成为源代码建模的基线模型体系结构。转换器通常在大型无监督语料库上进行预训练，学习与建模一般可用文本相关的令牌表示和转换，然后在感兴趣的特定下游任务上进行微调。虽然微调是将模型适应新领域的一种行之有效的方法——例如，对给定主题的问答——但泛化仍然是一个持续的挑战。在本文中，我们探索并评估了变压器模型的个性化微调。在为Java方法生成单元测试的上下文中，我们使用几种个性化技术来评估针对特定软件项目的个性化学习。我们考虑三种关键方法：（i）自定义微调，允许对所有模型参数进行微调；（ii）轻量级微调，其冻结模型的大部分参数，允许仅对令牌嵌入和softmax层进行调整或仅对最终层进行调整；（iii）前缀调整，它保持模型参数冻结，但优化小型项目特定的前缀向量。这些技术中的每一种都在总计算成本和预测性能方面进行了权衡，我们通过代码和任务特定的指标、训练时间和总计算操作来评估这些成本和性能。我们比较了代码生成的这些微调策略，并讨论了在各种部署场景中每种策略的潜在通用性和成本效益。,计算方法学，人工智能，自然语言处理，机器学习，学习范式，机器学习方法，信息系统，信息检索，检索任务和目标，推荐系统，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具,,,
TSKWNRV6,2023,https://doi.org/10.1145/3597926.3598104,ISSTA 2023,That's a Tough Call: Studying the Challenges of Call Graph Construction for WebAssembly,"WebAssembly is a low-level bytecode format that powers applications and libraries running in browsers, on the server side, and in standalone runtimes. Call graphs are at the core of many interprocedural static analysis and optimization techniques. However, WebAssembly poses some unique challenges for static call graph construction. Currently, these challenges are neither well understood, nor is it clear to what extent existing techniques address them. This paper presents the first systematic study of WebAssembly-specific challenges for static call graph construction and of the state-of-the-art in call graph analysis. We identify and classify 12 challenges, encode them into a suite of 24 executable microbenchmarks, and measure their prevalence in real-world binaries. These challenges reflect idiosyncrasies of WebAssembly, such as indirect calls via a mutable function table, interactions with the host environment, and unmanaged linear memory. We show that they commonly occur across a set of more than 8,000 real-world binaries. Based on our microbenchmarks and a set of executable real-world binaries, we then study the soundness and precision of four existing static analyses. Our findings include that, surprisingly, all of the existing techniques are unsound, without this being documented anywhere. We envision our work to provide guidance for improving static call graph construction for WebAssembly. In particular, the presented microbenchmarks will enable future work to check whether an analysis supports challenging language features, and to quantify its soundness and precision.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Formal methods,Automated static analysis",这是一个艰难的决定：研究WebAssembly调用图构建的挑战,WebAssembly是一种低级字节码格式，为在浏览器、服务器端和独立运行时运行的应用程序和库提供动力。调用图是许多过程间静态分析和优化技术的核心。然而，WebAssembly对静态调用图构建提出了一些独特的挑战。目前，这些挑战既没有得到很好的理解，也不清楚现有技术在多大程度上解决了这些挑战。本文首次系统地研究了WebAssembly对静态调用图构建的特定挑战以及调用图分析的最新技术。我们识别并分类了12个挑战，将它们编码为一套24个可执行的微基准，并测量它们在现实世界二进制文件中的流行率。这些挑战反映了WebAssembly的特性，例如通过可变函数表的间接调用、与主机环境的交互以及非托管线性内存。我们展示了它们通常发生在一组8000多个真实世界的二进制文件中。基于我们的微基准和一组可执行的现实世界二进制文件，我们研究了四种现有静态分析的可靠性和准确性。我们的发现包括，令人惊讶的是，所有现有的技术都是不健全的，而这在任何地方都没有记录在案。我们设想我们的工作为改进WebAssembly的静态调用图构建提供指导。特别是，所提出的微基准将使未来的工作能够检查分析是否支持具有挑战性的语言特征，并量化其可靠性和准确性。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，形式化方法，自动静态分析,,,
6E6E6MY7,2023,https://doi.org/10.1145/3597926.3598054,ISSTA 2023,API2Vec: Learning Representations of API Sequences for Malware Detection,"Analyzing malware based on API call sequence is an effective approach as the sequence reflects the dynamic execution behavior of malware.Recent advancements in deep learning have led to the application of these techniques for mining useful information from API call sequences. However, these methods mainly operate on raw sequences and may not effectively capture important information especially for multi-process malware, mainly due to the API call interleaving problem.  
","Security and privacy,Intrusion/anomaly detection and malware mitigation,Malware and its mitigation,Software and application security,Software security engineering",API2Verc:用于恶意软件检测的API序列的学习表示,基于API调用序列分析恶意软件是一种有效的方法，因为序列反映了恶意软件的动态执行行为。深度学习的最新进展导致了这些技术在从API调用序列中挖掘有用信息方面的应用。然而，这些方法主要对原始序列进行操作，可能无法有效捕获重要信息，尤其是对于多进程恶意软件，这主要是由于API调用交织问题。,安全和隐私，入侵/异常检测和恶意软件缓解，恶意软件及其缓解，软件和应用程序安全，软件安全工程,,,
6JHJ8BF6,2023,https://doi.org/10.1145/3597926.3598114,ISSTA 2023,AGORA: Automated Generation of Test Oracles for REST APIs,"Test case generation tools for REST APIs have grown in number and complexity in recent years. However, their advanced capabilities for automated input generation contrast with the simplicity of their test oracles, which limit the types of failures they can detect to crashes, regressions, and violations of the API specification or design best practices. In this paper, we present AGORA, an approach for the automated generation of test oracles for REST APIs through the detection of invariants—properties of the output that should always hold. In practice, AGORA aims to learn the expected behavior of an API by analyzing previous API requests and their corresponding responses. For this, we extended the Daikon tool for dynamic detection of likely invariants, including the definition of new types of invariants and the implementation of an instrumenter called Beet. Beet converts any OpenAPI specification and a collection of API requests and responses to a format processable by Daikon. As a result, AGORA currently supports the detection of up to 105 different types of invariants in REST APIs. AGORA achieved a total precision of 81.2% when tested on a dataset of 11 operations from 7 industrial APIs. More importantly, the test oracles generated by AGORA detected 6 out of every 10 errors systematically seeded in the outputs of the APIs under test. Additionally, AGORA revealed 11 bugs in APIs with millions of users: Amadeus, GitHub, Marvel, OMDb and YouTube. Our reports have guided developers in improving their APIs, including bug fixes and documentation updates in GitHub. Since it operates in black-box mode, AGORA can be seamlessly integrated into existing API testing tools.","Computer systems organization,Dependable and fault-tolerant systems and networks,Redundancy,Embedded and cyber-physical systems,Embedded systems,Robotics,Networks,Network properties,Network reliability",AGORA：REST API测试Oracle的自动生成,近年来，RESTAPI的测试用例生成工具的数量和复杂性都在增长。然而，他们自动化输入生成的高级功能与其测试预言器的简单性形成了鲜明对比，后者将检测到的故障类型限制为崩溃、回归和违反API规范或设计最佳实践。在本文中，我们介绍了AGORA，这是一种通过检测不变量自动生成RESTAPI测试预言器的方法，不变量是应该始终保持的输出属性。在实践中，AGORA旨在通过分析以前的API请求及其相应响应来了解API的预期行为。为此，我们扩展了Daikon工具，用于动态检测可能的不变量，包括定义新类型的不变量和实现一个名为Beet的工具。Beet将任何OpenAPI规范以及API请求和响应的集合转换为Daikon可处理的格式。因此，AGORA目前支持在RESTAPI中检测多达105种不同类型的不变量。AGORA在7种工业原料药的11种操作数据集上进行测试时，总精度达到81.2%。更重要的是，AGORA生成的测试预言机在被测试API的输出中系统地检测到每10个错误中就有6个。此外，AGORA披露了拥有数百万用户的API中的11个漏洞：Amadeus、GitHub、Marvel、OMDb和YouTube。我们的报告指导开发人员改进他们的API，包括GitHub中的错误修复和文档更新。由于AGORA在black-box模式下运行，因此可以无缝集成到现有的API测试工具中。,计算机系统组织，可靠和容错系统和网络，冗余，嵌入式和网络物理系统，嵌入式系统，机器人，网络，网络特性，网络可靠性,,,
KKJFZJXT,2023,https://doi.org/10.1145/3597926.3598118,ISSTA 2023,Finding Short Slow Inputs Faster with Grammar-Based Search,"Recent research has shown that mutational search with appropriate instrumentation can generate short inputs that demonstrate performance issues. Another thread of fuzzing research has shown that substituting subtrees from a forest of derivation trees is an effective grammar-based fuzzing technique for finding deep semantic bugs. We combine performance fuzzing with grammar-based search by generating length-limited derivation trees in which each subtree is labeled with its length. In addition we use performance instrumentation feedback to guide search. In contrast to fuzzing for security issues, for which fuzzing campaigns of many hours or even weeks can be appropriate, we focus on searches that are short enough (up to an hour with modest computational resources) to be part of a routine incremental test process. We have evaluated combinations of these approaches, with baselines including the best prior performance fuzzer. No single search technique dominates across all examples, but both Monte Carlo tree search and length-limited tree hybridization perform consistently well on example applications in which semantic performance bugs can be found with syntactically correct input. In the course of our evaluation we discovered a hang bug in LunaSVG, which the developers have acknowledged and corrected.","Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software performance",使用基于语法的搜索更快地找到短而慢的输入,最近的研究表明，使用适当的仪器进行突变搜索可以生成短输入，从而证明性能问题。模糊化研究的另一个线索表明，从派生树森林中替换子树是一种有效的基于语法的模糊化技术，可以发现深层语义缺陷。我们通过生成长度有限的派生树，将性能模糊与基于语法的搜索相结合，其中每个子树都标有其长度。此外，我们使用性能检测反馈来指导搜索。与针对安全问题的模糊化相比，针对安全问题进行数小时甚至数周的模糊化活动是合适的，我们专注于足够短的搜索（最多一小时，计算资源适中），可以作为常规增量测试过程的一部分。我们评估了这些方法的组合，基线包括最佳先验性能模糊器。没有一种单一的搜索技术在所有示例中占主导地位，但蒙特卡罗树搜索和长度受限树杂交在示例应用程序中始终表现良好，在这些应用程序中，通过语法正确的输入可以发现语义性能缺陷。在我们的评估过程中，我们发现LunaSVG中有一个挂起的错误，开发人员已经确认并纠正了这个错误。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，额外功能属性，软件性能,,,
SLUKT3PN,2023,https://doi.org/10.1145/3597926.3604925,ISSTA 2023,TreeLine and SlackLine: Grammar-Based Performance Fuzzing on Coffee Break,"TreeLine and SlackLine are grammar-based fuzzers for quickly finding performance problems in programs driven by richly structured text that can be described by context-free grammar. In contrast to long fuzzing campaigns to find (mostly invalid) inputs that trigger security vulnerabilities, TreeLine and SlackLine are designed to search for performance problems in the space of valid inputs in minutes rather than hours. The TreeLine and SlackLine front-ends differ in search strategy (Monte Carlo Tree Search or derivation tree splicing, respectively) but accept the same grammar specifications and rely on a common back-end for instrumented execution. Separation of concerns should facilitate use by other researchers who wish to explore alternatives and extensions of either the front or back ends.","Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software performance",树线和松弛线：基于语法的咖啡休息时的表现模糊,TreeLine和SlackLine是基于语法的模糊器，用于在由可通过上下文无关语法描述的丰富结构文本驱动的程序中快速发现性能问题。与寻找触发安全漏洞的（大部分无效）输入的长时间模糊活动不同，TreeLine和SlackLine旨在在几分钟而不是几小时内搜索有效输入空间中的性能问题。TreeLine和SlackLine前端的搜索策略不同（分别为蒙特卡洛树搜索或派生树拼接），但接受相同的语法规范，并依赖于一个通用的后端来执行指令。关注点的分离应便于其他希望探索前端或后端的替代方案和扩展的研究人员使用。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，额外功能属性，软件性能,,,
UZ8AQ36Y,2023,https://doi.org/10.1145/3597926.3598073,ISSTA 2023,In Defense of Simple Techniques for Neural Network Test Case Selection,"Although deep learning (DL) software has been pervasive in various applications, the brittleness of deep neural networks (DNN) hinders their deployment in many tasks especially high-stake ones. To mitigate the risk accompanied with DL software fault, a variety of DNN testing techniques have been proposed such as test case selection. Among those test case selection or prioritization methods, the uncertainty-based ones such as DeepGini have demonstrated their effectiveness in finding DNN’s faults. Recently, TestRank, a learning based test ranking method has shown their out-performance over simple uncertainty-based test selection methods. However, this is achieved with a more complicated design which needs to train a graph convolutional network and a multi-layer Perceptron. In this paper, we propose a novel and lightweight DNN test selection method to enhance the effectiveness of existing simple ones. Besides the DNN model’s uncertainty on test case itself, we take into account model’s uncertainty on its neighbors. This could diversify the selected test cases and improve the effectiveness of existing uncertainty-based test selection methods. Extensive experiments on 5 datasets demonstrate the effectiveness of our approach.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",为神经网络测试用例选择的简单技术辩护,尽管深度学习（DL）软件在各种应用中都很普遍，但深度神经网络（DNN）的脆弱性阻碍了其在许多任务中的部署，尤其是在高风险任务中。为了降低DL软件故障带来的风险，人们提出了各种DNN测试技术，如测试用例选择。在这些测试用例选择或优先级排序方法中，基于不确定性的方法，如DeepGini，已经证明了它们在发现DNN故障方面的有效性。最近，TestRank，一种基于学习的测试排名方法，已经显示出其优于简单的基于不确定性的测试选择方法的性能。然而，这是通过更复杂的设计来实现的，该设计需要训练图卷积网络和多层感知器。在本文中，我们提出了一种新的轻量级DNN测试选择方法，以提高现有简单方法的有效性。除了DNN模型对测试用例本身的不确定性外，我们还考虑了模型对其邻居的不确定性。这可以使所选测试用例多样化，并提高现有基于不确定性的测试选择方法的有效性。在5个数据集上进行的大量实验证明了我们方法的有效性。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
ZUK8J8RB,2023,https://doi.org/10.1145/3597926.3598055,ISSTA 2023,June: A Type Testability Transformation for Improved ATG Performance,"Strings are universal containers: they are flexible to use, abundant in code, and difficult to test. String-controlled programs are programs that make branching decisions based on string input. Automatically generating valid test inputs for these programs considering only character sequences rather than any underlying string-encoded structures, can be prohibitively expensive.  
We present June, a tool that enables Java developers to expose any present latent string structure to test generation tools. June is an annotation-driven testability transformation and an extensible library, JuneLib, of structured string definitions. The core JuneLib definitions are empirically derived and provide templates for all structured strings in our test set.  
June takes lightly annotated source code and injects code that permits an automated test generator (ATG) to focus on the creation of mutable substrings inside a structured string. Using June costs the developer little, with an average of 2.1 annotations per string-controlled class. June uses standard Java build tools and therefore deploys seamlessly within a Java project.  
By feeding string structure information to an ATG tool, June dramatically reduces wasted effort; branches are effortlessly covered that would otherwise be extremely difficult, or impossible, to cover. This waste reduction both increases and speeds coverage. EvoSuite, for example, achieves the same coverage on June-ed classes in 1 minute, on average, as it does in 9 minutes on the un-June-ed class. These gains increase over time. On our corpus, June-ing a program compresses 24 hours of execution time into ca. 2 hours. We show that many ATG tools can reuse the same June-ed code: a few June annotations, a one-off cost, benefit many different testing regimes.","Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation,Empirical software validation,Software notations and tools,General programming languages",六月：改进ATG性能的类型可测试性转换,字符串是通用容器：它们使用灵活，代码丰富，而且很难测试。字符串控制程序是根据字符串输入做出分支决策的程序。为这些程序自动生成有效的测试输入，只考虑字符序列，而不是任何底层的字符串编码结构，可能会非常昂贵。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认，经验软件验证，软件符号和工具，通用编程语言,,,
Q4UD9G7B,2023,https://doi.org/10.1145/3597926.3598084,ISSTA 2023,Security Checking of Trigger-Action-Programming Smart Home Integrations,"Internet of Things (IoT) has become prevalent in various fields, especially in the context of home automation (HA). To better control HA-IoT devices, especially to integrate several devices for rich smart functionalities, trigger-action programming, such as the If This Then That (IFTTT), has become a popular paradigm. Leveraging it, novice users can easily specify their intent in applets regarding how to control a device/service through another once a specific condition is met. Nevertheless, the users may design IFTTT-style integrations inappropriately, due to lack of security experience or  
unawareness of the security impact of cyber-attacks against individual devices. This has caused financial loss, privacy leakage, unauthorized access and other security issues. To address these problems, this work proposes a systematic framework named MEDIC to model smart home integrations and check their security. It automatically generates models incorporating the service/device behaviors and action rules of the applets, while taking into consideration the external attacks and in-device vulnerabilities. Our approach takes around one second to complete the modeling and checking of one integration. We carried out experiments based on 200 integrations created from a user study and a dataset crawled from ifttt.com. To our great surprise, nearly 83% of these integrations have security issues.","Computer systems organization,Embedded and cyber-physical systems,Security and privacy,Security in hardware,Embedded systems security,Software and its engineering,Software creation and management,Software verification and validation",触发动作编程智能家居集成的安全检查,物联网（IoT）已经在各个领域流行起来，尤其是在家庭自动化（HA）的背景下。为了更好地控制HA物联网设备，特别是集成多个设备以实现丰富的智能功能，触发动作编程，如If This Then That（IFTTT），已成为一种流行的模式。利用它，新手用户可以很容易地在小程序中指定他们的意图，即一旦满足特定条件，如何通过另一个设备/服务来控制设备/服务。然而，由于缺乏安全经验或,计算机系统组织，嵌入式和网络物理系统，安全和隐私，硬件安全，嵌入式系统安全，软件及其工程，软件创建和管理，软件验证和确认,,,
23B6LWRF,2023,https://doi.org/10.1145/3597926.3598148,ISSTA 2023,Improving Spectrum-Based Localization of Multiple Faults by Iterative Test Suite Reduction,"Spectrum-based fault localization (SBFL) works well for single-fault programs but its accuracy decays for increasing fault numbers. We present FLITSR (Fault Localization by Iterative Test Suite Reduction), a novel SBFL extension that improves the localization of a given base metric specifically in the presence of multiple faults. FLITSR iteratively selects reduced versions of the test suite that better localize the individual faults in the system. This allows it to identify and re-rank faults ranked too low by the base metric because they were masked by other program elements. ","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",通过迭代测试集约简改进基于频谱的多故障定位,基于频谱的故障定位（SBFL）适用于单故障程序，但其准确性随着故障数量的增加而下降。我们提出了FLITSR（迭代测试套件缩减故障定位），这是一种新的SBFL扩展，它改进了给定基本度量的定位，特别是在存在多个故障的情况下。FLITSR迭代地选择测试套件的精简版本，以便更好地定位系统中的单个故障。这使得它能够识别并重新排列根据基本度量排名过低的故障，因为它们被其他程序元素屏蔽了。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
9NYGN4XV,2023,https://doi.org/10.1145/3597926.3598051,ISSTA 2023,DeUEDroid: Detecting Underground Economy Apps Based on UTG Similarity,"In recent years, the underground economy is proliferating in the mobile system. These underground economy apps (UEware for short) make profits from providing non-compliant services, especially in sensitive areas (e.g., gambling, porn, loan). Unlike traditional malware, most of them (over 80%) do not have malicious payloads. Due to their unique characteristics, existing detection approaches  
cannot effectively and efficiently mitigate this emerging threat.  
To address this problem, we propose a novel approach to effectively and efficiently detect UEware by considering their UI transition graphs (UTGs). Based on the proposed approach, we design and implement a system, named DeUEDroid, to perform the detection. To evaluate DeUEDroid, we collect 25, 717 apps and build up the first large-scale ground-truth dataset (1, 700 apps) of UEware. The evaluation result based on the ground-truth dataset shows that DeUEDroid can cover new UI features and statically construct precise UTG. It achieves 98.22% detection F1-score and 98.97% classification accuracy, a significantly better performance than the traditional approaches. The evaluation result involving 24, 017 apps demonstrates the effectiveness and efficiency of UEware detection in real-world scenarios. Furthermore, the result also reveals that UEware are prevalent, i.e., 54% apps in the wild and 11% apps in the app stores are UEware. Our work sheds light on the future work of analyzing and detecting UEware. To engage the community, we have made our prototype system and the dataset available online.","Security and privacy,Software and application security,Software security engineering",DeUEDroid：基于UTG相似性的地下经济应用检测,近年来，地下经济在移动系统中激增。这些地下经济应用程序（简称UEware）通过提供不合规服务获利，尤其是在敏感领域（如赌博、色情、贷款）。与传统的恶意软件不同，它们中的大多数（超过80%）没有恶意负载。由于其独特的特性，现有的检测方法,安全和隐私，软件和应用程序安全，软件安全工程,,,
M9AJY254,2023,https://doi.org/10.1145/3597926.3598110,ISSTA 2023,SBDT: Search-Based Differential Testing of Certificate Parsers in SSL/TLS Implementations,"Certificate parsers, which are critical components of Secure Sockets Layer or Transport Layer Security (SSL/TLS) implementations, parse incomprehensible certificates into comprehensible inputs to certificate validators and humans. Thus, certificate parsers profoundly affect decision-makings of validators and humans, which in turn affect security. To guarantee the correctness of certificate parsers, an approach for search-based differential testing of certificate parsers, namely SBDT, is put forward. SBDT begins with modeling certificate structures, mutation operations, and bounds. Based on the initial model, SBDT searches for the most promising model node and mutation operator that trigger discrepancies, and generates a certificate from the node and operator it finds. Then, SBDT feeds the certificate to certificate parsers, and searches for multiple types of discrepancies after normalizing the results output by parsers. Distinct discrepancies are employed as feedback to update and prune the model. SBDT starts the next iteration from the updated and pruned model, unless all nodes and mutation operators have been pruned due to reaching their upper bounds. Our work has the following contributions: (1) To the best of our knowledge, this is the first time that testing of certificate parsers has been clearly distinguished from testing of certificate validators, which will facilitate accurate testing of certificate parsers and validators;	(2) SBDT is the first systematic and efficient approach for differential testing of certificate parsers by searching, updating, and pruning models; and (3) We have implemented an open-source prototype tool of SBDT, and experimental results show that SBDT is effective and efficient in finding new bugs and enhancements of certificate parsers.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",SBDT:SSSL/TLS实现中基于搜索的证书解析器差异测试,证书解析器是安全套接字层或传输层安全（SSL/TLS）实现的关键组件，它将不可理解的证书解析为证书验证器和人员的可理解输入。因此，证书解析器深刻地影响验证器和人员的决策，进而影响安全性。为了保证证书解析器的正确性，提出了一种基于搜索的证书解析器差分测试方法SBDT。SBDT从建模证书结构、变异操作和边界开始。SBDT在初始模型的基础上搜索最有前途的模型节点和触发差异的变异算子，并根据找到的节点和算子生成证书。然后，SBDT将证书提供给证书解析器，并在规范化解析器输出的结果后搜索多种类型的差异。使用明显的差异作为反馈来更新和修剪模型。SBDT从更新和修剪后的模型开始下一次迭代，除非所有节点和变异算子由于达到其上限而被修剪。我们的工作有以下贡献：（1）据我们所知，这是第一次将证书解析器的测试与证书验证器的测试明确区分开来，这将有助于证书解析器和验证器的准确测试；（2）SBDT是第一种通过搜索、更新和修剪模型来对证书解析器进行差分测试的系统而有效的方法；（3）我们实现了SBDT的开源原型工具，实验结果表明，SBDT在发现证书解析器的新缺陷和增强方面是有效的。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
7ARKTQIC,2023,https://doi.org/10.1145/3597926.3598082,ISSTA 2023,DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph Optimization,"The deep learning (DL) compiler serves as a vital infrastructure component to enable the deployment of deep neural networks on diverse hardware platforms such as mobile devices and Raspberry Pi. DL compiler’s primary function is to translate DNN programs written in high-level DL frameworks such as PyTorch and TensorFlow into portable executables. These executables can then be flexibly executed by the deployed host programs. However, existing DL compilers rely on a tracing mechanism, which involves feeding a runtime input to a neural network program and tracing the program execution paths to generate the computational graph necessary for compilation. Unfortunately, this mechanism falls short when dealing with modern dynamic neural networks (DyNNs) that possess varying computational graphs depending on the inputs. Consequently, conventional DL compilers struggle to accurately compile DyNNs into executable code. To address this limitation, we propose DyCL, a general approach that enables any existing DL compiler to successfully compile DyNNs. DyCL tackles the dynamic nature of DyNNs by introducing a compilation mechanism that redistributes the control and data flow of the original DNN programs during the compilation process. Specifically, DyCL develops program analysis and program transformation techniques to convert a dynamic neural network into multiple sub-neural networks. Each sub-neural network is devoid of conditional statements and is compiled independently. Furthermore, DyCL synthesizes a host module that models the control flow of the DyNNs and facilitates the invocation of the sub-neural networks. Our evaluation demonstrates the effectiveness of DyCL, achieving a 100% success rate in compiling all dynamic neural networks. Moreover, the compiled executables generated by DyCL exhibit significantly improved performance, running between 1.12× and 20.21× faster than the original DyNNs executed on general-purpose DL frameworks.","Computing methodologies,Machine learning,Software and its engineering,Software notations and tools,Compilers",DyCL：通过程序重写和图形优化的动态神经网络编译,深度学习（DL）编译器是一个重要的基础设施组件，可以在移动设备和Raspberry Pi等各种硬件平台上部署深度神经网络。DL编译器的主要功能是将用PyTorch和TensorFlow等高级DL框架编写的DNN程序转换为可移植的可执行文件。这些可执行文件然后可以由部署的主机程序灵活地执行。然而，现有的DL编译器依赖于跟踪机制，该机制涉及向神经网络程序提供运行时输入，并跟踪程序执行路径以生成编译所需的计算图。不幸的是，在处理现代动态神经网络（DyNN）时，这种机制达不到要求，因为动态神经网络根据输入具有不同的计算图。因此，传统的DL编译器很难将DyNN准确地编译成可执行代码。为了解决这一限制，我们提出了DyCL，这是一种通用方法，可以使任何现有的DL编译器成功编译DyNN。DyCL通过引入一种编译机制来解决DyNN的动态特性，该机制在编译过程中重新分配原始DNN程序的控制和数据流。具体而言，DyCL开发了程序分析和程序转换技术，将动态神经网络转换为多个子神经网络。每个子神经网络都没有条件语句，并且是独立编译的。此外，DyCL综合了一个主机模块，该模块对DyNN的控制流进行建模，并促进了子神经网络的调用。我们的评估证明了DyCL的有效性，在编译所有动态神经网络方面实现了100%的成功率。此外，由DyCL生成的编译的可执行文件表现出显著的性能改进，运行速度比在通用DL框架上执行的原始DyNN快1.12倍到20.21倍。,计算方法论，机器学习，软件及其工程，软件符号和工具，编译器,,,
25UWXVHP,2023,https://doi.org/10.1145/3597926.3598119,ISSTA 2023,Transforming Test Suites into Croissants,"Software developers often rely on regression testing to ensure that recent changes made to the source code do not introduce bugs. Flaky tests, which non-deterministically pass or fail regardless of any change to the code, can negatively impact the effectiveness of the regression testing. While state-of-the-art is advancing the techniques for test-flakiness detection and mitigation, the community is missing a systematic approach for generating high-quality benchmarks of flaky tests to compare the effectiveness of such techniques. Inspired by the power of mutation testing in evaluating the fault-detection ability of tests, this paper proposes Croissant, a framework for injecting flakiness into the test suites to assess the effectiveness of test-flakiness detection tools in finding these tests. Croissant implements 18 flakiness-inducing mutation operators. We designed these operators to allow controlling the non-determinism involved in flakiness, i.e., making many mutants deterministically pass or fail to observe flaky behavior. Our extensive empirical evaluation of Croissant on the test suites of 15 real-world projects confirms the ability of designed mutation operators to generate high-quality mutants, and their effectiveness in challenging test-flakiness detection tools in revealing flaky tests.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",将测试套房改造成羊角面包,软件开发人员通常依靠回归测试来确保最近对源代码所做的更改不会引入错误。无论代码有什么变化，缺陷测试都会非决定性地通过或失败，这可能会对回归测试的有效性产生负面影响。虽然最先进的技术正在推进测试片状检测和缓解技术，但社区缺乏一种系统的方法来生成片状测试的高质量基准，以比较这些技术的有效性。受突变测试在评估测试故障检测能力方面的力量的启发，本文提出了Croissant，这是一个将片状物注入测试套件的框架，用于评估测试片状物检测工具在发现这些测试时的有效性。Croissant实现了18个薄片诱导突变算子。我们设计了这些算子，以允许控制与片状有关的非决定性，即使许多突变体决定性地通过或未能观察到片状行为。我们在15个真实世界项目的测试套件上对Croissant进行了广泛的实证评估，证实了设计的突变算子产生高质量突变体的能力，以及它们在挑战测试片状检测工具以揭示片状测试方面的有效性。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
M9CC6WCL,2023,https://doi.org/10.1145/3597926.3598062,ISSTA 2023,Detecting Vulnerabilities in Linux-Based Embedded Firmware with SSE-Based On-Demand Alias Analysis,"Although the importance of using static taint analysis to detect taint-style vulnerabilities in Linux-based embedded firmware is widely recognized, existing approaches are plagued by following major limitations: (a) Existing works cannot properly handle indirect call on the path from attacker-controlled sources to security-sensitive sinks, resulting in lots of false negatives. (b) They employ heuristics to identify mediate taint source and it is not accurate enough, which leads to high false positives.  
","Security and privacy,Software and application security,Software security engineering",基于SSE的按需别名分析检测Linux嵌入式固件漏洞,尽管使用静态污点分析来检测基于Linux的嵌入式固件中的污点式漏洞的重要性已被广泛认可，但现有的方法受到以下主要限制的困扰：（a）现有的工作无法正确处理从攻击者控制的源到安全敏感的汇点的路径上的间接调用，导致了大量的假阴性。（b） 他们采用启发式方法来识别中间污染源，但不够准确，这导致了高误报率。,安全和隐私，软件和应用程序安全，软件安全工程,,,
GY3ZYELP,2023,https://doi.org/10.1145/3597926.3598072,ISSTA 2023,BehAVExplor: Behavior Diversity Guided Testing for Autonomous Driving Systems,"Testing Autonomous Driving Systems (ADSs) is a critical task for ensuring the reliability and safety of autonomous vehicles. Existing methods mainly focus on searching for safety violations while the diversity of the generated test cases is ignored, which may generate many redundant test cases and failures. Such redundant failures can reduce testing performance and increase failure analysis costs. In this paper, we present a novel behavior-guided fuzzing technique (BehAVExplor) to explore the different behaviors of the ego vehi- cle (i.e., the vehicle controlled by the ADS under test) and detect diverse violations. Specifically, we design an efficient unsupervised model, called BehaviorMiner, to characterize the behavior of the ego vehicle. BehaviorMiner extracts the temporal features from the given scenarios and performs a clustering-based abstraction to group behaviors with similar features into abstract states. A new test case will be added to the seed corpus if it triggers new behav- iors (e.g., cover new abstract states). Due to the potential conflict between the behavior diversity and the general violation feedback, we further propose an energy mechanism to guide the seed selec- tion and the mutation. The energy of a seed quantifies how good it is. We evaluated BehAVExplor on Apollo, an industrial-level ADS, and LGSVL simulation environment. Empirical evaluation results show that BehAVExplor can effectively find more diverse violations than the state-of-the-art.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",BehAVExlor：自动驾驶系统的行为多样性引导测试,测试自动驾驶系统（ADS）是确保自动驾驶汽车可靠性和安全性的关键任务。现有的方法主要侧重于搜索安全违规行为，而忽略了生成的测试用例的多样性，这可能会生成许多冗余的测试用例和失败。这种冗余故障会降低测试性能并增加故障分析成本。在本文中，我们提出了一种新的行为引导模糊技术（BehAVExplor）来探索自我车辆（即受测试的ADS控制的车辆）的不同行为，并检测各种违规行为。具体来说，我们设计了一个高效的无监督模型，称为BehaviorMiner，来表征自我载体的行为。BehaviorMiner从给定场景中提取时间特征，并执行基于聚类的抽象，将具有相似特征的行为分组为抽象状态。如果新的测试用例触发了新的行为（例如，覆盖新的抽象状态），则它将被添加到种子语料库中。由于行为多样性和一般违规反馈之间的潜在冲突，我们进一步提出了一种能量机制来指导种子选择和突变。种子的能量可以量化它有多好。我们在Apollo、工业级ADS和LGSVL模拟环境中评估了BehAVExlor。经验评估结果表明，BehAVExplor可以有效地发现比最先进的更多样的违规行为。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
9I8WGUS2,2023,https://doi.org/10.1145/3597926.3598039,ISSTA 2023,Icicle: A Re-designed Emulator for Grey-Box Firmware Fuzzing,"Emulation-based fuzzers enable testing binaries without source code and facilitate testing embedded applications where automated execution on the target hardware architecture is difficult and slow. The instrumentation techniques added to extract feedback and guide input mutations towards generating effective test cases is at the core of modern fuzzers. But, modern emulation-based fuzzers have evolved by re-purposing general-purpose emulators; consequently, developing and integrating fuzzing techniques, such as instrumentation methods, is difficult and often added in an ad-hoc manner, specific to an instruction set architecture (ISA). This limits state-of-the-art fuzzing techniques to a few ISAs such as x86/x86-64 or ARM/AArch64; a significant problem for firmware fuzzing of diverse ISAs. ","Security and privacy,Security in hardware,Embedded systems security,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",Icicle：一个重新设计的灰盒固件引信模拟器,基于仿真的模糊器可以在没有源代码的情况下测试二进制文件，并有助于测试在目标硬件架构上自动执行困难且缓慢的嵌入式应用程序。为提取反馈并引导输入突变生成有效测试用例而添加的仪器技术是现代模糊器的核心。但是，现代基于仿真的模糊器是通过重新使用通用仿真器而发展起来的；因此，开发和集成模糊技术（如插装方法）是困难的，并且通常是以特定于指令集体系结构（ISA）的特殊方式添加的。这将最先进的模糊化技术限制在少数ISAs，例如x86/x86-64或ARM/AArch64；这是不同ISAs的固件模糊化的一个重要问题。,安全与隐私，硬件安全，嵌入式系统安全，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
R2KY6R8S,2023,https://doi.org/10.1145/3597926.3598050,ISSTA 2023,Beware of the Unexpected: Bimodal Taint Analysis,"Static analysis is a powerful tool for detecting security vulnerabilities and other programming problems. Global taint tracking, in particular, can spot vulnerabilities arising from complicated data flow across multiple functions. However, precisely identifying which flows are problematic is challenging, and sometimes depends on factors beyond the reach of pure program analysis, such as conventions and informal knowledge. For example, learning that a parameter name of an API function locale ends up in a file path is surprising and potentially problematic. In contrast, it would be completely unsurprising to find that a parameter command passed to an API function execaCommand is eventually interpreted as part of an operating-system command. This paper presents Fluffy, a bimodal taint analysis that combines static analysis, which reasons about data flow, with machine learning, which probabilistically determines which flows are potentially problematic. The key idea is to let machine learning models predict from natural language information involved in a taint flow, such as API names, whether the flow is expected or unexpected, and to inform developers only about the latter. We present a general framework and instantiate it with four learned models, which offer different trade-offs between the need to annotate training data and the accuracy of predictions. We implement Fluffy on top of the CodeQL analysis framework and apply it to 250K JavaScript projects. Evaluating on five common vulnerability types, we find that Fluffy achieves an F1 score of 0.85 or more on four of them across a variety of datasets.","Security and privacy,Software and application security,Web application security,Software and its engineering,Software creation and management,Software verification and validation,Software notations and tools",小心意外：双峰污染分析,静态分析是检测安全漏洞和其他编程问题的强大工具。特别是，全局污染跟踪可以发现跨多个功能的复杂数据流所产生的漏洞。然而，准确识别哪些流有问题是一项挑战，有时取决于纯粹程序分析无法触及的因素，如惯例和非正式知识。例如，了解到API函数区域设置的参数名称最终出现在文件路径中是令人惊讶的，并且可能存在问题。相反，传递给API函数execaCommand的参数命令最终被解释为操作系统命令的一部分，这完全不足为奇。本文介绍了Fluffy，这是一种双峰污染分析，它结合了静态分析和机器学习，静态分析是数据流的原因，机器学习可能确定哪些流可能存在问题。关键思想是让机器学习模型从污染流中涉及的自然语言信息（如API名称）中预测流是预期的还是意外的，并只通知开发人员后者。我们提出了一个通用框架，并用四个学习模型对其进行了实例化，这些模型在注释训练数据的需要和预测的准确性之间提供了不同的权衡。我们在CodeQL分析框架的基础上实现了Fluffy，并将其应用于250K JavaScript项目。通过对五种常见漏洞类型的评估，我们发现Fluffy在各种数据集中的四种漏洞类型的F1得分达到0.85或更高。,安全和隐私，软件和应用程序安全，Web应用程序安全，软件及其工程，软件创建和管理，软件验证和确认，软件符号和工具,,,
4ZLBGMTI,2023,https://doi.org/10.1145/3597926.3598123,ISSTA 2023,Guided Retraining to Enhance the Detection of Difficult Android Malware,"The popularity of Android OS has made it an appealing target for malware developers. To evade detection, including by ML-based techniques, attackers invest in creating malware that closely resemble legitimate apps, challenging the state of the art with difficult-to-detect samples. In this paper, we propose Guided Retraining, a supervised representation learning-based method for boosting the performance of malware detectors. To that end, we first split the experimental dataset into subsets of “easy” and “difficult” samples, where difficulty is associated to the prediction probabilities yielded by a malware detector. For the subset of “easy” samples, the base malware detector is used to make the final predictions since the error rate on that subset is low by construction. Our work targets the second subset containing “difficult” samples, for which the probabilities are such that the classifier is not confident on the predictions, which have high error rates. We apply our Guided Retraining method on these difficult samples to improve their classification. Guided Retraining leverages the correct predictions and the errors made by the base malware detector to guide the retraining process. Guided Retraining learns new embeddings of the difficult samples using Supervised Contrastive Learning and trains an auxiliary classifier for the final predictions. We validate our method on four state-of-the-art Android malware detection approaches using over 265k malware and benign apps. Experimental results show that Guided Retraining can boost state-of-the-art detectors by eliminating up to 45.19% of the prediction errors that they make on difficult samples. We note furthermore that our method is generic and designed to enhance the performance of binary classifiers for other tasks beyond Android malware detection.","Computing methodologies,Machine learning,Security and privacy,Intrusion/anomaly detection and malware mitigation,Malware and its mitigation",引导式再培训可增强对Android疑难恶意软件的检测,安卓操作系统的流行使其成为恶意软件开发者的一个有吸引力的目标。为了逃避检测，包括通过基于ML的技术，攻击者投资创建与合法应用程序非常相似的恶意软件，用难以检测的样本挑战现有技术。在本文中，我们提出了引导式再训练，这是一种基于监督表示学习的方法，用于提高恶意软件检测器的性能。为此，我们首先将实验数据集划分为“容易”和“困难”样本的子集，其中困难与恶意软件检测器产生的预测概率有关。对于“简单”样本的子集，由于该子集的错误率较低，因此使用基本恶意软件检测器进行最终预测。我们的工作针对的是包含“困难”样本的第二个子集，对于这些样本，概率使得分类器对具有高错误率的预测不自信。我们将我们的引导式再培训方法应用于这些困难的样本，以改进它们的分类。引导式再培训利用基本恶意软件检测器的正确预测和错误来指导再培训过程。引导式再训练使用监督对比学习来学习困难样本的新嵌入，并为最终预测训练辅助分类器。我们使用超过26.5万个恶意软件和良性应用程序，在四种最先进的安卓恶意软件检测方法上验证了我们的方法。实验结果表明，引导式再训练可以消除最先进的检测器在困难样本上产生的45.19%的预测误差，从而提高检测器的性能。我们还注意到，我们的方法是通用的，旨在增强二进制分类器在Android恶意软件检测之外的其他任务中的性能。,计算方法，机器学习，安全和隐私，入侵/异常检测和恶意软件缓解，恶意软件及其缓解,,,
N6B33RJN,2023,https://doi.org/10.1145/3597926.3598067,ISSTA 2023,Large Language Models Are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models,"Deep Learning (DL) systems have received exponential growth in popularity and have become ubiquitous in our everyday life. Such systems are built on top of popular DL libraries, e.g., TensorFlow and PyTorch which provide APIs as building blocks for DL systems. Detecting bugs in these DL libraries is critical for almost all downstream DL systems in ensuring effectiveness/safety for end users. Meanwhile, traditional fuzzing techniques can be hardly effective for such a challenging domain since the input DL programs need to satisfy both the input language (e.g., Python) syntax/semantics and the DL API input/shape constraints for tensor computations.  
To address these limitations, we propose TitanFuzz – the first approach to directly leveraging Large Language Models (LLMs) to generate input programs for fuzzing DL libraries. LLMs are titanic models trained on billions of code snippets and can autoregressively generate human-like code snippets. Our key insight is that modern LLMs can also include numerous code snippets invoking DL library APIs in their training corpora, and thus can implicitly learn both language syntax/semantics and intricate DL API constraints for valid DL program generation. More specifically, we use both generative and infilling LLMs (e.g., Codex/InCoder) to generate and mutate valid/diverse input DL programs for fuzzing. Our experimental results demonstrate that TitanFuzz can achieve 30.38%/50.84% higher code coverage than state-of-the-art fuzzers on TensorFlow/PyTorch. Furthermore, TitanFuzz is able to detect 65 bugs, with 44 already confirmed as previously unknown bugs.  
This paper demonstrates that modern titanic LLMs can be leveraged to directly perform both generation-based and mutation-based fuzzing studied for decades, while being fully automated, generalizable, and applicable to domains challenging for traditional approaches (such as DL systems). We hope TitanFuzz can stimulate more work in this promising direction of LLMs for fuzzing.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software reliability",大型语言模型是零样本模糊器：通过大型语言模型模糊深层学习库,深度学习（DL）系统的受欢迎程度呈指数级增长，并在我们的日常生活中无处不在。这样的系统建立在流行的DL库之上，例如TensorFlow和PyTorch，它们提供API作为DL系统的构建块。在确保最终用户的有效性/安全性方面，检测这些DL库中的错误对于几乎所有下游DL系统都至关重要。同时，由于输入DL程序需要满足输入语言（例如Python）语法/语义和用于张量计算的DL API输入/形状约束，因此传统的模糊化技术对于这样一个具有挑战性的领域很难有效。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，额外功能属性，软件可靠性,,,
UK9GCZHE,2023,https://doi.org/10.1145/3597926.3598035,ISSTA 2023,CONCORD: Clone-Aware Contrastive Learning for Source Code,"Deep Learning (DL) models to analyze source code have shown immense promise during the past few years.  
More recently, self-supervised pre-training has gained traction for learning generic code representations valuable for many downstream SE tasks, such as clone and bug detection.  
","Computing methodologies,Artificial intelligence,Knowledge representation and reasoning,Software and its engineering,Software notations and tools,General programming languages,Language features",CONCORD:源代码的克隆感知对比学习,分析源代码的深度学习（DL）模型在过去几年中显示出了巨大的前景。,计算方法论，人工智能，知识表示和推理，软件及其工程，软件符号和工具，通用编程语言，语言特性,,,
USGZQ8MS,2023,https://doi.org/10.1145/3597926.3598048,ISSTA 2023,CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation,"General-purpose code generation aims to automatically convert the natural language description to code snippets in a general-purpose programming language (GPL) such as Python. In the process of code generation, it is essential to guarantee the generated code satisfies grammar constraints of GPL. However, existing sequence-to-sequence (Seq2Seq) approaches neglect grammar rules when generating GPL code. In this paper, we devise a pushdown automaton (PDA)-based methodology to make the first attempt to consider grammatical Seq2Seq models for general-purpose code generation, exploiting the principle that PL is a subset of PDA recognizable language and code accepted by PDA is grammatical. Specifically, we construct a PDA module and design an algorithm to constrain the generation of Seq2Seq models to ensure grammatical correctness. Guided by this methodology, we further propose CODEP, a code generation framework equipped with a PDA module, to integrate the deduction of PDA into deep learning. This framework leverages the state of PDA deduction (including state representation, state prediction task, and joint prediction with state) to assist models in learning PDA deduction. To comprehensively evaluate CODEP, we construct a PDA for Python and conduct extensive experiments on four public benchmark datasets. CODEP can employ existing sequence-based models as base models, and we show that it achieves 100% grammatical correctness percentage on these benchmark datasets. Consequently, CODEP relatively improves 17% CodeBLEU on CONALA, 8% EM on DJANGO, and 15% CodeBLEU on JUICE-10K compared to base models. Moreover, PDA module also achieves significant improvements on the pre-trained models.","Computing methodologies,Artificial intelligence,Software and its engineering,Software creation and management",CODEP：用于通用代码生成的语法Seq2Seq模型,通用代码生成旨在自动将自然语言描述转换为通用编程语言（GPL）（如Python）中的代码片段。在代码生成过程中，必须保证生成的代码满足GPL的语法约束。然而，现有的序列对序列（Seq2Seq）方法在生成GPL代码时忽略了语法规则。在本文中，我们设计了一种基于下推自动机（PDA）的方法，首次尝试考虑用于通用代码生成的语法Seq2Seq模型，利用PL是PDA可识别语言的子集并且PDA接受的代码是语法的这一原理。具体来说，我们构建了一个PDA模块，并设计了一个算法来约束Seq2Seq模型的生成，以确保语法正确性。在这种方法论的指导下，我们进一步提出了CODEP，一个配备PDA模块的代码生成框架，将PDA的推导集成到深度学习中。该框架利用PDA推导的状态（包括状态表示、状态预测任务以及与状态的联合预测）来帮助模型学习PDA推导。为了全面评估CODEP，我们为Python构建了一个PDA，并在四个公共基准数据集上进行了广泛的实验。CODEP可以使用现有的基于序列的模型作为基础模型，我们证明它在这些基准数据集上实现了100%的语法正确率。因此，与基本模型相比，CODEP在CONALA上相对提高了17%的CodeBLEU，在DJANGO上提高了8%的EM，在JUICE-10K上提高了15%的CodeBLEU。此外，PDA模块还对预先训练的模型进行了显著改进。,计算方法论，人工智能，软件及其工程，软件创建和管理,,,
SKQRY9CQ,2023,https://doi.org/10.1145/3597926.3605231,ISSTA 2023,Quantitative Robustness Analysis of Neural Networks,"Neural networks are an increasingly common tool for solving problems that require complex analysis and pattern matching, such as identifying stop signs or processing medical imagery. Accordingly, verification of neural networks for safety and correctness is of great importance, as mispredictions can have catastrophic results in safety critical domains. One metric for verification is robustness, which answers whether or not a misclassified input exists in a given input neighborhood. I am focusing my research at quantitative robustness---finding not only if there exist misclassified inputs within a given neighborhood but also how many exist as a proportion of the neighborhood size. My overall goal is to expand the research on quantitative neural network robustness verification and create a variety of quantitative verification tools geared towards expanding our understanding of neural network robustness.","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software defect analysis,Software testing and debugging",神经网络的定量鲁棒性分析,神经网络是一种越来越常见的工具，用于解决需要复杂分析和模式匹配的问题，例如识别停车标志或处理医学图像。因此，验证神经网络的安全性和正确性非常重要，因为预测失误可能会在安全关键领域产生灾难性后果。验证的一个指标是稳健性，它回答在给定的输入邻域中是否存在错误分类的输入。我的研究重点是定量稳健性——不仅要找出给定邻域内是否存在错误分类的输入，还要找出有多少输入与邻域大小成比例。我的总体目标是扩大对定量神经网络稳健性验证的研究，并创建各种定量验证工具，以扩大我们对神经网络稳健性的理解。,计算方法论，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件验证和确认，正式软件验证，软件缺陷分析，软件测试和调试,,,
KF2TS9TV,2023,https://doi.org/10.1145/3597926.3598090,ISSTA 2023,To Kill a Mutant: An Empirical Study of Mutation Testing Kills,"Mutation testing has been used and studied for over four decades as a method to assess the strength of a test suite. This technique adds an artificial bug (i.e., a mutation) to a program to produce a mutant, and the test suite is run to determine if any of its test cases are sufficient to detect this mutation (i.e., kill the mutant). In this situation, a test case that fails is the one that kills the mutant. However, little is known about the nature of these kills. In this paper, we present an empirical study that investigates the nature of these kills. We seek to answer questions, such as: How are test cases failing so that they contribute to mutant kills? How many test cases fail for each killed mutant, given that only a single failure is required to kill that mutant? How do program crashes contribute to kills, and what are the origins and nature of these crashes? We found several revealing results across all subjects, including the substantial contribution of ""crashes"" to test failures leading to mutant kills, the existence of diverse causes for test failures even for a single mutation, and the specific types of exceptions that commonly instigate crashes. We posit that this study and its results should likely be taken into account for practitioners in their use of mutation testing and interpretation of its mutation score, and for researchers who study and leverage mutation testing in their future work.","Software and its engineering,Software creation and management,Software verification and validation",杀死一个突变株：突变测试致死的实证研究,突变测试作为一种评估测试套件强度的方法已经使用和研究了40多年。这项技术在程序中添加了一个人工错误（即突变）来产生突变，并运行测试套件来确定其任何测试用例是否足以检测到这种突变（即杀死突变）。在这种情况下，失败的测试用例就是杀死突变体的测试用例。然而，人们对这些杀戮的性质知之甚少。在本文中，我们提出了一项实证研究，调查这些死亡的性质。我们试图回答一些问题，比如：测试病例是如何失败的，从而导致变异株死亡的？考虑到只需要一次失败就可以杀死每个被杀死的突变体，有多少测试案例失败？程序崩溃是如何导致死亡的，这些崩溃的起源和性质是什么？我们在所有受试者中发现了几个具有启发性的结果，包括“崩溃”对导致突变死亡的测试失败的重大贡献，即使是单个突变也存在测试失败的多种原因，以及通常引发崩溃的特定类型的异常。我们认为，从业者在使用突变测试和解释其突变评分时，以及在未来工作中研究和利用突变测试的研究人员，可能应该考虑这项研究及其结果。,软件及其工程，软件创建和管理，软件验证和确认,,,
WFGH3U2S,2023,https://doi.org/10.1145/3597926.3598078,ISSTA 2023,Quantitative Policy Repair for Access Control on the Cloud,"With the growing prevalence of cloud computing, providing secure access to information stored in the cloud has become a critical problem. Due to the complexity of access control policies, administrators may inadvertently allow unintended access to private information, and this is a common source of data breaches in cloud based services. In this paper, we present a quantitative symbolic analysis approach for automated policy repair in order to fix overly permissive policies. We encode the semantics of the access control policies using SMT formulas and assess their permissiveness using model counting. Given a policy, a permissiveness bound, and a set of requests that should be allowed, we iteratively repair the policy through permissiveness reduction and refinement, so that the permissiveness bound is reached while the given set of requests are still allowed. We demonstrate the effectiveness of our automated policy repair technique by applying it to policies written in Amazon's AWS Identity and Access Management (IAM) policy language.","Security and privacy,Formal methods and theory of security,Logic and verification,Security services,Access control",云访问控制的量化策略修复,随着云计算的日益普及，提供对存储在云中的信息的安全访问已成为一个关键问题。由于访问控制策略的复杂性，管理员可能会无意中允许对私人信息进行意外访问，这是基于云的服务中数据泄露的常见来源。在本文中，我们提出了一种用于自动策略修复的定量符号分析方法，以修复过于宽松的策略。我们使用SMT公式对访问控制策略的语义进行编码，并使用模型计数来评估其允许性。给定一个策略、一个允许性边界和一组应该被允许的请求，我们通过减少和细化允许性来迭代修复策略，从而在给定的请求集仍然被允许的情况下达到允许性边界。我们通过将自动策略修复技术应用于用亚马逊AWS身份和访问管理（IAM）策略语言编写的策略，展示了该技术的有效性。,安全与隐私，安全的形式化方法与理论，逻辑与验证，安全服务，访问控制,,,
ZWLKLWR3,2023,https://doi.org/10.1145/3597926.3598115,ISSTA 2023,Fuzzing Embedded Systems using Debug Interfaces,"Fuzzing embedded systems is hard. Their key components – microcontrollers – are highly diverse and cannot be easily virtualized; their software may not be changed or instrumented. However, we observe that many, if not most, microcontrollers feature a debug interface through which a debug probe (typically controllable via GDB, the GNU debugger) can set a limited number of hardware breakpoints. Using these, we extract partial coverage feedback even for uninstrumented binary code; and thus enable effective fuzzing for embedded systems through a generic, widespread mechanism. In its evaluation on four different microcontroller boards, our prototypical implementation GDBFuzz quickly reaches high code coverage and detects known and new vulnerabilities. As it can be applied to any program and system that GDB can debug, GDBFuzz is one of the least demanding and most versatile coverage-guided fuzzers.","Security and privacy,Security in hardware,Embedded systems security,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",使用调试接口对嵌入式系统进行引信化,模糊化嵌入式系统很难。它们的关键组件——微控制器——高度多样化，无法轻松虚拟化；其软件可能不会被更改或插入指令。然而，我们观察到，许多（如果不是大多数的话）微控制器都具有调试接口，调试探针（通常可通过GDB、GNU调试器控制）可以通过该接口设置有限数量的硬件断点。使用这些，我们提取部分覆盖反馈，即使是对于未经检测的二进制代码；从而能够通过通用的、广泛的机制对嵌入式系统进行有效的模糊处理。在四个不同的微控制器板上进行评估时，我们的原型实现GDBFuzz快速达到高代码覆盖率，并检测已知和新的漏洞。由于GDBFuzz可以应用于GDB可以调试的任何程序和系统，因此它是要求最低、用途最广泛的覆盖引导模糊器之一。,安全与隐私，硬件安全，嵌入式系统安全，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
WQVMG9IU,2023,https://doi.org/10.1145/3597926.3598130,ISSTA 2023,GrayC: Greybox Fuzzing of Compilers and Analysers for C,"Fuzzing of compilers and code analysers has led to a large number of  
bugs being found and fixed in widely-used frameworks such as LLVM, GCC  
and Frama-C. Most such fuzzing techniques have taken  
a blackbox approach, with compilers and code analysers starting to  
become relatively immune to such fuzzers.  
","Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Compilers",GrayC:C编译程序和分析器的灰盒模糊处理,编译器和代码分析器的模糊化导致了大量,软件及其工程，软件创建和管理，软件开发后问题，软件维护，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，编译器,,,
7SJJJDNE,2023,https://doi.org/10.1145/3597926.3598049,ISSTA 2023,Concept-Based Automated Grading of CS-1 Programming Assignments,"Due to the increasing enrolments in Computer Science programs, teaching of introductory programming needs to be scaled up. This places significant strain on teaching resources for programming courses for tasks such as grading of submitted programming assignments. Conventional attempts at automated grading of programming assignment rely on test-based grading which assigns scores based on the number of passing tests in a given test-suite. Since test-based grading may not adequately capture the student's understanding of the programming concepts needed to solve a programming task, we propose the notion of a concept graph which is essentially an abstracted control flow graph. Given the concept graphs extracted from a student's solution and a reference solution, we define concept graph matching and comparing of differing concepts. Our experiments on 1540 student submissions from a publicly available dataset show the efficacy of concept-based grading vis-a-vis test-based grading. Specifically, the concept based grading is (experimentally) shown to be closer to the grade manually assigned by the tutor. Apart from grading, the concept graph used by our approach is also useful for providing feedback to struggling students, as confirmed by our user study among tutors.","Applied computing,Education,Computer-assisted instruction,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",基于概念的CS-1编程作业自动评分,由于计算机科学课程的入学人数不断增加，入门编程的教学需要扩大。这给编程课程的教学资源带来了巨大的压力，比如对提交的编程作业进行评分。编程分配的自动评分的传统尝试依赖于基于测试的评分，该评分基于给定测试套件中通过测试的次数来分配分数。由于基于测试的评分可能无法充分捕捉学生对解决编程任务所需的编程概念的理解，我们提出了概念图的概念，它本质上是一个抽象的控制流图。给定从学生解决方案和参考解决方案中提取的概念图，我们定义了不同概念的概念图匹配和比较。我们对来自公开数据集的1540名学生提交的材料进行的实验表明，基于概念的评分与基于测试的评分相比是有效的。具体来说，基于概念的评分（实验）显示更接近导师手动分配的分数。除了评分，我们的方法使用的概念图也有助于向陷入困境的学生提供反馈，我们在导师中的用户研究证实了这一点。,应用计算，教育，计算机辅助教学，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
BVUDJ5GV,2023,https://doi.org/10.1145/3597926.3598125,ISSTA 2023,"Beyond ""Protected"" and ""Private"": An Empirical Security Analysis of Custom Function Modifiers in Smart Contracts","A smart contract is a piece of application-layer code running on blockchain ledgers and it provides programmatic logic via transaction-based execution of pre-defined functions. Smart contract functions are by default invokable by any party. To safeguard them, the mainstream smart contract language, i.e., Solidity of the popular Ethereum blockchain, proposed a unique language-level keyword called “modifier,” which allows developers to define custom function access control policies beyond the traditional “protected” and “private” modifiers in classic programming languages.  
","Security and privacy,Software and its engineering",超越“受保护”和“私有”：智能合约中自定义函数修改器的安全性实证分析,智能合约是运行在区块链账本上的一段应用层代码，它通过基于事务的预定义功能执行提供编程逻辑。默认情况下，任何一方都可以调用智能合约功能。为了保护它们，主流智能合约语言，即流行的以太坊区块链的Solidity，提出了一个名为“修饰符”的独特语言级关键字，允许开发者在经典编程语言中定义传统的“受保护”和“私有”修饰符之外的自定义功能访问控制策略。,安全和隐私，软件及其工程,,,
RPLNIILB,2023,https://doi.org/10.1145/3597926.3598091,ISSTA 2023,iSyn: Semi-automated Smart Contract Synthesis from Legal Financial Agreements,"Embracing software-driven smart contracts to fulfill legal agreements is a promising direction for digital transformation in the legal sector. Existing solutions mostly consider smart contracts as simple add-ons, without leveraging the programmability of smart contracts to realize complex semantics of legal agreements. In this paper, we propose iSyn, the first end-to-end system that synthesizes smart contracts to fulfill the semantics of financial legal agreements, with minimal human interventions. The design of iSyn centers around a novel intermediate representation (SmartIR) that closes the gap between the natural language sentences and smart contract statements. Specifically, iSyn includes a synergistic pipeline that unifies multiple NLP-techniques to accurately construct SmartIR instances given legal agreements, and performs template-based synthesis based on the SmartIR instances to synthesize smart contracts. We also design a validation framework to verify the correctness and detect known vulnerabilities of the synthesized smart contracts.We evaluate iSyn using legal agreements centering around financial transactions. The results show that iSyn-synthesized smart contracts are syntactically similar and semantically correct (or within a few edits), compared with the “ground truth” smart contracts manually developed by inspecting the legal agreements.","Computing methodologies,Artificial intelligence,Natural language processing,Software and its engineering,Software notations and tools,Compilers,Source code generation",iSyn：基于法律金融协议的半自动化智能合约综合,采用软件驱动的智能合约来履行法律协议是法律部门数字化转型的一个有希望的方向。现有的解决方案大多将智能合约视为简单的附加组件，而没有利用智能合约的可编程性来实现法律协议的复杂语义。在本文中，我们提出了iSyn，这是第一个端到端的系统，它综合了智能合约，以实现金融法律协议的语义，只需最少的人工干预。iSyn的设计围绕着一种新颖的中间表示（SmartIR）展开，该表示缩小了自然语言语句和智能合约语句之间的差距。具体而言，iSyn包括一个协同管道，该管道统一了多种NLP技术，在给定法律协议的情况下准确构建SmartIR实例，并基于SmartIR实例执行基于模板的合成，以合成智能合约。我们还设计了一个验证框架来验证合成智能合约的正确性和检测已知漏洞。我们使用围绕金融交易的法律协议来评估iSyn。结果表明，与通过检查法律协议手动开发的“基本事实”智能合约相比，iSyn合成的智能合约在语法上相似，语义上正确（或在几次编辑内）。,计算方法论，人工智能，自然语言处理，软件及其工程，软件符号和工具，编译器，源代码生成,,,
DP2BFMWC,2023,https://doi.org/10.1145/3597926.3604926,ISSTA 2023,"Oven: Safe and Live Communication Protocols in Scala, using Synthetic Behavioural Type Analysis","We present Oven: a toolset to assure safety and liveness of communication 
protocols among threads in concurrent programs in Scala. 
","Software and its engineering,Software notations and tools",Oven：使用综合行为类型分析的Scala中的安全和实时通信协议,我们展示Oven：一个确保通信安全和活跃的工具集,软件及其工程，软件符号和工具,,,
MR3YCK8Q,2023,https://doi.org/10.1145/3597926.3598075,ISSTA 2023,Vectorizing Program Ingredients for Better JVM Testing,"JVM testing is one of the most widely-used methodologies for guaranteeing the quality of JVMs. Among various JVM testing techniques, synthesis-based JVM testing, which constructs a test program by synthesizing various code snippets (also called program ingredients), has been demonstrated state-of-the-art. The existing synthesis-based JVM testing work puts more efforts in ensuring the validity of synthesized test programs, but ignores the influence of huge ingredient space, which largely limits the ingredient exploration efficiency as well as JVM testing performance. In this work, we propose Vectorized JVM Testing (called VECT) to further promote the performance of synthesis-based JVM testing. Its key insight is to reduce the huge ingredient space by clustering semantically similar ingredients via vectorizing ingredients using state-of-the-art code representation. To make VECT complete and more effective, based on vectorized ingredients, VECT further designs a feedback-driven ingredient selection strategy and an enhanced test oracle. We conducted an extensive study to evaluate VECT on three popular JVMs (i.e., HotSpot, OpenJ9, and Bisheng JDK) involving five OpenJDK versions. The results demonstrate VECT detects 115.03% ~ 776.92% more unique inconsistencies than the state-of-the-art JVM testing technique during the same testing time. In particular, VECT detects 26 previously unknown bugs for them, 15 of which have already been confirmed/fixed by developers.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",矢量化程序成分以实现更好的JVM测试,JVM测试是保证JVM质量的最广泛使用的方法之一。在各种JVM测试技术中，基于合成的JVM测试已经被证明是最先进的，它通过合成各种代码片段（也称为程序成分）来构建测试程序。现有的基于合成的JVM测试工作更多地致力于确保合成测试程序的有效性，但忽略了巨大的成分空间的影响，这在很大程度上限制了成分探索效率和JVM测试性能。在这项工作中，我们提出了矢量化JVM测试（VECT），以进一步提高基于合成的JVM测试的性能。它的关键见解是通过使用最先进的代码表示对成分进行矢量化，对语义相似的成分进行聚类，从而减少巨大的成分空间。为了使VECT更完整、更有效，在矢量化成分的基础上，VECT进一步设计了反馈驱动的成分选择策略和增强的测试预言机。我们在三个流行的JVM（即HotSpot、OpenJ9和必胜JDK）上进行了广泛的研究，评估了VECT，涉及五个OpenJDK版本。结果表明，在相同的测试时间内，VECT检测到的独特不一致性比最先进的JVM测试技术多115.03%～776.92%。特别是，VECT检测到26个以前未知的错误，其中15个已经被开发人员确认/修复。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
2YNVG8GX,2023,https://doi.org/10.1145/3597926.3598071,ISSTA 2023,CILIATE: Towards Fairer Class-Based Incremental Learning by Dataset and Training Refinement,"Due to the model aging problem, Deep Neural Networks (DNNs) need updates to adjust them to new data distributions. The common practice leverages incremental learning (IL), e.g., Class-based Incremental Learning (CIL) that updates output labels, to update the model with new data and a limited number of old data. This avoids heavyweight training (from scratch) using conventional methods and saves storage space by reducing the number of old data to store. But it also leads to poor performance in fairness. In this paper, we show that CIL suffers both dataset and algorithm bias problems, and existing solutions can only partially solve the problem. We propose a novel framework, CILIATE, that fixes both dataset and algorithm bias in CIL. It features a novel differential analysis guided dataset and training refinement process that identifies unique and important samples overlooked by existing CIL and enforces the model to learn from them. Through this process, CILIATE improves the fairness of CIL by 17.03%, 22.46%, and 31.79% compared to state-of-the-art methods, iCaRL, BiC, and WA, respectively, based on our evaluation on three popular datasets and widely used ResNet models. Our code is available at https://github.com/Antimony5292/CILIATE.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",CILIATE：通过数据集和训练优化实现更公平的基于类的增量学习,由于模型老化问题，深度神经网络需要更新以适应新的数据分布。常见做法利用增量学习（IL），例如，更新输出标签的基于类的增量学习（CIL），用新数据和有限数量的旧数据更新模型。这避免了使用传统方法进行重量级训练（从头开始），并通过减少要存储的旧数据数量来节省存储空间。但这也会导致公平性表现不佳。在本文中，我们证明了CIL同时存在数据集和算法偏差问题，并且现有的解决方案只能部分解决该问题。我们提出了一个新的框架CILIATE，它修复了CIL中的数据集和算法偏差。它具有一个新颖的差分分析引导的数据集和训练精化过程，该过程可以识别现有CIL忽略的独特和重要样本，并强制模型从中学习。基于我们对三个流行数据集和广泛使用的ResNet模型的评估，通过这个过程，与最先进的方法iCaRL、BiC和WA相比，CILIATE分别将CIL的公平性提高了17.03%、22.46%和31.79%。我们的代码可在https://github.com/Antimony5292/CILIATE.,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
X6B8EM5Y,2023,https://doi.org/10.1145/3597926.3598065,ISSTA 2023,Type Batched Program Reduction,"Given a program with a property of interest, program reduction searches for a smaller program that preserves the property and is easier to understand. Domain agnostic program reducers can reduce programs of multiple languages without extra domain knowledge. Despite their reusability, they may still take hours to run, hindering productivity and scalability. This paper proposes type batched program reduction, which uses machine learning to suggest portions of a program, or batches, that are most likely to be advantageous to reduce at a particular point in the reduction. We also extend this to jointly reduce multiple portions of a program at once, improving the performance further. Suggesting an appropriate order for removing batches from a program along with their potential simultaneous removal enables our reducer to outperform the state of the art reducers in reduction time over a set of large programs from multiple programming languages. This work lays foundations for further improvements in ML guided program reduction.","Software and its engineering,Software notations and tools",类型批量程序缩减,给定一个具有感兴趣属性的程序，程序约简会搜索一个较小的程序，该程序可以保留该属性并且更容易理解。领域不可知程序缩减器可以在没有额外领域知识的情况下缩减多种语言的程序。尽管它们具有可重用性，但它们可能仍需要数小时才能运行，从而阻碍了生产力和可扩展性。本文提出了类型批处理程序约简，它使用机器学习来建议程序的部分或批，这些部分或批最有可能有利于在约简的特定点进行约简。我们还将其扩展为同时联合减少程序的多个部分，从而进一步提高性能。建议从程序中删除批处理的适当顺序，以及它们可能同时删除的顺序，使我们的reducer在减少时间方面优于现有技术的reducers，而不是来自多种编程语言的一组大型程序。这项工作为进一步改进ML引导的程序缩减奠定了基础。,软件及其工程，软件符号和工具,,,
VR5GVLLR,2023,https://doi.org/10.1145/3597926.3605235,ISSTA 2023,Fairness Testing for Recommender Systems,"The topic of fairness in recommender systems (RSs) is gaining significant attention.  
However, current fairness metrics and testing approaches primarily cater to classification systems and are not suitable for RSs.  
To bridge this gap, we aim to address the specific challenges involved in fairness testing for RSs.  
In this paper, we present a novel testing approach specifically designed for RSs, which enables us to achieve accurate results while maintaining high efficiency.  
Additionally, we suggest potential avenues for further research in the realm of fairness testing for RSs.","Information systems,Information retrieval,Retrieval tasks and goals,Recommender systems,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",推荐系统的公平性测试,推荐系统中的公平性问题越来越受到人们的关注。,信息系统，信息检索，检索任务和目标，推荐系统，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
KTSF8QQF,2023,https://doi.org/10.1145/3597926.3598058,ISSTA 2023,FairRec: Fairness Testing for Deep Recommender Systems,"Deep learning-based recommender systems (DRSs) are increasingly and widely deployed in the industry, which brings significant convenience to people’s daily life in different ways. However, recommender systems are also shown to suffer from multiple issues, e.g., the echo chamber and the Matthew effect, of which the notation of “fairness” plays a core role. For instance, the system may be regarded as unfair to 1) a specific user, if the user gets worse recommendations than other users, or 2) an item (to recommend), if the item is much less likely to be exposed to the users than other items. While many fairness notations and corresponding fairness testing approaches have been developed for traditional deep classification models, they are essentially hardly applicable to DRSs. One major challenge is that there still lacks a systematic understanding and mapping between the existing fairness notations and the diverse testing requirements for deep recommender systems, not to mention further testing or debugging activities. To address the gap, we propose FairRec, a unified framework that supports fairness testing of DRSs from multiple customized perspectives, e.g., model utility, item diversity, item popularity, etc. We also propose a novel, efficient search-based testing approach to tackle the new challenge, i.e., double-ended discrete particle swarm optimization (DPSO) algorithm, to effectively search for hidden fairness issues in the form of certain disadvantaged groups from a vast number of candidate groups. Given the testing report, by adopting a simple re-ranking mitigation strategy on these identified disadvantaged groups, we show that the fairness of DRSs can be significantly improved. We conducted extensive experiments on multiple industry-level DRSs adopted by leading companies. The results confirm that FairRec is effective and efficient in identifying the deeply hidden fairness issues, e.g., achieving ∼95% testing accuracy with ∼half to 1/8 time.","Information systems,Information retrieval,Retrieval tasks and goals,Recommender systems,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",FairRec：深度推荐系统的公平性测试,基于深度学习的推荐系统（DRS）在行业中的部署越来越广泛，以不同的方式给人们的日常生活带来了极大的便利。然而，推荐系统也存在多个问题，例如回音室和马太效应，其中“公平”的符号起着核心作用。例如，该系统可能被认为对1）特定用户不公平，如果该用户获得的推荐比其他用户差，或者2）某个项目（要推荐），如果该项目比其他项目更不可能暴露给用户。虽然已经为传统的深度分类模型开发了许多公平性符号和相应的公平性测试方法，但它们基本上不适用于DRS。一个主要的挑战是，在现有的公平性符号和深度推荐系统的不同测试需求之间仍然缺乏系统的理解和映射，更不用说进一步的测试或调试活动了。为了解决这一差距，我们提出了FairRec，这是一个统一的框架，从多个定制的角度支持DRS的公平性测试，例如模型效用、项目多样性、项目流行度等。我们还提出了一种新颖、高效的基于搜索的测试方法来应对新的挑战，即双端离散粒子群优化（DPSO）算法，从大量的候选群体中有效地以某些弱势群体的形式寻找隐藏的公平问题。根据测试报告，通过对这些已确定的弱势群体采取简单的重新排序缓解策略，我们表明DRS的公平性可以显著提高。我们对领先公司采用的多个行业级DRS进行了广泛的实验。结果证实，FairRec在识别隐藏得很深的公平性问题方面是有效的，例如，在大约一半到1/8的时间内实现了大约95%的测试准确率。,信息系统，信息检索，检索任务和目标，推荐系统，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
VDL2QMNP,2023,https://doi.org/10.1145/3597926.3598041,ISSTA 2023,Reducing the Memory Footprint of IFDS-Based Data-Flow Analyses using Fine-Grained Garbage Collection,"The IFDS algorithm can be both memory- and compute-intensive for large programs as it needs to store a huge amount of path edges in memory and process them until a fixed point. In general, an IFDS-based data-flow analysis, such as taint analysis, aims to discover only the data-flow facts at some program points. Maintaining a huge amount of path edges (with many visited only once) wastes memory resources, and consequently, reduces its scalability and efficiency (due to frequent re-hashings for the path-edge data structure used). ","Software and its engineering,Software organization and properties,Software functional properties,Formal methods,Automated static analysis",使用细粒度垃圾收集减少基于IFDS的数据流分析的内存占用,对于大型程序来说，IFDS算法可能是内存密集型和计算密集型的，因为它需要在内存中存储大量的路径边，并处理它们直到固定点。通常，基于IFDS的数据流分析，如污点分析，旨在只发现某些程序点的数据流事实。维护大量的路径边缘（许多路径边缘只访问一次）会浪费内存资源，从而降低其可扩展性和效率（由于对所使用的路径边缘数据结构进行频繁的重新散列）。,软件及其工程，软件组织和属性，软件功能属性，形式化方法，自动静态分析,,,
YQSK3RMA,2023,https://doi.org/10.1145/3597926.3598096,ISSTA 2023,COME: Commit Message Generation with Modification Embedding,"Commit messages concisely describe code changes in natural language and are important for program comprehension and maintenance. Previous studies proposed some approaches for automatic commit message generation, but their performance is limited due to inappropriate representation of code changes and improper combination of translation-based and retrieval-based approaches. To address these problems, this paper introduces a novel framework named COME, in which modification embeddings are used to represent code changes in a fine-grained way, a self-supervised generative task is designed to learn contextualized code change representation, and retrieval-based and translation-based methods are combined through a decision algorithm. The average improvement of COME over the state-of-the-art approaches is 9.2% on automatic evaluation metrics and 8.0% on human evaluation metrics. We also analyse the effectiveness of COME's three main components and each of them results in an improvement of 8.6%, 8.7% and 5.2%.","Software and its engineering,Software notations and tools,Software maintenance tools",COME：嵌入修改的提交消息生成,提交消息简洁地描述了自然语言中的代码更改，对程序的理解和维护很重要。先前的研究提出了一些自动提交消息生成方法，但由于对代码更改的不恰当表示以及基于翻译和基于检索的方法的不恰当组合，它们的性能受到限制。为了解决这些问题，本文引入了一个名为COME的新框架，其中使用修改嵌入来以细粒度的方式表示代码变化，设计了一个自监督生成任务来学习上下文化的代码变化表示，并通过决策算法将基于检索和基于翻译的方法相结合。与最先进的方法相比，COME在自动评估指标上的平均改进率为9.2%，在人工评估指标上为8.0%。我们还分析了COME的三个主要组成部分的有效性，它们分别提高了8.6%、8.7%和5.2%。,软件及其工程，软件符号和工具，软件维护工具,,,
ZNV56W9M,2023,https://doi.org/10.1145/3597926.3598064,ISSTA 2023,Eunomia: Enabling User-Specified Fine-Grained Search in Symbolically Executing WebAssembly Binaries,"Although existing techniques have proposed automated approaches to alleviate the path explosion problem of symbolic execution, users still need to optimize symbolic execution by applying various searching strategies carefully. As existing approaches mainly support only coarse-grained global searching strategies, they cannot efficiently traverse through complex code structures. In this paper, we propose Eunomia, a symbolic execution technique that supports fine-grained search with local domain knowledge. Eunomia uses Aes, a DSL that lets users specify local searching strategies for different parts of the program. Eunomia also isolates the context of variables for different local searching strategies, avoiding conflicts. We implement Eunomia for WebAssembly, which can analyze applications written in various languages. Eunomia is the first symbolic execution engine that supports the full features of WebAssembly. We evaluate Eunomia with a microbenchmark suite and six real-world applications. Our evaluation shows that Eunomia improves bug detection by up to three orders of magnitude. We also conduct a user study that shows the benefits of using Aes. Moreover, Eunomia verifies six known bugs and detects two new zero-day bugs in Collections-C.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation",Eunomia：在符号执行的WebAssembly二进制文件中实现用户指定的细粒度搜索,尽管现有技术已经提出了自动化的方法来缓解符号执行的路径爆炸问题，但用户仍然需要通过仔细应用各种搜索策略来优化符号执行。由于现有方法主要只支持粗粒度的全局搜索策略，因此无法有效地遍历复杂的代码结构。在本文中，我们提出了Eunomia，这是一种符号执行技术，支持使用本地域知识进行细粒度搜索。Eunomia使用Aes，这是一种DSL，用户可以为程序的不同部分指定本地搜索策略。Eunomia还为不同的局部搜索策略隔离了变量的上下文，避免了冲突。我们为WebAssembly实现了Eunomia，它可以分析用各种语言编写的应用程序。Eunomia是第一个支持WebAssembly全部功能的符号执行引擎。我们使用一个微基准套件和六个真实世界的应用程序来评估Eunomia。我们的评估表明，Eunomia将错误检测提高了三个数量级。我们还进行了一项用户研究，展示了使用Aes的好处。此外，Eunomia验证了Collection-C中的六个已知错误，并检测到两个新的零日错误。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和确认,,,
Z59S4BL5,2023,https://doi.org/10.1145/3597926.3598069,ISSTA 2023,PhysCov: Physical Test Coverage for Autonomous Vehicles,"Adequately exercising the behaviors of autonomous vehicles is fundamental to their validation. However, quantifying an autonomous vehicle’s testing adequacy is challenging as the system’s behavior is influenced both by its state as well as its physical environment. To address this challenge, our work builds on two insights. First, data sensed by an autonomous vehicle provides a unique spatial signature of the physical environment inputs. Second, given the vehicle’s current state, inputs residing outside the autonomous vehicle’s physically reachable regions are less relevant to its behavior. Building on those insights, we introduce an abstraction that enables the computation of a physical environment-state coverage metric, PhysCov. The abstraction combines the sensor readings with a physical reachability analysis based on the vehicle’s state and dynamics to determine the region of the environment that may affect the autonomous vehicle. It then characterizes that region through a parameterizable geometric approximation that can trade quality for cost. Tests with the same characterizations are deemed to have had similar internal states and exposed to similar environments and thus likely to exercise the same set of behaviors, while tests with distinct characterizations will increase PhysCov. A study on two simulated and one real system’s dataset examines PhysCovs’s ability to quantify an autonomous vehicle’s test suite, showcases its characterization cost and precision, investigates its correlation with failures found and potential for test selection, and assesses its ability to distinguish among real-world scenarios.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",PhysCov：自动驾驶汽车的物理测试覆盖范围,充分行使自动驾驶汽车的行为是其验证的基础。然而，量化自动驾驶汽车的测试充分性是一项挑战，因为系统的行为受到其状态和物理环境的影响。为了应对这一挑战，我们的工作建立在两个见解的基础上。首先，自动驾驶汽车感测到的数据提供了物理环境输入的独特空间特征。其次，考虑到车辆的当前状态，位于自动驾驶汽车物理可达区域之外的输入与其行为的相关性较小。基于这些见解，我们引入了一种抽象，可以计算物理环境状态覆盖度量PhysCov。该抽象将传感器读数与基于车辆状态和动力学的物理可达性分析相结合，以确定可能影响自动驾驶车辆的环境区域。然后，它通过可参数化的几何近似来表征该区域，该几何近似可以用质量换取成本。具有相同特征的测试被认为具有相似的内部状态，暴露在相似的环境中，因此可能会行使相同的行为集，而具有不同特征的测试会增加PhysCov。一项针对两个模拟和一个真实系统数据集的研究考察了PhysCovs量化自动驾驶汽车测试套件的能力，展示了其表征成本和精度，调查了其与发现的故障的相关性和测试选择的潜力，并评估了其在现实世界场景中的区分能力。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
Y8LKJBFT,2023,https://doi.org/10.1145/3597926.3598112,ISSTA 2023,\(ømega\)Test: WebView-Oriented Testing for Android Applications,"WebView is a UI widget that helps integrate web applications into the native context of Android apps. It provides powerful mechanisms for bi-directional interactions between the native-end (Java) and the web-end (JavaScript) of an Android app. However, these interaction mechanisms are complicated and have induced various types of bugs. To mitigate the problem, various techniques have been proposed to detect WebView-induced bugs via dynamic analysis, which heavily relies on executing tests to explore WebView behaviors. Unfortunately, these techniques either require manual effort or adopt random test generation approaches, which are not able to effectively explore diverse WebView behaviors. In this paper, we study the problem of test generation for WebViews in Android apps. Effective test generation for WebViews requires identifying the essential program properties to be covered by the generated tests. To this end, we propose WebView-specific properties to characterize WebView behaviors, and devise a cross-language dynamic analysis method to identify these properties. We develop ωTest, a test generation technique that searches for event sequences covering the identified WebView-specific properties. An evaluation on 74 real-world open-/closed-source Android apps shows that ωTest can cover diverse WebView behaviors and detect WebView-induced bugs effectively. ωTest detected 36 previously-unknown bugs. From the 22 bugs that we have reported to the app developers, 13 bugs were confirmed, 9 of which were fixed.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",\（ømega\）测试：面向WebView的Android应用程序测试,WebView是一个UI小部件，有助于将web应用程序集成到Android应用程序的本地上下文中。它为Android应用程序的本地端（Java）和web端（JavaScript）之间的双向交互提供了强大的机制。然而，这些交互机制是复杂的，并引发了各种类型的错误。为了缓解这个问题，已经提出了各种技术来通过动态分析来检测WebView引起的错误，动态分析在很大程度上依赖于执行测试来探索WebView行为。不幸的是，这些技术要么需要手动操作，要么采用随机测试生成方法，无法有效地探索不同的WebView行为。在本文中，我们研究了Android应用程序中WebViews的测试生成问题。WebViews的有效测试生成需要确定生成的测试所涵盖的基本程序属性。为此，我们提出了特定于WebView的属性来表征WebView行为，并设计了一种跨语言动态分析方法来识别这些属性。我们开发了ωTest，这是一种测试生成技术，用于搜索覆盖已识别WebView特定属性的事件序列。对74个真实世界的开源/闭源Android应用程序的评估表明，ωTest可以覆盖不同的WebView行为，并有效地检测WebView引发的错误。ω测试检测到36个以前未知的错误。在我们向应用程序开发人员报告的22个错误中，有13个错误得到了确认，其中9个已经修复。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
C3242RFE,2023,https://doi.org/10.1145/3597926.3598145,ISSTA 2023,Interpreters for GNN-Based Vulnerability Detection: Are We There Yet?,"Traditional vulnerability detection methods have limitations due to their need for extensive manual labor. Using automated means for vulnerability detection has attracted research interest, especially deep learning, which has achieved remarkable results. Since graphs can better convey the structural feature of code than text, graph neural network (GNN) based vulnerability detection is significantly better than text-based approaches. Therefore, GNN-based vulnerability detection approaches are becoming popular. However, GNN models are close to black boxes for security analysts, so the models cannot provide clear evidence to explain why a code sample is detected as vulnerable or secure. At this stage, many GNN interpreters have been proposed. However, the explanations provided by these interpretations for vulnerability detection models are highly inconsistent and unconvincing to security experts. To address the above issues, we propose principled guidelines to assess the quality of the interpretation approaches for GNN-based vulnerability detectors based on concerns in vulnerability detection, namely, stability, robustness, and effectiveness. We conduct extensive experiments to evaluate the interpretation performance of six famous interpreters (GNN-LRP, DeepLIFT, GradCAM, GNNExplainer, PGExplainer, and SubGraphX) on four vulnerability detectors (DeepWukong, Devign, IVDetect, and Reveal). The experimental results show that the target interpreters achieve poor performance in terms of effectiveness, stability, and robustness. For effectiveness, we find that the instance-independent methods outperform others due to their deep insight into the detection model. In terms of stability, the perturbation-based interpretation methods are more resilient to slight changes in model parameters as they are model-agnostic. For robustness, the instance-independent approaches provide more consistent interpretation results for similar vulnerabilities.","Security and privacy,Systems security,Vulnerability management,Vulnerability scanners",基于GNN的漏洞检测的解释器：我们还存在吗？,传统的漏洞检测方法由于需要大量的体力劳动而具有局限性。使用自动化手段进行漏洞检测引起了研究兴趣，尤其是深度学习，并取得了显著的成果。由于图比文本更能传达代码的结构特征，因此基于图神经网络的漏洞检测明显优于基于文本的方法。因此，基于GNN的漏洞检测方法正变得越来越流行。然而，对于安全分析师来说，GNN模型几乎是黑匣子，因此这些模型无法提供明确的证据来解释为什么代码样本被检测为易受攻击或安全。在这个阶段，已经提出了许多GNN口译员。然而，这些解释对漏洞检测模型的解释高度不一致，安全专家无法信服。为了解决上述问题，我们提出了原则性指南，根据漏洞检测中的问题，即稳定性、稳健性和有效性，评估基于GNN的漏洞检测器的解释方法的质量。我们进行了大量的实验来评估六位著名口译员（GNN-LRP、DeepLIFT、GradCAM、GNNEexplainer、PGExplainer和SubGraphX）对四个漏洞检测器（DeepWukong、Devign、IVDetect和Reveal）的解释性能。实验结果表明，目标解释器在有效性、稳定性和鲁棒性方面表现不佳。就有效性而言，我们发现独立于实例的方法优于其他方法，因为它们对检测模型有深入的了解。就稳定性而言，基于扰动的解释方法对模型参数的微小变化更有弹性，因为它们与模型无关。对于稳健性，独立于实例的方法为类似的漏洞提供了更一致的解释结果。,安全和隐私，系统安全，漏洞管理，漏洞扫描器,,,
7DM22QBI,2023,https://doi.org/10.1145/3597926.3598040,ISSTA 2023,Fine-Grained Code Clone Detection with Block-Based Splitting of Abstract Syntax Tree,"Code clone detection aims to find similar code fragments and gains increasing importance in the field of software engineering. There are several types of techniques for detecting code clones. Text-based or token-based code clone detectors are scalable and efficient but lack consideration of syntax, thus resulting in poor performance in detecting syntactic code clones. Although some tree-based methods have been proposed to detect syntactic or semantic code clones with decent performance, they are mostly time-consuming and lack scalability. In addition, these detection methods can not realize fine-grained code clone detection. They are unable to distinguish the concrete code blocks that are cloned. In this paper, we design Tamer, a scalable and fine-grained tree-based syntactic code clone detector. Specifically, we propose a novel method to transform the complex abstract syntax tree into simple subtrees. It can accelerate the process of detection and implement the fine-grained analysis of clone pairs to locate the concrete clone parts of the code. To examine the detection performance and scalability of Tamer, we evaluate it on a widely used dataset BigCloneBench. Experimental results show that Tamer outperforms ten state-of-the-art code clone detection tools (i.e., CCAligner, SourcererCC, Siamese, NIL, NiCad, LVMapper, Deckard, Yang2018, CCFinder, and CloneWorks).","Software and its engineering,Software notations and tools,Software maintenance tools",基于抽象语法树分块的细粒度代码克隆检测,代码克隆检测旨在寻找相似的代码片段，在软件工程领域越来越重要。有几种类型的技术用于检测代码克隆。基于文本或基于令牌的代码克隆检测器可扩展且高效，但缺乏对语法的考虑，因此导致检测语法代码克隆的性能较差。尽管已经提出了一些基于树的方法来检测性能良好的语法或语义代码克隆，但它们大多耗时且缺乏可扩展性。此外，这些检测方法无法实现细粒度的代码克隆检测。他们无法区分被克隆的具体代码块。在本文中，我们设计了Tamer，一个可扩展的、细粒度的基于树的语法代码克隆检测器。具体来说，我们提出了一种将复杂的抽象语法树转换为简单子树的新方法。它可以加快检测过程，并实现克隆对的细粒度分析，以定位代码的具体克隆部分。为了检查Tamer的检测性能和可扩展性，我们在广泛使用的数据集BigCloneBench上对其进行了评估。实验结果表明，Tamer优于十种最先进的代码克隆检测工具（即CCAligner、SourcererCC、Siamese、NIL、NiCad、LVMapper、Deckard、Yang2018、CCFinder和CloneWorks）。,软件及其工程，软件符号和工具，软件维护工具,,,
65A3Q9PL,2023,https://doi.org/10.1145/3597926.3598046,ISSTA 2023,GDsmith: Detecting Bugs in Cypher Graph Database Engines,"Graph database engines stand out in the era of big data for their  
efficiency of modeling and processing linked data. To assure high quality of graph database engines, it is highly critical to conduct automatic test generation for graph database engines, e.g., random test generation, the most commonly adopted approach in practice. However, random test generation faces the challenge of generating complex inputs (i.e., property graphs and queries) for producing non-empty query results; generating such type of inputs is important especially for detecting wrong-result bugs. To address this challenge, in this paper, we propose GDsmith, the first approach for testing Cypher graph database engines. GDsmith ensures that each randomly generated query satisfies the semantic requirements. To increase the probability of producing complex queries that return non-empty results, GDsmith includes two new techniques: graph-guided generation of complex pattern combinations and data-guided generation of complex conditions. Our evaluation results demonstrate that GDsmith is effective and efficient for producing complex queries that return non-empty results for bug detection, and substantially outperforms the baselines. GDsmith successfully detects 28 bugs on the released versions of three highly popular open-source graph database engines and receives positive feedback from their developers.","Information systems,Data management systems,Database management system engines,Database query processing,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",GDsmith:检测Cypher图形数据库引擎中的错误,图形数据库引擎在大数据时代脱颖而出,信息系统，数据管理系统，数据库管理系统引擎，数据库查询处理，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
CMUXIJDI,2023,https://doi.org/10.1145/3597926.3604929,ISSTA 2023,PExReport-Maven: Creating Pruned Executable Cross-Project Failure Reports in Maven Build System,"Modern Java software development extensively depends on existing libraries written by other developer teams from the same or a different organization. When a developer executes the test, the execution trace may go across the boundaries of multiple dependencies and create cross-project failures (CPFs). A readable, executable, and concise CPF report may enable the most effective communication, but creating such a report is often challenging in Java ecosystems. We developed PExReport-Maven to automatically create the ideal CPF reports in the Maven build system. PExReport-Maven leverages the Maven build system to prune source code, dependencies, and the build environment to create a concise stand-alone executable CPF reproduction package from the original CPF project. The reproduction package includes the source code, dependencies, and build environment necessary to reproduce the CPF, making it an ideal CPF report. We performed an evaluation on 74 software project issues with 198 cross-project failures, and the evaluation results show that PExReport can create pruned reproduction packages for 184 out of the 198 test failures in our dataset, with an average reduction of 72.97% in Java classes. A future study will be conducted based on user feedback from using this tool to report real-world CPFs. 
PExReport-Maven is publicly available at https://github.com/wereHuang/PExReport-Maven. The tool demo is available on the PExReport website: https://sites.google.com/view/pexreport/home.","Software and its engineering,Software notations and tools,Software maintenance tools",PExReport Maven：在Maven构建系统中创建修剪的可执行文件跨项目失败报告,现代Java软件开发在很大程度上依赖于来自同一或不同组织的其他开发团队编写的现有库。当开发人员执行测试时，执行跟踪可能会跨越多个依赖项的边界，并产生跨项目失败（CPF）。可读、可执行和简洁的CPF报告可以实现最有效的沟通，但在Java生态系统中，创建这样的报告通常很有挑战性。我们开发了PExReport-Maven，以便在Maven构建系统中自动创建理想的CPF报告。PExReport-Maven利用Maven构建系统修剪源代码、依赖项和构建环境，从原始CPF项目中创建一个简洁的独立可执行CPF复制包。复制包包括复制CPF所需的源代码、依赖项和构建环境，使其成为理想的CPF报告。我们对74个软件项目问题和198个跨项目失败进行了评估，评估结果表明，PExReport可以为我们数据集中198个测试失败中的184个创建修剪后的复制包，Java类的平均减少72.97%。未来将根据用户使用该工具报告真实世界CPF的反馈进行研究。,软件及其工程，软件符号和工具，软件维护工具,,,
WCDHPIPK,2023,https://doi.org/10.1145/3597926.3598074,ISSTA 2023,ConfFix: Repairing Configuration Compatibility Issues in Android Apps,"XML configuration files are widely-used to specify the user interfaces (UI) of Android apps. Configuration compatibility (CC) issues are induced owing to the inconsistent handling of such XML configuration files across different Android framework versions. CC issues can cause software crashes and inconsistent look-and-feels, severely impacting the user experience of Android apps. However, there is no universal solution to resolve CC issues and app developers need to handle CC issues case by case. Existing tools are designed based on predefined rules or visual features that are possibly manifested by CC issues. Unfortunately, they can fail or generate overfitting patches when the CC issues are beyond their capabilities. To fill the above research gaps, we first empirically studied the app developers' common strategies in patching real-world CC issues. Based on the findings, we propose ConfFix, an automatic approach to repair CC issues in Android apps. ConfFix is driven by the knowledge of how an XML element is handled inconsistently in different versions of the Android framework and generates patches to eliminate such inconsistencies. We evaluated ConfFix on a set of 77 reproducible CC issues in 13 open-source Android apps. The results show that ConfFix outperforms baselines in successfully repairing 64 CC issues with a high precision. Encouragingly, the patches for 38 CC issues have been confirmed and merged by app developers.","Software and its engineering,Software creation and management,Software post-development issues,Software evolution",ConfFix:修复Android应用程序中的配置兼容性问题,XML配置文件被广泛用于指定Android应用程序的用户界面。由于不同Android框架版本对此类XML配置文件的处理不一致，导致了配置兼容性（CC）问题。CC问题可能会导致软件崩溃和外观不一致，严重影响安卓应用程序的用户体验。然而，没有通用的解决方案来解决CC问题，应用程序开发人员需要逐案处理CC问题。现有的工具是基于预定义的规则或视觉特征设计的，这些规则或视觉特性可能由CC问题表现出来。不幸的是，当CC问题超出其能力范围时，它们可能会失败或生成过拟合补丁。为了填补上述研究空白，我们首先实证研究了应用程序开发人员在修补现实世界CC问题时的常见策略。基于这些发现，我们提出了ConfFix，这是一种自动修复Android应用程序中CC问题的方法。ConfFix的驱动因素是了解在不同版本的Android框架中如何不一致地处理XML元素，并生成补丁来消除这种不一致。我们评估了ConfFix在13个开源Android应用程序中的77个可复制CC问题。结果表明，ConfFix在高精度成功修复64个CC问题方面优于基线。令人鼓舞的是，应用程序开发人员已经确认并合并了38个CC问题的补丁。,软件及其工程，软件创建和管理，软件后开发问题，软件演化,,,
2ZR36IW6,2023,https://doi.org/10.1145/3597926.3598147,ISSTA 2023,Understanding Breaking Changes in the Wild,"Modern software applications rely heavily on the usage of libraries, which provide reusable functionality, to accelerate the development process. As libraries evolve and release new versions, the software systems that depend on those libraries (the clients) should update their dependencies to use these new versions as the new release could, for example, include critical fixes for security vulnerabilities. However, updating is not always a smooth process, as it can result in software failures in the clients if the new version includes breaking changes. Yet, there is little research on how these breaking changes impact the client projects in the wild. To identify if changes between two library versions cause breaking changes at the client end, we perform an empirical study on Java projects built using Maven. For the analysis, we used 18,415 Maven artifacts, which declared 142,355 direct dependencies, of which 71.60% were not up-to-date. We updated these dependencies and found that 11.58% of the dependency updates contain breaking changes that impact the client. We further analyzed these changes in the library which impact the client projects and examine if libraries have adhered to the semantic versioning scheme when introducing breaking changes in their releases. Our results show that changes in transitive dependencies were a major factor in introducing breaking changes during dependency updates and almost half of the detected client impacting breaking changes violate the semantic versioning scheme by introducing breaking changes in non-Major updates.","Software and its engineering,Software creation and management,Software development techniques,Reusability,Software post-development issues,Maintaining software,Software evolution,Software notations and tools,Software libraries and repositories",了解野外的突破性变化,现代软件应用程序在很大程度上依赖于库的使用来加速开发过程，库提供了可重用的功能。随着库的发展和发布新版本，依赖这些库的软件系统（客户端）应该更新其依赖项以使用这些新版本，因为例如，新版本可能包括安全漏洞的关键修复。然而，更新并不总是一个顺利的过程，因为如果新版本包含破坏性更改，可能会导致客户端的软件故障。然而，很少有研究表明这些突破性的变化会对客户项目产生怎样的影响。为了确定两个库版本之间的更改是否会导致客户端发生突破性的更改，我们对使用Maven构建的Java项目进行了实证研究。在分析中，我们使用了18415个Maven工件，其中声明了142355个直接依赖项，其中71.60%不是最新的。我们更新了这些依赖关系，发现11.58%的依赖关系更新包含影响客户端的破坏性更改。我们进一步分析了库中影响客户端项目的这些更改，并检查库在其发布中引入突破性更改时是否遵守了语义版本控制方案。我们的结果表明，可传递依赖关系的变化是在依赖关系更新期间引入中断变化的主要因素，并且几乎一半检测到的影响中断变化的客户端通过在非主要更新中引入中断变化而违反了语义版本控制方案。,软件及其工程，软件创建和管理，软件开发技术，可重用性，软件开发后问题，维护软件，软件进化，软件符号和工具，软件库和存储库,,,
H8VM3M3M,2023,https://doi.org/10.1145/3597926.3598034,ISSTA 2023,Improving Bit-Blasting for Nonlinear Integer Constraints,"Nonlinear integer constraints are common and difficult in the verification and analysis of software/hardware. SMT(QF_NIA) generalizes such constraints, which is a boolean combination of nonlinear integer arithmetic constraints. A classical method to solve SMT(QF_NIA) is bit-blasting, which reduces them to boolean satisfiability problems. Currently, the existing pure bit-blasting based solvers are noncompetitive with other state-of-the-art SMT solvers. The bit-blasting based methods have some problems: First, the bit-blasting method is hampered by nonlinear multiplication operations; second, it sometimes does not search in a proper search space; and third, it contains some redundancy. ","Hardware,Mathematics of computing,Mathematical analysis,Mathematical optimization,Mixed discrete-continuous optimization,Integer programming,Numerical analysis,Mathematical software,Solvers,Theory of computation,Design and analysis of algorithms,Mathematical optimization,Logic,Automated reasoning,Logic and verification",非线性整数约束下钻头爆破的改进,非线性整数约束在软硬件的验证和分析中是常见而困难的。SMT（QF_NIA）推广了这种约束，它是非线性整数算术约束的布尔组合。求解SMT（QF_NIA）的一种经典方法是比特爆破，它将它们简化为布尔可满足性问题。目前，现有的基于纯钻头爆破的求解器与其他最先进的SMT求解器没有竞争力。基于钻头爆破的方法存在一些问题：首先，钻头爆破方法受到非线性乘法运算的阻碍；其次，它有时没有在适当的搜索空间中进行搜索；第三，它包含一些冗余。,硬件，计算数学，数学分析，数学优化，混合离散-连续优化，可编程，数值分析，数学软件，求解器，计算理论，算法设计和分析，数学优化，逻辑，自动推理，逻辑和验证,,,
V2ZFGMYH,2023,https://doi.org/10.1145/3597926.3598143,ISSTA 2023,Third-Party Library Dependency for Large-Scale SCA in the C/C++ Ecosystem: How Far Are We?,"Existing software composition analysis (SCA) techniques for the C/C++ ecosystem tend to identify the reused components through feature matching between target software project and collected third-party libraries (TPLs). However, feature duplication caused by internal code clone can cause inaccurate SCA results. To mitigate this issue, Centris, a state-of-the-art SCA technique for the C/C++ ecosystem, was proposed to adopt function-level code clone detection to derive the TPL dependencies for eliminating the redundant features before performing SCA tasks. Although Centris has been shown effective in the original paper, the accuracy of the derived TPL dependencies is not evaluated. Additionally, the dataset to evaluate the impact of TPL dependency on SCA is limited. To further investigate the efficacy and limitations of Centris, we first construct two large-scale ground-truth datasets for evaluating the accuracy of deriving TPL dependency and SCA results respectively. Then we extensively evaluate Centris where the evaluation results suggest that the accuracy of TPL dependencies derived by Centris may not well generalize to our evaluation dataset. We further infer the key factors that degrade the performance can be the inaccurate function birth time and the threshold-based recall. In addition, the impact on SCA from the TPL dependencies derived by Centris can be somewhat limited. Inspired by our findings, we propose TPLite with function-level origin TPL detection and graph-based dependency recall to enhance the accuracy of TPL reuse detection in the C/C++ ecosystem. Our evaluation results indicate that TPLite effectively increases the precision from 35.71% to 88.33% and the recall from 49.44% to 62.65% of deriving TPL dependencies compared with Centris. Moreover, TPLite increases the precision from 21.08% to 75.90% and the recall from 57.62% to 64.17% compared with the SOTA academic SCA tool B2SFinder and even outperforms the well-adopted commercial SCA tool BDBA, i.e., increasing the precision from 72.46% to 75.90% and the recall from 58.55% to 64.17%.","Software and its engineering,Software creation and management,Software development techniques,Reusability,Software notations and tools,Software libraries and repositories",C/C++生态系统中大规模SCA的第三方库依赖：我们有多远？,针对C/C++生态系统的现有软件组成分析（SCA）技术倾向于通过目标软件项目和收集的第三方库（TPL）之间的特性匹配来识别重用组件。但是，内部代码克隆导致的功能重复可能会导致SCA结果不准确。为了缓解这个问题，针对C/C++生态系统的最先进的SCA技术Centris被提议采用功能级别的代码克隆检测来推导TPL依赖关系，以便在执行SCA任务之前消除冗余功能。尽管Centris在最初的论文中已经被证明是有效的，但推导的TPL依赖关系的准确性没有得到评估。此外，用于评估TPL依赖性对SCA的影响的数据集是有限的。为了进一步研究Centris的有效性和局限性，我们首先构建了两个大规模的基本事实数据集，分别用于评估推导TPL依赖性和SCA结果的准确性。然后，我们对Centris进行了广泛的评估，其中评估结果表明，Centris导出的TPL依赖性的准确性可能无法很好地推广到我们的评估数据集。我们进一步推断，降低性能的关键因素可能是不准确的功能出生时间和基于阈值的回忆。此外，Centris派生的TPL依赖关系对SCA的影响可能有些有限。受我们研究结果的启发，我们提出了具有函数级起源TPL检测和基于图的依赖召回的TPLite，以提高C/C++生态系统中TPL重用检测的准确性。我们的评估结果表明，与Centris相比，TPLite有效地将推导TPL依赖性的精确度从35.71%提高到88.33%，召回率从49.44%提高到62.65%。此外，与SOTA学术SCA工具B2SFinder相比，TPLite将精度从21.08%提高到75.90%，召回率从57.62%提高到64.17%，甚至优于广泛采用的商业SCA工具BDBA，即精度从72.46%提高到75.90%，召回率由58.55%提高到64.17%。,软件及其工程，软件创建和管理，软件开发技术，可重用性，软件符号和工具，软件库和存储库,,,
5RIAAEHW,2023,https://doi.org/10.1145/3597926.3598141,ISSTA 2023,CodeGrid: A Grid Representation of Code,"Code representation is a key step in the application of AI in software engineering. Generic NLP representations are effective but do not exploit all the rich structure inherent to code. Recent work has focused on extracting abstract syntax trees (AST) and integrating their structural information into code representations.These AST-enhanced representations advanced the state of the art and accelerated new applications of AI to software engineering. ASTs, however, neglect important aspects of code structure, notably control and data flow, leaving some potentially relevant code signal unexploited. For example, purely image-based representations perform nearly as well as AST-based representations, despite the fact that they must learn to even recognize tokens, let alone their semantics. This result, from prior work, is strong evidence that these new code representations can still be improved; it also raises the question of just what signal image-based approaches are exploiting. ","Computing methodologies,Computer graphics,Image manipulation,Image processing,Machine learning,General and reference,Cross-computing tools and techniques,Software and its engineering,Software creation and management,Software organization and properties,Software system structures",CodeGrid：代码的网格表示,代码表示是人工智能在软件工程中应用的关键一步。通用NLP表示是有效的，但不能利用代码固有的所有丰富结构。最近的工作集中在提取抽象语法树（AST）并将其结构信息集成到代码表示中。这些AST增强的表示提升了技术水平，加速了人工智能在软件工程中的新应用。然而，AST忽略了代码结构的重要方面，尤其是控制和数据流，留下了一些潜在的相关代码信号未被利用。例如，纯基于图像的表示的性能几乎与基于AST的表示一样好，尽管它们必须学会识别标记，更不用说它们的语义了。从先前的工作来看，这一结果有力地证明了这些新的代码表示仍然可以改进；它还提出了一个问题，即基于信号图像的方法正在利用什么。,计算方法论，计算机图形学，图像处理，机器学习，概论和参考，交叉计算工具和技术，软件及其工程，软件创建和管理，软件组织和性质，软件系统结构,,,
NHRP93NW,2023,https://doi.org/10.1145/3597926.3598044,ISSTA 2023,Testing Graph Database Engines via Query Partitioning,"Graph Database Management Systems (GDBMSs) store data as graphs and allow the efficient querying of nodes and their relationships. Logic bugs are bugs that cause a GDBMS to return an incorrect result for a given query (e.g., by returning incorrect nodes or relationships). The impact of such bugs can be severe, as they often go unnoticed. The core insight of this paper is that Query Partitioning, a test oracle that has been proposed to test Relational Database Systems, is applicable to testing GDBMSs as well. The core idea of Query Partitioning is that, given a query, multiple queries are derived whose results can be combined to reconstruct the given query’s result. Any discrepancy in the result indicates a logic bug. We have implemented this approach as a practical tool named GDBMeter and evaluated GDBMeter on three popular GDBMSs and found a total of 40 unique, previously unknown bugs. We consider 14 of them to be logic bugs, the others being error or crash bugs. Overall, 27 of the bugs have been fixed, and 35 confirmed. We compared our approach to the state-of-the-art approach to testing GDBMS, which relies on differential testing; we found that it results in a high number of false alarms, while Query Partitioning reported actual logic bugs without any false alarms. Furthermore, despite the previous efforts in testing Neo4j and JanusGraph, we found 18 additional bugs. The developers appreciate our work and plan to integrate GDBMeter into their testing process. We expect that this simple, yet effective approach and the practical tool will be used to test other GDBMSs.","Information systems,Data management systems,Database management system engines,Database query processing,Software and its engineering,Software creation and management,Software verification and validation",通过查询分区测试图形数据库引擎,图形数据库管理系统（GDBMS）将数据存储为图形，并允许有效查询节点及其关系。逻辑错误是指导致GDBMS为给定查询返回不正确结果的错误（例如，通过返回不正确的节点或关系）。这种错误的影响可能是严重的，因为它们经常被忽视。本文的核心见解是，Query Partitioning，一个被提出用于测试关系数据库系统的测试预言机，也适用于测试GDBMS。查询分区的核心思想是，给定一个查询，可以派生多个查询，这些查询的结果可以组合起来重建给定查询的结果。结果中的任何差异都表明存在逻辑错误。我们已经将这种方法作为一个名为GDBMeter的实用工具来实现，并在三个流行的GDBMS上评估了GDBMeter，总共发现了40个以前未知的独特错误。我们认为其中14个是逻辑错误，其他是错误或崩溃错误。总的来说，27个错误已经修复，35个已经确认。我们将我们的方法与最先进的GDBMS测试方法进行了比较，后者依赖于差异测试；我们发现它会导致大量的假警报，而查询分区报告的实际逻辑错误没有任何假警报。此外，尽管之前对Neo4j和JanusGraph进行了测试，但我们发现了18个额外的错误。开发人员感谢我们的工作，并计划将GDBMeter集成到他们的测试过程中。我们希望这种简单而有效的方法和实用的工具将用于测试其他GDBMS。,信息系统，数据管理系统，数据库管理系统引擎，数据库查询处理，软件及其工程，软件创建和管理，软件验证和确认,,,
K9R9QL8A,2023,https://doi.org/10.1145/3597926.3598103,ISSTA 2023,A Bayesian Framework for Automated Debugging,"Debugging takes up a significant portion of developer time. As a result, automated debugging techniques including Fault Localization (FL) and Automated Program Repair (APR) have garnered significant attention due to their potential to aid developers in debugging tasks. With the recent advance in techniques that treat the two tasks as closely coupled, such as Unified Debugging, a framework to formally express these two tasks together would heighten our understanding of automated debugging and provide a way to formally analyze techniques and approaches. To this end, we propose a Bayesian framework of understanding automated debugging. We find that the Bayesian framework, along with a concrete statement of the objective of automated debugging, can recover maximal fault localization formulae from prior work, as well as analyze existing APR techniques and their underlying  
assumptions.  
As a means of empirically demonstrating our framework, we further propose BAPP, a Bayesian Patch Prioritization technique that incorporates intermediate program values to analyze likely patch locations and repair actions, with its core equations being derived by our Bayesian framework. We find that incorporating program values allows BAPP to identify correct patches more precisely: the rankings produced by BAPP reduced the number of required patch evaluations by 68% and consequently reduced the repair time by 34 minutes on average. Further, our Bayesian framework suggests a number of changes to the way fault localization information is used in program repair, which we validate is useful for BAPP. These results highlight the potential of value-cognizant automated debugging techniques, and further verifies our theoretical framework.","Software and its engineering,Software creation and management",一种用于自动调试的贝叶斯框架,调试占用了开发人员大量的时间。因此，包括故障定位（FL）和自动程序修复（APR）在内的自动调试技术由于其帮助开发人员完成调试任务的潜力而引起了人们的极大关注。随着将这两个任务视为紧密耦合的技术（如统一调试）的最新进展，将这两项任务正式表示在一起的框架将提高我们对自动化调试的理解，并提供一种正式分析技术和方法的方法。为此，我们提出了一个理解自动化调试的贝叶斯框架。我们发现，贝叶斯框架，以及自动化调试目标的具体陈述，可以从先前的工作中恢复最大故障定位公式，并分析现有的APR技术及其基础,软件及其工程，软件创建和管理,,,
DCC3DTTA,2023,https://doi.org/10.1145/3597926.3598137,ISSTA 2023,CGuard: Scalable and Precise Object Bounds Protection for C,"Spatial safety violations are the root cause of many security attacks and unexpected behavior of applications. Existing techniques to enforce spatial safety work broadly at either object or pointer granularity. Object-based approaches tend to incur high CPU overheads, whereas pointer-based approaches incur both high CPU and memory overheads.  
SGXBounds, an object-based approach, provides precise out-of-bounds protection for objects at a lower overhead compared to other tools with similar precision. However, a major drawback of this approach is that it cannot support address space larger than 32-bit.  
","Security and privacy,Systems security",CGuard：可扩展且精确的C对象边界保护,空间安全违规是许多安全攻击和应用程序意外行为的根本原因。现有的强制空间安全的技术在对象或指针粒度上广泛工作。基于对象的方法往往会产生高CPU开销，而基于指针的方法则会产生高的CPU和内存开销。,安全和隐私，系统安全,,,
AAWSV6I3,2023,https://doi.org/10.1145/3597926.3598131,ISSTA 2023,Enhancing REST API Testing with NLP Techniques,"RESTful services are commonly documented using OpenAPI specifications. Although numerous automated testing techniques have been proposed that leverage the machine-readable part of these specifications to guide test generation, their human-readable part has been mostly neglected. This is a missed opportunity, as natural language descriptions in the specifications often contain relevant information, including example values and inter-parameter dependencies, that can be used to improve test generation. In this spirit, we propose NLPtoREST, an automated approach that applies natural language processing techniques to assist REST API testing. Given an API and its specification, NLPtoREST extracts additional OpenAPI rules from the human-readable part of the specification. It then enhances the original specification by adding these rules to it. Testing tools can transparently use the enhanced specification to perform better test case generation. Because rule extraction can be inaccurate, due to either the intrinsic ambiguity of natural language or mismatches between documentation and implementation, NLPtoREST also incorporates a validation step aimed at eliminating spurious rules. We performed studies to assess the effectiveness of  
our rule extraction and validation approach, and the impact of enhanced specifications on the performance of eight state-of-the-art REST API testing tools. Our results are encouraging and show that NLPtoREST can extract many relevant rules with high accuracy, which can in turn significantly improve testing tools’ performance.","Information systems,World Wide Web,Web services,RESTful web services,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",用NLP技术增强REST API测试,RESTful服务通常使用OpenAPI规范进行记录。尽管已经提出了许多自动化测试技术，利用这些规范的机器可读部分来指导测试生成，但它们的人类可读部分大多被忽视了。这是一个错失的机会，因为规范中的自然语言描述通常包含可用于改进测试生成的相关信息，包括示例值和参数间依赖关系。本着这种精神，我们提出了NLPtoREST，这是一种应用自然语言处理技术来帮助RESTneneneba API测试的自动化方法。给定一个API及其规范，NLPtoREST从规范的可人工修改部分提取额外的OpenAPI规则。然后，它通过添加这些规则来增强原始规范。测试工具可以透明地使用增强的规范来执行更好的测试用例生成。由于自然语言的内在歧义或文档和实现之间的不匹配，规则提取可能不准确，NLPtoREST还包含了一个旨在消除虚假规则的验证步骤。我们进行了研究，以评估,信息系统，万维网，Web服务，REST风格的Web服务，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
V9TSR53V,2023,https://doi.org/10.1145/3597926.3598101,ISSTA 2023,Automated Program Repair from Fuzzing Perspective,"In this work, we present a novel approach that connects two closely-related topics: fuzzing and automated program repair (APR). The paper is divided into two parts. In the first part, we describe the similarities between fuzzing and APR both of which can be viewed as a search problem. In the second part, we introduce a new patch-scheduling algorithm called Casino, which is designed from a fuzzing perspective to enhance search efficiency. Our experiments demonstrate that Casino outperforms existing algorithms. We also promote open science by sharing SimAPR, a simulation tool that can be used to evaluate new patch-scheduling algorithms.","Software and its engineering,Software creation and management,Software development techniques,Automatic programming,Software verification and validation,Software defect analysis,Software testing and debugging",引信视角下的程序自动修复,在这项工作中，我们提出了一种新的方法，将两个密切相关的主题联系起来：模糊化和自动程序修复（APR）。本文分为两个部分。在第一部分中，我们描述了模糊化和APR之间的相似性，这两者都可以被视为一个搜索问题。在第二部分中，我们介绍了一种新的补丁调度算法Casino，它是从模糊的角度设计的，以提高搜索效率。我们的实验表明Casino的性能优于现有算法。我们还通过共享SimAPR来促进开放科学，SimAPR是一种可用于评估新补丁调度算法的模拟工具。,软件及其工程，软件创建和管理，软件开发技术，自动编程，软件验证和确认，软件缺陷分析，软件测试和调试,,,
NPSAD47M,2023,https://doi.org/10.1145/3597926.3598124,ISSTA 2023,DeFiTainter: Detecting Price Manipulation Vulnerabilities in DeFi Protocols,"DeFi protocols are programs that manage high-value digital assets on blockchain. The price manipulation vulnerability is one of the common vulnerabilities in DeFi protocols, which allows attackers to gain excessive profits by manipulating token prices. In this paper, we propose DeFiTainter, an inter-contract taint analysis framework for detecting price manipulation vulnerabilities. DeFiTainter features two innovative mechanisms to ensure its effectiveness. The first mechanism is to construct a call graph for inter-contract taint analysis by restoring call information, not only from code constants but also from contract storage and function parameters. The second mechanism is a high-level semantic induction tailored for detecting price manipulation vulnerabilities, which accurately identifies taint sources and sinks and tracks taint data across contracts. Extensive evaluation of real-world incidents and high-value DeFi protocols shows that DeFiTainter outperforms existing approaches and achieves state-of-the-art performance with a precision of 96% and a recall of 91.3% in detecting price manipulation vulnerabilities. Furthermore, DeFiTainter uncovers three previously undisclosed price manipulation vulnerabilities.","Software and its engineering,Software creation and management,Software verification and validation",DeFiTainter:检测DeFi协议中的价格操纵漏洞,DeFi协议是管理区块链上高价值数字资产的程序。价格操纵漏洞是DeFi协议中常见的漏洞之一，它允许攻击者通过操纵代币价格来获得过度利润。在本文中，我们提出了DeFiTaint，一个用于检测价格操纵漏洞的合同间污染分析框架。DeFiTainter有两个创新机制来确保其有效性。第一种机制是通过恢复调用信息来构建用于合同间污染分析的调用图，调用信息不仅来自代码常量，还来自合同存储和函数参数。第二种机制是为检测价格操纵漏洞而定制的高级语义归纳，它准确地识别污染源和汇，并跟踪合同中的污染数据。对真实世界事件和高价值DeFi协议的广泛评估表明，DeFiTaint在检测价格操纵漏洞方面优于现有方法，并以96%的准确率和91.3%的召回率实现了最先进的性能。此外，DeFiTaint还发现了三个之前未公开的价格操纵漏洞。,软件及其工程，软件创建和管理，软件验证和确认,,,
UCD2E5ZI,2023,https://doi.org/10.1145/3597926.3604917,ISSTA 2023,Behaviorally Typed State Machines in TypeScript for Heterogeneous Swarms,"A heterogeneous swarm system is a distributed system where participants come and go, communication topology may change at any time, data replication is asynchronous and partial, and local agents behave differently between nodes. These systems are hard to design and reason about, mainly because we desire a particular class of behaviors to emerge from the interplay of heterogeneous individual agents. Nevertheless, mission-critical operations like manufacturing process orchestration in factories use such systems due to their uncompromising availability and resilience of computing services. ","Software and its engineering,Software notations and tools,General programming languages,Language types,Distributed programming languages,Software organization and properties,Software system structures,Distributed systems organizing principles,Theory of computation,Models of computation,Concurrency,Distributed computing models",异构Swarm的TypeScript中的行为类型状态机,异构集群系统是一个分布式系统，参与者来来往往，通信拓扑可能随时变化，数据复制是异步和部分的，本地代理在节点之间的行为不同。这些系统很难设计和推理，主要是因为我们希望从异构个体的相互作用中产生一类特定的行为。尽管如此，像工厂中的制造流程编排这样的关键任务操作使用这样的系统，因为它们具有无与伦比的可用性和计算服务的弹性。,软件及其工程，软件符号和工具，通用编程语言，语言类型，分布式编程语言，软件组织和属性，软件系统结构，分布式系统组织原理，计算理论，计算模型，并发，分布式计算模型,,,
RA3NBXMD,2023,https://doi.org/10.1145/3597926.3598126,ISSTA 2023,Synthesizing Speech Test Cases with Text-to-Speech? An Empirical Study on the False Alarms in Automated Speech Recognition Testing,"Recent studies have proposed the use of Text-To-Speech (TTS) systems to automatically synthesise speech test cases on a scale and uncover a large number of failures in ASR systems. However, the failures uncovered by synthetic test cases may not reflect the actual performance of an ASR system when it transcribes human audio, which we refer to as false alarms. Given a failed test case synthesised from TTS systems, which consists of TTS-generated audio and the corresponding ground truth text, we feed the human audio stating the same text to an ASR system. If human audio can be correctly transcribed, an instance of a false alarm is detected. ","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",用文本到语音合成语音测试用例？语音识别测试中误报现象的实证研究,最近的研究已经提出使用文本到语音（TTS）系统来自动合成一定规模的语音测试用例，并发现ASR系统中的大量故障。然而，合成测试用例发现的故障可能无法反映ASR系统在转录人类音频时的实际性能，我们称之为假警报。给定一个由TTS系统合成的失败测试用例，该用例由TTS生成的音频和相应的基本事实文本组成，我们将陈述相同文本的人类音频提供给ASR系统。如果人类音频能够被正确转录，则检测到错误警报的实例。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
9GFAMLQF,2023,https://doi.org/10.1145/3597926.3598095,ISSTA 2023,GPUHarbor: Testing GPU Memory Consistency at Large (Experience Paper),"Memory consistency specifications (MCSs) are a difficult, yet critical, part of a concurrent programming framework. Existing MCS testing tools are not immediately accessible, and thus, have only been applied to a limited number of devices. However, in the post-Dennard scaling landscape, there has been an explosion of new architectures and frameworks. Studying the shared memory behaviors of these new platforms is important to understand their behavior and ensure conformance to framework specifications.  
","Computing methodologies,Computer graphics,Graphics systems and interfaces,Graphics processors,Parallel computing methodologies,Parallel programming languages,Software and its engineering,Software creation and management,Software verification and validation,Empirical software validation",GPUHarbor:大规模测试GPU内存一致性（经验论文）,内存一致性规范（MCS）是并发编程框架中一个困难但关键的部分。现有的MCS测试工具无法立即访问，因此只能应用于数量有限的设备。然而，在后Dennard的扩展环境中，出现了新架构和框架的爆炸式增长。研究这些新平台的共享内存行为对于理解它们的行为并确保符合框架规范非常重要。,计算方法，计算机图形学，图形系统和接口，图形处理器，并行计算方法，并行编程语言，软件及其工程，软件创建和管理，软件验证和验证，经验软件验证,,,
W5WEJJQW,2023,https://doi.org/10.1145/3597926.3598140,ISSTA 2023,An Empirical Study on Concurrency Bugs in Interrupt-Driven Embedded Software,"Interrupt-driven embedded software is widely used in aerospace, automotive electronics, medical equipment, IoT, and other industrial fields. This type of software is usually programmed with interrupts to interact with hardware and respond to external stimuli on time. However, uncertain interleaving execution of interrupts may cause concurrency bugs, resulting in task failure or serious safety issues. A deep understanding of real-world concurrency bugs in embedded software will significantly improve the ability of techniques in combating concurrency bugs, such as bug detection, testing and fixing.","Computing methodologies,Concurrent computing methodologies,Concurrent programming languages,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",中断驱动嵌入式软件并发错误的实证研究,中断驱动嵌入式软件广泛应用于航空航天、汽车电子、医疗设备、物联网等工业领域。这种类型的软件通常通过中断进行编程，以与硬件交互并及时响应外部刺激。然而，不确定的中断交错执行可能会导致并发错误，导致任务失败或严重的安全问题。深入了解嵌入式软件中真实世界的并发错误将显著提高对抗并发错误的技术能力，如错误检测、测试和修复。,计算方法论，并发计算方法论，并发编程语言，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
7M6UQC78,2023,https://doi.org/10.1145/3597926.3598083,ISSTA 2023,Systematically Producing Test Orders to Detect Order-Dependent Flaky Tests,"Software testing suffers from the presence of flaky tests, which can pass or fail when run on the same version of code. Order- dependent tests (OD tests) are flaky tests whose outcome depends on the order in which they are run. An OD test can be detected if specific tests are run or not run before it, resulting in a difference in test outcome. While prior work has proposed rerunning tests in different random test orders, this approach does not provide guarantees toward detecting all OD tests. Later work that proposed a more systematic approach to ordering tests still fails to account for the relationships between all tests in the test suite.  
We propose three new techniques to detect OD tests through a more systematic means of producing test orders. Our techniques build upon prior work in Tuscan squares to cover test pairs in a minimal set of test orders while also obeying the constraints of how tests can be positioned in a test order w.r.t. their test classes. Further, as there are many test pairs that need to be covered, we develop a technique that can take a specified set of test pairs to cover and produce test orders that aim to cover just those test pairs. Our evaluation with 289 known OD tests across 47 test suites from open-source projects shows that our most cost-effective technique can detect 97.2% of the known OD tests with 104.7 test orders, on average, per subject. While all techniques produce a relatively large number of test orders, our analysis of the minimal set of test orders needed to detect OD tests shows a tremendous reduction in the test orders needed to detect OD tests – representing an opportunity for future work to prioritize test orders.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",系统生成测试订单以检测订单相关的缺陷测试,软件测试存在漏洞测试，当在同一版本的代码上运行时，这些测试可能通过也可能失败。顺序相关测试（OD测试）是一种片状测试，其结果取决于它们的运行顺序。如果在OD测试之前运行或未运行特定测试，则可以检测到OD测试，从而导致测试结果的差异。虽然先前的工作已经提出以不同的随机测试顺序重新运行测试，但这种方法并不能保证检测到所有OD测试。后来的工作提出了一种更系统的测试排序方法，但仍然没有考虑到测试套件中所有测试之间的关系。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
ZAP6IZ96,2023,https://doi.org/10.1145/3597926.3598100,ISSTA 2023,Simulation-Based Validation for Autonomous Driving Systems,"We investigate a rigorous simulation and testing-based validation method for autonomous driving systems that integrates an existing industrial simulator and a formally defined testing environment. The environment includes a scenario generator that drives the simulation process and a monitor that checks at runtime the observed behavior of the system against a set of system properties to be validated. The validation method consists in extracting from the simulator a semantic model of the simulated system including a metric graph, which is a mathematical model of the environment in which the vehicles of the system evolve. The monitor can verify properties formalized in a first-order linear temporal logic and provide diagnostics explaining their non-satisfaction. Instead of exploring the system behavior randomly as many simulators do, we propose a method to systematically generate sets of scenarios that cover potentially risky situations, especially for different types of junctions where specific traffic rules must be respected. We show that the systematic exploration of risky situations has uncovered many flaws in the real simulator that would have been very difficult to discover by a random exploration process.","Computer systems organization,Embedded and cyber-physical systems,Computing methodologies,Artificial intelligence,Control methods,Modeling and simulation,Software and its engineering,Software creation and management,Software verification and validation,Software organization and properties",基于仿真的自动驾驶系统验证,我们研究了一种严格的基于模拟和测试的自动驾驶系统验证方法，该方法集成了现有的工业模拟器和正式定义的测试环境。该环境包括一个驱动模拟过程的场景生成器和一个在运行时对照一组待验证的系统属性检查观察到的系统行为的监视器。验证方法包括从模拟器中提取模拟系统的语义模型，该语义模型包括度量图，度量图是系统车辆进化环境的数学模型。监视器可以验证在一阶线性时间逻辑中形式化的属性，并提供解释其不满足的诊断。我们没有像许多模拟器那样随机探索系统行为，而是提出了一种方法来系统地生成涵盖潜在风险情况的场景集，特别是对于必须遵守特定交通规则的不同类型的路口。我们表明，对危险情况的系统探索发现了真实模拟器中的许多缺陷，这些缺陷很难通过随机探索过程发现。,计算机系统组织，嵌入式和信息物理系统，计算方法学，人工智能，控制方法，建模和仿真，软件及其工程，软件创建和管理，软件验证和确认，软件组织和属性,,,
JWW2GK8H,2023,https://doi.org/10.1145/3597926.3598129,ISSTA 2023,Systematic Testing of the Data-Poisoning Robustness of KNN,"Data poisoning aims to compromise a machine learning based software component by contaminating its training set to change its prediction results for test inputs. Existing methods for deciding data-poisoning robustness have either poor accuracy or long running time and, more importantly, they can only certify some of the truly-robust cases, but remain inconclusive when certification fails. In other words, they cannot falsify the truly-non-robust cases. To overcome this limitation, we propose a systematic testing based method, which can falsify as well as certify data-poisoning robustness for a widely used supervised-learning technique named k-nearest neighbors (KNN). Our method is faster and more accurate than the baseline enumeration method, due to a novel over-approximate analysis in the abstract domain, to quickly narrow down the search space, and systematic testing in the concrete domain, to find the actual violations. We have evaluated our method on a set of supervised-learning datasets. Our results show that the method significantly outperforms state-of-the-art techniques, and  
can decide data-poisoning robustness of KNN prediction results for most of the test inputs.","Computing methodologies,Machine learning,Learning paradigms,Supervised learning,Security and privacy,Formal methods and theory of security,Logic and verification,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification",KNN数据中毒鲁棒性的系统测试,数据中毒旨在通过污染基于机器学习的软件组件的训练集来改变其对测试输入的预测结果，从而危害该组件。现有的确定数据中毒稳健性的方法要么准确性差，要么运行时间长，更重要的是，它们只能证明一些真正稳健的案例，但当证明失败时，仍然没有结论。换言之，他们不能伪造真正不可靠的案例。为了克服这一限制，我们提出了一种基于系统测试的方法，该方法可以伪造并证明一种广泛使用的名为k近邻（KNN）的监督学习技术的数据中毒鲁棒性。我们的方法比基线枚举方法更快、更准确，因为它在抽象域中进行了一种新颖的过近似分析，可以快速缩小搜索空间，并在具体域中进行系统测试，以发现实际违规行为。我们已经在一组监督学习数据集上评估了我们的方法。我们的结果表明，该方法显著优于最先进的技术，并且,计算方法学，机器学习，学习范式，监督学习，安全和隐私，安全的形式化方法和理论，逻辑和验证，软件及其工程，软件创建和管理，软件验证和确认，形式化软件验证,,,
D677VA7Y,2023,https://doi.org/10.1145/3597926.3598076,ISSTA 2023,What You See Is What You Get? It Is Not the Case! Detecting Misleading Icons for Mobile Applications,"With the prevalence of smartphones, people nowadays can access a wide variety of services through diverse apps. A good Graphical User Interface (GUI) can make an app more appealing and competitive in app markets. Icon widgets, as an essential part of an app’s GUI, leverage icons to visually convey their functionalities to facilitate user interactions. Whereas, designing intuitive icon widgets can be a non-trivial job. Developers should follow a series of guidelines and make appropriate choices from a plethora of possibilities. Inappropriately designed or misused icons may cause user confusion, lead to wrong operations, and even result in security risks (e.g., revenue loss and privacy leakage). To investigate the problem, we manually checked 9,075 icons of 1,111 top-ranked commercial apps from Google Play and found 640 misleading icons in 312 ( ‍28%) of these apps. This shows that misleading icons are prevalent among real-world apps, even the top ones. ","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",你所见即所得？事实并非如此！检测移动应用程序的误导图标,随着智能手机的普及，如今人们可以通过各种应用程序访问各种各样的服务。一个好的图形用户界面（GUI）可以使应用程序在应用程序市场上更具吸引力和竞争力。图标小部件作为应用程序GUI的重要组成部分，利用图标直观地传达其功能，以方便用户交互。然而，设计直观的图标小部件可能是一项不平凡的工作。开发人员应该遵循一系列的指导原则，并从众多的可能性中做出适当的选择。设计不当或误用的图标可能会导致用户混淆，导致错误操作，甚至导致安全风险（如收入损失和隐私泄露）。为了调查这个问题，我们手动检查了谷歌Play排名第一的1111个商业应用程序中的9075个图标，在312个中发现了640个误导性图标( ‍28%）。这表明，误导性图标在现实世界的应用程序中普遍存在，即使是顶级应用程序也是如此。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
Y4UIN9BQ,2023,https://doi.org/10.1145/3597926.3598111,ISSTA 2023,SmartState: Detecting State-Reverting Vulnerabilities in Smart Contracts via Fine-Grained State-Dependency Analysis,"Smart contracts written in Solidity are widely used in different  
blockchain platforms such as Ethereum, TRON and BNB Chain.  
One of the unique designs in Solidity smart contracts is its statereverting  
mechanism for error handling and access control. Unfortunately,  
a number of recent security incidents showed that  
adversaries also utilize this mechanism to manipulate critical states  
of smart contracts, and hence, bring security consequences such as  
illegal profit-gain and Deny-of-Service (DoS). In this paper, we call  
such vulnerabilities as the State-reverting Vulnerability (SRV). Automatically  
identifying SRVs poses unique challenges, as it requires  
an in-depth analysis and understanding of the state-dependency  
relations in smart contracts.  
","Software and its engineering,Software creation and management,Software verification and validation",SmartState：通过细粒度状态依赖分析检测智能合约中的状态恢复漏洞,用Solidity编写的智能合约广泛用于,软件及其工程，软件创建和管理，软件验证和确认,,,
5DRQTUNK,2023,https://doi.org/10.1145/3597926.3598043,ISSTA 2023,Green Fuzzing: A Saturation-Based Stopping Criterion using Vulnerability Prediction,"Fuzzing is a widely used automated testing technique that uses random inputs to provoke program crashes indicating security breaches. A difficult but important question is when to stop a fuzzing campaign. Usually, a campaign is terminated when the number of crashes and/or covered code elements has not increased over a certain period of time. To avoid premature termination when a ramp-up time is needed before vulnerabilities are reached, code coverage is often preferred over crash count to decide when to terminate a campaign. However, a campaign might only increase the coverage on non-security-critical code or repeatedly trigger the same crashes. For these reasons, both code coverage and crash count tend to overestimate the fuzzing effectiveness, unnecessarily increasing the duration and thus the cost of the testing process.  
","Security and privacy,Software and application security,Software security engineering",绿色引信：一种基于饱和的基于脆弱性预测的停止准则,模糊化是一种广泛使用的自动化测试技术，它使用随机输入来引发表明安全漏洞的程序崩溃。一个困难但重要的问题是何时停止一场模糊的运动。通常，当崩溃和/或覆盖的代码元素的数量在一段时间内没有增加时，活动就会终止。为了避免在到达漏洞之前需要一段时间来提前终止，在决定何时终止活动时，代码覆盖率通常比崩溃计数更可取。然而，活动可能只会增加非安全关键代码的覆盖范围，或者重复触发相同的崩溃。由于这些原因，代码覆盖率和崩溃次数都倾向于高估模糊化的有效性，不必要地增加了持续时间，从而增加了测试过程的成本。,安全和隐私，软件和应用程序安全，软件安全工程,,,
BABAERY8,2023,https://doi.org/10.1145/3597926.3598080,ISSTA 2023,Towards More Realistic Evaluation for Neural Test Oracle Generation,"Unit testing has become an essential practice during software development and maintenance. Effective unit tests can help guard and improve software quality but require a substantial amount of time and effort to write and maintain. A unit test consists of a test prefix and a test oracle. Synthesizing test oracles, especially functional oracles, is a well-known challenging problem. Recent studies proposed to leverage neural models to generate test oracles, i.e., neural test oracle generation (NTOG), and obtained promising results. However, after a systematic inspection, we find there are some inappropriate settings in existing evaluation methods for NTOG. These settings could mislead the understanding of existing NTOG approaches’ performance. We summarize them as 1) generating test prefixes from bug-fixed program versions, 2) evaluating with an unrealistic metric, and 3) lacking a straightforward baseline. In this paper, we first investigate the impacts of these settings on evaluating and understanding the performance of NTOG approaches. We find that 1) unrealistically generating test prefixes from bug-fixed program versions inflates the number of bugs found by the state-of-the-art NTOG approach TOGA by 61.8%, 2) FPR (False Positive Rate) is not a realistic evaluation metric and the Precision of TOGA is only 0.38%, and 3) a straightforward baseline NoException, which simply expects no exception should be raised, can find 61% of the bugs found by TOGA with twice the Precision. Furthermore, we introduce an additional ranking step to existing evaluation methods and propose an evaluation metric named Found@K to better measure the cost-effectiveness of NTOG approaches in terms of bug-finding. We propose a novel unsupervised ranking method to instantiate this ranking step, significantly improving the cost-effectiveness of TOGA. Eventually, based on our experimental results and observations, we propose a more realistic evaluation method TEval+ for NTOG and summarize seven rules of thumb to boost NTOG approaches into their practical usages.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",面向更真实的神经测试Oracle生成评估,单元测试已经成为软件开发和维护过程中的一项重要实践。有效的单元测试可以帮助保护和提高软件质量，但需要大量的时间和精力来编写和维护。单元测试由一个测试前缀和一个测试预言器组成。综合测试预言机，尤其是功能预言机，是一个众所周知的具有挑战性的问题。最近的研究提出利用神经模型生成测试预言机，即神经测试预言机生成（NTOG），并获得了有希望的结果。然而，经过系统的检查，我们发现现有的NTOG评估方法存在一些不合适的设置。这些设置可能会误导对现有NTOG方法性能的理解。我们将其总结为1）从修复错误的程序版本生成测试前缀，2）使用不切实际的度量进行评估，以及3）缺乏直接的基线。在本文中，我们首先研究了这些设置对评估和理解NTOG方法性能的影响。我们发现，1）从错误修复程序版本不切实际地生成测试前缀，使最先进的NTOG方法TOGA发现的错误数量增加了61.8%，2）FPR（假阳性率）不是一个现实的评估指标，TOGA的精度仅为0.38%，3）一个简单的基线NoException，它只是期望不应该提出任何异常，可以以两倍的精度找到TOGA发现的61%的错误。此外，我们在现有的评估方法中引入了一个额外的排名步骤，并提出了一个名为Found@K以更好地衡量NTOG方法在漏洞发现方面的成本效益。我们提出了一种新的无监督排序方法来实例化该排序步骤，显著提高了TOGA的成本效益。最后，基于我们的实验结果和观察结果，我们提出了一种更现实的NTOG评估方法TEval+，并总结了七条经验法则，以促进NTOG方法的实际应用。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
SHFXG9XN,2023,https://doi.org/10.1145/3597926.3598086,ISSTA 2023,More Precise Regression Test Selection via Reasoning about Semantics-Modifying Changes,"Regression test selection (RTS) speeds up regression testing by only re-running tests that might be affected by code changes. Ideal RTS safely selects all affected tests and precisely selects only affected tests. But, aiming for this ideal is often slower than re-running all tests. So, recent RTS techniques use program analysis to trade precision for speed, i.e., lower regression testing time, or even use machine learning to trade safety for speed. We seek to make recent analysis-based RTS techniques more precise, to further speed up regression testing. Independent studies suggest that these techniques reached a “performance wall” in the speed-ups that they provide.  
We manually inspect code changes to discover those that do not require re-running tests that are only affected by such changes. We categorize 29 kinds of changes that we find from five projects into 13 findings, 11 of which are semantics-modifying. We enhance two RTS techniques---Ekstazi and STARTS---to reason about our findings. Using 1,150 versions of 23 projects, we evaluate the impact on safety and precision of leveraging such changes. We also evaluate if our findings from a few projects can speed up regression testing in other projects. The results show that our enhancements are effective and they can generalize. On average, they result in selecting 41.7% and 31.8% fewer tests, and take 33.7% and 28.7% less time than Ekstazi and STARTS, respectively, with no loss in safety.","Software and its engineering,Software creation and management,Software post-development issues,Software evolution,Software verification and validation,Software defect analysis,Software testing and debugging",基于语义修改变化推理的更精确回归测试选择,回归测试选择（RTS）通过仅重新运行可能受代码更改影响的测试来加快回归测试。理想RTS安全地选择所有受影响的测试，并精确地只选择受影响的试验。但是，实现这一理想往往比重新运行所有测试要慢。因此，最近的RTS技术使用程序分析以精度换取速度，即较低的回归测试时间，甚至使用机器学习以安全换取速度。我们试图使最近基于分析的RTS技术更加精确，以进一步加快回归测试。独立研究表明，这些技术在提供的加速中达到了“性能墙”。,软件及其工程，软件创建和管理，软件开发后问题，软件演化，软件验证和确认，软件缺陷分析，软件测试和调试,,,
ZUVVG2BA,2023,https://doi.org/10.1145/3597926.3598149,ISSTA 2023,Extracting Inline Tests from Unit Tests,"We recently proposed inline tests for validating individual program statements; they allow developers to provide test inputs, expected outputs, and test oracles immediately after a target statement. But, existing code can have many target statements. So, automatic generation of inline tests is an important next step towards increasing their adoption. We propose ExLi, the first technique for automatically generating inline tests. ExLi extracts inline tests from unit tests; it first records all variable values at a target statement while executing unit tests. Then, ExLi uses those values as test inputs and test oracles in an initial set of generated inline tests. Target statements that are executed many times could have redundant initial inline tests. So, ExLi uses a novel coverage-then-mutants based reduction process to remove redundant inline tests. We implement ExLi for Java and use it to generate inline tests for 718 target statements in 31 open-source programs. ExLi reduces 17,273 initially generated inline tests to 905 inline tests. The final set of generated inline tests kills up to 25.1% more mutants on target statements than developer written and automatically generated unit tests. That is, ExLi generates inline tests that can improve the fault-detection capability of the test suites from which they are extracted.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",从单元测试中提取内联测试,我们最近提出了用于验证单个程序语句的内联测试；它们允许开发人员在目标语句之后立即提供测试输入、预期输出和测试oracles。但是，现有的代码可能有许多目标语句。因此，内联测试的自动生成是提高其采用率的重要下一步。我们提出了ExLi，这是第一种自动生成内联测试的技术。ExLi从单元测试中提取内联测试；它首先在执行单元测试时记录目标语句中的所有变量值。然后，ExLi在生成的一组初始内联测试中使用这些值作为测试输入和测试预言器。执行多次的目标语句可能具有冗余的初始内联测试。因此，ExLi使用了一种新的覆盖率然后基于突变体的减少过程来消除冗余的内联测试。我们为Java实现了ExLi，并使用它为31个开源程序中的718个目标语句生成内联测试。ExLi将17273个最初生成的内联测试减少到905个内联测试。最后一组生成的内联测试在目标语句上杀死的突变体比开发人员编写和自动生成的单元测试多25.1%。也就是说，ExLi生成的内联测试可以提高从中提取测试套件的故障检测能力。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
RH8T2X7A,2023,https://doi.org/10.1145/3597926.3598056,ISSTA 2023,A Comprehensive Study on Quality Assurance Tools for Java,"Quality assurance (QA) tools are receiving more and more attention and are widely used by developers. Given the wide range of solutions for QA technology, it is still a question of evaluating QA tools. Most existing research is limited in the following ways: (i) They compare tools without considering scanning rules analysis. (ii) They disagree on the effectiveness of tools due to the study methodology and benchmark dataset. (iii) They do not separately analyze the role of the warnings. (iv) There is no large-scale study on the analysis of time performance. To address these problems, in the paper, we systematically select 6 free or open-source tools for a comprehensive study from a list of 148 existing Java QA tools. To carry out a comprehensive study and evaluate tools in multi-level dimensions, we first mapped the scanning rules to the CWE and analyze the coverage and granularity of the scanning rules. Then we conducted an experiment on 5 benchmarks, including 1,425 bugs, to investigate the effectiveness of these tools. Furthermore, we took substantial effort to investigate the effectiveness of warnings by comparing the real labeled bugs with the warnings and investigating their role in bug detection. Finally, we assessed these tools’ time performance on 1,049 projects. The useful findings based on our comprehensive study can help developers improve their tools and provide users with suggestions for selecting QA tools.","General and reference,Cross-computing tools and techniques,Empirical studies,Software and its engineering,Software notations and tools,Software maintenance tools",Java质量保证工具的综合研究,质量保证（QA）工具正受到越来越多的关注，并被开发人员广泛使用。鉴于QA技术的解决方案范围广泛，评估QA工具仍然是一个问题。大多数现有的研究在以下方面受到限制：（i）他们在不考虑扫描规则分析的情况下比较工具。（ii）由于研究方法和基准数据集，他们对工具的有效性存在分歧。（iii）他们没有单独分析警告的作用。（iv）没有对时间表现的分析进行大规模研究。为了解决这些问题，在本文中，我们从148个现有的Java QA工具中系统地选择了6个免费或开源工具进行全面研究。为了在多层次维度上对工具进行全面的研究和评估，我们首先将扫描规则映射到CWE，并分析扫描规则的覆盖范围和粒度。然后，我们对5个基准测试进行了实验，其中包括1425个bug，以研究这些工具的有效性。此外，我们花了大量的精力来调查警告的有效性，将真正标记的错误与警告进行比较，并调查它们在错误检测中的作用。最后，我们评估了这些工具在1049个项目中的时间性能。基于我们全面研究的有用发现可以帮助开发人员改进他们的工具，并为用户选择QA工具提供建议。,通则和参考，交叉计算工具和技术，经验研究，软件及其工程，软件符号和工具，软件维护工具,,,
T32DK3I2,2023,https://doi.org/10.1145/3597926.3598060,ISSTA 2023,Who Judges the Judge: An Empirical Study on Online Judge Tests,"Online Judge platforms play a pivotal role in education, competitive programming, recruitment, career training, and large language model training. They rely on predefined test suites to judge the correctness of submitted solutions. It is therefore important that the solution judgement is reliable and free from potentially misleading false positives (i.e., incorrect solutions that are judged as correct). In this paper, we conduct an empirical study of 939 coding problems with 541,552 solutions, all of which are judged to be correct according to the test suites used by the platform, finding that 43.4% of the problems include false positive solutions (3,440 bugs are revealed in total). We also find that test suites are, nevertheless, of high quality according to widely-studied test effectiveness measurements: 88.2% of false positives have perfect (100%) line coverage, 78.9% have perfect branch coverage, and 32.5% have a perfect mutation score. Our findings indicate that more work is required to weed out false positive solutions and to further improve test suite effectiveness. We have released the detected false positive solutions and the generated test inputs to facilitate future research.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",谁评判法官——基于在线法官测试的实证研究,在线法官平台在教育、竞技节目、招聘、职业培训和大型语言模型培训中发挥着关键作用。它们依赖于预定义的测试套件来判断提交的解决方案的正确性。因此，重要的是，解决方案的判断是可靠的，没有潜在的误导性误报（即被判断为正确的不正确解决方案）。在本文中，我们对939个编码问题进行了实证研究，共有541552个解决方案，根据平台使用的测试套件，所有这些问题都被判断为正确的，发现43.4%的问题包括假阳性解决方案（共发现3440个错误）。然而，我们还发现，根据广泛研究的测试有效性测量，测试套件具有高质量：88.2%的假阳性具有完美（100%）的行覆盖率，78.9%具有完美的分支覆盖率，32.5%具有完美的突变得分。我们的研究结果表明，需要做更多的工作来清除假阳性解决方案，并进一步提高测试套件的有效性。我们已经发布了检测到的假阳性解决方案和生成的测试输入，以促进未来的研究。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
PQRJY8MF,2023,https://doi.org/10.1145/3597926.3598042,ISSTA 2023,Hybrid Inlining: A Framework for Compositional and Context-Sensitive Static Analysis,"Context-sensitivity is essential for achieving good precision in inter-procedural static analysis. To be context-sensitive, top-down analysis needs to fully inline all the statements in a callee at all its callsites, leading to statement explosion. Compositional analysis, which inlines summaries of all the callees, scales up but often loses precision, as it is not strictly context-sensitive. We propose a compositional and strictly context-sensitive framework for static analysis. This framework is based on a key observation: a compositional analysis often loses precision only on some critical statements that need to be analyzed context-sensitively. Our approach hybridly inlines the critical statements and the summaries of non-critical statements of each callee, thus avoiding re-analyzing non-critical ones. In addition, our analysis lazily summarizes the critical statements, by stopping propagating the critical statements once the calling context accumulated is adequate. We have designed and implemented several analyses (including a pointer analysis) based on this framework. Our evaluation on the pointer analysis shows that it can analyze large Java programs from the DaCapo benchmark suite and industry in minutes. Compared to context-insensitive analysis, Hybrid Inlining introduces only 65% and 1% additional time overheads on DaCapo and industrial applications, respectively.","Software and its engineering,Software organization and properties,Software functional properties,Formal methods,Automated static analysis",混合嵌入：一个用于组合和上下文敏感静态分析的框架,上下文敏感性对于实现程序间静态分析的良好精度至关重要。为了区分上下文，自上而下的分析需要在被调用方的所有调用位置完全内联被调用方中的所有语句，从而导致语句爆炸。组合分析内联所有被调用方的摘要，它可以扩展，但往往会失去精度，因为它不是严格区分上下文的。我们提出了一个组合的、严格上下文敏感的静态分析框架。这个框架基于一个关键的观察结果：成分分析往往只在一些需要敏感地分析上下文的关键语句上失去准确性。我们的方法混合内联了每个被调用者的关键语句和非关键语句的摘要，从而避免了重新分析非关键语句。此外，我们的分析通过在调用上下文累积足够后停止传播关键语句来懒惰地总结关键语句。基于这个框架，我们设计并实现了一些分析（包括指针分析）。我们对指针分析的评估表明，它可以在几分钟内分析来自DaCapo基准套件和行业的大型Java程序。与上下文不敏感分析相比，Hybrid Inlining在DaCapo和工业应用程序上分别只引入了65%和1%的额外时间开销。,软件及其工程，软件组织和属性，软件功能属性，形式化方法，自动静态分析,,,
HW8SZ56J,2023,https://doi.org/10.1145/3597926.3598092,ISSTA 2023,RefBERT: A Two-Stage Pre-trained Framework for Automatic Rename Refactoring,"Refactoring is an indispensable practice of improving the quality and maintainability of source code in software evolution. Rename refactoring is the most frequently performed refactoring that suggests a new name for an identifier to enhance readability when the identifier is poorly named. However, most existing works only identify renaming activities between two versions of source code, while few works express concern about how to suggest a new name. In this paper, we study automatic rename refactoring on variable names, which is considered more challenging than other rename refactoring activities. We first point out the connections between rename refactoring and various prevalent learning paradigms and the difference between rename refactoring and general text generation in natural language processing. Based on our observations, we propose RefBERT, a two-stage pre-trained framework for rename refactoring on variable names. RefBERT first predicts the number of sub-tokens in the new name and then generates sub-tokens accordingly. Several techniques, including constrained masked language modeling, contrastive learning, and the bag-of-tokens loss, are incorporated into RefBERT to tailor it for automatic rename refactoring on variable names. Through extensive experiments on our constructed refactoring datasets, we show that the generated variable names of RefBERT are more accurate and meaningful than those produced by the existing method. Our implementation and data are available at https://github.com/KDEGroup/RefBERT.","Computing methodologies,Artificial intelligence,Natural language processing,Software and its engineering,Software creation and management,Software notations and tools",RefBERT:一个用于自动重命名重构的两阶段预训练框架,重构是软件进化中提高源代码质量和可维护性的一种必不可少的实践。重命名重构是最常执行的重构，当标识符命名不正确时，它会为标识符建议一个新名称，以增强可读性。然而，大多数现有的工作只确定两个版本的源代码之间的重命名活动，而很少有工作对如何建议新名称表示担忧。在本文中，我们研究了变量名的自动重命名重构，这被认为比其他重命名重构活动更具挑战性。我们首先指出了重命名重构与各种流行的学习范式之间的联系，以及自然语言处理中重命名重构与一般文本生成之间的区别。基于我们的观察，我们提出了RefBERT，这是一个两阶段的预训练框架，用于对变量名进行重命名重构。RefBERT首先预测新名称中的子令牌的数量，然后相应地生成子令牌。RefBERT中包含了几种技术，包括受约束的掩蔽语言建模、对比学习和令牌袋丢失，以便对变量名进行自动重命名重构。通过在我们构建的重构数据集上进行大量实验，我们表明RefBERT生成的变量名比现有方法生成的变量名称更准确、更有意义。我们的实施和数据可在https://github.com/KDEGroup/RefBERT.,计算方法学，人工智能，自然语言处理，软件及其工程，软件创建和管理，软件符号和工具,,,
RWJLZM6C,2023,https://doi.org/10.1145/3597926.3598068,ISSTA 2023,Exploring Missed Optimizations in WebAssembly Optimizers,"The prosperous trend of deploying complex applications to web browsers has  
boosted the development of WebAssembly (wasm) compilation toolchains. Software  
written in different high-level programming languages are compiled into wasm  
executables, which can be executed fast and safely in a virtual machine. The  
performance of wasm executables depends highly on compiler optimizations.  
Despite the prosperous use of wasm executables, recent research has indicated  
that real-world wasm applications are slower than anticipated, suggesting  
deficiencies in wasm optimizations.  
","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Compilers",WebAssembly优化程序中遗漏优化的探索,将复杂应用程序部署到web浏览器的繁荣趋势,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，编译器,,,
JMA4EE42,2023,https://doi.org/10.1145/3597926.3598053,ISSTA 2023,Fuzzing Deep Learning Compilers with HirGen,"Deep Learning (DL) compilers are widely adopted to optimize advanced DL models for efficient deployment on diverse hardware.  
Their quality has a profound effect on the quality of compiled  
DL models. A recent bug study shows that the optimization of  
high-level intermediate representations (IRs) is the most error-prone  
compilation stage and bugs in this stage account for 44.92% of the  
whole collected ones. However, existing testing techniques do not  
consider the features related to high-level optimization (e.g., the  
high-level IR), and are therefore weak in exposing bugs at this stage.  
To bridge this gap, we propose HirGen, an automated testing technique that effectively exposes coding mistakes in the optimization  
of high-level IRs. The design of HirGen includes 1) three coverage  
criteria to generate diverse and valid computational graphs; 2) the  
use of the high-level IR’s language features to generate diverse IRs;  
3) three test oracles of which two are inspired by metamorphic  
testing and differential testing. HirGen has successfully detected  
21 bugs that occur at TVM, with 17 bugs confirmed and 12 fixed.  
Further, we construct four baselines using state-of-the-art DL compiler fuzzers that can cover the high-level optimization stage. Our  
experiment results show that HirGen can detect 10 crashes and  
inconsistencies that cannot be detected by the baselines in 48 hours.  
We also evaluate the usefulness of our proposed coverage criteria  
and test oracles.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",用HirGen模糊化深度学习编译器,深度学习（DL）编译器被广泛用于优化高级DL模型，以便在各种硬件上进行高效部署。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
CA6TTV2E,2023,https://doi.org/10.1145/3597926.3598052,ISSTA 2023,Dependency-Aware Metamorphic Testing of Datalog Engines,"Datalog is a declarative query language with wide applicability,  
 especially in program analysis. Queries are evaluated by Datalog  
 engines, which are complex and thus prone to returning  
 incorrect results. Such bugs, called query bugs, may compromise the  
 soundness of upstream program analyzers, having potentially  
 detrimental consequences in safety-critical settings.  
","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",数据日志引擎的依赖感知变形测试,Datalog是一种具有广泛适用性的声明性查询语言，,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
APT7JTDP,2023,https://doi.org/10.1145/3597926.3598133,ISSTA 2023,Toward Automated Detecting Unanticipated Price Feed in Smart Contract,"Decentralized finance (DeFi) based on smart contracts has reached a total value locked (TVL) of over USD 200 billion in 2022. In DeFi ecosystems, price oracles play a critical role in providing real-time price feeds for cryptocurrencies to ensure accurate asset pricing in smart contracts. However, the price oracle also faces security issues, including the possibility of unanticipated price feeds, which can lead to imbalances in debt and assets in the DeFi protocol. However, existing solutions cannot effectively combine transactions and code for real-time monitoring of price oracles.  
","Software and its engineering,Software creation and management,Software verification and validation",智能合约中的自动检测非预期价格馈送,2022年，基于智能合约的去中心化金融（DeFi）锁定总价值已超过2000亿美元。在DeFi生态系统中，价格神谕在为加密货币提供实时价格源以确保智能合约中的资产定价准确方面发挥着关键作用。然而，价格预言机也面临着安全问题，包括意外价格馈送的可能性，这可能导致DeFi协议中的债务和资产失衡。然而，现有的解决方案无法有效地结合交易和代码来实时监控价格预言机。,软件及其工程，软件创建和管理，软件验证和确认,,,
4738C7FR,2023,https://doi.org/10.1145/3597926.3604928,ISSTA 2023,EvoSpex: A Search-Based Tool for Postcondition Inference,"Postconditions are predicates that specify the intended behavior of a program by capturing properties about the program state when the program finishes its execution. Although postconditions can help to improve many software reliability analyses, they are seldom found accompanying source code. Thus, tools that assist developers in specifying postconditions are useful. This tool demo paper presents EvoSpex, a tool based on evolutionary computation that automatically infers postconditions of Java methods. Given a target Java method and a test suite for it, our tool executes the test suite to obtain valid pre/post state pairs for the method under analysis. Then, these pairs are mutated to obtain (allegedly) invalid ones, and finally a postcondition assertion characterizing the current method behavior is produced, by using an evolutionary algorithm that searches for an assertion that is satisfied by the valid pre/post state pairs and leaves out the invalid ones. EvoSpex implements a classic genetic algorithm that explores the space of candidate postconditions over a JML-like specification language. The algorithm is guided by a fitness function that aims at precisely capturing the valid state pairs, rejecting the invalid ones, and that also favors more succinct assertions.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,System description languages,Specification languages",EvoSpex：一种基于搜索的后条件推理工具,后条件是指通过在程序完成执行时捕获有关程序状态的属性来指定程序预期行为的谓词。尽管后置条件可以帮助改进许多软件可靠性分析，但它们很少与源代码一起使用。因此，帮助开发人员指定后置条件的工具非常有用。该工具演示文件介绍了EvoSpex，这是一个基于进化计算的工具，可以自动推断Java方法的后条件。给定一个目标Java方法和它的测试套件，我们的工具执行测试套件以获得被分析方法的有效前/后状态对。然后，对这些对进行突变以获得（据称）无效的断言，最后通过使用进化算法来产生表征当前方法行为的后条件断言，该算法搜索由有效的前/后状态对满足的断言并排除无效的断言。EvoSpex实现了一种经典的遗传算法，该算法在类似JML的规范语言上探索候选后条件的空间。该算法由适应度函数指导，该函数旨在准确捕获有效状态对，拒绝无效状态对，并且还支持更简洁的断言。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，系统描述语言，规范语言,,,
SNJK2HTD,2023,https://doi.org/10.1145/3597926.3605232,ISSTA 2023,Automatic Testing and Benchmarking for Configurable Static Analysis Tools,"Static analysis is an important tool for detecting bugs in real-world software. The advent of numerous analysis algorithms with their own tradeoffs has led to the proliferation of configurable static analysis tools, but their complex, undertested configuration spaces are obstacles to their widespread adoption. To improve the reliability of these tools, my research focuses on developing new approaches to automatically test and debug them. First, I describe an empirical study that helps to understand the performance and behavior of configurable taint analysis tools for Android. The findings of this study motivate the development of ECSTATIC, a framework for testing and debugging that goes beyond taint analysis to test any configurable static analysis tool. The next steps for this research involve the automatic creation of real-world benchmarks for static analysis with associated ground truths and analysis features.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Formal methods,Automated static analysis",可配置静态分析工具的自动测试和基准测试,静态分析是检测现实软件中错误的重要工具。许多具有自身权衡的分析算法的出现导致了可配置静态分析工具的激增，但其复杂、测试不足的配置空间阻碍了其广泛采用。为了提高这些工具的可靠性，我的研究重点是开发新的方法来自动测试和调试它们。首先，我描述了一项实证研究，该研究有助于理解Android可配置污染分析工具的性能和行为。这项研究的发现推动了ECSTATIC的开发，这是一个测试和调试框架，它超越了污染分析，可以测试任何可配置的静态分析工具。这项研究的下一步涉及自动创建用于静态分析的真实世界基准，以及相关的基本事实和分析特征。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，形式化方法，自动静态分析,,,
IZ9DCUS5,2023,https://doi.org/10.1145/3597926.3604918,ISSTA 2023,ECSTATIC: Automatic Configuration-Aware Testing and Debugging of Static Analysis Tools,"Static analyses are powerful tools that can serve as a complement to dynamic approaches such as testing. In order to ensure generality, many static analysis tools are configurable. However, these configurations can make testing and debugging more difficult. To address this issue, we introduce a new tool, ECSTATIC, which leverages partial order relations between analysis configuration options to automatically test and debug static analyzers, even without ground truths. ECSTATIC’s results are reproducible by virtue of running within Docker containers, and ECSTATIC provides clear extension interfaces for users to add their own tools and input programs. We evaluated ECSTATIC on four popular dataflow analysis tools, and found 74 bugs in all four tools. We also found that ECSTATIC’s novel two-staged delta debugging was able to reduce real-world programs by 50%, compared to a baseline of 6%.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software functional properties,Formal methods,Automated static analysis",ECSTATIC：静态分析工具的自动配置感知测试和调试,静态分析是功能强大的工具，可以作为测试等动态方法的补充。为了确保通用性，许多静态分析工具都是可配置的。但是，这些配置可能会使测试和调试更加困难。为了解决这个问题，我们引入了一种新的工具ECSTATIC，它利用分析配置选项之间的偏序关系来自动测试和调试静态分析器，即使没有基本事实。通过在Docker容器中运行，ECSTATIC的结果是可复制的，并且ECSTATIC为用户添加自己的工具和输入程序提供了清晰的扩展接口。我们在四个流行的数据流分析工具上评估了ECSTATIC，在所有四个工具中发现了74个错误。我们还发现，ECSTATIC新颖的两阶段delta调试能够将现实世界中的程序减少50%，而基线为6%。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件功能属性，形式化方法，自动静态分析,,,
VBN9D9KU,2023,https://doi.org/10.1145/3597926.3598037,ISSTA 2023,Understanding and Tackling Label Errors in Deep Learning-Based Vulnerability Detection (Experience Paper),"Software system complexity and security vulnerability diversity are plausible sources of the persistent challenges in software vulnerability research. Applying deep learning methods for automatic vulnerability detection has been proven an effective means to complement traditional detection approaches. Unfortunately, lacking well-qualified benchmark datasets could critically restrict the effectiveness of deep learning-based vulnerability detection techniques. Specifically, the long-term existence of erroneous labels in the existing vulnerability datasets may lead to inaccurate, biased, and even flawed results. ","Computing methodologies,Machine learning,General and reference,Cross-computing tools and techniques,Empirical studies,Software and its engineering,Software creation and management,Software verification and validation,Empirical software validation",理解和解决基于深度学习的漏洞检测中的标签错误（经验论文）,软件系统的复杂性和安全漏洞的多样性是软件漏洞研究中持续挑战的可能来源。将深度学习方法应用于漏洞自动检测已被证明是补充传统检测方法的有效手段。不幸的是，缺乏合格的基准数据集可能会严重限制基于深度学习的漏洞检测技术的有效性。具体而言，现有漏洞数据集中长期存在错误标签可能导致不准确、有偏见甚至有缺陷的结果。,计算方法论，机器学习，概论和参考，交叉计算工具和技术，经验研究，软件及其工程，软件创建和管理，软件验证和确认，经验软件验证,,,
NQDLXL58,2023,https://doi.org/10.1145/3597926.3598144,ISSTA 2023,Green Fuzzer Benchmarking,"Over the last decade, fuzzing has been increasingly gaining  
 traction due to its effectiveness in finding  
 bugs. Nevertheless, fuzzer evaluations have been challenging  
 during this time, mainly due to lack of standardized  
 benchmarking. Aiming to alleviate this issue, in 2020, Google  
 released FuzzBench, an open-source benchmarking platform, that  
 is widely used for accurate fuzzer benchmarking.  
","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",绿色引信基准,在过去的十年里，模糊处理越来越多,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
FNA25QW4,2023,https://doi.org/10.1145/3597926.3598132,ISSTA 2023,Automated Generation of Security-Centric Descriptions for Smart Contract Bytecode,"Smart contract and DApp users are taking great risks, as they do not  
obtain necessary knowledge that can help them avoid using vulnera-  
ble and malicious contract code. In this paper, we develop a novel  
system Tx2TXT that can automatically create security-centric textual  
descriptions directly from smart contract bytecode. To capture the  
security aspect of financial applications, we formally define a funds  
transfer graph to model critical funds flows in smart contracts. To  
ensure the expressiveness and conciseness of the descriptions de-  
rived from these graphs, we employ a GCN-based model to identify  
security-related condition statements and selectively add them to our  
graph models. To convert low-level bytecode instructions to human-  
readable textual scripts, we leverage robust API signatures to recover  
bytecode semantics. We have evaluated Tx2TXT on 890 well-labeled  
vulnerable, malicious and safe contracts where developer-crafted  
descriptions are available. Our results have shown that Tx2TXT out-  
performs state-of-the-art solutions and can effectively help end users  
avoid risky contracts","Security and privacy,Software and application security",智能合约字节码安全中心描述的自动生成,智能合约和DApp用户正在承担巨大的风险，因为他们没有,安全和隐私，软件和应用程序安全,,,
8FJCMTT3,2023,https://doi.org/10.1145/3597926.3598097,ISSTA 2023,OCFI: Make Function Entry Identification Hard Again,"Function entry identification is a crucial yet challenging task for binary disassemblers that has been the focus of research in the past decades. However, recent researches show that call frame information (CFI) provides accurate and almost complete function entries. With the aid of CFI, disassemblers have significant improvements in function entry detection. CFI is specifically designed for efficient stack unwinding, and every function has corresponding CFI in x64 and aarch64 architectures. Nevertheless, not every function and instruction unwinds the stack at runtime, and this observation has led to the development of techniques such as obfuscation to complicate function detection by disassemblers.  
","Security and privacy,Software and application security,Software reverse engineering,Software security engineering",OCFI：再次使函数入口识别变得困难,函数入口识别是二进制反汇编程序的一项关键但具有挑战性的任务，在过去几十年中一直是研究的焦点。然而，最近的研究表明，调用帧信息（CFI）提供了准确且几乎完整的函数条目。在CFI的帮助下，反汇编程序在函数入口检测方面有了显著的改进。CFI是专门为高效的堆栈展开而设计的，在x64和aarch64架构中，每个功能都有相应的CFI。然而，并不是每个函数和指令都在运行时解开堆栈，这一观察结果导致了模糊处理等技术的发展，使反汇编程序的函数检测复杂化。,安全和隐私，软件和应用程序安全，软件逆向工程，软件安全工程,,,
BBNPJ5ER,2023,https://doi.org/10.1145/3597926.3604924,ISSTA 2023,RobotBT: Behavior-Tree-Based Test-Case Specification for the Robot Framework,"The Robot Framework is a popular and widely used test automation framework that abstracts test case specifications toward natural language specifications. This makes it well suited for implementing high-level test cases, at least as long as the functions provided by Robot can support the intended functionality. For more complicated test cases, custom and often deeply nested functionality specifications are required, and the readability of Robot test cases tends to decrease. We present RobotBT, a library for the Robot framework that addresses these shortcomings by adding support for specifying test cases using behavior trees. Behavior trees are a comprehensive method for specifying complex behaviors based on a control flow model that orchestrates the execution of functionality. We evaluated RobotBT on a test suite for GUI testing from G~DATA CyberDefense AG and interviewed their engineers about the usability, readability, and applicability of RobotBT. Our results show that BTs improve the expressiveness and readability of Robot Framework test cases and are applicable to practical problems.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Context specific languages,Domain specific languages,General programming languages,Language features,Control structures",RobotBT：基于行为树的机器人框架测试用例规范,Robot框架是一个流行且广泛使用的测试自动化框架，它将测试用例规范抽象为自然语言规范。这使得它非常适合实现高级测试用例，至少只要Robot提供的功能能够支持预期的功能。对于更复杂的测试用例，需要自定义且通常嵌套很深的功能规范，并且Robot测试用例的可读性往往会降低。我们介绍了RobotBT，一个用于Robot框架的库，它通过添加对使用行为树指定测试用例的支持来解决这些缺点。行为树是一种基于协调功能执行的控制流模型来指定复杂行为的综合方法。我们在G~DATA CyberDefense AG的GUI测试套件上评估了RobotBT，并就RobotBT的可用性、可读性和适用性采访了他们的工程师。我们的结果表明，BT提高了Robot Framework测试用例的表达性和可读性，适用于实际问题。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，上下文特定语言，领域特定语言，通用编程语言，语言特征，控制结构,,,
VRN8MPUH,2023,https://doi.org/10.1145/3597926.3598128,ISSTA 2023,SlipCover: Near Zero-Overhead Code Coverage for Python,"Coverage analysis is widely used but can suffer from high overhead. This overhead is especially acute in the context of Python, which is already notoriously slow (a recent study observes a roughly 30x slowdown vs. native code). We find that the state-of-the-art coverage tool for Python, coverage.py, introduces a median overhead of 180% with the standard Python interpreter. Slowdowns are even more extreme when using PyPy, a JIT-compiled Python implementation, with coverage.py imposing a median overhead of 1,300%. This performance degradation reduces the utility of coverage analysis in most use cases, including testing and fuzzing, and precludes its use in deployment.  
This paper presents SlipCover, a novel, near-zero overhead coverage analyzer for Python. SlipCover works without modifications to either the Python interpreter or PyPy. It first processes a program's AST to accurately identify all branches and lines. SlipCover then dynamically rewrites Python bytecodes to add lightweight instrumentation to each identified branch and line. At run time, SlipCover periodically de-instruments already-covered lines and branches. The result is extremely low overheads -- a median of just 5% -- making SlipCover suitable for use in deployment. We show its efficiency can translate to significant increases in the speed of coverage-based clients. As a proof of concept, we integrate SlipCover into TPBT, a targeted property-based testing system, and observe a 22x speedup.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",SlipCover：Python的接近零开销代码覆盖率,覆盖率分析被广泛使用，但是可能遭受高开销。这种开销在Python的环境中尤其严重，因为Python已经是出了名的慢（最近的一项研究观察到，与本机代码相比，它的速度大约慢了30倍）。我们发现，最先进的Python覆盖工具coverage.py在标准Python解释器中引入了180%的平均开销。当使用PyPy（一种JIT编译的Python实现）时，速度减慢甚至更为极端，coverage.py的开销中值为1300%。这种性能下降降低了覆盖率分析在大多数用例中的实用性，包括测试和模糊化，并阻止了它在部署中的使用。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
XS46XFR8,2023,https://doi.org/10.1145/3597926.3604923,ISSTA 2023,MetaData262: Automatic Test Suite Selection for Partial JavaScript Implementations,"Despite the large number of partial reference implementations of the JavaScript language, there is currently no automatic mechanism for selecting the appropriate official tests for such implementations. To fill this gap, we introduce a new format for presenting the metadata associated with the tests included in Test262, the official JavaScript test suite, and present MetaData262, a new tool for both computing the metadata of Test262 tests and filtering tests according to their respective metadata properties.","Software and its engineering,Software notations and tools,Software libraries and repositories",MetaData262：部分JavaScript实现的自动测试套件选择,尽管JavaScript语言有大量的部分引用实现，但目前还没有为这些实现选择适当的官方测试的自动机制。为了填补这一空白，我们引入了一种新的格式来呈现与官方JavaScript测试套件Test262中包含的测试相关的元数据，并推出了MetaData262，这是一种新工具，用于计算Test262测试的元数据并根据其各自的元数据属性过滤测试。,软件及其工程，软件符号和工具，软件库和存储库,,,
VGTIZFRH,2023,https://doi.org/10.1145/3597926.3598146,ISSTA 2023,An Empirical Study on the Effects of Obfuscation on Static Machine Learning-Based Malicious JavaScript Detectors,"Machine learning is increasingly being applied to malicious JavaScript detection in response to the growing number of Web attacks and the attendant costly manual identification. In practice, to hide their malicious behaviors or protect intellectual copyrights, both malicious and benign scripts tend to obfuscate their own code before uploading. While obfuscation is beneficial, it also introduces some additional code features (e.g., dead code) into the code. When machine learning is employed to learn a malicious JavaScript detector, these additional features can affect the model to make it less effective. However, there is still a lack of clear understanding of how robust existing machine learning-based detectors are on different obfuscators.  
In this paper, we conduct the first empirical study to figure out how obfuscation affects machine learning detectors based on static features. Through the results, we observe several findings: 1) Obfuscation has a significant impact on the effectiveness of detectors, causing an increase both in false negative rate (FNR) and false positive rate (FPR), and the bias of obfuscation in the training  
set induces detectors to detect obfuscation rather than malicious behaviors. 2) The common measures such as improving the quality of the training set by adding relevant obfuscated samples and leveraging state-of-the-art deep learning models can not work well.3) The root cause of obfuscation effects on these detectors is that feature spaces they use can only reflect shallow differences in code, not about the nature of benign and malicious, which can be easily affected by the differences brought by obfuscation. 4) Obfuscation has a similar effect on realistic detectors in VirusTotal, indicating that this is a common real-world problem.","Security and privacy,Intrusion/anomaly detection and malware mitigation,Malware and its mitigation",混淆对基于静态机器学习的恶意JavaScript检测器影响的实证研究,机器学习正越来越多地应用于恶意JavaScript检测，以应对日益增多的网络攻击和随之而来的昂贵的手动识别。在实践中，为了隐藏他们的恶意行为或保护知识版权，恶意和良性脚本都倾向于在上传之前混淆自己的代码。虽然模糊处理是有益的，但它也在代码中引入了一些额外的代码特性（例如，死代码）。当使用机器学习来学习恶意JavaScript检测器时，这些附加功能可能会影响模型，使其效率降低。然而，对于现有的基于机器学习的检测器在不同的模糊器上的鲁棒性，仍然缺乏明确的理解。,安全和隐私，入侵/异常检测和恶意软件缓解，恶意软件及其缓解,,,
TU3QQHDZ,2023,https://doi.org/10.1145/3597926.3605237,ISSTA 2023,Type Automata,"PL researchers have a profound understanding of automata theory but fail to grasp the subtleties and nuances of the type systems used in modern programming languages.  
My research pursues new insights into the computational power of type systems by connecting them with well-founded classes of automata through type automata---machines that employ program types as control and storage.  
In addition, I demonstrate advanced type-level metaprogramming applications of type automata that coerce the compiler into performing computations at compile time.","Software and its engineering,Software notations and tools,Context specific languages,API languages,Domain specific languages,Formal language definitions,General programming languages",类型自动机,PL研究人员对自动机理论有着深刻的理解，但未能理解现代编程语言中使用的类型系统的微妙之处。,软件及其工程，软件符号和工具，上下文特定语言，API语言，领域特定语言，形式语言定义，通用编程语言,,,
2APGTSJL,2023,https://doi.org/10.1145/3597926.3598134,ISSTA 2023,Virtual Reality (VR) Automated Testing in the Wild: A Case Study on Unity-Based VR Applications,"Virtual Reality (VR) is an emerging technique that provides a unique real-time experience for users. VR technologies have provided revolutionary user experiences in various scenarios (e.g., training, education, gaming, etc.). However, testing VR applications is challenging due to their nature which necessitates physical interactivity, and their reliance on specific hardware systems. Despite the recent advancements in VR technology and its usage scenarios, we still know little about VR application testing. To fill up this knowledge gap, we performed an empirical study on 314 open-source VR applications. Our analysis identified that 79% of the VR projects evaluated did not have any automatic tests, and for the VR projects that did, the median functional-method to test-method ratios were lower than those of other project types. Moreover, we uncovered tool support issues concerning the measurement of VR code coverage, and the assertion density results we were able to generate were relatively low, with an average of 17.63%. Finally, through a manual analysis of 370 test cases, we identified the different categories of test cases being used to validate VR application quality attributes. Furthermore, we extracted which of these categories are VR-attention, meaning that test writers need to pay special attention to VR characteristics when writing tests of these categories. We believe that our findings constitute a call to action for the VR development community to improve their automatic testing practices and provide directions for software engineering researchers to develop advanced techniques for automatic test case generation and test quality analysis for VR applications. Our replication package containing the dataset we used, software tools we developed, and the results we found, is accessible at ‍https://doi.org/10.6084/m9.figshare.19678938.","Human-centered computing,Human computer interaction (HCI),Interaction paradigms,Virtual reality,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",虚拟现实（VR）野外自动化测试——基于Unity的VR应用案例研究,虚拟现实（VR）是一种新兴技术，为用户提供独特的实时体验。VR技术在各种场景（如培训、教育、游戏等）中提供了革命性的用户体验。然而，测试VR应用程序具有挑战性，因为它们的本质需要物理交互，并且依赖于特定的硬件系统。尽管虚拟现实技术及其使用场景最近取得了进步，但我们对虚拟现实应用测试的了解仍然很少。为了填补这一知识空白，我们对314个开源VR应用程序进行了实证研究。我们的分析表明，79%的评估VR项目没有任何自动测试，而对于有自动测试的VR项目，功能方法与测试方法的比值中值低于其他项目类型。此外，我们还发现了有关VR代码覆盖率测量的工具支持问题，我们能够生成的断言密度结果相对较低，平均为17.63%。最后，通过对370个测试用例的手动分析，我们确定了用于验证VR应用质量属性的不同类别的测试用例。此外，我们提取了这些类别中哪些是VR注意力，这意味着测试作者在编写这些类别的测试时需要特别注意VR特征。我们相信，我们的发现呼吁VR开发社区采取行动，改进他们的自动测试实践，并为软件工程研究人员开发用于VR应用程序的自动测试用例生成和测试质量分析的先进技术提供指导。我们的复制包包含我们使用的数据集、我们开发的软件工具以及我们发现的结果，可访问 ‍https://doi.org/10.6084/m9.figshare.19678938.,以人为中心的计算，人机交互，交互范式，虚拟现实，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
7KFAS8UQ,2023,https://doi.org/10.1145/3597926.3604922,ISSTA 2023,EDHOC-Fuzzer: An EDHOC Protocol State Fuzzer,"EDHOC is a compact and lightweight authenticated key exchange protocol proposed by the IETF, whose design focuses on small message sizes, in order to be suitable for constrained IoT communication technologies. In this tool paper, we overview EDHOC-Fuzzer, a protocol state fuzzer for implementations of EDHOC clients and servers. It employs model learning to generate a state machine model of an EDHOC implementation, capturing its input/output behavior. This model can then be used for model-based testing, for fingerprinting, or can be analyzed for non-conformances, state machine bugs and security vulnerabilities. We overview the architecture and use of EDHOC-Fuzzer, and present some examples of models produced by the tool and our current findings.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",EDHOC引信：一种EDHOC协议状态引信,EDHOC是IETF提出的一种紧凑、轻量级的认证密钥交换协议，其设计侧重于小消息大小，以适用于受约束的物联网通信技术。在本文中，我们概述了EDHOC Fuzzer，一种用于实现EDHOC客户端和服务器的协议状态模糊器。它采用模型学习来生成EDHOC实现的状态机模型，捕捉其输入/输出行为。然后，该模型可以用于基于模型的测试、指纹识别，或者可以分析不符合项、状态机错误和安全漏洞。我们概述了EDHOC Fuzzer的体系结构和使用，并介绍了该工具生成的一些模型示例和我们目前的发现。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
SUNKGWAZ,2023,https://doi.org/10.1145/3597926.3598136,ISSTA 2023,Rare Path Guided Fuzzing,"Starting with a random initial seed, fuzzers search for inputs that trigger bugs or vulnerabilities. However, fuzzers often fail to generate inputs for program paths guarded by restrictive branch conditions. In this paper, we show that by first identifying rare-paths in programs (i.e., program paths with path constraints that are unlikely to be satisfied by random input generation), and then, generating inputs/seeds that trigger rare-paths, one can improve the coverage of fuzzing tools. In particular, we present techniques 1) that identify rare paths using quantitative symbolic analysis, and 2) generate inputs that can explore these rare paths using path-guided concolic execution. We provide these inputs as initial seed sets to three state of the art fuzzers. Our experimental evaluation on a set of programs shows that the fuzzers achieve better coverage with the rare-path based seed set compared to a random initial seed.","Software and its engineering,Software organization and properties,Extra-functional properties,Software reliability,Software functional properties,Formal methods,Software verification",稀有路径制导引信,从一个随机的初始种子开始，模糊器搜索触发bug或漏洞的输入。然而，模糊器通常无法为受限制分支条件保护的程序路径生成输入。在本文中，我们表明，通过首先识别程序中的稀有路径（即具有路径约束的程序路径，随机输入生成不太可能满足这些路径约束），然后生成触发稀有路径的输入/种子，可以提高模糊工具的覆盖率。特别地，我们提出了技术1）使用定量符号分析来识别稀有路径，以及2）生成可以使用路径引导的一致执行来探索这些稀有路径的输入。我们将这些输入作为初始种子集提供给三个最先进的模糊器。我们对一组程序的实验评估表明，与随机初始种子相比，基于稀有路径的种子集的模糊器实现了更好的覆盖。,软件及其工程，软件组织和属性，额外功能属性，软件可靠性，软件功能属性，形式化方法，软件验证,,,
KN2MSRSI,2023,https://doi.org/10.1145/3597926.3605238,ISSTA 2023,Quantitative Symbolic Similarity Analysis,"Similarity analysis plays a crucial role in various software engineering tasks, such as detecting software changes, version merging, identifying plagiarism, and analyzing binary code. Equivalence analysis, a stricter form of similarity, focuses on determining whether different programs or versions of the same program behave identically. While extensive research exists on code and binary similarity as well as equivalence analysis, there is a lack of quantitative reasoning in these areas. Non-equivalence is a spectrum that requires deeper exploration, as it can manifest in different ways across the input domain space. This paper emphasizes the importance of quantitative reasoning on non-equivalence which arises due to semantic differences. By quantitatively reasoning about non-equivalence, it becomes possible to identify specific input ranges for which programs are equivalent or non-equivalent. We aim to address the gap in quantitative reasoning in symbolic similarity analysis, enabling a more comprehensive understanding of program behavior.","Software and its engineering,Software organization and properties,Extra-functional properties,Software reliability,Software functional properties,Formal methods,Software verification",定量符号相似性分析,相似性分析在各种软件工程任务中发挥着至关重要的作用，如检测软件更改、版本合并、识别剽窃和分析二进制代码。等价性分析是一种更严格的相似性形式，侧重于确定不同程序或同一程序的版本是否表现相同。虽然对代码和二进制相似性以及等价性分析进行了广泛的研究，但在这些领域缺乏定量推理。不等价是一个需要更深入探索的频谱，因为它可以在输入域空间中以不同的方式表现出来。本文强调了定量推理对因语义差异而产生的不对等现象的重要性。通过对不等价性进行定量推理，可以确定程序等价或不等价的特定输入范围。我们的目标是解决符号相似性分析中定量推理的差距，使人们能够更全面地理解程序行为。,软件及其工程，软件组织和属性，额外功能属性，软件可靠性，软件功能属性，形式化方法，软件验证,,,
HI65D7YD,2023,https://doi.org/10.1145/3597926.3604921,ISSTA 2023,KDAlloc: The KLEE Deterministic Allocator: Deterministic Memory Allocation during Symbolic Execution and Test Case Replay,"The memory allocator can have an important impact in symbolic execution.  
Taking a user-centric view, this tool demonstration paper discusses some of the main benefits provided by KLEE's new allocator KDAlloc in terms of improved deterministic execution and bug-finding capabilities.  
We then introduce a new replay tool for KLEE which enables the native execution to integrate KDAlloc and receive the same heap addresses as during symbolic execution.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",KDAlloc:KLEE确定性分配器：符号执行和测试用例回放期间的确定性内存分配,内存分配器可以在符号执行中产生重要影响。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
G87GGERK,2023,https://doi.org/10.1145/3597926.3598045,ISSTA 2023,Semantic-Based Neural Network Repair,"Recently, neural networks have spread into numerous fields including many safety-critical systems. Neural networks are built (and trained) by programming in frameworks such as TensorFlow and PyTorch. Developers apply a rich set of pre-defined layers to manually program neural networks or to automatically generate them (e.g., through AutoML). Composing neural networks with different layers is error-prone due to the non-trivial constraints that must be satisfied in order to use those layers. In this work, we propose an approach to automatically repair erroneous neural networks. The challenge is in identifying a minimal modification to the network so that it becomes valid. Modifying a layer might have cascading effects on subsequent layers and thus our approach must search recursively to identify a ''globally'' minimal modification. Our approach is based on an executable semantics of deep learning layers and focuses on four kinds of errors which are common in practice. We evaluate our approach for two usage scenarios, i.e., repairing automatically generated neural networks and manually written ones suffering from common model bugs. The results show that we are able to repair 100% of a set of randomly generated neural networks (which are produced with an existing AI framework testing approach) effectively and efficiently (with an average repair time of 21.08s) and 93.75% of a collection of real neural network bugs (with an average time of 3min 40s).","Computing methodologies,Artificial intelligence,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Theory of computation,Logic,Constraint and logic programming",基于语义的神经网络修复,最近，神经网络已经扩展到许多领域，包括许多安全关键系统。神经网络是通过在TensorFlow和PyTorch等框架中编程来构建（和训练）的。开发人员应用一组丰富的预定义层来手动编程神经网络或自动生成神经网络（例如，通过AutoML）。由于必须满足非平凡的约束才能使用这些层，因此用不同层组成神经网络很容易出错。在这项工作中，我们提出了一种自动修复错误神经网络的方法。挑战在于确定对网络的最小修改，使其变得有效。修改一个层可能会对后续层产生级联效应，因此我们的方法必须递归搜索，以确定“全局”最小修改。我们的方法基于深度学习层的可执行语义，重点关注实践中常见的四种错误。我们针对两种使用场景评估了我们的方法，即修复自动生成的神经网络和手动编写的存在常见模型错误的神经网络。结果表明，我们能够100%有效、高效地修复一组随机生成的神经网络（使用现有的人工智能框架测试方法生成）（平均修复时间为21.08s）和93.75%的真实神经网络错误集合（平均时间为3分40秒）。,计算方法论，人工智能，软件及其工程，软件创建和管理，软件验证和确认，形式软件验证，计算理论，逻辑，约束和逻辑编程,,,
9ZTVEQPV,2023,https://doi.org/10.1145/3597926.3604919,ISSTA 2023,RustSmith: Random Differential Compiler Testing for Rust,"We present RustSmith, the first Rust randomised program generator for end-to-end testing of Rust compilers. RustSmith generates programs that conform to the advanced type system of Rust, respecting rules related to borrowing and lifetimes, and that are guaranteed to yield a well-defined result. This makes RustSmith suitable for differential testing between compilers or across optimisation levels. By applying RustSmith to a series of versions of the official Rust compiler, rustc, we show that it can detect insidious historical bugs that evaded detection for some time. We have also used RustSmith to find previously-unknown bugs in an alternative Rust compiler implementation, mrustc. In a controlled experiment, we assess statement and mutation coverage achieved by RustSmith vs. the rustc optimisation test suite.","Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Compilers",RustSmith:Rust的随机微分编译器测试,我们介绍了RustSmith，第一个用于Rust编译器端到端测试的Rust随机程序生成器。RustSmith生成的程序符合Rust的高级类型系统，尊重与借用和生存期相关的规则，并保证产生定义良好的结果。这使得RustSmith适用于编译器之间或优化级别之间的差异测试。通过将RustSmith应用于官方Rust编译器rustc的一系列版本，我们表明它可以检测到一段时间内逃避检测的潜在历史错误。我们还使用RustSmith在另一个Rust编译器实现mrustc中查找以前未知的错误。在对照实验中，我们评估了RustSmith与rustc优化测试套件实现的陈述和突变覆盖率。,软件及其工程，软件创建和管理，软件开发后问题，软件维护，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，编译器,,,
PHW75HIC,2023,https://doi.org/10.1145/3597926.3598036,ISSTA 2023,Towards Efficient Fine-Tuning of Pre-trained Code Models: An Experimental Study and Beyond,"Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that only the representations of the top two layers change most during fine-tuning for various downstream tasks. (3) Based on the above findings, we propose Telly to efficiently fine-tune pre-trained code models via layer freezing. The extensive experimental results on five various downstream tasks demonstrate that training parameters and the corresponding time cost are greatly reduced, while performances are similar or better.","Software and its engineering,Software creation and management,Software development techniques,Reusability",实现预训练代码模型的有效微调：实验研究及展望,最近，在许多软件测试和分析任务中，对预训练的代码模型（如CodeBERT）进行下游任务的微调已经取得了巨大成功。虽然有效且普遍，但微调预先训练的参数会产生巨大的计算成本。在本文中，我们进行了广泛的实验研究，以探索在微调过程中，分层预训练的表示及其编码的代码知识会发生什么。然后，基于上述发现，我们提出了对大型预训练代码模型进行微调的有效替代方案。我们的实验研究表明：（1）源代码的词汇、句法和结构特性分别编码在较低、中间和较高的层中，而语义特性横跨整个模型。（2） 微调过程保留了大部分代码属性。具体而言，在微调期间，较低层和中间层捕获的基本代码属性仍然保留。此外，我们发现，在各种下游任务的微调过程中，只有顶部两层的表示变化最大。（3） 基于上述发现，我们建议Telly通过层冻结来有效地微调预训练的代码模型。在五个不同下游任务上的大量实验结果表明，训练参数和相应的时间成本大大降低，同时性能相似或更好。,软件及其工程，软件创建和管理，软件开发技术，可重用性,,,
96J88FZC,2023,https://doi.org/10.1145/3597926.3598088,ISSTA 2023,ACETest: Automated Constraint Extraction for Testing Deep Learning Operators,"Deep learning (DL) applications are prevalent nowadays as they can help with multiple tasks. DL libraries are essential for building DL applications. Furthermore, DL operators are the important building blocks of the DL libraries, that compute the multi-dimensional data (tensors). Therefore, bugs in DL operators can have great impacts. Testing is a practical approach for detecting bugs in DL operators. In order to test DL operators effectively, it is essential that the test cases pass the input validity check and are able to reach the core function logic of the operators. Hence, extracting the input validation constraints is required for generating high-quality test cases. Existing techniques rely on either human effort or documentation of DL library APIs to extract the constraints. They cannot extract complex constraints and the extracted constraints may differ from the actual code implementation.  
To address the challenge, we propose ACETest, a technique to automatically extract input validation constraints from the code to build valid yet diverse test cases which can effectively unveil bugs in the core function logic of DL operators. For this purpose, ACETest can automatically identify the input validation code in DL operators, extract the related constraints and generate test cases according to the constraints. The experimental results on popular DL libraries, TensorFlow and PyTorch, demonstrate that ACETest can extract constraints with higher quality than state-of-the-art (SOTA) techniques. Moreover, ACETest is capable of extracting 96.4% more constraints and detecting 1.95 to 55 times more bugs than SOTA techniques. In total, we have used ACETest to detect 108 previously unknown bugs on TensorFlow and PyTorch, with 87 of them confirmed by the developers. Lastly, five of the bugs were assigned with CVE IDs due to their security impacts.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software reliability",ACETest：用于测试深度学习算子的自动约束提取,深度学习（DL）应用程序现在很流行，因为它们可以帮助完成多项任务。DL库对于构建DL应用程序至关重要。此外，DL运算符是DL库的重要组成部分，用于计算多维数据（张量）。因此，DL运营商中的错误可能会产生巨大影响。测试是检测DL运算符中错误的一种实用方法。为了有效地测试DL操作符，测试用例必须通过输入有效性检查，并能够到达操作符的核心功能逻辑。因此，为了生成高质量的测试用例，需要提取输入验证约束。现有技术依靠人工努力或DL库API的文档来提取约束。它们不能提取复杂的约束，并且提取的约束可能与实际的代码实现不同。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，额外功能属性，软件可靠性,,,
XSF7YP9J,2023,https://doi.org/10.1145/3597926.3598059,ISSTA 2023,ItyFuzz: Snapshot-Based Fuzzer for Smart Contract,"Smart contracts are critical financial instruments, and their security is of utmost importance. However, smart contract programs are difficult to fuzz due to the persistent blockchain state behind all transactions. Mutating sequences of transactions are complex and often lead to a suboptimal exploration for both input and program spaces. In this paper, we introduce a novel snapshot-based fuzzer ItyFuzz for testing smart contracts. In ItyFuzz, instead of storing sequences of transactions and mutating from them, we snapshot states and singleton transactions. To explore interesting states, ItyFuzz introduces a dataflow waypoint mechanism to identify states with more potential momentum. ItyFuzz also incorporates comparison waypoints to prune the space of states. By maintaining snapshots of the states, ItyFuzz can synthesize concrete exploits like reentrancy attacks quickly. Because ItyFuzz has second-level response time to test a smart contract, it can be used for on-chain testing, which has many benefits compared to local development testing. Finally, we evaluate ItyFuzz on real-world smart contracts and some hacked on-chain DeFi projects. ItyFuzz outperforms existing fuzzers in terms of instructional coverage and can find and generate realistic exploits for on-chain projects quickly.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",ItyFuzz：基于快照的智能合约模糊器,智能合约是重要的金融工具，其安全性至关重要。然而，由于所有交易背后的持续区块链状态，智能合约程序很难模糊。事务序列的突变是复杂的，并且经常导致对输入和程序空间的次优探索。在本文中，我们介绍了一种新的基于快照的模糊器ItyFuzz，用于测试智能合约。在ItyFuzz中，我们不存储事务序列并从中进行更改，而是快照状态和单例事务。为了探索有趣的状态，ItyFuzz引入了一种数据流航路点机制，以识别具有更多潜在动量的状态。ItyFuzz还结合了比较路点来修剪状态空间。通过维护状态的快照，ItyFuzz可以快速合成具体的漏洞，如重新进入攻击。因为ItyFuzz有二级响应时间来测试智能合约，所以它可以用于链上测试，与本地开发测试相比，这有很多好处。最后，我们对ItyFuzz在现实世界中的智能合约和一些被黑客入侵的链上DeFi项目进行了评估。ItyFuzz在教学覆盖率方面优于现有的模糊器，可以快速找到并生成链上项目的真实漏洞。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
6J3V8BKJ,2023,https://doi.org/10.1145/3597926.3605233,ISSTA 2023,Harnessing Large Language Models for Simulink Toolchain Testing and Developing Diverse Open-Source Corpora of Simulink Models for Metric and Evolution Analysis,"MATLAB/Simulink is a de-facto standard tool in several safety-critical industries such as automotive, aerospace, healthcare, and industrial automation for system modeling and analysis, compiling models to code, and deploying code to embedded hardware. On one hand, testing cyber-physical system (CPS) development tools such as MathWorks’ Simulink is important as a bug in the toolchain may propagate to the artifacts they produce. On the other hand, it is equally important to understand modeling practices and model evolution to support engineers and scientists as they are widely used in design, simulation, and verification of CPS models. Existing work in this area is limited by two main factors, i.e., (1) inefficiencies of state-of-the-art testing schemes in finding critical tool-chain bugs and (2) the lack of a reusable corpus of public Simulink models. In my thesis, I propose to (1) curate a large reusable corpus of Simulink models to help understand modeling practices and model evolution and (2) leverage such a corpus with deep-learning based language models to test the toolchain.","Computer systems organization,Embedded and cyber-physical systems,Computing methodologies,Machine learning,Learning paradigms,Multi-task learning,Transfer learning,Information systems,Information retrieval,Retrieval models and ranking,Language models,Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Software system structures,Software system models,Model-driven software engineering",利用大型语言模型进行Simulink工具链测试并开发用于度量和进化分析的Simulink模型的多种开源库,MATLAB/Simulink是汽车、航空航天、医疗保健和工业自动化等几个安全关键行业的事实上的标准工具，用于系统建模和分析、将模型编译为代码以及将代码部署到嵌入式硬件。一方面，测试网络物理系统（CPS）开发工具（如MathWorks的Simulink）很重要，因为工具链中的错误可能会传播到它们产生的工件中。另一方面，了解建模实践和模型演化以支持工程师和科学家同样重要，因为它们被广泛用于CPS模型的设计、模拟和验证。该领域的现有工作受到两个主要因素的限制，即（1）最先进的测试方案在发现关键工具链错误方面效率低下，以及（2）缺乏可重复使用的公共Simulink模型语料库。在我的论文中，我建议（1）策划一个大型可重复使用的Simulink模型语料库，以帮助理解建模实践和模型进化；（2）利用这样一个语料库与基于深度学习的语言模型来测试工具链。,计算机系统组织，嵌入式和网络物理系统，计算方法，机器学习，学习范式，多任务学习，迁移学习，信息系统，信息检索，检索模型和排名，语言模型，软件及其工程，软件创建和管理，软件开发协作，开源模型，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，软件系统结构，软件系统模型，模型驱动软件工程,,,
GTP5VGK8,2023,https://doi.org/10.1145/3597926.3598116,ISSTA 2023,Splendor: Static Detection of Stored XSS in Modern Web Applications,"In modern websites, stored Cross-Site Scripting (XSS) is the most dangerous XSS vulnerability, which can store payloads in the web system and be triggered directly by the victim. Database (DB) as the most commonly used storage medium for data on websites is therefore also the most common place where stored XSS occurs. Due to the modularity of modern programming architectures, the complex underlying database operations will often be encapsulated and abstracted as a Data Access Layer (DAL) to provide unified data access services to the business layer. The heavy use of Object-Oriented (OO) and dynamic language features involved in the encapsulation makes it increasingly challenging for static taint analysis tools to understand how tainted data flows between the source code and the exact locations in database. ","Security and privacy,Software and application security,Web application security",Splendor:现代Web应用程序中存储XSS的静态检测,在现代网站中，存储的跨站点脚本（XSS）是最危险的XSS漏洞，它可以将有效负载存储在web系统中，并由受害者直接触发。数据库（DB）作为网站上最常用的数据存储介质，因此也是存储XSS最常见的地方。由于现代编程体系结构的模块化，复杂的底层数据库操作通常会被封装和抽象为数据访问层（DAL），为业务层提供统一的数据访问服务。封装中大量使用面向对象（OO）和动态语言功能，这使得静态污染分析工具越来越难以理解污染数据如何在源代码和数据库中的确切位置之间流动。,安全和隐私，软件和应用程序安全，Web应用程序安全,,,
J9QEGHD9,2023,https://doi.org/10.1145/3597926.3598120,ISSTA 2023,Tai-e: A Developer-Friendly Static Analysis Framework for Java by Harnessing the Good Designs of Classics,"Static analysis is a mature field with applications to bug detection, security analysis, program understanding, optimization, and more. To facilitate these applications, static analysis frameworks play an essential role by providing a series of fundamental services such as intermediate representation (IR) generation, control flow graph construction, points-to/alias information computation, and so on. However, although static analysis has made great strides and several well-known frameworks have emerged in this field over the past decades, these frameworks are not that easy to learn and use for developers who rely on them to create and implement analyses. In that sense, it is far from trivial to build a developer-friendly static analysis framework, because compared to the knowledge required for static analysis itself, we have significantly less knowledge designing and implementing static analysis frameworks.  
","Software and its engineering,Software organization and properties,Software functional properties,Formal methods,Automated static analysis",Tai-e：一个开发人员友好的Java静态分析框架，利用经典的优秀设计,静态分析是一个成熟的领域，应用于漏洞检测、安全分析、程序理解、优化等。为了促进这些应用，静态分析框架通过提供一系列基本服务发挥着重要作用，如中间表示（IR）生成、控制流图构建、点到/别名信息计算等。然而，尽管在过去的几十年里，静态分析已经取得了长足的进步，并且在这个领域出现了一些著名的框架，但对于依赖它们来创建和实现分析的开发人员来说，这些框架并不容易学习和使用。从这个意义上说，构建一个开发人员友好的静态分析框架绝非易事，因为与静态分析本身所需的知识相比，我们设计和实现静态分析框架的知识要少得多。,软件及其工程，软件组织和属性，软件功能属性，形式化方法，自动静态分析,,,
SBSPUE8J,2023,https://doi.org/10.1145/3597926.3604927,ISSTA 2023,SymRustC: A Hybrid Fuzzer for Rust,"We present SymRustC, a hybrid fuzzer for Rust. SymRustC is hybrid  
 in the sense that it combines fuzzing and concolic execution.  
 SymRustC leverages an existing tool called SymCC for  
 its concolic execution capability and another existing tool  
 called LibAFL for its fuzzing capability.  
 Since SymCC instruments LLVM IR (Intermediate  
 Representation) for concolic execution and the Rust compiler  
 uses LLVM as a backend, we integrate SymCC with the  
 Rust compiler to instrument Rust programs for concolic  
 execution. LibAFL provides a framework to develop a  
 fuzzer, and we use it to develop a hybrid fuzzer that  
 combines fuzzing and our concolic execution. We discuss our  
 implementation as well as four case studies to demonstrate  
 that SymRustC can generate inputs that discover errors in  
 Rust programs.","Software and its engineering,Software creation and management,Software verification and validation,Empirical software validation,Software defect analysis,Software testing and debugging",SymRustC:一种用于Rust的混合引信,我们介绍了SymRustC，一个用于Rust的混合模糊器。SymRustC是混合动力,软件及其工程，软件创建和管理，软件验证和确认，经验软件确认，软件缺陷分析，软件测试和调试,,,
2ZTVTV4Z,2023,https://doi.org/10.1145/3597926.3598117,ISSTA 2023,Applying and Extending the Delta Debugging Algorithm for Elevator Dispatching Algorithms (Experience Paper),"Elevator systems are one kind of Cyber-Physical Systems (CPSs), and as such, test cases are usually complex and long in time. This is mainly because realistic test scenarios are employed (e.g., for testing elevator dispatching algorithms, typically a full day of passengers traveling through a system of elevators is used). However, in such a context, when needing to reproduce a failure, it is of high benefit to provide the minimal test input to the software developers. This way, analyzing and trying to localize the root-cause of the failure is easier and more agile. Delta debugging has been found to be an efficient technique to reduce failure-inducing test inputs. In this paper, we enhance this technique by first monitoring the environment at which the CPS operates as well as its physical states. With the monitored information, we search for stable states of the CPS during the execution of the simulation. In a second step, we use such identified stable states to help the delta debugging algorithm isolate the failure-inducing test inputs more efficiently.  
","Computer systems organization,Embedded and cyber-physical systems,Embedded systems,Software and its engineering,Software creation and management,Software verification and validation,Empirical software validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software performance",增量调试算法在电梯调度算法中的应用与扩展（经验论文）,电梯系统是网络物理系统（CPS）的一种，因此，测试用例通常复杂且时间长。这主要是因为采用了现实的测试场景（例如，为了测试电梯调度算法，通常使用一整天的乘客通过电梯系统）。然而，在这种情况下，当需要再现故障时，向软件开发人员提供最小的测试输入是非常有益的。这样，分析和尝试定位故障的根本原因更容易，也更灵活。德尔塔调试已被发现是一种有效的技术，可以减少引起故障的测试输入。在本文中，我们通过首先监测CPS运行的环境及其物理状态来增强这项技术。利用监测到的信息，我们在模拟执行过程中搜索CPS的稳定状态。在第二步中，我们使用这样识别的稳定状态来帮助delta调试算法更有效地隔离引起故障的测试输入。,计算机系统组织，嵌入式和网络物理系统，嵌入式系统，软件及其工程，软件创建和管理，软件验证和确认，经验软件验证，软件缺陷分析，软件测试和调试，软件组织和属性，额外功能属性，软件性能,,,
2RZJ7P85,2023,https://doi.org/10.1145/3597926.3598107,ISSTA 2023,Guiding Greybox Fuzzing with Mutation Testing,"Greybox fuzzing and mutation testing are two popular but mostly independent fields of software testing research that have so far had limited overlap. Greybox fuzzing, generally geared towards  
searching for new bugs, predominantly uses code coverage for selecting inputs to save. Mutation testing is primarily used as a stronger alternative to code coverage in assessing the quality of regression tests; the idea is to evaluate tests for their ability to identify artificially injected faults in the target program. But what if we wanted to use greybox fuzzing to synthesize high-quality  
regression tests?  
","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",用突变测试指导灰盒引信,灰盒模糊和突变测试是软件测试研究中两个流行但大多独立的领域，迄今为止重叠有限。Greybox绒毛，通常针对,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
43SJK5ZP,2023,https://doi.org/10.1145/3597926.3598079,ISSTA 2023,Validating Multimedia Content Moderation Software via Semantic Fusion,"The exponential growth of social media platforms, such as Facebook, Instagram, Youtube, and TikTok, has revolutionized communication and content publication in human society. Users on these platforms can publish multimedia content that delivers information via the combination of text, audio, images, and video. Meanwhile, the multimedia content release facility has been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography. To this end, content moderation software has been widely deployed on these platforms to detect and blocks toxic content. However, due to the complexity of content moderation models and the difficulty of understanding information across multiple modalities, existing content moderation software can fail to detect toxic content, which often leads to extremely negative impacts (e.g., harmful effects on teen mental health).  
We introduce Semantic Fusion, a general, effective methodology for validating multimedia content moderation software. Our key idea is to fuse two or more existing single-modal inputs (e.g., a textual sentence and an image) into a new input that combines the semantics of its ancestors in a novel manner and has toxic nature by construction. This fused input is then used for validating multimedia content moderation software. We realized Semantic Fusion as DUO, a practical content moderation software testing tool. In our evaluation, we employ DUO to test five commercial content moderation software and two state-of-the-art models against three kinds of toxic contents. The results show that DUO achieves up to 100% error finding rate (EFR) when testing moderation software and it obtains up to 94.1% EFR when testing the state-of-the-art models. In addition, we leverage the test cases generated by DUO to retrain the two models we explored, which largely improves model robustness (2.5%∼5.7% EFR) while maintaining the accuracy on the original test set.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",通过语义融合验证多媒体内容调节软件,Facebook、Instagram、Youtube和TikTok等社交媒体平台的指数级增长，彻底改变了人类社会的传播和内容发布。这些平台上的用户可以发布多媒体内容，通过文本、音频、图像和视频的组合传递信息。与此同时，多媒体内容发布设施越来越多地被用来传播有毒内容，如仇恨言论、恶意广告和色情内容。为此，内容审核软件已被广泛部署在这些平台上，用于检测和屏蔽有毒内容。然而，由于内容审核模型的复杂性和难以理解多种模式的信息，现有的内容审核软件可能无法检测到有毒内容，这往往会导致极其负面的影响（例如，对青少年心理健康的有害影响）。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
AFR44CCD,2023,https://doi.org/10.1145/3597926.3598081,ISSTA 2023,Back Deduction Based Testing for Word Sense Disambiguation Ability of Machine Translation Systems,"Machine translation systems have penetrated our daily lives, providing translation services from source language to target language to millions of users online daily. Word Sense Disambiguation (WSD) is one of the essential functional requirements of machine translation systems, which aims to determine the exact sense of polysemes in the given context. Commercial machine translation systems (e.g., Google Translate) have been shown to fail in identifying the proper sense and consequently cause translation errors. However, to our knowledge, no prior studies focus on testing such WSD bugs for machine translation systems. ","Computing methodologies,Artificial intelligence,Natural language processing,Machine translation,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",基于回溯推理的机器翻译系统词义消歧能力测试,机器翻译系统已经渗透到我们的日常生活中，每天为数百万在线用户提供从源语言到目标语言的翻译服务。词义消歧（WSD）是机器翻译系统的基本功能要求之一，旨在确定特定上下文中多义词的确切含义。商业机器翻译系统（例如谷歌翻译）已被证明无法识别正确的含义，从而导致翻译错误。然而，据我们所知，以前没有任何研究专注于测试机器翻译系统的WSD漏洞。,计算方法论，人工智能，自然语言处理，机器翻译，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
JQBSN79E,2023,https://doi.org/10.1145/3597926.3598105,ISSTA 2023,GenCoG: A DSL-Based Approach to Generating Computation Graphs for TVM Testing,"TVM is a popular deep learning (DL) compiler. It is designed for compiling DL models, which are naturally computation graphs, and as well promoting the efficiency of DL computation. State-of-the-art methods, such as Muffin and NNSmith, allow developers to generate computation graphs for testing DL compilers. However, these techniques are inefficient — their generated computation graphs are either type-invalid or inexpressive, and hence not able to test the core functionalities of a DL compiler.  
","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Compilers,Context specific languages,Domain specific languages",GenCoG:一种基于DSL的TVM测试计算图生成方法,TVM是一种流行的深度学习（DL）编译器。它是为了编译DL模型而设计的，DL模型自然是计算图，并提高DL计算的效率。最先进的方法，如Muffin和NNSmith，允许开发人员生成用于测试DL编译器的计算图。然而，这些技术效率低下——它们生成的计算图要么类型无效，要么无法表达，因此无法测试DL编译器的核心功能。,计算方法，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，编译器，上下文特定语言，领域特定语言,,,
V2GGXWT4,2023,https://doi.org/10.1145/3597926.3604920,ISSTA 2023,KeenTune: Automated Tuning Tool for Cloud Application Performance Testing and Optimization,"The performance testing and optimization of cloud applications is challenging, because manual tuning of cloud computing stacks is tedious and automated tuning tools are rare used for cloud services. To address this issue, we introduce KeenTune, an automated tuning tool designed to optimize application performance and facilitate performance testing. KeenTune is a lightweight and flexible tool that can be deployed with to-be-tuned applications with negligible impact on their performance. Specifically, KeenTune uses a surrogate model that can be implemented with machine learning models to filter out less relevant parameters for efficient tuning. Our empirical evaluation shows that KeenTune significantly enhances the throughput performance of Nginx web servers, resulting in performance improvements of up to 90.43% and 117.23% in certain cases. This study highlights the benefits of using KeenTune for achieving efficient and effective performance testing of cloud applications. The video and source code for KeenTune are provided as supplementary materials.","Software and its engineering,Software notations and tools,Software maintenance tools,Software organization and properties,Extra-functional properties,Software performance",KeenTune：用于云应用程序性能测试和优化的自动调整工具,云应用程序的性能测试和优化具有挑战性，因为手动调整云计算堆栈是乏味的，而自动调整工具很少用于云服务。为了解决这个问题，我们介绍了KeenTune，这是一种自动调优工具，旨在优化应用程序性能并促进性能测试。KeenTune是一个轻量级且灵活的工具，可以与待调优的应用程序一起部署，对其性能的影响可以忽略不计。具体而言，KeenTune使用了一个可以与机器学习模型一起实现的代理模型，以过滤掉不太相关的参数，从而实现有效的调整。我们的经验评估表明，KeenTune显著提高了Nginx web服务器的吞吐量性能，在某些情况下，性能分别提高了90.43%和117.23%。这项研究强调了使用KeenTune实现云应用程序高效性能测试的好处。KeenTune的视频和源代码作为补充材料提供。,软件及其工程，软件符号和工具，软件维护工具，软件组织和属性，额外功能属性，软件性能,,,
AS8S4PF2,2023,https://doi.org/10.1145/3597926.3598122,ISSTA 2023,Data Constraint Mining for Automatic Reconciliation Scripts Generation,"Fund loss is an increasingly critical problem caused by the misbehavior of software, especially in fintech and e-commerce platforms. Data reconciliation is one of the most commonly used approaches in detecting and preventing fund loss by executing reconciliation scripts on data storage systems (e.g., database and cache systems). The core of reconciliation scripts is the data constraints, which can be expressed as implications with two parts: preconditions and assertions. However, due to the complexity and diversity of business, the construction of data constraints and reconciliation scripts usually heavily relies on business experts. To this end, we propose AutoReconciler to mine data constraints from business data and generate reconciliation scripts automatically. It can mine assertions via enhanced symbolic regression, discover preconditions via association rule mining, and generate reconciliation scripts in SQL form. We have performed extensive experiments on the synthesized data. The result shows that our approach outperforms the baseline by a large margin (an average improvement in precision and recall of 22.1% and 51.6%, respectively), especially for complex data constraints. Our solution has been implemented, deployed, and adopted in production, and we conducted several case studies further to confirm the benefits of our solution in industrial scenarios.","Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation",用于自动生成对账脚本的数据约束挖掘,资金损失是软件不当行为造成的一个日益严重的问题，尤其是在金融科技和电子商务平台。数据对账是通过在数据存储系统（如数据库和缓存系统）上执行对账脚本来检测和防止资金损失的最常用方法之一。协调脚本的核心是数据约束，它可以表示为包含两部分的含义：先决条件和断言。然而，由于业务的复杂性和多样性，数据约束和对账脚本的构建通常在很大程度上依赖于业务专家。为此，我们建议AutoReconciler从业务数据中挖掘数据约束，并自动生成对账脚本。它可以通过增强的符号回归挖掘断言，通过关联规则挖掘发现前提条件，并以SQL形式生成协调脚本。我们对合成的数据进行了大量实验。结果表明，我们的方法在很大程度上优于基线（精度和召回率分别平均提高了22.1%和51.6%），尤其是在复杂的数据约束下。我们的解决方案已在生产中实施、部署和采用，我们还进一步进行了几项案例研究，以确认我们的解决方法在工业场景中的优势。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认,,,
RTKDB8R4,2023,https://doi.org/10.1145/3597926.3598135,ISSTA 2023,How Effective Are Neural Networks for Fixing Security Vulnerabilities,"Security vulnerability repair is a difficult task that is in dire need of automation. Two groups of techniques have shown promise: (1) large code language models (LLMs) that have been pre-trained on source code for tasks such as code completion, and (2) automated program repair (APR) techniques that use deep learning (DL) models to automatically fix software bugs. ","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software development techniques,Automatic programming,Software verification and validation,Software defect analysis,Software testing and debugging",神经网络修复安全漏洞的有效性,安全漏洞修复是一项艰巨的任务，迫切需要自动化。有两组技术显示出了前景：（1）大型代码语言模型（LLM），它已经在源代码上进行了预训练，用于完成代码等任务；（2）自动程序修复（APR）技术，它使用深度学习（DL）模型来自动修复软件错误。,计算方法，机器学习，机器学习方法，神经网络，安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件开发技术，自动编程，软件验证和确认，软件缺陷分析，软件测试和调试,,,
N2J4L945,2023,https://doi.org/10.1145/3597926.3598033,ISSTA 2023,CydiOS: A Model-Based Testing Framework for iOS Apps,"To make an app stand out in an increasingly competitive market, developers must ensure its quality to deliver a better user experience. UI testing is a popular technique for quality assurance, which can thoroughly test the app from the users’ perspective. However, while considerable research has already studied UI testing on the Android platform, there is no research on iOS. This paper introduces CydiOS, a novel approach to performing model-based testing for iOS apps. CydiOS enhances the existing static analysis to build a more complete static model for the app under test. We propose an approach to retrieve runtime information to obtain real-time app context that can be mapped in the model. To improve the effectiveness of UI testing, we also introduce a potential-aware search algorithm to guide testing execution. We compare CydiOS with four representative algorithms(i.e., random, depth-first, stoat, and ape). We have evaluated CydiOS on 50 popular apps from App Store, and the results show that CydiOS outperforms other tools, achieving both higher code coverage and screen coverage. We open source CydiOS at https://github.com/SoftWare2022Testing/CydiOS, and a demo video can be found there.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",CydiOS:一个基于模型的iOS应用程序测试框架,为了让应用程序在竞争日益激烈的市场中脱颖而出，开发人员必须确保其质量，以提供更好的用户体验。UI测试是一种流行的质量保证技术，它可以从用户的角度彻底测试应用程序。然而，尽管已经有相当多的研究在Android平台上进行了UI测试，但还没有对iOS进行研究。本文介绍了CydiOS，这是一种为iOS应用程序执行基于模型的测试的新方法。CydiOS增强了现有的静态分析，为测试中的应用程序构建了一个更完整的静态模型。我们提出了一种检索运行时信息的方法，以获得可以映射到模型中的实时应用程序上下文。为了提高UI测试的有效性，我们还引入了一种潜在的感知搜索算法来指导测试执行。我们将CydiOS与四种有代表性的算法（即随机、深度优先、stoat和ape）进行了比较。我们在App Store的50个流行应用程序上评估了CydiOS，结果显示CydiOS的性能优于其他工具，实现了更高的代码覆盖率和屏幕覆盖率。我们在https://github.com/SoftWare2022Testing/CydiOS，并且可以在那里找到演示视频。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
GQ3W4IMI,2023,https://doi.org/10.1145/3597926.3598099,ISSTA 2023,Latent Imitator: Generating Natural Individual Discriminatory Instances for Black-Box Fairness Testing,"Machine learning (ML) systems have achieved remarkable performance across a wide area of applications. However, they frequently exhibit unfair behaviors in sensitive application domains (e.g., employment and loan), raising severe fairness concerns. To evaluate and test fairness, engineers often generate individual discriminatory instances to expose unfair behaviors before model deployment. However, existing baselines ignore the naturalness of generation and produce instances that deviate from the real data distribution, which may fail to reveal the actual model fairness since these unnatural discriminatory instances are unlikely to appear in practice. To address the problem, this paper proposes a framework named Latent Imitator (LIMI) to generate more natural individual discriminatory instances with the help of a generative adversarial network (GAN), where we imitate the decision boundary of the target model in the semantic latent space of GAN and further samples latent instances on it. Specifically, we first derive a surrogate linear boundary to coarsely approximate the decision boundary of the target model, which reflects the nature of the original data distribution. Subsequently, to obtain more natural instances, we manipulate random latent vectors to the surrogate boundary with a one-step movement, and further conduct vector calculation to probe two potential discriminatory candidates that may be more closely located in the real decision boundary. Extensive experiments on various datasets demonstrate that our LIMI outperforms other baselines largely in effectiveness (×9.42 instances), efficiency (×8.71 speeds), and naturalness (+19.65%) on average. In addition, we empirically demonstrate that retraining on test samples generated by our approach can lead to improvements in both individual fairness (45.67% on IFr and 32.81% on IFo) and group fairness (9.86% on SPD and 28.38% on AOD). Our codes can be found on our website.","Software and its engineering,Software creation and management",潜在模仿者：生成用于黑箱公平性测试的自然个体判别实例,机器学习（ML）系统在广泛的应用领域中取得了显著的性能。然而，他们在敏感的申请领域（如就业和贷款）经常表现出不公平行为，引发了严重的公平问题。为了评估和测试公平性，工程师经常在模型部署之前生成个别歧视性实例来揭露不公平行为。然而，现有的基线忽略了生成的自然性，并产生了偏离真实数据分布的实例，这可能无法揭示实际的模型公平性，因为这些非自然的歧视性实例在实践中不太可能出现。为了解决这个问题，本文提出了一个名为“潜在模仿者”（LIMI）的框架，以借助生成对抗性网络（GAN）生成更自然的个体歧视实例，在GAN的语义潜在空间中模拟目标模型的决策边界，并在其上对潜在实例进行进一步采样，我们首先导出一个代理线性边界来粗略地近似目标模型的决策边界，它反映了原始数据分布的性质。随后，为了获得更多的自然实例，我们通过一步移动将随机潜在向量操纵到代理边界，并进一步进行向量计算，以探测两个可能更接近真实决策边界的潜在判别候选。在各种数据集上的大量实验表明，我们的LIMI在有效性（×9.42个实例）、效率（×8.71个速度）和自然度（+19.65%）方面大大优于其他基线。此外，我们实证证明，对我们的方法生成的测试样本进行再培训可以提高个人公平性（IFr为45.67%，IFo为32.81%）和群体公平性（SPD为9.86%，AOD为28.38%）。我们的代码可以在我们的网站上找到。,软件及其工程，软件创建和管理,,,
I2K4MRD4,2023,https://doi.org/10.1145/3597926.3598061,ISSTA 2023,Precise and Efficient Patch Presence Test for Android Applications against Code Obfuscation,"Third-party libraries (TPLs) are widely utilized by Android developers to implement new apps. Unfortunately, TPLs are often suffering from various vulnerabilities, which could be exploited by attackers to cause catastrophic consequences for app users. Therefore, testing whether a vulnerability has been patched in target apps is crucial. However, existing techniques are unable to effectively test patch presence for obfuscated apps while obfuscation is pervasive in practice. To address the new challenges introduced by code obfuscation, this study presents PHunter, which is a system that captures obfuscation-resilient semantic features of patch-related methods to identify the presence of the patch in target apps. Specifically, PHunter utilizes coarse-grained features to locate patch-related methods, and compares the fine-grained semantic similarity to determine whether the code has been patched. Extensive evaluations on 94 CVEs and 200 apps show that PHunter can outperform state-of-the-art tools, achieving an average accuracy of 97.1% with high efficiency and low false positive rates. Besides, PHunter is able to be resilient to different obfuscation strategies. More importantly, PHunter is useful in eliminating the false alarms generated by existing TPL detection tools. In particular, it can help reduce up to 25.2% of the false alarms with an accuracy of 95.3%.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software notations and tools,Software libraries and repositories",针对代码混乱的Android应用程序的精确高效补丁存在性测试,Android开发人员广泛使用第三方库来实现新的应用程序。不幸的是，TPL经常存在各种漏洞，攻击者可能会利用这些漏洞给应用程序用户造成灾难性后果。因此，测试目标应用程序中是否修补了漏洞至关重要。然而，现有技术无法有效测试混淆应用程序的补丁存在，而混淆在实践中普遍存在。为了应对代码模糊带来的新挑战，本研究提出了PHunter，这是一个捕捉补丁相关方法的模糊弹性语义特征的系统，以识别目标应用程序中是否存在补丁。具体来说，PHunter利用粗粒度特征来定位与补丁相关的方法，并比较细粒度的语义相似性来确定代码是否已被补丁。对94个CVE和200个应用程序的广泛评估表明，PHunter的性能优于最先进的工具，平均准确率为97.1%，效率高，假阳性率低。此外，PHunter能够适应不同的混淆策略。更重要的是，PHunter有助于消除现有TPL检测工具产生的错误警报。特别是，它可以帮助减少高达25.2%的假警报，准确率为95.3%。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件符号和工具，软件库和存储库,,,
JQNTZDSW,2023,https://doi.org/10.1145/3597926.3598138,ISSTA 2023,An Empirical Study of Functional Bugs in Android Apps,"Android apps are ubiquitous and serve many aspects of our daily lives. Ensuring their functional correctness is crucial for their success. To date, we still lack a general and in-depth understanding of functional bugs, which hinders the development of practices and techniques to tackle functional bugs. To fill this gap, we conduct the first systematic study on 399 functional bugs from 8 popular open-source and representative Android apps to investigate the root causes, bug symptoms, test oracles, and the capabilities and limitations of existing testing techniques. This study took us substantial effort. It reveals several new interesting findings and implications which help shed light on future research on tackling functional bugs. Furthermore, findings from our study guided the design of a proof-of-concept differential testing tool, RegDroid, to automatically find functional bugs in Android apps. We applied RegDroid on 5 real-world popular apps, and successfully discovered 14 functional bugs, 10 of which were previously unknown and affected the latest released versions—all these 10 bugs have been confirmed and fixed by the app developers. Specifically, 10 out of these 14 found bugs cannot be found by existing testing techniques. We have made all the artifacts (including the dataset of 399 functional bugs and RegDroid) in our work publicly available at https://github.com/Android-Functional-bugs-study/home.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",安卓应用程序功能缺陷的实证研究,安卓应用程序无处不在，服务于我们日常生活的许多方面。确保它们的功能正确性对它们的成功至关重要。到目前为止，我们仍然缺乏对功能缺陷的全面和深入的理解，这阻碍了解决功能缺陷的实践和技术的发展。为了填补这一空白，我们对8款流行的开源和有代表性的Android应用程序中的399个功能错误进行了首次系统研究，以调查根本原因、错误症状、测试预言机以及现有测试技术的能力和局限性。这项研究花费了我们大量的精力。它揭示了一些新的有趣的发现和启示，有助于阐明未来解决功能缺陷的研究。此外，我们的研究结果指导了概念验证差异测试工具RegDroid的设计，该工具可以自动查找Android应用程序中的功能错误。我们在5个现实世界中流行的应用程序上应用了RegDroid，成功发现了14个功能错误，其中10个以前未知，并影响了最新发布的版本——所有这10个错误都已被应用程序开发人员确认并修复。具体来说，在这14个发现的bug中，有10个是现有测试技术无法发现的。我们已经将工作中的所有工件（包括399个功能错误和RegDroid的数据集）公开在https://github.com/Android-Functional-bugs-study/home.,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
IBDPNXSQ,2023,https://doi.org/10.1145/3597926.3598121,ISSTA 2023,Improving Binary Code Similarity Transformer Models by Semantics-Driven Instruction Deemphasis,"Given a function in the binary executable form, binary code similarity analysis determines a set of similar functions from a large pool of candidate functions. These similar functions are usually compiled from the same source code with different compilation setups. Such analysis has a large number of applications, such as malware detection, code clone detection, and automatic software patching. The state-of-the art methods utilize complex Deep Learning models such as Transformer models. We observe that these models suffer from undesirable instruction distribution biases caused by specific compiler conventions. We develop a novel technique to detect such biases and repair them by removing the corresponding instructions from the dataset and finetuning the models. This entails synergy between Deep Learning model analysis and program analysis. Our results show that we can substantially improve the state-of-the-art models’ performance by up to 14.4% in the most challenging cases where test data may be out of the distributions of training data.","Computing methodologies,Machine learning,Security and privacy,Software and application security,Software reverse engineering",用语义驱动的指令去强调改进二进制代码相似性变换器模型,给定二进制可执行形式的函数，二进制代码相似性分析从大量候选函数中确定一组相似函数。这些类似的函数通常是用不同的编译设置从相同的源代码编译而来的。这种分析有大量的应用，如恶意软件检测、代码克隆检测和自动软件修补。现有技术的方法利用复杂的深度学习模型，例如Transformer模型。我们观察到，这些模型受到由特定编译器约定引起的不希望的指令分布偏差的影响。我们开发了一种新的技术来检测这种偏差，并通过从数据集中删除相应的指令和微调模型来修复它们。这需要深度学习模型分析和程序分析之间的协同作用。我们的结果表明，在测试数据可能不在训练数据分布范围内的最具挑战性的情况下，我们可以将最先进的模型的性能大幅提高14.4%。,计算方法，机器学习，安全和隐私，软件和应用程序安全，软件逆向工程,,,
G2I57INV,2023,https://doi.org/10.1145/3597926.3598085,ISSTA 2023,LiResolver: License Incompatibility Resolution for Open Source Software,"Open source software (OSS) licenses regulate the conditions under which OSS can be legally reused, distributed, and modified. However, a common issue arises when incorporating third-party OSS accompanied with licenses, i.e., license incompatibility, which occurs when multiple licenses exist in one project and there are conflicts between them. Despite being problematic, fixing license incompatibility issues requires substantial efforts due to the lack of license understanding and complex package dependency. In this paper, we propose LiResolver, a fine-grained, scalable, and flexible tool to resolve license incompatibility issues for open source software. Specifically, it first understands the semantics of licenses through fine-grained entity extraction and relation extraction. Then, it detects and resolves license incompatibility issues by recommending official licenses in priority. When no official licenses can satisfy the constraints, it generates a custom license as an alternative solution. Comprehensive experiments demonstrate the effectiveness of LiResolver, with 4.09% false positive (FP) rate and 0.02% false negative (FN) rate for incompatibility issue localization, and 62.61% of 230 real-world incompatible projects resolved by LiResolver. We discuss the feedback from OSS developers and the lessons learned from this work. All the datasets and the replication package of LiResolver have been made publicly available to facilitate follow-up research.","Software and its engineering,Software creation and management,Collaboration in software development",LiResolver：开源软件的许可证不兼容解决方案,开放源码软件（OSS）许可证规定了OSS可以合法重复使用、分发和修改的条件。然而，在合并带有许可证的第三方OSS时会出现一个常见问题，即许可证不兼容，当一个项目中存在多个许可证，并且它们之间存在冲突时就会出现这种情况。尽管存在问题，但由于缺乏对许可证的理解和复杂的包依赖性，解决许可证不兼容问题需要付出大量努力。在本文中，我们提出了LiResolver，这是一种细粒度、可扩展和灵活的工具，用于解决开源软件的许可证不兼容问题。具体来说，它首先通过细粒度的实体提取和关系提取来理解许可证的语义。然后，它通过优先推荐官方许可证来检测并解决许可证不兼容问题。当没有官方许可证能够满足这些限制时，它会生成一个自定义许可证作为替代解决方案。综合实验证明了LiResolver的有效性，其不兼容问题定位的假阳性（FP）率为4.09%，假阴性（FN）率为0.02%，在LiResolver解决的230个现实世界不兼容项目中占62.61%。我们讨论了OSS开发人员的反馈以及从这项工作中吸取的教训。LiResolver的所有数据集和复制包都已公开，以便于后续研究。,软件及其工程，软件创建和管理，软件开发中的协作,,,
MYQFBLLN,2023,https://doi.org/10.1145/3597926.3598127,ISSTA 2023,A Tale of Two Approximations: Tightening Over-Approximation for DNN Robustness Verification via Under-Approximation,"The robustness of deep neural networks (DNNs) is crucial to the hosting system’s reliability and security. Formal verification has been demonstrated to be effective in providing provable robustness guarantees. To improve its scalability, over-approximating the non-linear activation functions in DNNs by linear constraints has been widely adopted, which transforms the verification problem into an efficiently solvable linear programming problem. Many efforts have been dedicated to defining the so-called tightest approximations to reduce overestimation imposed by over-approximation. In this paper, we study existing approaches and identify a dominant factor in defining tight approximation, namely the approximation domain of the activation function. We find out that tight approximations defined on approximation domains may not be as tight as the ones on their actual domains, yet existing approaches all rely only on approximation domains. Based on this observation, we propose a novel dual-approximation approach to tighten overapproximations, leveraging an activation function’s underestimated domain to define tight approximation bounds. We implement our approach with two complementary algorithms based respectively on Monte Carlo simulation and gradient descent into a tool called DualApp. We assess it on a comprehensive benchmark of DNNs with different architectures. Our experimental results show that DualApp significantly outperforms the state-of-the-art approaches with 100% − 1000% improvement on the verified robustness ratio and 10.64% on average (up to 66.53%) on the certified lower bound.","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification",两个近似的故事：通过欠近似对DNN鲁棒性验证的紧过近似,深度神经网络的鲁棒性对托管系统的可靠性和安全性至关重要。形式验证已被证明在提供可证明的稳健性保证方面是有效的。为了提高其可扩展性，通过线性约束对DNN中的非线性激活函数进行过近似已被广泛采用，这将验证问题转化为一个有效可解的线性规划问题。许多努力都致力于定义所谓的最紧近似，以减少过度近似带来的过高估计。在本文中，我们研究了现有的方法，并确定了定义紧近似的一个主导因素，即激活函数的近似域。我们发现，在近似域上定义的紧近似可能不如在其实际域上的紧近似，但现有的方法都只依赖于近似域。基于这一观察结果，我们提出了一种新的对偶逼近方法来收紧过逼近，利用激活函数的低估域来定义紧逼近边界。我们使用两种互补算法来实现我们的方法，这两种算法分别基于蒙特卡罗模拟和梯度下降到一个名为DualApp的工具中。我们根据不同架构的DNN的综合基准对其进行评估。我们的实验结果表明，DualApp显著优于最先进的方法，在验证的鲁棒性比率上提高了100%-1000%，在验证下限上平均提高了10.64%（高达66.53%）。,计算方法，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件验证和确认，正式软件验证,,,
B2WK6SGD,2023,https://doi.org/10.1145/3597926.3598087,ISSTA 2023,Silent Compiler Bug De-duplication via Three-Dimensional Analysis,"Compiler testing is an important task for assuring the quality of compilers, but investigating test failures is very time-consuming. This is because many test failures are caused by the same compiler bug (known as bug duplication problem). In particular, this problem becomes much more challenging on silent compiler bugs (also called wrong code bugs), since these bugs can provide little information (unlike crash bugs that can produce error messages) for bug de-duplication. In this work, we propose a novel technique (called D3) to solve the duplication problem on silent compiler bugs. Its key insight is to characterize the silent bugs from the testing process and identify three-dimensional information (i.e., test program, optimizations, and test execution) for bug de-duplication. However, there are huge amount of bug-irrelevant details on the three dimensions, D3 then systematically conducts causal analysis to identify bug-causal features from each of the three dimensions for more accurate bug de-duplication. Finally, D3 ranks the test failures that are more likely to be caused by different silent bugs higher by measuring the distance among test failures based on the three-dimensional bug-causal features. Our experimental results on four datasets (including duplicate bugs of both GCC and LLVM) demonstrate the significant superiority of D3 over the two state-of-the-art compiler bug de-duplication techniques, achieving the average improvement of 19.36% and 51.43% in identifying unique silent compiler bugs when analyzing the same number of test failures.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",通过三维分析消除静默编译器错误,编译器测试是确保编译器质量的一项重要任务，但调查测试失败非常耗时。这是因为许多测试失败都是由同一个编译器错误（称为错误重复问题）引起的。特别是，这个问题在静默编译器错误（也称为错误代码错误）上变得更具挑战性，因为这些错误可以为错误消除提供很少的信息（不像崩溃错误会产生错误消息）。在这项工作中，我们提出了一种新的技术（称为D3）来解决静默编译器错误的重复问题。它的关键见解是从测试过程中表征静默的bug，并确定用于bug消除的三维信息（即测试程序、优化和测试执行）。然而，在三个维度上有大量与bug无关的细节，D3然后系统地进行因果分析，从三个维度中的每个维度识别bug因果特征，以实现更准确的bug去重。最后，D3通过基于三维错误因果特征测量测试失败之间的距离，将更可能由不同的静默错误引起的测试失败排名更高。我们在四个数据集上的实验结果（包括GCC和LLVM的重复错误）证明了D3相对于两种最先进的编译器错误消除技术的显著优势，在分析相同数量的测试失败时，在识别独特的静默编译器错误方面实现了19.36%和51.43%的平均改进。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
E72PP2L3,2023,https://doi.org/10.1145/3597926.3598063,ISSTA 2023,Definition and Detection of Defects in NFT Smart Contracts,"Recently, the birth of non-fungible tokens (NFTs) has attracted great attention. NFTs are capable of representing users’ ownership on the blockchain and have experienced tremendous market sales due to their popularity. Unfortunately, the high value of NFTs also makes them a target for attackers. The defects in NFT smart contracts could be exploited by attackers to harm the security and reliability of the NFT ecosystem. Despite the significance of this issue, there is a lack of systematic work that focuses on analyzing NFT smart contracts, which may raise worries about the security of users’ NFTs. To address this gap, in this paper, we introduce 5 defects in NFT smart contracts. Each defect is defined and illustrated with a code example highlighting its features and consequences, paired with possible solutions to fix it. Furthermore, we propose a tool named NFTGuard to detect our defined defects based on a symbolic execution framework. Specifically, NFTGuard extracts the information of the state variables from the contract abstract syntax tree (AST), which is critical for identifying variable-loading and storing operations during symbolic execution. Furthermore, NFTGuard recovers source-code-level features from the bytecode to effectively locate defects and report them based on predefined detection patterns. We run NFTGuard on 16,527 real-world smart contracts and perform an evaluation based on the manually labeled results. We find that 1,331 contracts contain at least one of the 5 defects, and the overall precision achieved by our tool is 92.6%.","Software and its engineering,Software creation and management,Software verification and validation",NFT智能合约缺陷的定义和检测,最近，不可替代代币（NFT）的诞生引起了人们的极大关注。NFT能够代表用户在区块链上的所有权，并因其受欢迎而经历了巨大的市场销售。不幸的是，NFT的高价值也使其成为攻击者的目标。攻击者可能会利用NFT智能合约中的缺陷来损害NFT生态系统的安全性和可靠性。尽管这个问题意义重大，但缺乏专注于分析NFT智能合约的系统性工作，这可能会引发对用户NFT安全性的担忧。为了解决这一差距，本文介绍了NFT智能合约中的5个缺陷。每个缺陷都通过一个代码示例进行定义和说明，突出其特征和后果，并结合可能的解决方案进行修复。此外，我们提出了一个名为NFTGuard的工具，用于基于符号执行框架检测我们定义的缺陷。具体来说，NFTGuard从合同抽象语法树（AST）中提取状态变量的信息，这对于识别符号执行期间的变量加载和存储操作至关重要。此外，NFTGuard从字节码中恢复源代码级别的特征，以有效地定位缺陷并基于预定义的检测模式报告缺陷。我们在16527个真实世界的智能合约上运行NFTGuard，并根据手动标记的结果进行评估。我们发现，1331份合同至少包含5个缺陷中的一个，我们的工具实现的总体精度为92.6%。,软件及其工程，软件创建和管理，软件验证和确认,,,
EW4QN348,2023,https://doi.org/10.1145/3597926.3598102,ISSTA 2023,1dFuzz: Reproduce 1-Day Vulnerabilities with Directed Differential Fuzzing,"1-day vulnerabilities are common in practice and have posed severe threats to end users, as adversaries could learn from released patches to find them and exploit them. Reproducing 1-day vulnerabilities is also crucial for defenders, e.g., to block attack traffic against 1-day vulnerabilities. A core question that affects the effectiveness of recognizing and triggering 1-day vulnerabilities is what is the unique feature of a security patch. After conducting a large-scale empirical study, we point out that a common and unique feature of patches is the trailing call sequence (TCS) and present a novel directed differential fuzzing solution 1dFuzz to efficiently reproduce 1-day vulnerabilities in this paper. Based on the TCS feature, we present a locator 1dLoc able to find candidate patch locations via static analysis, a novel TCS-based distance metric for directed fuzzing, and a novel sanitizer 1dSan able to catch PoCs for 1-day vulnerabilities during fuzzing. We have systematically evaluated 1dFuzz on a set of real-world software vulnerabilities in 11 different settings. Results show that 1dFuzz significantly outperforms state-of-the-art (SOTA) baselines and could find up to 2.26x more 1-day vulnerabilities with a 43% shorter time.","Security and privacy,Systems security",1dFuzz：通过定向差分引信再现1天漏洞,1天漏洞在实践中很常见，对最终用户构成了严重威胁，因为对手可以从发布的补丁中学习，找到它们并加以利用。再现1天漏洞对防御者来说也是至关重要的，例如，阻止针对1天漏洞的攻击流量。影响识别和触发1天漏洞有效性的一个核心问题是安全补丁的独特功能是什么。在进行了大规模的实证研究后，我们指出补丁的一个共同和独特的特征是拖尾调用序列（TCS），并在本文中提出了一种新的有向差分模糊解决方案1dFuzz来有效地再现1天的漏洞。基于TCS功能，我们提出了一种能够通过静态分析找到候选补丁位置的定位器1dLoc，一种用于定向模糊的新的基于TCS的距离度量，以及一种能够在模糊过程中捕获1天漏洞的PoC的新消毒剂1dSan。我们系统地评估了1dFuzz在11种不同设置中的一组真实世界软件漏洞。结果显示，1dFuzz显著优于最先进的（SOTA）基线，可以在43%的时间内发现2.26倍以上的1天漏洞。,安全和隐私，系统安全,,,
A3VTFX96,2023,https://doi.org/10.1145/3597926.3598057,ISSTA 2023,Detecting State Inconsistency Bugs in DApps via On-Chain Transaction Replay and Fuzzing,"Decentralized applications (DApps) consist of multiple smart contracts running on Blockchain. With the increasing popularity of the DApp ecosystem, vulnerabilities in DApps could bring significant impacts such as financial losses. Identifying vulnerabilities in DApps is by no means trivial, as modern DApps consist of complex interactions across multiple contracts. Previous research suffers from either high false positives or false negatives, due to the lack of precise contextual information which is mandatory for confirming smart contract vulnerabilities when analyzing smart contracts. ","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",通过事务回放和模糊化检测DApps中的状态不一致错误,去中心化应用程序（DApps）由运行在区块链上的多个智能合约组成。随着DApp生态系统的日益普及，DApp中的漏洞可能会带来重大影响，如财务损失。识别DApp中的漏洞绝非易事，因为现代DApp由多个合约之间的复杂交互组成。由于缺乏准确的上下文信息，以前的研究要么存在高误报，要么存在误报，这是在分析智能合约时确认智能合约漏洞所必需的。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
D9CGWM88,2023,https://doi.org/10.1145/3597926.3598047,ISSTA 2023,Loop Invariant Inference through SMT Solving Enhanced Reinforcement Learning,"Inferring loop invariants is one of the most challenging problems in program verification. It is highly desired to incorporate machine learning when inferring. This paper presents a Reinforcement Learning (RL) pruning framework to infer loop invariants over a general nonlinear hypothesis space. The key idea is to synergize the RL-based pruning and SMT solving to generate candidate invariants efficiently. To address the sparse reward problem in learning, we design a novel two-dimensional reward mechanism that enables the RL pruner to recognize the capability boundary of SMT solvers and learn the pruning heuristics in a few rounds. We have implemented our approach with Z3 SMT solver in the tool called LIPuS and conducted extensive experiments over the linear and nonlinear benchmarks. Experiment results show that LIPuS can solve the most cases compared to the state-of-the-art loop invariant inference tools such as Code2Inv, ICE-DT, GSpacer, SymInfer, ImplCheck, and Eldarica. Especially, LIPuS outperforms them significantly on nonlinear benchmarks.","Computing methodologies,Machine learning,Learning paradigms,Reinforcement learning,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification",基于SMT求解的循环不变量推理增强强化学习,推导循环不变量是程序验证中最具挑战性的问题之一。在推断时，非常希望结合机器学习。本文提出了一种在一般非线性假设空间上推断循环不变量的强化学习（RL）修剪框架。其关键思想是将基于RL的修剪和SMT求解协同起来，有效地生成候选不变量。为了解决学习中的稀疏奖励问题，我们设计了一种新的二维奖励机制，使RL修剪器能够识别SMT求解器的能力边界，并在几轮中学习修剪启发式算法。我们已经在名为LIPuS的工具中使用Z3 SMT求解器实现了我们的方法，并在线性和非线性基准上进行了大量实验。实验结果表明，与Code2Inv、ICE-DT、GSpacer、SymInfer、ImplCheck和Eldarica等最先进的循环不变量推理工具相比，LIPuS可以解决大多数情况。特别是，LIPuS在非线性基准测试中的表现显著优于它们。,计算方法，机器学习，学习范例，强化学习，软件及其工程，软件创建和管理，软件验证和确认，正式软件验证,,,
SL8F4BLW,2023,https://doi.org/10.1145/3597926.3605239,ISSTA 2023,Reasoning about MLIR Semantics through Effects and Handlers,"MLIR is a novel framework for developing intermediate representations (IRs) of compilers. At its core, MLIR is a framework for the specification of syntax fragments (dialects) and optimisations, which can be combined à−la−carte to form customised IRs. Through this, MLIR allows IR abstractions to be shared across different domains. With rapid adoption of MLIR across industry, techniques for formalised semantics which matches the flexibility and extensibility offered by MLIR are urgently needed. We propose a framework for MLIR semantics based on effect handlers, which allows for dialect semantics to be specified in a modular and composable way, parallel to MLIR. We also describe several research directions continuing on from handlers-based MLIR semantics.","Software and its engineering,Software notations and tools,Compilers,Interpreters,Formal language definitions,Semantics",基于效果和处理程序的MLIR语义推理,MLIR是一种用于开发编译器中间表示（IR）的新框架。MLIR的核心是一个用于规范语法片段（方言）和优化的框架，这些片段和优化可以按菜单组合，形成定制的IR。通过这种方式，MLIR允许在不同的域之间共享IR抽象。随着MLIR在整个行业的快速采用，迫切需要与MLIR提供的灵活性和可扩展性相匹配的形式化语义技术。我们提出了一个基于效果处理程序的MLIR语义框架，该框架允许以模块化和可组合的方式指定方言语义，与MLIR并行。我们还描述了基于处理程序的MLIR语义的几个研究方向。,软件及其工程，软件符号和工具，编译器，解释器，形式语言定义，语义学,,,
AMTE9PW6,2023,https://doi.org/10.1145/3597926.3598094,ISSTA 2023,ROME: Testing Image Captioning Systems via Recursive Object Melting,"Image captioning (IC) systems aim to generate a text description of the salient objects in an image. In recent years, IC systems have been increasingly integrated into our daily lives, such as assistance for visually-impaired people and description generation in Microsoft Powerpoint. However, even the cutting-edge IC systems (e.g., Microsoft Azure Cognitive Services) and algorithms (e.g., OFA) could produce erroneous captions, leading to incorrect captioning of important objects, misunderstanding, and threats to personal safety. The existing testing approaches either fail to handle the complex form of IC system output (i.e., sentences in natural language) or generate unnatural images as test cases. To address these problems, we introduce Recursive Object MElting (ROME), a novel metamorphic testing approach for validating IC systems. Different from existing approaches that generate test cases by inserting objects, which easily make the generated images unnatural, ROME melts (i.e., remove and inpaint) objects. ROME assumes that the object set in the caption of an image includes the object set in the caption of a generated image after object melting. Given an image, ROME can recursively remove its objects to generate different pairs of images. We use ROME to test one widely-adopted image captioning API and four state-of-the-art (SOTA) algorithms. The results show that the test cases generated by ROME look much more natural than the SOTA IC testing approach and they achieve comparable naturalness to the original images. Meanwhile, by generating test pairs using 226 seed images, ROME reports a total of 9,121 erroneous issues with high precision (86.47%-92.17%). In addition, we further utilize the test cases generated by ROME to retrain the Oscar, which improves its performance across multiple evaluation metrics.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",ROME:通过递归对象熔化测试图像字幕系统,图像字幕（IC）系统旨在生成图像中显著对象的文本描述。近年来，IC系统越来越多地融入我们的日常生活，例如为视障人士提供帮助和在Microsoft Powerpoint中生成描述。然而，即使是尖端的IC系统（如Microsoft Azure Cognitive Services）和算法（如OFA）也可能产生错误的字幕，导致重要对象的字幕错误、误解和人身安全威胁。现有的测试方法要么无法处理复杂形式的IC系统输出（即自然语言的句子），要么生成不自然的图像作为测试用例。为了解决这些问题，我们引入了递归对象MElting（ROME），这是一种用于验证IC系统的新的变形测试方法。与现有的通过插入对象生成测试用例的方法不同，ROME融化（即移除和修复）对象，这很容易使生成的图像变得不自然。ROME假设在图像的标题中设置的对象包括在对象熔化之后生成的图像的标题内设置的对象。给定一个图像，ROME可以递归地移除其对象以生成不同的图像对。我们使用ROM测试了一种广泛采用的图像字幕API和四种最先进的（SOTA）算法。结果表明，ROME生成的测试用例看起来比SOTA IC测试方法自然得多，并且它们实现了与原始图像相当的自然度。同时，通过使用226个种子图像生成测试对，ROME报告了总计9121个高精度（86.47%-92.17%）的错误问题。此外，我们进一步利用ROME生成的测试用例对Oscar进行了再培训，从而提高了其在多个评估指标上的性能。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
ZJB7SFLE,2023,https://doi.org/10.1145/3597926.3598038,ISSTA 2023,Pattern-Based Peephole Optimizations with Java JIT Tests,"We present JOG, a framework that facilitates developing Java JIT  
peephole optimizations alongside JIT tests. JOG enables developers  
to write a pattern, in Java itself, that specifies desired code transformations by writing code before and after the optimization, as  
well as any necessary preconditions. Such patterns can be written  
in the same way that tests of the optimization are already written  
in OpenJDK. JOG translates each pattern into C/C++ code that  
can be integrated as a JIT optimization pass. JOG also generates  
Java tests for optimizations from patterns. Furthermore, JOG can  
automatically detect possible shadow relation between a pair of  
optimizations where the effect of the shadowed optimization is  
overridden by another. Our evaluation shows that JOG makes it  
easier to write readable JIT optimizations alongside tests without  
decreasing the effectiveness of JIT optimizations. We wrote 162  
patterns, including 68 existing optimizations in OpenJDK, 92 new  
optimizations adapted from LLVM, and two new optimizations that  
we proposed. We opened eight pull requests (PRs) for OpenJDK,  
including six for new optimizations, one on removing shadowed  
optimizations, and one for newly generated JIT tests; seven PRs  
have already been integrated into the master branch of OpenJDK.","Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software defect analysis,Software testing and debugging,Software notations and tools,Compilers,Just-in-time compilers,Source code generation,Context specific languages,Domain specific languages",基于模式的Java JIT测试Peephole优化,我们介绍了JOG，一个促进Java JIT开发的框架,软件及其工程，软件创建和管理，软件验证和确认，正式软件验证，软件缺陷分析，软件测试和调试，软件符号和工具，编译器，即时编译器，源代码生成，上下文特定语言，领域特定语言,,,
SAYCLAW4,2023,https://doi.org/10.1145/3597926.3598070,ISSTA 2023,Building Critical Testing Scenarios for Autonomous Driving from Real Accidents,"One of the aims of the development and spread of autonomous driving technology is to reduce traffic accidents caused by human factors.  
But recently reported data on fatal accidents involving autonomous driving system (ADS) shows that this important goal has not been achieved.  
So there is an emerge requirement on more comprehensive and targeted testing especially on safe driving.  
In this paper, we propose an approach to automatically building critical testing scenarios from real-world accident data.  
Firstly, we propose a new model called M-CPS (Multi-channel Panoptic Segmentation) to extract the effective information from the accident record (such as images or videos), and separate the independent individuals of different traffic participants for further scene recovery.  
Compared with the traditional panoramic segmentation models, M-CPS model is able to effectively handle segmentation challenges due to the shooting angle, image quality, pixel overlap and other problems existing in the accident record.  
Next, the extracted core information is then connected with the virtual testing platform to generate the original scene set.  
Besides, we also design a mutation testing solution on the basis of the original scene set, thus greatly enriching the scene library for testing.  
In our experiments,  
the M-CPS model reaches a result of 66.1% PQ on CityScapes test set, shows that our model has only slight fluctuations on performance compared with the best benchmark model on pure panoptic segmentation task.  
It also reaches a result of 84.5% IoU for semantic segmentation branch and 40.3% mAP for instance segmentation branch on SHIFT dataset.  
Then we use UCF-Crime, CADP and US-Accidents datasets to generate the original and mutated scene set.  
Those generated scene sets are connected to Apollo and Carla simulation platforms to test ADS prototypes.  
We find three types of scenarios that can lead to accidents of ADS prototypes, which indicates that the existing ADS prototype has defects.  
Our solution provides a new possible direction for the recovery of key scenarios in ADS testing, and can improve the efficiency in related fields.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",从实际事故中构建自动驾驶的关键测试场景,自动驾驶技术发展和传播的目的之一是减少人为因素造成的交通事故。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
WZINT354,2023,https://doi.org/10.1145/3597926.3598093,ISSTA 2023,CoopHance: Cooperative Enhancement for Robustness of Deep Learning Systems,"Adversarial attacks have been a threat to Deep Learning (DL) systems to be reckoned with. By adding human-imperceptible perturbation to benign inputs, adversarial attacks can cause the incorrect behavior of DL systems. Considering the popularity of DL systems in the industry, it is critical and urgent for developers to enhance the robustness of DL systems against adversarial attacks.  
","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Security and privacy,Software and application security,Domain-specific security and privacy architectures",CoopHance:深度学习系统鲁棒性的协同增强,对抗性攻击对深度学习（DL）系统的威胁不容忽视。通过将人类无法察觉的扰动添加到良性输入中，对抗性攻击可能会导致DL系统的错误行为。考虑到DL系统在行业中的流行，开发人员提高DL系统对对抗性攻击的鲁棒性是至关重要和紧迫的。,计算方法，机器学习，机器学习方法，神经网络，安全和隐私，软件和应用程序安全，特定领域的安全和隐私体系结构,,,
LDW5CPIB,2023,https://doi.org/10.1145/3597926.3598106,ISSTA 2023,Alligator in Vest: A Practical Failure-Diagnosis Framework via Arm Hardware Features,"Failure diagnosis in practical systems is difficult, and the main obstacle is that the information a developer has access to is limited. This information is usually not enough to help developers fix or even locate the related bug. Moreover, due to the vast difference between the development and production environments, it is not trivial to reproduce failures from the production environment in the development environment. When failures are caused by non-deterministic events such as race conditions or unforeseen inputs, reproducing them is even more challenging.  
","Security and privacy,Software and application security,Systems security",背心鳄鱼：一个基于手臂硬件特征的实用故障诊断框架,实际系统中的故障诊断很困难，主要障碍是开发人员所能获得的信息有限。这些信息通常不足以帮助开发人员修复甚至定位相关的错误。此外，由于开发环境和生产环境之间的巨大差异，在开发环境中重现生产环境中的故障并非易事。当故障是由非确定性事件（如比赛条件或不可预见的输入）引起时，再现它们更具挑战性。,安全和隐私，软件和应用程序安全，系统安全,,,
DR4MH5UK,2023,https://doi.org/10.1145/3597926.3598098,ISSTA 2023,Catamaran: Low-Overhead Memory Safety Enforcement via Parallel Acceleration,"Memory safety issues are the intrinsic diseases of C/C++ programs.  
Dynamic memory safety enforcement as the dominant approach has an advantage in high effectiveness, yet suffers from prohibitively high runtime overhead.  
Existing attempts to reduce the overhead are either labor-intensive, tightly dependent on specific hardware/compiler support, or poorly effective.  
","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software performance,Theory of computation,Semantics and reasoning,Program reasoning,Program analysis",双体船：通过平行加速实现低头顶记忆安全,内存安全问题是C/C++程序的内在问题。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件组织和属性，额外功能属性，软件性能，计算理论，语义和推理，程序推理，程序分析,,,
DRS9CMH2,2023,https://doi.org/10.1145/3597926.3598066,ISSTA 2023,Automatically Reproducing Android Bug Reports using Natural Language Processing and Reinforcement Learning,"As part of the process of resolving issues submitted by users via bug reports, Android developers attempt to reproduce and observe the crashes described by the bug reports. Due to the low-quality of bug reports and the complexity of modern apps, the reproduction process is non-trivial and time-consuming. Therefore, automatic approaches that can help reproduce Android bug reports are in great need. However, current approaches to help developers automatically reproduce bug reports are only able to handle limited forms of natural language text and struggle to successfully reproduce crashes for which the initial bug report had missing or imprecise steps. In this paper, we introduce a new fully automated approach to reproduce crashes from Android bug reports that addresses these limitations. Our approach accomplishes this by leveraging natural language processing techniques to more holistically and accurately analyze the natural language in Android bug reports and designing new techniques, based on reinforcement learning, to guide the search for successful reproducing steps. We conducted an empirical evaluation of our approach on 77 real world bug reports. Our approach achieved 67% precision and 77% recall in accurately extracting reproduction steps from bug reports, reproduced 74% of the total bug reports, and reproduced 64% of the bug reports that contained missing steps, significantly outperforming state of the art techniques.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",使用自然语言处理和强化学习自动复制Android Bug报告,作为解决用户通过错误报告提交的问题的过程的一部分，Android开发人员试图重现和观察错误报告所描述的崩溃。由于错误报告的质量低，以及现代应用程序的复杂性，复制过程非常耗时。因此，非常需要能够帮助重现Android错误报告的自动方法。然而，目前帮助开发人员自动复制错误报告的方法只能处理有限形式的自然语言文本，并且很难成功复制最初错误报告缺少或不精确步骤的崩溃。在本文中，我们介绍了一种新的全自动方法，从Android错误报告中再现崩溃，以解决这些限制。我们的方法通过利用自然语言处理技术来更全面、准确地分析Android错误报告中的自然语言，并设计基于强化学习的新技术来指导搜索成功的复制步骤，从而实现这一点。我们对77份真实世界的bug报告进行了实证评估。我们的方法在从错误报告中准确提取再现步骤方面实现了67%的准确率和77%的召回率，再现了74%的错误报告总数，再现了64%的包含缺失步骤的错误报告，显著优于现有技术。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
SZ55LQMW,2023,https://doi.org/10.1145/3597926.3598142,ISSTA 2023,Detecting Condition-Related Bugs with Control Flow Graph Neural Network,"Automated bug detection is essential for high-quality software development and has attracted much attention over the years. Among the various bugs, previous studies show that the condition expressions are quite error-prone and the condition-related bugs are commonly found in practice. Traditional approaches to automated bug detection are usually limited to compilable code and require tedious manual effort. Recent deep learning-based work tends to learn general syntactic features based on Abstract Syntax Tree (AST) or apply the existing Graph Neural Networks over program graphs. However, AST-based neural models may miss important control flow information of source code, and existing Graph Neural Networks for bug detection tend to learn local neighbourhood structure information. Generally, the condition-related bugs are highly influenced by control flow knowledge, therefore we propose a novel CFG-based Graph Neural Network (CFGNN) to automatically detect condition-related bugs, which includes a graph-structured LSTM unit to efficiently learn the control flow knowledge and long-distance context information.  
We also adopt the API-usage attention mechanism to leverage the API knowledge. To evaluate the proposed approach, we collect real-world bugs in popular GitHub repositories and build a large-scale condition-related bug dataset. The experimental results show that our proposed approach significantly outperforms the state-of-the-art methods for detecting condition-related bugs.","Software and its engineering,Software notations and tools,Software maintenance tools",用控制流图神经网络检测状态相关错误,自动化错误检测对于高质量的软件开发至关重要，多年来一直备受关注。在各种错误中，先前的研究表明，条件表达式非常容易出错，并且与条件相关的错误在实践中很常见。自动错误检测的传统方法通常局限于可编译的代码，并且需要繁琐的手动工作。最近基于深度学习的工作倾向于基于抽象语法树（AST）来学习一般语法特征，或者将现有的图神经网络应用于程序图。然而，基于AST的神经模型可能会错过源代码的重要控制流信息，并且现有的用于错误检测的图神经网络倾向于学习局部邻域结构信息。通常情况下，与条件相关的错误受到控制流知识的高度影响，因此我们提出了一种新的基于CFG的图神经网络（CFGNN）来自动检测与条件有关的错误，该网络包括一个图结构的LSTM单元，以有效地学习控制流知识和远程上下文信息。,软件及其工程，软件符号和工具，软件维护工具,,,
9QBCY3UF,2023,https://doi.org/10.1145/3597926.3598108,ISSTA 2023,Testing Automated Driving Systems by Breaking Many Laws Efficiently,"An automated driving system (ADS), as the brain of an autonomous vehicle (AV), should be tested thoroughly ahead of deployment.  
ADS must satisfy a complex set of rules to ensure road safety, e.g., the existing traffic laws and possibly future laws that are dedicated to AVs.  
To comprehensively test an ADS, we would like to systematically discover diverse scenarios in which certain traffic law is violated. The challenge is that (1) there are many traffic laws (e.g., 13 testable articles in Chinese traffic laws and 16 testable articles in Singapore traffic laws, with 81 and 43 violation situations respectively); and (2) many of traffic laws are only relevant in complicated specific scenarios.  
","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",通过有效地打破许多法律来测试自动驾驶系统,自动驾驶系统（ADS）作为自动驾驶汽车（AV）的大脑，应在部署前进行彻底测试。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
Z87ZGJ4Q,2023,https://doi.org/10.1145/3597926.3598077,ISSTA 2023,Testing the Compiler for a New-Born Programming Language: An Industrial Case Study (Experience Paper),"Due to the critical role of compilers, many compiler testing techniques have been proposed, two most notable categories among which are grammar-based and metamorphic-based techniques. All of them have been extensively studied for testing mature compilers. However, it is typical to develop a new compiler for a new-born programming language in practice. In this scenario, the existing techniques are hardly applicable due to some major reasons: (1) no reference compilers to support differential testing, (2) lack of program analysis tools to support most of metamorphic-based compiler testing, (3) substantial implementation effort incurred by different programming language features. Hence, it is unknown how the existing techniques perform in this new scenario.  
","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Compilers",测试一种新兴编程语言的编译器：一个工业案例研究（经验论文）,由于编译器的关键作用，人们提出了许多编译器测试技术，其中最著名的两类是基于语法的技术和基于变形的技术。所有这些都经过了广泛的研究，用于测试成熟的编译器。然而，在实践中，为新生的编程语言开发一个新的编译器是很典型的。在这种情况下，现有的技术几乎不适用，这主要是由于以下原因：（1）没有支持差分测试的参考编译器，（2）缺乏支持大多数基于变形的编译器测试的程序分析工具，（3）不同编程语言特性带来的大量实现工作。因此，目前尚不清楚现有技术在这种新情况下的表现。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，编译器,,,
FXES456J,2023,https://doi.org/10.1145/3597926.3598139,ISSTA 2023,NodeRT: Detecting Races in Node.js Applications Practically,"Node.js has become one of the most popular development platforms due to its superior concurrency support. However, races induced by the nondeterministic execution order of event handlers may occur in Node.js applications, causing serious runtime failures. The state-of-the-art Node.js race detector NRace builds a happens-before (HB) graph before detection with a set of HB relation rules. In detection, NRace utilizes a heavy-weight BFS-based algorithm to query the reachability between resource operations, which introduces substantial overhead in practice, causing NRace inapplicable to real-world Node.js application test processes. This paper proposes a more practical Node.js dynamic race detection approach called NodeRT (Node.js Race Tracker). To reduce unnecessary overhead, NodeRT simplifies the HB relation rules, and divides the detection into three stages: trace collection stage, race candidate detection stage, and false positive removal stage. In the trace collection stage, NodeRT constructs a partial HB graph called asynchronous call tree (ACTree), enabling efficient reachability queries between event handlers. In the race candidate detection stage, NodeRT performs detection on the ACTree, which effectively eliminates most non-racing event handlers and outputs race candidates. In the false positive removal stage, NodeRT utilizes matching rules derived from HB relation rules and features of resources to reduce false positives in the race candidates. In experiments, NodeRT detects all known races and 9 unknown harmful races in real-world applications, whereas NRace only detects 3 of the unknown harmful races, with 64× more time consumption on average. Compared with NRace, NodeRT significantly reduces the overhead, making it practical to be integrated into real-world test processes.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",NodeRT：实际检测Node.js应用程序中的竞争,Node.js由于其卓越的并发支持而成为最受欢迎的开发平台之一。然而，在Node.js应用程序中，由于事件处理程序的执行顺序不确定，可能会引发竞争，从而导致严重的运行时故障。最先进的Node.js种族检测器NRace在检测之前使用一组HB关系规则构建了一个发生前（HB）图。在检测中，NRace使用基于重权重BFS的算法来查询资源操作之间的可达性，这在实践中引入了大量开销，导致NRace不适用于真实世界的Node.js应用程序测试过程。本文提出了一种更实用的Node.js动态种族检测方法NodeRT（Node.js race Tracker）。为了减少不必要的开销，NodeRT简化了HB关系规则，并将检测分为三个阶段：痕迹收集阶段、种族候选检测阶段和假阳性去除阶段。在跟踪收集阶段，NodeRT构建了一个称为异步调用树（ACTree）的部分HB图，从而在事件处理程序之间实现高效的可达性查询。在竞赛候选检测阶段，NodeRT对ACTree进行检测，有效地消除了大多数非竞赛事件处理程序，并输出竞赛候选。在假阳性去除阶段，NodeRT利用从HB关系规则和资源特征导出的匹配规则来减少候选种族中的假阳性。在实验中，NodeRT在现实世界的应用程序中检测到所有已知种族和9个未知的有害种族，而NRace只检测到3个未知的危害种族，平均多消耗64倍的时间。与NRace相比，NodeRT显著降低了开销，使其能够集成到真实世界的测试过程中。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
I2RVZSUE,2023,https://doi.org/10.1145/3597926.3598113,ISSTA 2023,ModelObfuscator: Obfuscating Model Information to Protect Deployed ML-Based Systems,"More and more edge devices and mobile apps are leveraging deep learning (DL) capabilities. Deploying such models on devices – referred to as on-device models – rather than as remote cloud-hosted services, has gained popularity because it avoids transmitting user’s data off of the device and achieves high response time. However, on-device models can be easily attacked, as they can be accessed by unpacking corresponding apps and the model is fully exposed to attackers. Recent studies show that attackers can easily generate white-box-like attacks for an on-device model or even inverse its training data. To protect on-device models from white-box attacks, we propose a novel technique called model obfuscation. Specifically, model obfuscation hides and obfuscates the key information – structure, parameters and attributes – of models by renaming, parameter encapsulation, neural structure obfuscation, shortcut injection, and extra layer injection. We have developed a prototype tool ModelObfuscator to automatically obfuscate on-device TFLite models. Our experiments show that this proposed approach can dramatically improve model security by significantly increasing the difficulty of parsing models’ inner information, without increasing the latency of DL models. Our proposed on-device model obfuscation has the potential to be a fundamental technique for on-device model deployment. Our prototype tool is publicly available at https://github.com/zhoumingyi/ModelObfuscator.","Software and its engineering,Software organization and properties,Extra-functional properties,Software reliability,Software safety",ModelObfuscator：混淆模型信息以保护已部署的基于ML的系统,越来越多的边缘设备和移动应用程序正在利用深度学习（DL）功能。在设备上部署此类模型（称为设备上模型），而不是作为远程云托管服务，已经越来越受欢迎，因为它避免了将用户的数据传输到设备之外，并实现了高响应时间。然而，设备上的模型很容易受到攻击，因为可以通过打开相应的应用程序来访问它们，并且该模型完全暴露在攻击者面前。最近的研究表明，攻击者可以很容易地为设备上的模型生成类似白盒的攻击，甚至可以反转其训练数据。为了保护设备上的模型免受白盒攻击，我们提出了一种称为模型模糊的新技术。具体而言，模型模糊处理通过重命名、参数封装、神经结构模糊处理、快捷方式注入和额外层注入来隐藏和模糊模型的关键信息——结构、参数和属性。我们开发了一个原型工具ModelObfuscator，用于自动模糊设备上的TFLite模型。我们的实验表明，这种方法可以在不增加DL模型延迟的情况下，显著增加解析模型内部信息的难度，从而显著提高模型的安全性。我们提出的设备上模型模糊有可能成为设备上模型部署的基本技术。我们的原型工具可在https://github.com/zhoumingyi/ModelObfuscator.,软件及其工程，软件组织和属性，额外功能属性，软件可靠性，软件安全,,,
BN8UPDRM,2023,https://doi.org/10.1145/3597926.3598089,ISSTA 2023,DDLDroid: Efficiently Detecting Data Loss Issues in Android Apps,"Data loss issues in Android apps triggered by activity restart or app relaunch significantly reduce the user experience and undermine the app quality. While data loss detection has received much attention, the state-of-the-art techniques still miss many data loss issues due to the inaccuracy of the static analysis or the low coverage of the dynamic exploration. To this end, we present DDLDroid, a static analysis approach and an open-source tool, to systematically and efficiently detect data loss issues based on the data flow analysis. DDLDroid is bootstrapped by a saving-restoring bipartite graph which correlates variables that need saving to the corresponding variables that need restoring according to their carrier widgets. The missed or broken saving or restoring data flows lead to data loss issues. The experimental evaluation on 66 Android apps demonstrates the effectiveness and efficiency of our approach: DDLDroid successfully detects 302 true data loss issues in 73 minutes, 180 of which are previously unknown.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis",DDLDroid：有效检测Android应用程序中的数据丢失问题,安卓应用程序中由活动重启或应用程序重新启动引发的数据丢失问题会显著降低用户体验并破坏应用程序质量。尽管数据丢失检测受到了广泛关注，但由于静态分析的不准确或动态探索的覆盖率低，现有技术仍然遗漏了许多数据丢失问题。为此，我们提出了DDLDroid，这是一种静态分析方法和开源工具，可以在数据流分析的基础上系统有效地检测数据丢失问题。DDLDroid由保存-恢复二分图引导，该图将需要保存的变量与需要恢复的相应变量根据其载体窗口小部件进行关联。丢失或损坏的保存或恢复数据流会导致数据丢失问题。对66款安卓应用程序的实验评估证明了我们方法的有效性和效率：DDLDroid在73分钟内成功检测到302个真实的数据丢失问题，其中180个以前是未知的。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析,,,
VJAI5VGM,2023,https://doi.org/10.1145/3597926.3604916,ISSTA 2023,DDLDroid: A Static Analyzer for Automatically Detecting Data Loss Issues in Android Applications,"DDLDroid is a static analyzer for detecting data loss issues in Android apps during activity restart or app relaunch. It is bootstrapped by a saving-restoring bipartite graph which correlates variables that need saving to those that need restoring according to their carrier widgets, and is based on the analysis of saving and restoring data flows. It reports data loss issues once missed or broken data flows are identified. DDLDroid detects 302 true data loss issues out of 66 Android apps in 73 minutes, including 180 previously-unknown issues, demonstrating its effectiveness and efficiency.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis",DDLDroid:一个用于自动检测Android应用程序中数据丢失问题的静态分析器,DDLDroid是一个静态分析器，用于在活动重启或应用程序重新启动期间检测Android应用程序中的数据丢失问题。它是由保存-恢复二分图引导的，该图根据载体部件将需要保存的变量与需要恢复的变量关联起来，并基于对保存和恢复数据流的分析。一旦发现丢失或损坏的数据流，它就会报告数据丢失问题。DDLDroid在73分钟内检测到66个Android应用程序中的302个真实数据丢失问题，其中包括180个以前未知的问题，证明了其有效性和效率。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析,,,
9QZT9A6J,2023,https://doi.org/10.1145/3597926.3598109,ISSTA 2023,DeepAtash: Focused Test Generation for Deep Learning Systems,"When deployed in the operation environment, Deep Learning (DL) systems often experience the so-called development to operation (dev2op) data shift, which causes a lower prediction accuracy on field data as compared to the one measured on the test set during development. To address the dev2op shift, developers must obtain new data with the newly observed features, as these are under-represented in the train/test set, and must use them to fine tune the DL model, so as to reach the desired accuracy level.  
In this paper, we address the issue of acquiring new data with the specific features observed in operation, which caused a dev2op shift, by proposing DeepAtash, a novel search-based focused testing approach for DL systems.  
DeepAtash targets a cell in the feature space, defined as a combination of feature ranges, to generate misbehaviour-inducing inputs with predefined features.  
Experimental results show that DeepAtash was able to generate up to 29X more targeted, failure-inducing inputs than the baseline approach. The inputs generated by DeepAtash were useful to significantly improve the quality of the original DL systems through fine tuning not only on data with the targeted features, but quite surprisingly also on inputs drawn from the original distribution.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",DeepAtash：深度学习系统的集中测试生成,当部署在操作环境中时，深度学习（DL）系统通常会经历所谓的开发到操作（dev2op）数据转移，这导致与开发期间在测试集上测量的数据相比，现场数据的预测精度较低。为了解决dev2op的转变，开发人员必须获得具有新观察到的特征的新数据，因为这些特征在训练/测试集中表现不足，并且必须使用它们来微调DL模型，以达到所需的精度水平。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
YVBUPM2Y,2023,https://doi.org/10.1109/TSE.2023.3243262,TSE 2023,Plumber: Boosting the Propagation of Vulnerability Fixes in the npm Ecosystem,"Vulnerabilities are known reported security threats that affect a large amount of packages in the npm ecosystem. To mitigate these security threats, the open-source community strongly suggests vulnerable packages to timely publish vulnerability fixes and recommends affected packages to update their dependencies. However, there are still serious lags in the propagation of vulnerability fixes in the ecosystem. In our preliminary study on the latest versions of 356,283 active npm packages, we found that 20.0% of them can still introduce vulnerabilities via direct or transitive dependencies although the involved vulnerable packages have already published fix versions for over a year. Prior study by (Chinthanet et al. 2021) lays the groundwork for research on how to mitigate propagation lags of vulnerability fixes in an ecosystem. They conducted an empirical investigation to identify lags that might occur between the vulnerable package release and its fixing release. They found that factors such as the branch upon which a fix landed and the severity of the vulnerability had a small effect on its propagation trajectory throughout the ecosystem. To ensure quick adoption and propagation of a release that contains the fix, they gave several actionable advice to developers and researchers. However, it is still an open question how to design an effective technique to accelerate the propagation of vulnerability fixes. Motivated by this problem, in this paper, we conducted an empirical study to learn the scale of packages that block the propagation of vulnerability fixes in the ecosystem and investigate their evolution characteristics. Furthermore, we distilled the remediation strategies that have better effects on mitigating the fix propagation lags. Leveraging our empirical findings, we propose an ecosystem-level technique, Plumber, for deriving feasible remediation strategies to boost the propagation of vulnerability fixes. To precisely diagnose the causes of fix propagation blocking, Plumber models the vulnerability metadata, and npm dependency metadata and continuously monitors their evolution. By analyzing a full-picture of the ecosystem-level dependency graph and the corresponding fix propagation statuses, it derives remediation schemes for pivotal packages. In the schemes, Plumber provides customized remediation suggestions with vulnerability impact analysis to arouse package developers’ awareness. We applied Plumber to generating 268 remediation reports for the identified pivotal packages, to evaluate its remediation effectiveness based on developers’ feedback. Encouragingly, 47.4% our remediation reports received positive feedback from many well-known npm projects, such as Tensorflow/tfjs, Ethers.js, and GoogleChrome/workbox. Our reports have boosted the propagation of vulnerability fixes into 16,403 root packages through 92,469 dependency paths. On average, each remediated package version is receiving 72,678 downloads per week by the time of this work.","Empirical study,  $npm$   n p m      ecosystem,vulnerable dependencies",管道工：促进漏洞修复在npm生态系统中的传播,已知的漏洞报告了影响npm生态系统中大量包的安全威胁。为了减轻这些安全威胁，开源社区强烈建议易受攻击的软件包及时发布漏洞修复程序，并建议受影响的软件包更新其依赖关系。然而，脆弱性修复在生态系统中的传播仍然存在严重滞后。在我们对356283个活动npm包的最新版本的初步研究中，我们发现其中20.0%的包仍然可以通过直接或传递依赖关系引入漏洞，尽管相关的易受攻击包已经发布了一年多的修复版本。（Chinthant等人，2021）之前的研究为研究如何减轻生态系统中脆弱性修复的传播滞后奠定了基础。他们进行了一项实证调查，以确定易受攻击的软件包发布和修复发布之间可能出现的滞后。他们发现，修复所在的分支和脆弱性的严重程度等因素对其在整个生态系统中的传播轨迹影响很小。为了确保快速采用和传播包含修复程序的版本，他们向开发人员和研究人员提供了一些可行的建议。然而，如何设计一种有效的技术来加速漏洞修复的传播仍然是一个悬而未决的问题。受此问题的启发，在本文中，我们进行了一项实证研究，以了解阻碍脆弱性修复在生态系统中传播的一揽子计划的规模，并调查其进化特征。此外，我们提炼出了在缓解修复传播滞后方面效果更好的补救策略。利用我们的经验发现，我们提出了一种生态系统级别的技术，Plumber，用于推导可行的补救策略，以促进漏洞修复的传播。为了准确诊断修复传播阻塞的原因，Plumber对漏洞元数据和npm依赖元数据进行建模，并持续监控它们的演变。通过分析生态系统级依赖图的全貌和相应的修复传播状态，它导出了关键包的修复方案。在这些方案中，Plumber通过漏洞影响分析提供定制的补救建议，以唤起包开发人员的意识。我们应用Plumber为已确定的关键包生成268份补救报告，以根据开发人员的反馈评估其补救效果。令人鼓舞的是，47.4%的补救报告得到了许多知名npm项目的积极反馈，如Tensorflow/tfjs、Ethers.js和GoogleChrome/workbox。我们的报告通过92469个依赖路径，促进了漏洞修复到16403个根包中的传播。到本工作完成时，每个修正的软件包版本平均每周收到72678次下载。,实证研究，$npm$npm生态系统，脆弱的依赖性,,,
ISYCZYPM,2023,https://doi.org/10.1109/TSE.2023.3287297,TSE 2023,Decomposition of Monolith Applications Into Microservices Architectures: A Systematic Review,"Microservices architecture has gained significant traction, in part owing to its potential to deliver scalable, robust, agile, and failure-resilient software products. Consequently, many companies that use large and complex software systems are actively looking for automated solutions to decompose their monolith applications into microservices. This paper rigorously examines 35 research papers selected from well-known databases using a Systematic Literature Review (SLR) protocol and snowballing method, extracting data to answer the research questions, and presents the following four contributions. First, the Monolith to Microservices Decomposition Framework (M2MDF) which identifies the major phases and key elements of decomposition. Second, a detailed analysis of existing decomposition approaches, tools and methods. Third, we identify the metrics and datasets used to evaluate and validate monolith to microservice decomposition processes. Fourth, we propose areas for future research. Overall, the findings suggest that monolith decomposition into microservices remains at an early stage and there is an absence of methods for combining static, dynamic, and evolutionary data. Insufficient tool support is also in evidence. Furthermore, standardised metrics, datasets, and baselines have yet to be established. These findings can assist practitioners seeking to understand the various dimensions of monolith decomposition and the community's current capabilities in that endeavour. The findings are also of value to researchers looking to identify areas to further extend research in the monolith decomposition space.","Monolith application decomposition,monolith to microservices migration,microservices architecture,microservices identification,static analysis,dynamic analysis",将单片应用程序分解为微服务体系结构：系统综述,微服务体系结构获得了巨大的吸引力，部分原因是它有潜力提供可扩展、健壮、敏捷和故障恢复的软件产品。因此，许多使用大型复杂软件系统的公司都在积极寻找自动化解决方案，将其单片应用程序分解为微服务。本文使用系统文献综述（SLR）协议和滚雪球方法，从知名数据库中筛选了35篇研究论文，提取数据来回答研究问题，并提出了以下四点贡献。首先，单片到微服务分解框架（M2MDF）确定了分解的主要阶段和关键元素。其次，详细分析了现有的分解方法、工具和方法。第三，我们确定了用于评估和验证单体到微服务分解过程的指标和数据集。第四，我们提出了未来研究的领域。总体而言，研究结果表明，整体分解为微服务仍处于早期阶段，并且缺乏将静态、动态和进化数据相结合的方法。工具支持不足也是明显的。此外，标准化指标、数据集和基线尚未建立。这些发现可以帮助从业者了解巨石分解的各个方面以及社区目前在这方面的能力。这些发现对研究人员也有价值，他们希望确定进一步扩展巨石分解空间研究的领域。,单片应用程序分解，单片到微服务迁移，微服务架构，微服务识别，静态分析，动态分析,,,
UGUS3NYL,2023,https://doi.org/10.1109/TSE.2022.3154717,TSE 2023,Evaluation of Static Vulnerability Detection Tools With Java Cryptographic API Benchmarks,"Several studies showed that misuses of cryptographic APIs are common in real-world code (e.g., Apache projects and Android apps). There exist several open-sourced and commercial security tools that automatically screen Java programs to detect misuses. To compare their accuracy and security guarantees, we develop two comprehensive benchmarks named CryptoAPI-Bench and ApacheCryptoAPI-Bench. CryptoAPI-Bench consists of 181 unit test cases that cover basic cases, as well as complex cases, including interprocedural, field sensitive, multiple class test cases, and path sensitive data flow of misuse cases. The benchmark also includes correct cases for testing false-positive rates. The ApacheCryptoAPI-Bench consists of 121 cryptographic cases from 10 Apache projects. We evaluate four tools, namely, SpotBugs, CryptoGuard, CrySL, and another tool (anonymous) using both benchmarks. We present their performance and comparative analysis. The ApacheCryptoAPI-Bench also examines the scalability of the tools. Our benchmarks are useful for advancing state-of-the-art solutions in the space of misuse detection.","Cryptographic API misuses,benchmark,Java",基于Java加密API基准的静态漏洞检测工具评估,几项研究表明，对加密API的滥用在现实世界的代码中很常见（例如，Apache项目和Android应用程序）。有几种开源和商业安全工具可以自动筛选Java程序以检测误用。为了比较它们的准确性和安全保证，我们开发了两个全面的基准测试，分别命名为CryptoAPI Bench和ApacheCryptoAPI Bench。CryptoAPI Bench由181个单元测试用例组成，涵盖基本用例和复杂用例，包括过程间、字段敏感、多类测试用例和误用用例的路径敏感数据流。该基准还包括检测假阳性率的正确案例。ApacheCryptoAPI工作台由10个Apache项目的121个加密案例组成。我们评估了四个工具，即SpotBugs、CryptoGuard、CrySL和另一个使用这两个基准的工具（匿名）。我们介绍了他们的表现和比较分析。ApacheCryptoAPI工作台还检查了这些工具的可扩展性。我们的基准对于在误用检测领域推进最先进的解决方案非常有用。,加密API误用，基准测试，Java,,,
KUJX75FE,2023,https://doi.org/10.1109/TSE.2023.3243522,TSE 2023,Black-Box Testing of Deep Neural Networks through Test Case Diversity,"Deep Neural Networks (DNNs) have been extensively used in many areas including image processing, medical diagnostics and autonomous driving. However, DNNs can exhibit erroneous behaviours that may lead to critical errors, especially when used in safety-critical systems. Inspired by testing techniques for traditional software systems, researchers have proposed neuron coverage criteria, as an analogy to source code coverage, to guide the testing of DNNs. Despite very active research on DNN coverage, several recent studies have questioned the usefulness of such criteria in guiding DNN testing. Further, from a practical standpoint, these criteria are white-box as they require access to the internals or training data of DNNs, which is often not feasible or convenient. Measuring such coverage requires executing DNNs with candidate inputs to guide testing, which is not an option in many practical contexts. In this paper, we investigate diversity metrics as an alternative to white-box coverage criteria. For the previously mentioned reasons, we require such metrics to be black-box and not rely on the execution and outputs of DNNs under test. To this end, we first select and adapt three diversity metrics and study, in a controlled manner, their capacity to measure actual diversity in input sets. We then analyze their statistical association with fault detection using four datasets and five DNNs. We further compare diversity with state-of-the-art white-box coverage criteria. As a mechanism to enable such analysis, we also propose a novel way to estimate fault detection in DNNs. Our experiments show that relying on the diversity of image features embedded in test input sets is a more reliable indicator than coverage criteria to effectively guide DNN testing. Indeed, we found that one of our selected black-box diversity metrics far outperforms existing coverage criteria in terms of fault-revealing capability and computational time. Results also confirm the suspicions that state-of-the-art coverage criteria are not adequate to guide the construction of test input sets to detect as many faults as possible using natural inputs.","Coverage,deep neural network,diversity,faults,test",基于测试用例多样性的深度神经网络黑盒测试,深度神经网络（DNN）已被广泛应用于图像处理、医学诊断和自动驾驶等领域。然而，DNN可能表现出错误行为，可能导致关键错误，尤其是在安全关键系统中使用时。受传统软件系统测试技术的启发，研究人员提出了神经元覆盖标准，作为对源代码覆盖的类比，以指导DNN的测试。尽管对DNN覆盖率进行了非常积极的研究，但最近的几项研究对此类标准在指导DNN测试方面的有用性提出了质疑。此外，从实践的角度来看，这些标准是白盒的，因为它们需要访问DNN的内部或训练数据，这通常是不可行或不方便的。测量这种覆盖率需要使用候选输入执行DNN来指导测试，这在许多实际情况下不是一种选择。在本文中，我们研究了作为白盒覆盖标准的替代方案的多样性度量。由于前面提到的原因，我们要求这些指标是黑盒的，而不是依赖于测试中DNN的执行和输出。为此，我们首先选择并调整三种多样性指标，并以可控的方式研究它们测量输入集中实际多样性的能力。然后，我们使用四个数据集和五个DNN来分析它们与故障检测的统计关联。我们进一步将多样性与最先进的白盒覆盖标准进行了比较。作为实现这种分析的机制，我们还提出了一种新的方法来估计DNN中的故障检测。我们的实验表明，依靠嵌入测试输入集中的图像特征的多样性是一个比覆盖标准更可靠的指标，可以有效地指导DNN测试。事实上，我们发现，我们选择的一个黑匣子多样性指标在故障揭示能力和计算时间方面远远优于现有的覆盖标准。结果还证实了人们的怀疑，即最先进的覆盖标准不足以指导测试输入集的构建，从而使用自然输入检测尽可能多的故障。,覆盖范围，深度神经网络，多样性，故障，测试,,,
FU5L5EJY,2023,https://doi.org/10.1109/TSE.2022.3212635,TSE 2023,SynShine: Improved Fixing of Syntax Errors,"Novice programmers struggle with the complex syntax of modern programming languages like Java, and make lot of syntax errors. The diagnostic syntax error messages from compilers and IDEs are sometimes useful, but often the messages are cryptic and puzzling. Novices could be helped, and instructors’ time saved, by automated repair suggestions when dealing with syntax errors. Large samples of novice errors and fixes are now available, offering the possibility of data-driven machine-learning approaches to help novices fix syntax errors. Current machine-learning approaches do a reasonable job fixing syntax errors in shorter programs, but don't work as well even for moderately longer programs. We introduce SynShine, a machine-learning based tool that substantially improves on the state-of-the-art, by learning to use compiler diagnostics, employing a very large neural model that leverages unsupervised pre-training, and relying on multi-label classification rather than autoregressive synthesis to generate the (repaired) output. We describe SynShine's architecture in detail, and provide a detailed evaluation. We have built SynShine into a free, open-source version of Visual Studio Code (VSCode); we make all our source code and models freely available.","Deep learning,program repair,naturalness",SynShine:改进的语法错误修复,新手程序员很难处理像Java这样的现代编程语言的复杂语法，并会犯很多语法错误。来自编译器和IDE的诊断语法错误消息有时是有用的，但这些消息往往是神秘和令人费解的。在处理语法错误时，自动修复建议可以帮助新手，并节省教师的时间。现在有大量的新手错误和修复样本，提供了数据驱动的机器学习方法来帮助新手修复语法错误的可能性。当前的机器学习方法在修复较短程序中的语法错误方面做得很好，但即使对于中等长度的程序也不能很好地工作。我们介绍了SynShine，这是一种基于机器学习的工具，通过学习使用编译器诊断，使用利用无监督预训练的非常大的神经模型，并依靠多标签分类而不是自回归合成来生成（修复的）输出，大大改进了最先进的工具。我们详细描述了SynShine的架构，并提供了详细的评估。我们已经将SynShine构建成一个免费的、开源的Visual Studio代码（VSCode）版本；我们免费提供所有的源代码和模型。,深度学习，程序修复，自然,,,
H6LYCJTV,2023,https://doi.org/10.1109/TSE.2022.3188898,TSE 2023,VID2XML: Automatic Extraction of a Complete XML Data From Mobile Programming Screencasts,"Developers often refer to video-hosting online platforms to find screencasts that provide a step-by-step guide to help them solve a programming task at hand or learn a new concept. More specifically, developers search for resources that help them design and implement effective mobile graphical user interfaces (GUI) using XML. Although mobile programming screencasts contain a vast amount of XML data at developers’ disposal, they cannot be easily found and copied-pasted due to the image nature of videos. Given that the most common task developers perform online is copy-pasting, mobile programming screencasts must support that and be complemented with XML data in a textual format. To overcome this challenge and aid developers, this paper presents vid2XML, which is a three-phase approach that leverages both visual and textual information of video frames to locate XML region in video frames, locate the currently opened file, and extract XML data for each file presented in video frames. We evaluated each phase of vid2XML in a comprehensive empirical evaluation on videos collected from YouTube. The results reveal that vid2XML is able to accurately (i) locate XML regions, outperforming four previous work, (ii) locate the bounding box of the selected file, and (iii) extract, fix, and merge XML data for each file opened/created in a video.","Programming screencasts,computer vision,Android programming,programming video tutorials,video mining",VID2XML：从移动编程屏幕广播中自动提取完整的XML数据,开发人员经常参考视频托管在线平台，寻找提供分步指南的屏幕广播，帮助他们解决手头的编程任务或学习新概念。更具体地说，开发人员搜索帮助他们使用XML设计和实现有效的移动图形用户界面（GUI）的资源。尽管移动编程屏幕截图包含大量可供开发人员使用的XML数据，但由于视频的图像性质，这些数据无法轻易找到并复制粘贴。考虑到开发人员在线执行的最常见任务是复制粘贴，移动编程屏幕广播必须支持这一点，并用文本格式的XML数据进行补充。为了克服这一挑战并帮助开发人员，本文提出了vid2XML，这是一种分三阶段的方法，它利用视频帧的视觉和文本信息来定位视频帧中的XML区域，定位当前打开的文件，并为视频帧中呈现的每个文件提取XML数据。我们对从YouTube上收集的视频进行了全面的实证评估，评估了vid2XML的每个阶段。结果表明，vid2XML能够准确地（i）定位XML区域，优于之前的四项工作，（ii）定位所选文件的边界框，以及（iii）提取、修复和合并视频中打开/创建的每个文件的XML数据。,编程屏幕广播，计算机视觉，安卓编程，编程视频教程，视频挖掘,,,
4G5TIKBW,2023,https://doi.org/10.1109/TSE.2022.3214764,TSE 2023,Managing Technical Debt Using Intelligent Techniques - A Systematic Mapping Study,"Technical Debt (TD) is a metaphor reflecting technical compromises that can yield short-term benefits but might hurt the long-term health of a software system. With the increasing amount of data generated when performing software development activities, an emergent research field has gained attention: applying Intelligent Techniques to solve Software Engineering problems. Intelligent Techniques were used to explore data for knowledge discovery, reasoning, learning, planning, perception, or supporting decision-making. Although these techniques can be promising, there is no structured understanding related to their application to support Technical Debt Management (TDM) activities. Within this context, this study aims to investigate to what extent the literature has proposed and evaluated solutions based on Intelligent Techniques to support TDM activities. To this end, we performed a Systematic Mapping Study (SMS) to investigate to what extent the literature has proposed and evaluated solutions based on Intelligent Techniques to support TDM activities. In total, 150 primary studies were identified and analyzed, dated from 2012 to 2021. The results indicated a growing interest in applying Intelligent Techniques to support TDM activities, the most used: Machine Learning and Reasoning under uncertainty. Intelligent Techniques aimed to assist mainly TDM activities related to identification, measurement, and monitoring. Design TD, Code TD, and Architectural TD are the TD types in the spotlight. Most studies were categorized at automation levels 1 and 2, meaning that existing approaches still require substantial human intervention. Symbolists and Analogizers are levels of explanation presented by most Intelligent Techniques, implying that these solutions conclude a general truth after considering a sufficient number of particular cases. Moreover, we also cataloged the empirical research types, contributions, and validation strategies described in primary studies. Based on our findings, we argue that there is still room to improve the use of Intelligent Techniques to support TDM activities. The open issues that emerged from this study can represent future opportunities for practitioners and researchers.","Technical debt,intelligent techniques,technical debt management activities,systematic mapping study",利用智能技术管理技术债务——一项系统映射研究,技术债务（TD）是一个隐喻，反映了技术妥协，可以产生短期利益，但可能损害软件系统的长期健康。随着软件开发活动中产生的数据量的不断增加，一个新兴的研究领域引起了人们的关注：应用智能技术解决软件工程问题。智能技术用于探索知识发现、推理、学习、计划、感知或支持决策的数据。尽管这些技术可能很有前景，但对于它们在支持技术债务管理（TDM）活动方面的应用，还没有一个结构化的理解。在此背景下，本研究旨在调查文献在多大程度上提出并评估了基于智能技术的解决方案，以支持TDM活动。为此，我们进行了一项系统映射研究（SMS），以调查文献在多大程度上提出并评估了基于智能技术的解决方案，以支持TDM活动。2012年至2021年，共确定并分析了150项主要研究。研究结果表明，人们对应用智能技术支持TDM活动越来越感兴趣，其中最常用的是：不确定性下的机器学习和推理。智能技术旨在主要协助与识别、测量和监测相关的TDM活动。设计TD、规范TD和建筑TD是聚光灯下的TD类型。大多数研究被归类为自动化水平1和2，这意味着现有的方法仍然需要大量的人工干预。象征主义者和类比者是大多数智能技术所提供的解释水平，这意味着这些解决方案在考虑了足够多的特定情况后得出了一个普遍的真理。此外，我们还对初级研究中描述的实证研究类型、贡献和验证策略进行了编目。根据我们的研究结果，我们认为智能技术的使用仍有改进的空间，以支持TDM活动。这项研究中出现的悬而未决的问题可以代表从业者和研究人员未来的机会。,技术债务，智能技术，技术债务管理活动，系统制图研究,,,
UGEC67X3,2023,https://doi.org/10.1109/TSE.2022.3231242,TSE 2023,autoMPI: Automated Multiple Perspective Attack Investigation With Semantics Aware Execution Partitioning,"Multiple Perspective attack Investigation (MPI) is a technique to partition application dependencies based on high-level semantics. It facilitates provenance analysis by generating succinct causal graphs. It involves an annotation process that identifies variables and data structures corresponding to the partitions and the communication channels between them. Though the amount of annotation is small, this process requires a detailed understanding of the source code. In this work, autoMPI, we extend the capability of MPI by automating the identifying annotation requirements. We leverage a hybrid analysis approach, performing a differential analysis based on crafted inputs. Static analysis is conducted to identify the annotation sites within the application code afterward automatically. Our evaluation shows the proposed approach can significantly facilitate the annotation process. It correctly identifies all required annotation sites within an average 16 seconds analysis time for the majority of analyzed programs with average precision and recall 72.5% and 100%, respectively.","Annotation,dynamic analysis,provenance,static analysis",autoMPI：具有语义感知执行分区的自动多视角攻击调查,多视角攻击调查（MPI）是一种基于高级语义划分应用程序依赖关系的技术。它通过生成简洁的因果图来促进出处分析。它涉及一个注释过程，该过程识别与分区相对应的变量和数据结构以及它们之间的通信通道。尽管注释的数量很少，但这个过程需要对源代码有详细的了解。在这项名为autoMPI的工作中，我们通过自动识别注释需求来扩展MPI的功能。我们采用混合分析方法，根据精心编制的输入进行差异分析。之后进行静态分析以自动识别应用程序代码中的注释站点。我们的评估表明，所提出的方法可以显著地促进注释过程。对于大多数分析程序，它在平均16秒的分析时间内正确识别了所有所需的注释位置，平均精度和召回率分别为72.5%和100%。,注释，动态分析，出处，静态分析,,,
DIS5GG8Q,2023,https://doi.org/10.1109/TSE.2022.3179294,TSE 2023,IoTCom: Dissecting Interaction Threats in IoT Systems,"Due to the growing presence of Internet of Things (IoT) apps and devices in smart homes and smart cities, there are more and more concerns about their security and privacy risks. IoT apps normally interact with each other and the physical world to offer utility to the users. In this paper, we investigate the safety and security risks brought by the interactive behaviors of IoT apps. Two major challenges ensue in identifying the interaction threats: i) how to discover the threats across both cyber and physical channels; and ii) how to ensure the scalability of the detection approach. To address these challenges, we first provide a taxonomy of interaction threats between IoT apps, which contains seven classes of coordination threats categorized based on their interaction behaviors. Then, we present IoTCom, a compositional threat detection system capable of automatically detecting and verifying unsafe interactions between IoT apps and devices. IoTCom applies static analysis to automatically infer relevant apps’ behaviors, and uses a novel strategy to trim the extracted app's behaviors prior to translating them into analyzable formal specifications, mitigating the state explosion associated with formal analysis. Our experiments with numerous bundles of real-world IoT apps have corroborated IoTCom's ability to effectively identify a broad spectrum of interaction threats triggered through cyber and physical channels, many of which were previously unknown. Finally, IoTCom uses an automatic verifier to validate the discovered threats. Our experimental results show that IoTCom significantly outperforms the existing techniques in terms of the computational time, and maintains the capability to perform its analysis across different IoT platforms.","Interaction threats,IoT safety,formal verification",IoTCom:剖析物联网系统中的交互威胁,由于物联网（IoT）应用程序和设备在智能家居和智能城市中的存在越来越多，人们越来越担心其安全和隐私风险。物联网应用程序通常相互交互，并与物理世界交互，为用户提供实用性。在本文中，我们调查了物联网应用程序的交互行为带来的安全和安保风险。识别交互威胁面临两大挑战：一）如何通过网络和物理渠道发现威胁；以及ii）如何确保检测方法的可扩展性。为了应对这些挑战，我们首先提供了物联网应用程序之间交互威胁的分类，其中包含七类基于交互行为分类的协调威胁。然后，我们介绍了IoTCom，这是一个组合威胁检测系统，能够自动检测和验证物联网应用程序和设备之间的不安全交互。IoTCom应用静态分析来自动推断相关应用程序的行为，并使用一种新颖的策略在将提取的应用程序行为转换为可分析的形式规范之前对其进行修剪，从而缓解与形式分析相关的状态爆炸。我们对大量现实世界物联网应用程序的实验证实了IoTCom有效识别通过网络和物理渠道引发的广泛交互威胁的能力，其中许多以前是未知的。最后，IoTCom使用一个自动验证器来验证发现的威胁。我们的实验结果表明，IoTCom在计算时间方面显著优于现有技术，并保持了在不同物联网平台上进行分析的能力。,交互威胁，物联网安全，正式验证,,,
CDSKGRCV,2023,https://doi.org/10.1109/TSE.2023.3276780,TSE 2023,Graph-of-Code: Semantic Clone Detection Using Graph Fingerprints,"The code clone detection issue has been researched using a number of explicit factors based on the tokens and contents and found effective results. However, exposing code contents may be an impractical option because of privacy and security factors. Moreover, the lack of scalability of past methods is an important challenge. The code flow states can be inferred by code structure and implicitly represented using empirical graphs. The assumption is that modelling of the code clone detection problem can be achieved without the content of the codes being revealed. Here, a Graph-of-Code concept for the code clone detection problem is introduced, which represents codes into graphs. While Graph-of-Code provides structural properties and quantification of its characteristics, it can exclude code contents or tokens to identify the clone type. The aim is to evaluate the impact of graph-of-code structural properties on the performance of code clone detection. This work employs a feature extraction-based approach for unlabelled graphs. The approach generates a “Graph Fingerprint” which represents different topological feature levels. The results of code clone detection indicate that code structure has a significant role in detecting clone types. We found different GoC-models outperform others. The models achieve between 96% to 99% in detecting code clones based on recall, precision, and F1-Score. The GoC approach is capable in detecting code clones with scalable dataset and with preserving codes privacy.","Code clones,software clones,semantic clones,graph-of-code,graph properties",代码图：使用图指纹的语义克隆检测,基于令牌和内容，使用许多显式因素对代码克隆检测问题进行了研究，并找到了有效的结果。然而，由于隐私和安全因素，公开代码内容可能是不切实际的选择。此外，过去的方法缺乏可扩展性也是一个重要的挑战。代码流状态可以通过代码结构来推断，并使用经验图来隐式表示。假设可以在不暴露代码内容的情况下实现代码克隆检测问题的建模。这里，介绍了一个用于代码克隆检测问题的代码图概念，它将代码表示为图。虽然“代码图”提供了其特征的结构属性和量化，但它可以排除代码内容或标记来识别克隆类型。目的是评估代码结构特性图对代码克隆检测性能的影响。这项工作采用了一种基于特征提取的方法来处理未标记图。该方法生成表示不同拓扑特征级别的“图形指纹”。代码克隆检测结果表明，代码结构在检测克隆类型方面具有重要作用。我们发现不同的GoC模型优于其他模型。基于召回率、精确度和F1分数，这些模型在检测代码克隆方面达到了96%至99%。GoC方法能够检测具有可扩展数据集的代码克隆，并保护代码隐私。,代码克隆，软件克隆，语义克隆，代码图，图属性,,,
3XFI4XMK,2023,https://doi.org/10.1109/TSE.2023.3314410,TSE 2023,Let's Go to the Whiteboard (Again): Perceptions From Software Architects on Whiteboard Architecture Meetings,"The whiteboard plays a crucial role in the day-to-day lives of software architects, as they frequently will organize meetings at the whiteboard to discuss a new architecture, some proposed changes to an existing architecture, a mismatch between a prescribed architecture and its code, and more. While much has been studied about software architects, the architectures they produce, and how they produce them, a detailed understanding of these whiteboards meetings is still lacking. In this paper, we contribute a mixed-methods study involving semi-structured interviews and a subsequent survey to understand the perceptions of software architects on whiteboard architecture meetings. We focus on four aspects: (1) why do they hold these meetings, (2) what is the impact of the experience levels of the participants in these meetings, (3) how do the architects document the meetings, and (4) what kinds of changes are made in downstream activities to the work produced after the meetings have concluded? In studying these aspects, we identify eleven observations related to both technical aspects and social aspects of the meetings. These insights have implications for further research, offer concrete advice to practitioners, and suggest ways of educating future software architects.","Software architecture,software architects,whiteboard meetings,architecture documentation,interviews,survey",让我们再次进入白板：软件架构师对白板架构会议的看法,白板在软件架构师的日常生活中发挥着至关重要的作用，因为他们经常在白板上组织会议，讨论新的体系结构、对现有体系结构的一些拟议更改、指定体系结构与其代码之间的不匹配等。尽管已经对软件架构师、他们产生的体系结构以及他们如何产生这些体系结构进行了大量研究，但对这些白板会议的详细了解仍然缺乏。在本文中，我们提出了一项混合方法研究，包括半结构化访谈和随后的调查，以了解软件架构师对白板架构会议的看法。我们关注四个方面：（1）他们为什么要举行这些会议，（2）这些会议参与者的经验水平有什么影响，（3）架构师如何记录这些会议，以及（4）会议结束后，下游活动对产生的工作做出了什么样的改变？在研究这些方面时，我们确定了与会议的技术方面和社会方面有关的11项意见。这些见解对进一步的研究具有启示意义，为从业者提供了具体的建议，并提出了教育未来软件架构师的方法。,软件体系结构，软件架构师，白板会议，体系结构文档，访谈，调查,,,
S4T44RZ4,2023,https://doi.org/10.1109/TSE.2022.3156182,TSE 2023,Using Metamorphic Testing to Improve the Quality of Tags in OpenStreetMap,"We present a metamorphic testing approach to validate the information included in OpenStreetMap, a collaborative effort to produce a free map of the world. We focus on the quality of the tags storing the information about the elements of the map. We identified metamorphic relations with the potential to detect different types of tagging errors. In particular, we carefully designed mechanisms to automatically generate follow-up inputs, a fundamental component in the successful application of a metamorphic testing approach. The intrinsic nature of automatically analysing tags implies that we will detect real errors but some false positives as well. In order to obtain a good trade-off between real errors and false positives, we introduce thresholds. Our MRs will raise an error associated with a certain value if, depending on the nature of the MR, we have a certain number of elements (not) fulfilling a given condition. In order to evaluate the goodness and versatility of our framework, we chose four cities in different continents with the goal of analysing very heterogeneous contributors adding information in different languages. The application of this framework to the analysis of the chosen cities revealed errors in all of them and in all the considered categories. In addition, around 66% of the errors found by our MRs in the analysed areas have not been previously reported by Osmose, the de facto standard OSM error checker.","Metamorphic testing,OpenStreetMap,quality of maps",使用变形测试提高OpenStreetMap中标签的质量,我们提出了一种变形测试方法来验证OpenStreetMap中包含的信息，这是一种制作免费世界地图的合作努力。我们关注的是存储地图元素信息的标签的质量。我们确定了具有检测不同类型标签错误潜力的变质关系。特别是，我们精心设计了自动生成后续输入的机制，这是成功应用变形测试方法的基本组成部分。自动分析标签的本质意味着我们会检测到真正的错误，但也会检测到一些误报。为了在真实错误和误报之间获得良好的权衡，我们引入了阈值。如果根据MR的性质，我们有一定数量的元素（不）满足给定条件，我们的MR将产生与某个值相关的错误。为了评估我们的框架的优点和多功能性，我们选择了不同大陆的四个城市，目的是分析以不同语言添加信息的异质贡献者。将这一框架应用于所选城市的分析表明，所有城市和所有考虑的类别都存在错误。此外，我们的MR在分析区域发现的大约66%的错误以前没有被事实上的标准OSM错误检查器Osmose报告过。,变形测试，OpenStreetMap，地图质量,,,
RQH355ZM,2023,https://doi.org/10.1109/TSE.2022.3150618,TSE 2023,ARTE: Automated Generation of Realistic Test Inputs for Web APIs,"Automated test case generation for web APIs is a thriving research topic, where test cases are frequently derived from the API specification. However, this process is only partially automated since testers are usually obliged to manually set meaningful valid test inputs for each input parameter. In this article, we present ARTE, an approach for the automated extraction of realistic test data for web APIs from knowledge bases like DBpedia. Specifically, ARTE leverages the specification of the API parameters to automatically search for realistic test inputs using natural language processing, search-based, and knowledge extraction techniques. ARTE has been integrated into RESTest, an open-source testing framework for RESTful APIs, fully automating the test case generation process. Evaluation results on 140 operations from 48 real-world web APIs show that ARTE can efficiently generate realistic test inputs for 64.9% of the target parameters, outperforming the state-of-the-art approach SAIGEN (31.8%). More importantly, ARTE supported the generation of over twice as many valid API calls (57.3%) as random generation (20%) and SAIGEN (26%), leading to a higher failure detection capability and uncovering several real-world bugs. These results show the potential of ARTE for enhancing existing web API testing tools, achieving an unprecedented level of automation.","Test data generation,automated testing,web APIs,web of data",ARTE：Web API真实测试输入的自动生成,web API的自动测试用例生成是一个蓬勃发展的研究课题，其中测试用例通常来源于API规范。然而，这个过程只是部分自动化的，因为测试人员通常必须为每个输入参数手动设置有意义的有效测试输入。在本文中，我们介绍了ARTE，这是一种从DBpedia等知识库中自动提取web API真实测试数据的方法。具体而言，ARTE利用API参数的规范，使用自然语言处理、基于搜索和知识提取技术自动搜索真实的测试输入。ARTE已经集成到RESTest中，RESTest是RESTful API的开源测试框架，完全自动化了测试用例生成过程。对48个真实世界web API的140个操作的评估结果显示，ARTE可以有效地为64.9%的目标参数生成真实的测试输入，优于最先进的方法SAIGEN（31.8%）。更重要的是，ARTE支持生成的有效API调用（57.3%）是随机生成（20%）和SAIGEN（26%）的两倍多，导致更高的故障检测能力，并发现了几个真实世界中的错误。这些结果显示了ARTE增强现有web API测试工具的潜力，实现了前所未有的自动化水平。,测试数据生成，自动化测试，web API，数据web,,,
8SXMCDI8,2023,https://doi.org/10.1109/TSE.2022.3166626,TSE 2023,Capabilities and Practices in DevOps: A Multivocal Literature Review,"Context. To meet the demands of customers and market, Information Technology (IT) organizations are seeking to implement DevOps. While many succeed in DevOps adoption, others lack the knowledge on how to incorporate DevOps culture, process, measurements, and techniques in their business. Thus, successful adoption is still inconsistent, highlighting the need to provide management with relevant information to support the development of DevOps Capabilities effectively. But what are these Capabilities? Unfortunately, there is still a lack of clarity about DevOps Capabilities and their relationships to DevOps Practices and Outcomes among researchers and practitioners. Objective. This research aims to gather community consensus on the relationship between Capabilities and Practices, so a better DevOps implementation can be mapped. Seeking to define DevOps Capabilities and Practices concepts and to identify, organize and summarize Capabilities as they relate to Practices. Method. A Multivocal Literature Review (MLR) is conducted, with 93 documents gathered and thoroughly examined from throughout the community, including books, scientific articles, white papers, and conferences, among others. Results. This survey contributes a list of 37 organized Capabilities, their mentions in literature, and their definitions. The concepts of Practices and Capabilities were mapped and categorized in an ordered taxonomy. It is concluded that industry research has much outweighed scientific research on this topic, with Capabilities evolving dynamically over time, reinforcing team collaboration and communication as the most crucial one. The study's Outcomes will assist researchers and practitioners understand how Capabilities and Practices are related at different levels and how to better implement them.","DevOps capabilities,DevOps practices,software engineering process,software release management and delivery,software development,multivocal literature review",DevOps的能力和实践：多语言文献综述,上下文为了满足客户和市场的需求，信息技术组织正在寻求实施DevOps。虽然许多人成功地采用了DevOps，但其他人缺乏如何将DevOps文化、流程、测量和技术融入业务的知识。因此，成功采用仍然不一致，突出了向管理层提供相关信息以有效支持DevOps能力开发的必要性。但这些能力是什么？不幸的是，研究人员和从业者对DevOps能力及其与DevOps实践和结果的关系仍然缺乏明确性。客观的本研究旨在收集社区对能力和实践之间关系的共识，从而制定更好的DevOps实施方案。寻求定义DevOps能力和实践概念，并识别、组织和总结与实践相关的能力。方法进行了一次多语言文献综述（MLR），从整个社区收集并彻底审查了93份文件，包括书籍、科学文章、白皮书和会议等。后果这项调查提供了一份37种组织能力的列表，它们在文献中被提及，以及它们的定义。实践和能力的概念被映射并分类在有序的分类法中。结论是，在这个主题上，行业研究远远超过了科学研究，能力随着时间的推移而动态发展，加强团队协作和沟通是最关键的。该研究的成果将帮助研究人员和从业者了解能力和实践在不同层面上的关系，以及如何更好地实施它们。,DevOps能力，DevOps实践，软件工程流程，软件发布管理和交付，软件开发，多语言文献综述,,,
YEJD37XK,2023,https://doi.org/10.1109/TSE.2022.3210076,TSE 2023,Effect of Requirements Analyst Experience on Elicitation Effectiveness: A Family of Quasi-Experiments,"Context. In software engineering there is a widespread assumption that experience improves requirements analyst effectiveness, although empirical studies demonstrate the opposite. Aim. Determine whether experience (interviews, eliciting, development, professional) influences requirements elicitation using interviews. Method. We ran 12 quasi-experiments recruiting 124 subjects in which we measured analyst effectiveness as the number of items (i.e., concepts, rules, processes) correctly elicited. The experimental task was to elicit requirements using the open interview technique followed by the consolidation of the elicited information in domains with which the analysts were and were not familiar. Results. In unfamiliar domains, interview experience, requirements experience, development experience, and professional experience does not have any relationship with analyst effectiveness. In familiar domains, effectiveness varies depending on the type of experience. Interview experience has a positive effect, whereas professional experience has a moderate negative effect. Requirements experience appears to have a moderately positive effect; however, the statistical power of the analysis is insufficient to be able to confirm this point. Development experience has no effect. Conclusion. Experience impacts analyst effectiveness differently depending on the problem domain type (familiar, unfamiliar). Generally, experience does not account for all the observed variability in effectiveness, so there are other influential factors.","Elicitation,requirements analyst,experience,effectiveness,problem domain,quasi-experiment",需求分析师经验对启发有效性的影响：一组准实验,上下文在软件工程中，人们普遍认为经验可以提高需求分析师的有效性，尽管实证研究表明情况恰恰相反。目标通过访谈确定经验（面试、启发、发展、专业）是否会影响需求启发。方法我们进行了12个准实验，招募了124名受试者，在这些实验中，我们测量了分析师的有效性，即正确引出的项目数量（即概念、规则、过程）。实验任务是使用开放式访谈技术引出需求，然后将引出的信息整合到分析师熟悉和不熟悉的领域中。后果在不熟悉的领域，面试经验、需求经验、开发经验和专业经验与分析师的有效性没有任何关系。在熟悉的领域，有效性因经验类型而异。面试经历具有积极影响，而职业经历具有适度的负面影响。需求经验似乎具有适度的积极影响；然而，分析的统计能力不足以证实这一点。发展经验没有影响。结论经验对分析师有效性的影响因问题领域类型（熟悉、不熟悉）而异。一般来说，经验并不能解释观察到的所有有效性变化，因此还有其他影响因素。,启发，需求分析师，经验，有效性，问题领域，准实验,,,
6WHJRLRS,2023,https://doi.org/10.1109/TSE.2022.3147975,TSE 2023,Learning How to Listen: Automatically Finding Bug Patterns in Event-Driven JavaScript APIs,"Event-driven programming is widely practiced in the JavaScript community, both on the client side to handle UI events and AJAX requests, and on the server side to accommodate long-running operations such as file or network I/O. Many popular event-based APIs allow event names to be specified as free-form strings without any validation, potentially leading to lost events for which no listener has been registered and dead listeners for events that are never emitted. In previous work, Madsen et al. presented a precise static analysis for detecting such problems, but their analysis does not scale because it may require a number of contexts that is exponential in the size of the program. Concentrating on the problem of detecting dead listeners, we present an approach to learn how to use event-based APIs by first mining a large corpus of JavaScript code using a simple static analysis to identify code snippets that register an event listener, and then applying statistical modeling to identify anomalous patterns, which often indicate incorrect API usage. In a large-scale evaluation on 127,531 open-source JavaScript code bases, our technique was able to detect 75 anomalous listener-registration patterns, while maintaining a precision of 90.9% and recall of 7.5% over a validation set, demonstrating that a learning-based approach to detecting event-handling bug patterns is feasible. In an additional experiment, we investigated instances of these patterns in 25 open-source projects, and reported 30 issues to the project maintainers, of which 7 have been confirmed as bugs.","Static analysis,JavaScript,event-driven programming,bug finding,API modeling",学习如何倾听：在事件驱动的JavaScript API中自动查找错误模式,事件驱动编程在JavaScript社区中得到了广泛的实践，既在客户端处理UI事件和AJAX请求，也在服务器端处理文件或网络I/O等长时间运行的操作。许多流行的基于事件的API允许在没有任何验证的情况下将事件名称指定为自由格式字符串，这可能会导致没有注册侦听器的事件丢失，以及从未发出的事件的侦听器失效。在之前的工作中，Madsen等人提出了一种精确的静态分析方法来检测此类问题，但他们的分析没有规模，因为它可能需要许多在程序规模上呈指数级的上下文。针对检测死侦听器的问题，我们提出了一种学习如何使用基于事件的API的方法，方法是首先使用简单的静态分析来挖掘大量JavaScript代码，以识别注册事件侦听器的代码片段，然后应用统计建模来识别异常模式，这些模式通常表明API使用不正确。在对127531个开源JavaScript代码库的大规模评估中，我们的技术能够检测到75个异常侦听器注册模式，同时在验证集中保持90.9%的精度和7.5%的召回率，这表明基于学习的方法检测事件处理错误模式是可行的。在另一个实验中，我们调查了25个开源项目中这些模式的实例，并向项目维护人员报告了30个问题，其中7个已被确认为错误。,静态分析，JavaScript，事件驱动编程，bug查找，API建模,,,
5IUZNF9W,2023,https://doi.org/10.1109/TSE.2022.3173678,TSE 2023,A Data Transfer and Relevant Metrics Matching Based Approach for Heterogeneous Defect Prediction,"Heterogeneous defect prediction (HDP) is a promising research area in the software defect prediction domain to handle the unavailability of the past homogeneous data. In HDP, the prediction is performed using source dataset in which the independent features (metrics) are entirely different than the independent features of target dataset. One important assumption in machine learning is that independent features of the source and target datasets should be relevant to each other for better prediction accuracy. However, these assumptions do not generally hold in HDP. Further in HDP, the selected source dataset for a given target dataset may be of small size causing insufficient training. To resolve these issues, we have proposed a novel heterogeneous data preprocessing method, namely, Transfer of Data from Target dataset to Source dataset selected using Relevance score (TDTSR), for heterogeneous defect prediction. In the proposed approach, we have used chi-square test to select the relevant metrics between source and target datasets and have performed experiments using proposed approach with various machine learning algorithms. Our proposed method shows an improvement of at least 14% in terms of AUC score in the HDP scenario compared to the existing state of the art models.","Heterogeneous defect prediction,heterogeneous metrics,chi square test,random forest,relevant metrics",一种基于数据传输和相关度量匹配的异构缺陷预测方法,异构缺陷预测（HDP）是软件缺陷预测领域中一个很有前途的研究领域，用于处理过去同质数据的不可用性。在HDP中，使用源数据集进行预测，其中独立特征（度量）与目标数据集的独立特征完全不同。机器学习中的一个重要假设是，源数据集和目标数据集的独立特征应该彼此相关，以获得更好的预测精度。然而，这些假设通常不适用于HDP。此外，在HDP中，为给定目标数据集选择的源数据集可能是小尺寸的，导致训练不足。为了解决这些问题，我们提出了一种新的异构数据预处理方法，即将数据从目标数据集转移到使用相关性评分（TDTSR）选择的源数据集，用于异构缺陷预测。在提出的方法中，我们使用卡方检验来选择源数据集和目标数据集之间的相关度量，并使用提出的方法和各种机器学习算法进行了实验。我们提出的方法显示，与现有的现有技术模型相比，在HDP场景中，AUC得分至少提高了14%。,异构缺陷预测，异构度量，卡方检验，随机森林，相关度量,,,
VB7Z8R37,2023,https://doi.org/10.1109/TSE.2023.3279774,TSE 2023,Function Call Graph Context Encoding for Neural Source Code Summarization,"Source code summarization is the task of writing natural language descriptions of source code. The primary use of these descriptions is in documentation for programmers. Automatic generation of these descriptions is a high value research target due to the time cost to programmers of writing these descriptions themselves. In recent years, a confluence of software engineering and artificial intelligence research has made inroads into automatic source code summarization through applications of neural models of that source code. However, an Achilles’ heel to a vast majority of approaches is that they tend to rely solely on the context provided by the source code being summarized. But empirical studies in program comprehension are quite clear that the information needed to describe code much more often resides in the context in the form of Function Call Graph surrounding that code. In this paper, we present a technique for encoding this call graph context for neural models of code summarization. We implement our approach as a supplement to existing approaches, and show statistically significant improvement over existing approaches. In a human study with 20 programmers, we show that programmers perceive generated summaries to generally be as accurate, readable, and concise as human-written summaries.","Automatic documentation generation,context-aware models,neural networks,source code summarization",用于神经源代码摘要的函数调用图上下文编码,源代码摘要是编写源代码的自然语言描述的任务。这些描述的主要用途是在程序员的文档中。由于程序员自己编写这些描述的时间成本，这些描述的自动生成是一个高价值的研究目标。近年来，软件工程和人工智能研究的融合通过应用源代码的神经模型，在源代码自动摘要方面取得了进展。然而，绝大多数方法的致命弱点是，它们往往只依赖于所总结的源代码提供的上下文。但程序理解中的实证研究非常清楚，描述代码所需的信息通常以围绕代码的函数调用图的形式存在于上下文中。在本文中，我们提出了一种为代码摘要的神经模型编码这种调用图上下文的技术。我们实施我们的方法是对现有方法的补充，并在统计上显示出比现有方法的显著改进。在一项针对20名程序员的人类研究中，我们发现程序员认为生成的摘要通常与人类编写的摘要一样准确、可读和简洁。,自动文档生成，上下文感知模型，神经网络，源代码摘要,,,
4JRRLLTD,2023,https://doi.org/10.1109/TSE.2022.3208864,TSE 2023,Test Flakiness Across Programming Languages,"Regression Testing (RT) is a quality-assurance practice commonly adopted in the software industry to check if functionality remains intact after code changes. Test flakiness is a serious problem for RT. A test is said to be flaky when it non-deterministically passes or fails on a fixed environment. Prior work studied test flakiness primarily on Java programs. It is unclear, however, how problematic is test flakiness for software written in other programming languages. This paper reports on a study focusing on three central aspects of test flakiness: concentration, similarity, and cost. Considering concentration, our results show that, for any given programming language that we studied (C, Go, Java, JS, and Python), most issues could be explained by a small fraction of root causes (5/13 root causes cover 78.07% of the issues) and could be fixed by a relatively small fraction of fix strategies (10/23 fix strategies cover 85.20% of the issues). Considering similarity, although there were commonalities in root causes and fixes across languages (e.g., concurrency and async wait are common causes of flakiness in most languages), we also found important differences (e.g., flakiness due to improper release of resources are more common in C), suggesting that there is opportunity to fine tuning analysis tools. Considering cost, we found that issues related to flaky tests are resolved either very early once they are posted ($&lt; $10 days), suggesting relevance, or very late ($&gt;$100 days), suggesting irrelevance.","Regression testing,test flakiness,programming languages",跨程序设计语言测试片状,回归测试（RT）是软件行业通常采用的一种质量保证实践，用于检查代码更改后功能是否保持完整。测试片状是RT的一个严重问题。当测试在固定环境中非决定性地通过或失败时，称为片状。之前的工作主要研究Java程序的测试片状性。然而，目前还不清楚用其他编程语言编写的软件的测试片状性有多大问题。本文报告了一项研究，重点关注测试片状度的三个核心方面：浓度、相似性和成本。考虑到集中度，我们的结果表明，对于我们研究的任何给定编程语言（C、Go、Java、JS和Python），大多数问题都可以用一小部分根本原因来解释（5/13个根本原因覆盖了78.07%的问题），并且可以用相对较小的修复策略来解决（10/23个修复策略覆盖了85.20%的问题）。考虑到相似性，尽管不同语言在根本原因和修复方面存在共性（例如，并发和异步等待是大多数语言中片状的常见原因），但我们也发现了重要的差异（例如，由于资源释放不当导致的片状在C中更为常见），这表明有机会对分析工具进行微调。考虑到成本，我们发现，与片状测试相关的问题要么在发布后很早（$&lt；$10天）得到解决，表明相关，要么很晚（$&gt；$100天）解决，表明无关。,回归测试，测试片状性，编程语言,,,
XNQ5Z2ZZ,2023,https://doi.org/10.1109/TSE.2022.3222119,TSE 2023,How Much Does Software Complexity Matter for Maintenance Productivity? The Link Between Team Instability and Diversity,"Software complexity decreases maintenance productivity, as do team attributes of instability and knowledge diversity. We know little about the extent to which the two team attributes interact with software complexity and shape productivity across systems of varying complexity. We address this gap by investigating whether and to what degree software complexity moderates the effects of team instability and knowledge diversity on maintenance productivity over the life of a system. We posit, given the exponential growth of code and task dependencies inherent in complex software systems, that system-level complexity has a significant nonlinear amplifying effect on the adverse effects of the two team attributes. To validate the presence of such an effect, we conduct a robust split-sample econometric analysis using three years of maintenance data from 426 mission-critical systems of a Fortune 100 company. The sampled systems vary in size (50KLOC to 2000KLOC, where 20% exceed 500KLOC), with a considerable portion of the sample manifesting “high” to “very high” software complexity. The analysis corroborates the known adverse effects of team instability, team knowledge diversity, and software complexity on maintenance productivity. More importantly, it shows—as theorized—that the adverse effects of the team attributes on maintenance productivity are significantly amplified only when software complexity grows high. We conclude with practical and research implications about how to manage software teams maintaining complex software over the life of a system.","Interaction effects,high software complexity,productivity,software complexity,software maintenance,team instability,team knowledge diversity",软件复杂性对维护效率有多重要？团队不稳定性与多样性之间的联系,软件复杂性降低了维护生产力，团队的不稳定性和知识多样性也是如此。我们对这两个团队属性在多大程度上与软件复杂性相互作用以及在不同复杂性的系统中塑造生产力知之甚少。我们通过调查软件复杂性是否以及在多大程度上调节团队不稳定性和知识多样性对系统寿命内维护生产力的影响来解决这一差距。我们假设，考虑到复杂软件系统中固有的代码和任务依赖性的指数增长，系统级复杂性对两个团队属性的不利影响具有显著的非线性放大效应。为了验证这种影响的存在，我们使用《财富》100强公司426个关键任务系统的三年维护数据进行了稳健的分样本计量经济分析。采样系统的大小各不相同（50KLOC到2000KLOC，其中20%超过500KLOC），相当一部分样本显示出“高”到“非常高”的软件复杂性。该分析证实了团队不稳定性、团队知识多样性和软件复杂性对维护生产力的已知不利影响。更重要的是，它表明——正如理论上的那样——只有当软件复杂性增加时，团队属性对维护生产力的不利影响才会显著放大。最后，我们对如何管理在系统生命周期内维护复杂软件的软件团队进行了实践和研究。,交互效应，高软件复杂性，生产力，软件复杂性，软件维护，团队不稳定性，团队知识多样性,,,
HT7IES8X,2023,https://doi.org/10.1109/TSE.2022.3192279,TSE 2023,Translating Video Recordings of Complex Mobile App UI Gestures into Replayable Scenarios,"Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S+, an automated approach for translating video recordings of Android app usages into replayable scenarios. V2S+ is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user gestures captured in a video, and convert these into a replayable test scenario. Given that V2S+ takes a computer vision-based approach, it is applicable to both hybrid and native Android applications. We performed an extensive evaluation of V2S+ involving 243 videos depicting 4,028 GUI-based actions collected from users exercising features and reproducing bugs from a collection of over 90 popular native and hybrid Android apps. Our results illustrate that V2S+ can accurately replay scenarios from screen recordings, and is capable of reproducing $\approx$ 90.2% of sequential actions recorded in native application scenarios on physical devices, and $\approx$ 83% of sequential actions recorded in hybrid application scenarios on emulators, both with low overhead. A case study with three industrial partners illustrates the potential usefulness of V2S+ from the viewpoint of developers.","Bug reporting,screen recordings,object detection",将复杂移动应用程序UI手势的视频录制转换为可回放场景,移动应用程序的屏幕记录很容易获得并捕获与软件开发人员相关的丰富信息（例如，错误或功能请求），使其成为众包应用程序反馈的流行机制。因此，这些视频正在成为开发人员必须管理的常见工件。鉴于独特的移动开发限制，包括快速的发布周期和快速发展的平台，用于分析所有类型的丰富软件工件的自动化技术为移动开发人员提供了好处。不幸的是，与其他类型的（文本）工件相比，由于其图形性质，自动分析屏幕记录带来了严重的挑战。为了应对这些挑战，本文介绍了V2S+，这是一种将Android应用程序使用的视频录制转换为可回放场景的自动化方法。V2S+主要基于计算机视觉技术，并采用最新的对象检测和图像分类解决方案来检测和分类视频中捕捉的用户手势，并将其转换为可回放的测试场景。鉴于V2S+采用了基于计算机视觉的方法，它适用于混合和原生Android应用程序。我们对V2S+进行了广泛的评估，涉及243个视频，描述了4028个基于GUI的动作，这些动作是从用户那里收集的，这些用户使用了90多个流行的原生和混合Android应用程序中的功能并复制了错误。我们的结果表明，V2S+可以准确地从屏幕记录中回放场景，并且能够在物理设备上再现本机应用场景中记录的$\approxy$90.2%的顺序动作，以及在模拟器上再现混合应用场景中录制的$\pproxy$83%的顺序动作。三个行业合作伙伴的案例研究从开发者的角度说明了V2S+的潜在用途。,错误报告，屏幕记录，对象检测,,,
QMFRYVBA,2023,https://doi.org/10.1109/TSE.2022.3160873,TSE 2023,A Taxonomy of Inter-Team Coordination Mechanisms in Large-Scale Agile,"In large-scale agile software development, many teams work together to achieve overarching project goals. The more teams, the greater the coordination requirements. Despite the growing popularity of large-scale agile, inter-team coordination is challenging to practice and research. We conducted a case study over 1.5 years in a large-scale software development firm to better understand which inter-team coordination mechanisms are used in large-scale agile and how they support inter-team coordination. Based on a thematic analysis of 31 interviews, 113 hours of observations, and supplemental material, we identified 27 inter-team coordination mechanisms. From this, we offer the following contributions. First, we propose a taxonomy of inter-team coordination with three categories: coordination meetings, such as communities of practice, inter-team stand-ups, and retrospectives; coordination roles, such as the program architects and the platform team; and coordination tools and artefacts, such as Slack and JIRA as well as inter-team task boards, product backlogs, and roadmaps. Second, the coordination mechanisms displayed combinations of four key characteristics, technical, organizational, physical, and social (TOPS), which form the basis of the TOPS framework to capture the multifaceted characteristics of coordination mechanisms. Technical relates to the software product and/or technical tools supporting software development. Organizational pertains to the structural aspects of the organization. Physical refers to tangible or spatial characteristics. Social captures interpersonal and community-based characteristics. Finally, the taxonomy and the TOPS framework provide a knowledge base and a structured approach for researchers to study as well as for software practitioners to understand and improve inter-team coordination in large-scale agile.","Large-scale agile,agile software development,inter-team coordination,case study,taxonomy",大规模敏捷中团队间协调机制的分类,在大规模敏捷软件开发中，许多团队协同工作以实现总体项目目标。团队越多，协调要求就越高。尽管大规模敏捷越来越受欢迎，但团队间协调在实践和研究中具有挑战性。我们在一家大型软件开发公司进行了一项为期1.5年的案例研究，以更好地了解哪些团队间协调机制在大规模敏捷中使用，以及它们如何支持团队间协调。根据对31次访谈、113小时的观察和补充材料的专题分析，我们确定了27个团队间协调机制。由此，我们提供以下贡献。首先，我们提出了一个团队间协调的分类法，分为三类：协调会议，如实践社区、团队间单口相声和回顾；协调角色，例如程序架构师和平台团队；以及协调工具和人工制品，如Slack和JIRA，以及团队间任务委员会、产品积压和路线图。其次，协调机制表现出四个关键特征的组合，即技术、组织、物理和社会（TOPS），这构成了TOPS框架的基础，以捕捉协调机制的多方面特征。技术是指支持软件开发的软件产品和/或技术工具。组织与组织的结构方面有关。物理是指有形的或空间的特征。社交捕捉了人际关系和社区特征。最后，分类法和TOPS框架为研究人员提供了一个知识库和一种结构化的方法，也为软件从业者了解和改进大规模敏捷中的团队间协调提供了一种知识库和结构化的方法。,大规模敏捷，敏捷软件开发，团队间协调，案例研究，分类,,,
372R6DJU,2023,https://doi.org/10.1109/TSE.2023.3310793,TSE 2023,CombTransformers: Statement-Wise Transformers for Statement-Wise Representations,"This study presents a novel category of Transformer architectures known as comb transformers, which effectively reduce the space complexity of the self-attention layer from a quadratic to a subquadratic level. This is achieved by processing sequence segments independently and incorporating $\mathcal{X}$-word embeddings to merge cross-segment information. The reduction in attention memory requirements enables the deployment of deeper architectures, potentially leading to more competitive outcomes. Furthermore, we design an abstract syntax tree (AST)-based code representation to effectively exploit comb transformer properties. To explore the potential of our approach, we develop nine specific instances based on three popular architectural concepts: funnel, hourglass, and encoder-decoder. These architectures are subsequently trained on three code-related tasks: method name generation, code search, and code summarization. These tasks encompass a range of capabilities: short/long sequence generation and classification. In addition to the proposed comb transformers, we also evaluate several baseline architectures for comparative analysis. Our findings demonstrate that the comb transformers match the performance of the baselines and frequently perform better.","Programming languages,machine learning,learning representations,code search and summarization,method name Gen",CombTransformers：陈述式表达的陈述式变压器,本研究提出了一种新的变压器架构类别，称为梳状变压器，它有效地将自注意层的空间复杂性从二次型降低到次二次型。这是通过独立处理序列段并结合$\mathcal｛X｝$字嵌入来合并跨段信息来实现的。注意力记忆需求的减少使得能够部署更深层次的架构，从而可能带来更具竞争力的结果。此外，我们设计了一个基于抽象语法树（AST）的代码表示，以有效地利用梳状变换器的特性。为了探索我们方法的潜力，我们基于三个流行的体系结构概念开发了九个特定的实例：漏斗、沙漏和编码器-解码器。这些体系结构随后被训练用于三个与代码相关的任务：方法名称生成、代码搜索和代码摘要。这些任务包括一系列功能：短/长序列生成和分类。除了所提出的梳状变压器外，我们还评估了几种基线架构进行比较分析。我们的研究结果表明，梳状变换器与基线的性能相匹配，并且经常表现得更好。,编程语言，机器学习，学习表示，代码搜索和摘要，方法名称Gen,,,
THKIZ64N,2023,https://doi.org/10.1109/TSE.2023.3242588,TSE 2023,Trace Diagnostics for Signal-Based Temporal Properties,"Trace checking is a verification technique widely used in Cyber-physical system (CPS) development, to verify whether execution traces satisfy or violate properties expressing system requirements. Often these properties characterize complex signal behaviors and are defined using domain-specific languages, such as SB-TemPsy-DSL, a pattern-based specification language for signal-based temporal properties. Most of the trace-checking tools only yield a Boolean verdict. However, when a property is violated by a trace, engineers usually inspect the trace to understand the cause of the violation; such manual diagnostic is time-consuming and error-prone. Existing approaches that complement trace-checking tools with diagnostic capabilities either produce low-level explanations that are hardly comprehensible by engineers or do not support complex signal-based temporal properties. In this paper, we propose TD-SB-TemPsy, a trace-diagnostic approach for properties expressed using SB-TemPsy-DSL. Given a property and a trace that violates the property, TD-SB-TemPsy determines the root cause of the property violation. TD-SB-TemPsy relies on the concepts of violation cause, which characterizes one of the behaviors of the system that may lead to a property violation, and diagnoses, which are associated with violation causes and provide additional information to help engineers understand the violation cause. As part of TD-SB-TemPsy, we propose a language-agnostic methodology to define violation causes and diagnoses. In our context, its application resulted in a catalog of 34 violation causes, each associated with one diagnosis, tailored to properties expressed in SB-TemPsy-DSL. We assessed the applicability of TD-SB-TemPsy on two datasets, including one based on a complex industrial case study. The results show that TD-SB-TemPsy could finish within a timeout of 1 min for $\approx 83.66\%$ of the trace-property combinations in the industrial dataset, yielding a diagnosis in $\approx 99.84\%$ of these cases; moreover, it also yielded a diagnosis for all the trace-property combinations in the other dataset. These results suggest that our tool is applicable and efficient in most cases.","cyber-physical systems,diagnostics,run-time verification,signals,specification patterns,temporal properties,trace checking",基于信号时间特性的轨迹诊断,跟踪检查是一种在网络物理系统（CPS）开发中广泛使用的验证技术，用于验证执行跟踪是否满足或违反表示系统需求的属性。这些特性通常表征复杂的信号行为，并使用特定于领域的语言来定义，例如SB TemPsy DSL，这是一种用于基于信号的时间特性的基于模式的规范语言。大多数跟踪检查工具只产生布尔判定。然而，当财产被痕迹侵犯时，工程师通常会检查痕迹，以了解侵犯的原因；这种手动诊断耗时且容易出错。现有的方法用诊断功能来补充跟踪检查工具，要么产生工程师难以理解的低级解释，要么不支持复杂的基于信号的时间特性。在本文中，我们提出了TD SB TemPsy，这是一种使用SB TemPsy DSL表示的属性的跟踪诊断方法。给定一个属性和违反该属性的跟踪，TD SB TemPsy将确定该属性违反的根本原因。TD SB TemPsy依赖于违规原因和诊断的概念，违规原因是系统可能导致财产违规的行为之一，诊断与违规原因相关，并提供额外信息帮助工程师了解违规原因。作为TD SB TemPsy的一部分，我们提出了一种语言不可知的方法来定义违规原因和诊断。在我们的上下文中，它的应用导致了34个违规原因的目录，每个原因都与一个诊断相关，并根据SB TemPsy DSL中表达的属性进行了定制。我们在两个数据集上评估了TD SB TemPsy的适用性，其中一个数据集基于复杂的工业案例研究。结果表明，TD SB TemPsy可以在1分钟的超时内完成工业数据集中$\约83.66\%$的痕量性质组合，在$\约99.84\%$中产生诊断；此外，它还对其他数据集中的所有跟踪属性组合进行了诊断。这些结果表明，我们的工具在大多数情况下是适用的和有效的。,赛博物理系统，诊断，运行时验证，信号，规范模式，时间特性，跟踪检查,,,
B92YZHEL,2023,https://doi.org/10.1109/TSE.2022.3175789,TSE 2023,Towards Reliable Online Just-in-Time Software Defect Prediction,"Throughout its development period, a software project experiences different phases, comprises modules with different complexities and is touched by many different developers. Hence, it is natural that problems such as Just-in-Time Software Defect Prediction (JIT-SDP) are affected by changes in the defect generating process (concept drifts), potentially hindering predictive performance. JIT-SDP also suffers from delays in receiving the labels of training examples (verification latency), potentially exacerbating the challenges posed by concept drift and further hindering predictive performance. However, little is known about what types of concept drift affect JIT-SDP and how they affect JIT-SDP classifiers in view of verification latency. This work performs the first detailed analysis of that. Among others, it reveals that different types of concept drift together with verification latency significantly impair the stability of the predictive performance of existing JIT-SDP approaches, drastically affecting their reliability over time. Based on the findings, a new JIT-SDP approach is proposed, aimed at providing higher and more stable predictive performance (i.e., reliable) over time. Experiments based on ten GitHub open source projects show that our approach was capable of produce significantly more stable predictive performances in all investigated datasets while maintaining or improving the predictive performance obtained by state-of-art methods.","Just-in-time software defect prediction,online learning,concept drift,verification latency,class imbalance learning",面向可靠的在线实时软件缺陷预测,在整个开发过程中，一个软件项目经历了不同的阶段，包括具有不同复杂性的模块，并受到许多不同开发人员的影响。因此，诸如实时软件缺陷预测（JIT-SDP）之类的问题自然会受到缺陷生成过程的变化（概念漂移）的影响，这可能会阻碍预测性能。JIT-SDP在接收训练示例的标签（验证延迟）方面也存在延迟，这可能会加剧概念漂移带来的挑战，并进一步阻碍预测性能。然而，从验证延迟的角度来看，关于什么类型的概念漂移会影响JIT-SDP以及它们如何影响JIT-SDP-分类器，我们知之甚少。这项工作进行了第一次详细的分析。除其他外，它揭示了不同类型的概念漂移以及验证延迟显著损害了现有JIT-SDP方法预测性能的稳定性，随着时间的推移，极大地影响了它们的可靠性。基于这些发现，提出了一种新的JIT-SDP方法，旨在随着时间的推移提供更高、更稳定的预测性能（即可靠性）。基于十个GitHub开源项目的实验表明，我们的方法能够在所有研究的数据集中产生更稳定的预测性能，同时保持或提高通过现有方法获得的预测性能。,实时软件缺陷预测，在线学习，概念漂移，验证延迟，类不平衡学习,,,
NUWB3QFX,2023,https://doi.org/10.1109/TSE.2022.3191353,TSE 2023,Towards Better Dependency Management: A First Look at Dependency Smells in Python Projects,"Managing cross-project dependencies is tricky in modern software development. A primary way to manage dependencies is using dependency configuration files, which brings convenience to the entire software ecosystem, including developers, maintainers, and users. However, developers may introduce dependency smells if dependency configuration files are not well written and maintained. Dependency smells are recurring violations of dependency management in dependency configuration files and can potentially lead to severe consequences. This paper provides an in-depth look at three dependency smells, namely, Missing Dependency, Bloated Dependency, and Version Constraint Inconsistency in Python projects. First, we implement a tool called Python Cross-project Dependency- PyCD to accurately extract dependency information from configuration files. The evaluation result on 212 Python projects shows that PyCD outperforms state-of-the-art tools. Then, we make an empirical study for three dependency smells in 132 Python projects to investigate the pervasiveness, causes, and evolution. The results show that: 1) dependency smells are prevalent in Python projects and exist inconsistently in different projects; 2) dependency smells are introduced into Python projects for different reasons, mainly due to the problems of synchronous update and collaborative development; and 3) dependency smells can be removed with different patterns according to different dependency smells. Furthermore, we report and get responses for 40 harmful dependency smell instances, 34 of which have been responded that these dependency smells do exist in the projects, and 10 instances are fixed or under process. The feedback from developers indicates that dependency smells can have a negative impact on project maintenance. Our study highlights that these dependency smells deserve the attention of developers.","Cross-project dependency,dependency smell,python",走向更好的依赖管理：Python项目中的依赖气味,在现代软件开发中，管理跨项目依赖关系是很棘手的。管理依赖关系的主要方法是使用依赖关系配置文件，这为整个软件生态系统（包括开发人员、维护人员和用户）带来了便利。但是，如果依赖配置文件编写和维护不好，开发人员可能会引入依赖气味。依赖项气味是依赖项配置文件中重复出现的违反依赖项管理的行为，可能会导致严重后果。本文深入研究了Python项目中的三种依赖气味，即缺失依赖、膨胀依赖和版本约束不一致。首先，我们实现了一个名为PythonCros-projectDependency-PyCD的工具，以准确地从配置文件中提取依赖关系信息。对212个Python项目的评估结果表明，PyCD的性能优于最先进的工具。然后，我们对132个Python项目中的三种依赖气味进行了实证研究，以研究其普遍性、原因和进化。结果表明：1）依赖气味在Python项目中普遍存在，并且在不同的项目中存在不一致性；2） 依赖气味被引入Python项目的原因各不相同，主要是由于同步更新和协同开发的问题；以及3）可以根据不同的依赖气味以不同的模式去除依赖气味。此外，我们报告并获得了40个有害依赖气味实例的响应，其中34个实例已得到响应，这些依赖气味确实存在于项目中，10个实例已修复或正在处理中。来自开发人员的反馈表明，依赖气味可能会对项目维护产生负面影响。我们的研究强调，这些依赖气味值得开发人员注意。,跨项目依赖，依赖气味，python,,,
PWPQCJNJ,2023,https://doi.org/10.1109/TSE.2023.3267446,TSE 2023,MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation,"Large language models have demonstrated the ability to generate both natural language and programming language text. Although contemporary code generation models are trained on corpora with several programming languages, they are tested using benchmarks that are typically monolingual. The most widely used code generation benchmarks only target Python, so there is little quantitative evidence of how code generation models perform on other programming languages. We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages. We use MultiPL-E to extend the HumanEval benchmark (Chen et al., 2021) and MBPP benchmark (Austin et al., 2021) to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex (Chen et al., 2021), CodeGen (Nijkamp et al., 2022) and InCoder (Fried et al., 2022). We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance. Finally, the MultiPL-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.","B.2.3 reliability, testing, and fault-tolerance,I.5.1.D neural nets",MultiPL-E:一种可扩展的Polyglot基准神经代码生成方法,大型语言模型已经证明了生成自然语言和编程语言文本的能力。尽管当代的代码生成模型是用几种编程语言在语料库上训练的，但它们是使用典型的单语基准测试的。最广泛使用的代码生成基准测试只针对Python，因此几乎没有定量证据表明代码生成模型在其他编程语言上的表现。我们提出了MultiPL-E，一个用于将单元测试驱动的代码生成基准转换为新语言的系统。我们使用MultiPL-E将两个流行的Python代码生成基准转换为另外18种编程语言，创建了第一个大规模的多语言代码生成基准。我们使用MultiPL-E将HumanEval基准测试（Chen et al.，2021）和MBPP基准测试（Austin等人，2021）扩展到18种语言，这些语言涵盖了一系列编程范式和流行性。使用这些新的并行基准，我们评估了三种最先进的代码生成模型的多语言性能：Codex（Chen et al.，2021）、CodeGen（Nijkamp et al.，2022）和InCoder（Fried et al.，2020）。我们发现Codex与其他几种语言在Python上的性能相匹配，甚至超过了它。MultiPL-E中表示的编程语言范围使我们能够探索语言频率和语言特征对模型性能的影响。最后，将代码生成基准编译到新编程语言的MultiPL-E方法具有可扩展性和可扩展性，可以直接评估新的模型、基准和语言。,B.2.3可靠性，测试和容错，I.5.1.D神经网络,,,
2HTBL7ND,2023,https://doi.org/10.1109/TSE.2022.3168672,TSE 2023,Program Synthesis for Cyber-Resilience,"Architectural tactics enable stakeholders to achieve cyber-resilience requirements. They permit systems to react, resist, detect, and recover from cyber incidents. This paper presents an approach to generate source code for architectural tactics typically used in safety and mission-critical systems. Our approach extensively relies on the use of the Event-B formal method and the EventB2Java code generation plugin of the Rodin platform. It leverages the modeling of architectural tactics in the Event-B formal language and uses a set of EventB2Java transformation rules to generate certified code implementations for the said tactics. Since resilience requirements are statements about a system over time, and because of the fact that the Event-B language does not provide (native) support for the writing of temporal specifications, we have implemented a novel Linear Temporal Logic (LTL) extension for Event-B. We support several architectural tactics for availability, performance, and security. The generated code is certified in the following sense: discharging proof obligations in Rodin - the platform we use for writing the Event-B models - attests to the soundness of the architectural tactics modelled in Event-B, and the soundness of the translation encoded by the EventB2Java tool attests to the code correctness. Finally, we demonstrate the usability of our resilience validation approach with the aid of an Autonomous Vehicle System. It further helped us increase our confidence in the soundness of our Event-B LTL extension.","Code synthesis,Event-B,formal methods,resilience,security,testing,verification",网络弹性程序综合,体系结构策略使利益相关者能够实现网络弹性要求。它们允许系统对网络事件做出反应、抵抗、检测和恢复。本文提出了一种为安全和任务关键型系统中通常使用的体系结构策略生成源代码的方法。我们的方法广泛依赖于使用Event-B形式化方法和Rodin平台的EventB2Java代码生成插件。它利用Event-B形式语言中的体系结构策略建模，并使用一组EventB2Java转换规则来生成所述策略的认证代码实现。由于弹性需求是关于一个系统随时间变化的陈述，并且由于Event-B语言不为编写时态规范提供（本地）支持，我们为Event-B实现了一个新的线性时态逻辑（LTL）扩展。我们支持针对可用性、性能和安全性的几种体系结构策略。生成的代码在以下意义上得到了验证：在我们用于编写Event-B模型的平台Rodin中履行证明义务证明了Event-B中建模的架构策略的可靠性，EventB2Java工具编码的翻译的可靠性证明了代码的正确性。最后，我们借助自动驾驶汽车系统证明了我们的弹性验证方法的可用性。它进一步帮助我们增强了对Event-B LTL扩展的可靠性的信心。,代码合成，Event-B，形式化方法，弹性，安全性，测试，验证,,,
BNB8VQGT,2023,https://doi.org/10.1109/TSE.2023.3288901,TSE 2023,NLP-Based Automated Compliance Checking of Data Processing Agreements Against GDPR,"When the entity processing personal data (the processor) differs from the one collecting personal data (the controller), processing personal data is regulated in Europe by the General Data Protection Regulation (GDPR) through data processing agreements (DPAs). Checking the compliance of DPAs contributes to the compliance verification of software systems as DPAs are an important source of requirements for software development involving the processing of personal data. However, manually checking whether a given DPA complies with GDPR is challenging as it requires significant time and effort for understanding and identifying DPA-relevant compliance requirements in GDPR and then verifying these requirements in the DPA. Legal texts introduce additional complexity due to convoluted language and inherent ambiguity leading to potential misunderstandings. In this paper, we propose an automated solution to check the compliance of a given DPA against GDPR. In close interaction with legal experts, we first built two artifacts: (i) the “shall” requirements extracted from the GDPR provisions relevant to DPA compliance and (ii) a glossary table defining the legal concepts in the requirements. Then, we developed an automated solution that leverages natural language processing (NLP) technologies to check the compliance of a given DPA against these “shall” requirements. Specifically, our approach automatically generates phrasal-level representations for the textual content of the DPA and compares them against predefined representations of the “shall” requirements. By comparing these two representations, the approach not only assesses whether the DPA is GDPR compliant but it further provides recommendations about missing information in the DPA. Over a dataset of 30 actual DPAs, the approach correctly finds 618 out of 750 genuine violations while raising 76 false violations, and further correctly identifies 524 satisfied requirements. The approach has thus an average precision of 89.1%, a recall of 82.4%, and an accuracy of 84.6%. Compared to a baseline that relies on off-the-shelf NLP tools, our approach provides an average accuracy gain of $\approx$20 percentage points. The accuracy of our approach can be improved to $\approx$94% with limited manual verification effort.","Requirements engineering (RE),the general data protection regulation (GDPR),regulatory compliance,natural language processing (NLP),data processing agreement (DPA),privacy",基于NLP的数据处理协议与GDPR的自动合规性检查,当处理个人数据的实体（处理者）与收集个人数据的主体（控制者）不同时，在欧洲，处理个人数据受《通用数据保护条例》（GDPR）通过数据处理协议（DPA）的监管。检查DPA的合规性有助于软件系统的合规验证，因为DPA是涉及个人数据处理的软件开发需求的重要来源。然而，手动检查给定的DPA是否符合GDPR是一项挑战，因为它需要大量的时间和精力来理解和确定GDPR中与DPA相关的合规要求，然后在DPA中验证这些要求。由于错综复杂的语言和导致潜在误解的内在歧义，法律文本带来了额外的复杂性。在本文中，我们提出了一种自动解决方案，用于检查给定DPA是否符合GDPR。在与法律专家的密切互动中，我们首先构建了两个工件：（i）从GDPR中提取的与DPA合规性相关的“应”要求，以及（ii）定义要求中法律概念的术语表。然后，我们开发了一个自动化解决方案，该解决方案利用自然语言处理（NLP）技术来检查给定DPA是否符合这些“应”要求。具体而言，我们的方法自动为DPA的文本内容生成短语级表示，并将其与“应”要求的预定义表示进行比较。通过比较这两种表述，该方法不仅评估了DPA是否符合GDPR，还进一步提供了有关DPA中缺失信息的建议。在30个实际DPA的数据集上，该方法正确地发现了750个真实违规中的618个，同时提出了76个虚假违规，并进一步正确地确定了524个满足的要求。因此，该方法的平均准确率为89.1%，召回率为82.4%，准确率为84.6%。与依赖现成NLP工具的基线相比，我们的方法提供了约20个百分点的平均准确度增益。通过有限的手动验证，我们的方法的准确性可以提高到$\approxy$94%。,需求工程（RE），通用数据保护条例（GDPR），法规遵从性，自然语言处理（NLP），数据处理协议（DPA），隐私,,,
NV9XI25R,2023,https://doi.org/10.1109/TSE.2023.3256322,TSE 2023,Metamorphic Testing for Web System Security,"Security testing aims at verifying that the software meets its security properties. In modern Web systems, however, this often entails the verification of the outputs generated when exercising the system with a very large set of inputs. Full automation is thus required to lower costs and increase the effectiveness of security testing. Unfortunately, to achieve such automation, in addition to strategies for automatically deriving test inputs, we need to address the oracle problem, which refers to the challenge, given an input for a system, of distinguishing correct from incorrect behavior (e.g., the response to be received after a specific HTTP GET request). In this paper, we propose Metamorphic Security Testing for Web-interactions (MST-wi), a metamorphic testing approach that integrates test input generation strategies inspired by mutational fuzzing and alleviates the oracle problem in security testing. It enables engineers to specify metamorphic relations (MRs) that capture many security properties of Web systems. To facilitate the specification of such MRs, we provide a domain-specific language accompanied by an Eclipse editor. MST-wi automatically collects the input data and transforms the MRs into executable Java code to automatically perform security testing. It automatically tests Web systems to detect vulnerabilities based on the relations and collected data. We provide a catalog of 76 system-agnostic MRs to automate security testing in Web systems. It covers 39% of the OWASP security testing activities not automated by state-of-the-art techniques; further, our MRs can automatically discover 102 different types of vulnerabilities, which correspond to 45% of the vulnerabilities due to violations of security design principles according to the MITRE CWE database. We also define guidelines that enable test engineers to improve the testability of the system under test with respect to our approach. We evaluated MST-wi effectiveness and scalability with two well-known Web systems (i.e., Jenkins and Joomla). It automatically detected 85% of their vulnerabilities and showed a high specificity (99.81% of the generated inputs do not lead to a false positive); our findings include a new security vulnerability detected in Jenkins. Finally, our results demonstrate that the approach scale, thus enabling automated security testing overnight.","System security testing,metamorphic testing,domain-specific languages",Web系统安全的变形测试,安全测试旨在验证软件是否符合其安全特性。然而，在现代Web系统中，这通常需要验证在使用大量输入的系统时生成的输出。因此，需要完全自动化来降低成本并提高安全测试的有效性。不幸的是，为了实现这种自动化，除了自动推导测试输入的策略外，我们还需要解决预言机问题，这是指在给定系统输入的情况下，区分正确和不正确行为（例如，在特定的HTTP GET请求后接收的响应）的挑战。在本文中，我们提出了Web交互的变形安全测试（MST-wi），这是一种变形测试方法，它集成了受变异模糊启发的测试输入生成策略，并缓解了安全测试中的预言机问题。它使工程师能够指定获取Web系统的许多安全属性的变形关系（MR）。为了便于规范此类MR，我们提供了一种特定于领域的语言，并附带了一个Eclipse编辑器。MST-wi自动收集输入数据，并将MR转换为可执行的Java代码，以自动执行安全测试。它根据关系和收集的数据自动测试Web系统以检测漏洞。我们提供了76个系统不可知的MR目录，用于自动化Web系统中的安全测试。它涵盖了39%的OWASP安全测试活动，这些活动不是由最先进的技术自动化的；此外，根据MITRE CWE数据库，我们的MR可以自动发现102种不同类型的漏洞，这相当于45%的漏洞是由于违反安全设计原则造成的。我们还定义了指导方针，使测试工程师能够根据我们的方法提高被测系统的可测试性。我们使用两个著名的Web系统（即Jenkins和Joomla）评估了MST-wi的有效性和可扩展性。它自动检测到85%的漏洞，并显示出高度的特异性（99.81%的生成输入不会导致假阳性）；我们的发现包括在Jenkins中检测到的一个新的安全漏洞。最后，我们的结果证明了该方法的规模，从而实现了一夜之间的自动化安全测试。,系统安全测试，变形测试，领域特定语言,,,
5W9M4SVU,2023,https://doi.org/10.1109/TSE.2022.3218264,TSE 2023,T-Evos: A Large-Scale Longitudinal Study on CI Test Execution and Failure,"Continuous integration is widely adopted in software projects to reduce the time it takes to deliver the changes to the market. To ensure software quality, developers also run regression test cases in a continuous fashion. The CI practice generates commit-by-commit software evolution data that provides great opportunities for future testing research. However, such data is often unavailable due to space limitation (e.g., developers only keep the data for a certain period) and the significant effort involved in re-running the test cases on a per-commit basis. In this paper, we present T-Evos, a dataset on test result and coverage evolution, covering 8,093 commits across 12 open-source Java projects. Our dataset includes the evolution of statement-level code coverage for every test case (either passed and failed), test result, all the builds information, code changes, and the corresponding bug reports. We conduct an initial analysis to demonstrate the overall dataset. In addition, we conduct an empirical study using T-Evos to study the characteristics of test failures in CI settings. We find that test failures are frequent, and while most failures are resolved within a day, some failures require several weeks to resolve. We highlight the relationship between code changes and test failure, and provide insights for future automated testing research. Our dataset may be used for future testing research and benchmarking in CI. Our findings provide an important first step in understanding code coverage evolution and test failures in a continuous environment.","Evolution and maintenance,mining software repositories,software testing",T-Evos：CI测试执行与失败的大规模纵向研究,持续集成在软件项目中被广泛采用，以减少向市场交付更改所需的时间。为了确保软件质量，开发人员还以连续的方式运行回归测试用例。CI实践生成逐个提交的软件进化数据，为未来的测试研究提供了巨大的机会。然而，由于空间限制（例如，开发人员只将数据保存一段时间）以及在每次提交的基础上重新运行测试用例所涉及的大量工作，这些数据通常不可用。在本文中，我们介绍了T-Evos，一个关于测试结果和覆盖率演变的数据集，涵盖了12个开源Java项目的8093次提交。我们的数据集包括每个测试用例（通过和失败）的语句级代码覆盖率的演变、测试结果、所有构建信息、代码更改以及相应的错误报告。我们进行了初步分析，以展示整个数据集。此外，我们使用T-Evos进行了一项实证研究，以研究CI环境中测试失败的特征。我们发现测试失败是经常发生的，虽然大多数失败在一天内得到解决，但有些失败需要几周才能解决。我们强调了代码更改和测试失败之间的关系，并为未来的自动化测试研究提供了见解。我们的数据集可用于CI的未来测试研究和基准测试。我们的发现为理解连续环境中的代码覆盖率演变和测试失败迈出了重要的第一步。,进化和维护，挖掘软件存储库，软件测试,,,
4ZQRHTT2,2023,https://doi.org/10.1109/TSE.2022.3159548,TSE 2023,Automatically Distilling Storyboard With Rich Features for Android Apps,"Before developing a new mobile app, the development team usually endeavors painstaking efforts to review many existing apps with similar purposes. The review process is crucial in the sense that it reduces market risks and provides inspirations for app development. However, manual exploration of hundreds of existing apps by different roles (e.g., product manager, UI/UX designer, developer, and tester) can be ineffective. For example, it is difficult to completely explore all the functionalities of the app from different aspects including design, implementation, and testing in a short period of time. However, existing reverse engineering tools only provide basic features such as AndroidManifest.xml and Java source files for users. Following the conception of storyboard in movie production, we propose a system, named StoryDistiller, to automatically generate the storyboards for Android apps with rich features through reverse engineering, and assist different roles to review and analyze apps effectively and efficiently. Specifically, we (1) propose a hybrid method to extract a relatively complete Activity transition graph (ATG), that is, it first extracts the ATG of Android apps through static analysis method first, and further leverages dynamic component exploration to augment ATG; (2) extract the required inter-component communication (ICC) data of each target Activity by leveraging static data-flow analysis and renders UI pages dynamically by using app instrumentation together with the extracted required ICC data; (3) obtain rich features including comprehensive ATG with rendered UI pages, semantic activity names, corresponding logic and layout code, etc. (4) implement the storyboard visualization as a web service with the rendered UI pages and the corresponding rich features. Our experiments unveil that StoryDistiller is effective and indeed useful to assist app exploration and review. We also conduct a comprehensive comparison study to demonstrate better performance over IC3, Gator, Stoat, and StoryDroid.","Android apps,app review,competitive analysis,reverse engineering,storyboard",为Android应用程序自动提取具有丰富功能的情节提要,在开发新的移动应用程序之前，开发团队通常会煞费苦心地审查许多具有类似目的的现有应用程序。审查过程至关重要，因为它降低了市场风险，并为应用程序开发提供了灵感。然而，由不同的角色（例如，产品经理、UI/UX设计师、开发人员和测试人员）手动探索数百个现有应用程序可能是无效的。例如，很难在短时间内从设计、实现和测试等不同方面全面探索应用程序的所有功能。然而，现有的逆向工程工具只为用户提供AndroidManifest.xml和Java源文件等基本功能。根据电影制作中故事板的概念，我们提出了一个名为StoryDistiller的系统，通过逆向工程自动生成功能丰富的Android应用程序的故事板，并帮助不同角色有效地审查和分析应用程序。具体而言，我们（1）提出了一种混合方法来提取相对完整的活动转换图（ATG），即首先通过静态分析方法提取安卓应用程序的ATG，并进一步利用动态组件探索来增强ATG；（2） 通过利用静态数据流分析来提取每个目标活动的所需组件间通信（ICC）数据，并通过将应用工具与所提取的所需ICC数据一起使用来动态地呈现UI页面；（3） 获得丰富的功能，包括具有渲染的UI页面、语义活动名称、相应的逻辑和布局代码等的全面ATG。（4）利用渲染的UI页和相应的丰富功能将故事板可视化实现为web服务。我们的实验表明，StoryDistiller在帮助应用程序探索和审查方面是有效的，确实很有用。我们还进行了一项全面的比较研究，以证明与IC3、Gator、Stoat和StoryDroid相比具有更好的性能。,安卓应用程序，应用程序审查，竞争分析，逆向工程，故事板,,,
X7MQXP66,2023,https://doi.org/10.1109/TSE.2023.3252259,TSE 2023,API Usage Recommendation Via Multi-View Heterogeneous Graph Representation Learning,"Developers often need to decide which APIs to use for the functions being implemented. With the ever-growing number of APIs and libraries, it becomes increasingly difficult for developers to find appropriate APIs, indicating the necessity of automatic API usage recommendation. Previous studies adopt statistical models or collaborative filtering methods to mine the implicit API usage patterns for recommendation. However, they rely on the occurrence frequencies of APIs for mining usage patterns, thus prone to fail for the low-frequency APIs. Besides, prior studies generally regard the API call interaction graph as homogeneous graph, ignoring the rich information (e.g., edge types) in the structure graph. In this work, we propose a novel method named MEGA for improving the recommendation accuracy especially for the low-frequency APIs. Specifically, besides call interaction graph, MEGA considers another two new heterogeneous graphs: global API co-occurrence graph enriched with the API frequency information and hierarchical structure graph enriched with the project component information. With the three multi-view heterogeneous graphs, MEGA can capture the API usage patterns more accurately. Experiments on three Java benchmark datasets demonstrate that MEGA significantly outperforms the baseline models by at least 19% with respect to the Success Rate@1 metric. Especially, for the low-frequency APIs, MEGA also increases the baselines by at least 55% regarding the Success Rate@1 score.","API recommendation,graph representation learning,multi-view heterogeneous graphs",基于多视图异构图表示学习的API使用推荐,开发人员通常需要决定要为正在实现的函数使用哪些API。随着API和库数量的不断增加，开发人员越来越难以找到合适的API，这表明自动推荐API使用的必要性。以往的研究采用统计模型或协同过滤方法来挖掘隐含的API使用模式进行推荐。然而，它们依赖于API的出现频率来挖掘使用模式，因此低频率API很容易失败。此外，先前的研究通常将API调用交互图视为同构图，忽略了结构图中丰富的信息（如边缘类型）。在这项工作中，我们提出了一种新的方法，称为MEGA，以提高推荐精度，特别是对低频API。具体来说，除了调用交互图之外，MEGA还考虑了另外两种新的异构图：富含API频率信息的全局API同现图和富含项目组件信息的分层结构图。通过三个多视图异构图，MEGA可以更准确地捕捉API的使用模式。在三个Java基准数据集上的实验表明，MEGA在成功方面显著优于基线模型至少19%Rate@1米制的特别是，对于低频API，MEGA还将关于成功的基线增加了至少55%Rate@1分数,API推荐，图形表示学习，多视图异构图形,,,
RWMI6QET,2023,https://doi.org/10.1109/TSE.2022.3147265,TSE 2023,Neural Transfer Learning for Repairing Security Vulnerabilities in C Code,"In this paper, we address the problem of automatic repair of software vulnerabilities with deep learning. The major problem with data-driven vulnerability repair is that the few existing datasets of known confirmed vulnerabilities consist of only a few thousand examples. However, training a deep learning model often requires hundreds of thousands of examples. In this work, we leverage the intuition that the bug fixing task and the vulnerability fixing task are related and that the knowledge learned from bug fixes can be transferred to fixing vulnerabilities. In the machine learning community, this technique is called transfer learning. In this paper, we propose an approach for repairing security vulnerabilities named VRepair which is based on transfer learning. VRepair is first trained on a large bug fix corpus and is then tuned on a vulnerability fix dataset, which is an order of magnitude smaller. In our experiments, we show that a model trained only on a bug fix corpus can already fix some vulnerabilities. Then, we demonstrate that transfer learning improves the ability to repair vulnerable C functions. We also show that the transfer learning model performs better than a model trained with a denoising task and fine-tuned on the vulnerability fixing task. To sum up, this paper shows that transfer learning works well for repairing security vulnerabilities in C compared to learning on a small dataset.","Vulnerability fixing,transfer learning,seq2seq learning",修复C代码安全漏洞的神经传递学习,在本文中，我们解决了使用深度学习自动修复软件漏洞的问题。数据驱动漏洞修复的主要问题是，已知已确认漏洞的少数现有数据集仅由几千个示例组成。然而，训练一个深度学习模型通常需要数十万个例子。在这项工作中，我们利用了错误修复任务和漏洞修复任务是相关的直觉，从错误修复中学到的知识可以转移到修复漏洞上。在机器学习社区中，这种技术被称为迁移学习。在本文中，我们提出了一种基于迁移学习的安全漏洞修复方法VRepair。VRepair首先在一个大型错误修复语料库上进行训练，然后在一个小一个数量级的漏洞修复数据集上进行调优。在我们的实验中，我们表明，仅在错误修复语料库上训练的模型已经可以修复一些漏洞。然后，我们证明了迁移学习提高了修复脆弱C函数的能力。我们还表明，迁移学习模型比用去噪任务训练并在漏洞修复任务上进行微调的模型表现更好。总之，本文表明，与在小数据集上学习相比，迁移学习在修复C中的安全漏洞方面效果良好。,漏洞修复，迁移学习，seq2seq学习,,,
3P7EBQZU,2023,https://doi.org/10.1109/TSE.2022.3164662,TSE 2023,Program Repair With Repeated Learning,"A key challenge in generate-and-validate automated program repair is directing the search for fixes so that it can efficiently find those that are more likely to be correct. To this end, several techniques use machine learning to capture the features of programmer-written fixes. In existing approaches, fitting the model typically takes place before fix generation and is independent of it: the fix generation process uses the learned model as one of its inputs. However, the intermediate outcomes of an ongoing fix generation process often provide valuable information about which candidate fixes were “better”; this information could profitably be used to retrain the model, so that each new iteration of the fixing process would also learn from the outcome of previous ones. In this paper, we propose the Liana technique for automated program repair, which is based on this idea of repeatedly learning the features of generated fixes. To this end, Liana uses a fine-grained model that combines information about fix characteristics, their relations to the fixing context, and the results of test execution. The model is initially trained offline, and then repeatedly updated online as the fix generation process unravels; at any step, the most up-to-date model is used to guide the search for fixes—prioritizing those that are more likely to include the right ingredients. In an experimental evaluation on 732 real-world Java bugs from 3 popular benchmarks, Liana built correct fixes for 134 faults (83 ranked as first in its output)— improving over several other generate-and-validate program repair tools according to various measures.","Automated program repair (APR),generate-and-validate APR,learning-to-rank,repeated learning",重复学习的程序修复,生成和验证自动程序修复的一个关键挑战是指导搜索修复程序，以便有效地找到更可能正确的修复程序。为此，有几种技术使用机器学习来捕捉程序员编写的修复程序的特性。在现有的方法中，拟合模型通常在修复生成之前进行，并且与修复无关：修复生成过程使用学习的模型作为其输入之一。然而，正在进行的修复生成过程的中间结果往往提供了关于哪些候选修复“更好”的宝贵信息；这些信息可以有益地用于重新训练模型，这样修复过程的每一次新迭代也可以从以前迭代的结果中学习。在本文中，我们提出了用于自动程序修复的Liana技术，该技术基于重复学习生成的修复的特征的思想。为此，Liana使用了一个细粒度模型，该模型结合了有关修复特性、它们与修复上下文的关系以及测试执行结果的信息。该模型最初是离线训练的，然后随着修复生成过程的展开而在线重复更新；在任何步骤中，都会使用最新的模型来指导搜索修复程序——对那些更有可能包含正确成分的修复程序进行优先级排序。在对来自3个流行基准的732个真实世界Java错误的实验评估中，Liana为134个错误（83个在其输出中排名第一）建立了正确的修复程序，根据各种衡量标准，它比其他几种生成和验证程序修复工具有所改进。,自动程序修复（APR），生成和验证APR，学习排名，重复学习,,,
LMDTWDJQ,2023,https://doi.org/10.1109/TSE.2022.3210580,TSE 2023,A Source-Level Instrumentation Framework for the Dynamic Analysis of Memory Safety,"Low-level control makes C unsafe, resulting in memory errors that can lead to data corruption, security vulnerabilities or program crashes. Dynamic analysis tools, which have been widely used for detecting memory errors at runtime, usually perform instrumentation at the IR or binary level. However, these non-source-level instrumentation frameworks and tools suffer from two inherent drawbacks: optimization sensitivity and platform dependence. Due to optimization sensitivity, the user of these tools must trade either performance for effectiveness by compiling the program at -O0 or effectiveness for performance by compiling the program at a higher optimization level, say, -O3. In this paper, we propose a new source-level instrumentation framework to overcome these two drawbacks, and implement it in a new dynamic analysis tool, called Movec, that adopts a pointer-based monitoring algorithm. We have evaluated Movec comprehensively by using the NIST's SARD benchmark suite (1152 programs), a set of 126 microbenchmarks (with ground truth), a set of 20 MiBench benchmarks and 5 pure-C SPEC CPU 2017 benchmarks. In terms of effectiveness, Movec outperforms three state-of-the-art dynamic analysis tools, AddressSanitizer, SoftBoundCETS and Valgrind, for all the standard optimization levels (from -O0 to -O3). In terms of performance, Movec outperforms SoftBoundCETS and Valgrind, and is slower than AddressSanitizer but consumes less memory.","Software quality,software testing,dynamic analysis,bug finding,memory errors,code instrumentation",用于内存安全动态分析的源级仪器框架,低级控制会使C变得不安全，从而导致内存错误，从而导致数据损坏、安全漏洞或程序崩溃。动态分析工具已被广泛用于在运行时检测内存错误，通常在IR或二进制级别执行检测。然而，这些非源代码级别的检测框架和工具存在两个固有的缺点：优化敏感性和平台依赖性。由于优化的敏感性，这些工具的用户必须通过在-O0下编译程序来换取性能，或者通过在更高的优化级别（例如-O3）下编译程序以换取性能。在本文中，我们提出了一种新的源代码级检测框架来克服这两个缺点，并在一个新的动态分析工具Movec中实现，该工具采用了基于指针的监测算法。我们使用NIST的SARD基准套件（1152个程序）、一组126个微基准（具有基本事实）、一套20个MiBench基准和5个pure-C SPEC CPU 2017基准对Movec进行了全面评估。就有效性而言，Movec在所有标准优化级别（从-O0到-O3）上都优于三种最先进的动态分析工具AddressSanitizer、SoftBoundCETS和Valgrind。就性能而言，Movec的性能优于SoftBoundCETS和Valgrind，并且比AddressSanitizer慢，但消耗的内存更少。,软件质量，软件测试，动态分析，错误查找，内存错误，代码检测,,,
SJGBY5T5,2023,https://doi.org/10.1109/TSE.2022.3156637,TSE 2023,SeqTrans: Automatic Vulnerability Fix Via Sequence to Sequence Learning,"Software vulnerabilities are now reported unprecedentedly due to the recent development of automated vulnerability hunting tools. However, fixing vulnerabilities still mainly depends on programmers’ manual efforts. Developers need to deeply understand the vulnerability and affect the system’s functions as little as possible. In this paper, with the advancement of Neural Machine Translation (NMT) techniques, we provide a novel approach called SeqTrans to exploit historical vulnerability fixes to provide suggestions and automatically fix the source code. To capture the contextual information around the vulnerable code, we propose to leverage data-flow dependencies to construct code sequences and feed them into the state-of-the-art transformer model. The fine-tuning strategy has been introduced to overcome the small sample size problem. We evaluate SeqTrans on a dataset containing 1,282 commits that fix 624 CVEs in 205 Java projects. Results show that the accuracy of SeqTrans outperforms the latest techniques and achieves 23.3% in statement-level fix and 25.3% in CVE-level fix. In the meantime, we look deep inside the result and observe that the NMT model performs very well in certain kinds of vulnerabilities like CWE-287 (Improper Authentication) and CWE-863 (Incorrect Authorization).","Machine learning,neural machine translation,software engineering,vulnerability fix",SeqTrans：通过序列到序列学习自动修复漏洞,由于最近开发了自动漏洞搜寻工具，软件漏洞的报告前所未有。然而，修复漏洞仍然主要取决于程序员的手动操作。开发人员需要深入了解该漏洞，并尽可能少地影响系统的功能。在本文中，随着神经机器翻译（NMT）技术的进步，我们提供了一种称为SeqTrans的新方法来利用历史漏洞修复来提供建议并自动修复源代码。为了捕获易受攻击代码的上下文信息，我们建议利用数据流依赖关系来构建代码序列，并将其输入到最先进的转换器模型中。为了克服小样本量的问题，引入了微调策略。我们在一个包含1282个提交的数据集上评估SeqTrans，该数据集修复了205个Java项目中的624个CVE。结果表明，SeqTrans的准确率优于最新技术，在语句级修复和CVE级修复中分别达到23.3%和25.3%。同时，我们深入研究了结果，发现NMT模型在某些类型的漏洞中表现得很好，如CWE-287（不当身份验证）和CWE-863（错误授权）。,机器学习，神经机器翻译，软件工程，漏洞修复,,,
6TZ664UE,2023,https://doi.org/10.1109/TSE.2022.3171202,TSE 2023,Data Preparation for Software Vulnerability Prediction: A Systematic Literature Review,"Software Vulnerability Prediction (SVP) is a data-driven technique for software quality assurance that has recently gained considerable attention in the Software Engineering research community. However, the difficulties of preparing Software Vulnerability (SV) related data is considered as the main barrier to industrial adoption of SVP approaches. Given the increasing, but dispersed, literature on this topic, it is needed and timely to systematically select, review, and synthesize the relevant peer-reviewed papers reporting the existing SV data preparation techniques and challenges. We have carried out a Systematic Literature Review (SLR) of SVP research in order to develop a systematized body of knowledge of the data preparation challenges, solutions, and the needed research. Our review of the 61 relevant papers has enabled us to develop a taxonomy of data preparation for SVP related challenges. We have analyzed the identified challenges and available solutions using the proposed taxonomy. Our analysis of the state of the art has enabled us identify the opportunities for future research. This review also provides a set of recommendations for researchers and practitioners of SVP approaches.","Data preparation,data quality,software vulnerability prediction,systematic literature review",软件漏洞预测的数据准备：系统文献综述,软件脆弱性预测（SVP）是一种用于软件质量保证的数据驱动技术，最近在软件工程研究界引起了相当大的关注。然而，准备软件漏洞（SV）相关数据的困难被认为是行业采用SVP方法的主要障碍。鉴于有关该主题的文献越来越多，但分散，有必要及时系统地选择、审查和综合报告现有SV数据准备技术和挑战的相关同行评审论文。我们对高级副总裁的研究进行了系统文献综述（SLR），以开发关于数据准备挑战、解决方案和所需研究的系统化知识体系。我们对61篇相关论文的综述使我们能够为SVP相关挑战制定数据准备的分类法。我们已经使用所提出的分类法分析了已确定的挑战和可用的解决方案。我们对最新技术的分析使我们能够确定未来研究的机会。这篇综述还为高级副总裁方法的研究人员和从业者提供了一系列建议。,数据准备，数据质量，软件漏洞预测，系统文献综述,,,
S9WU2RP3,2023,https://doi.org/10.1109/TSE.2023.3270708,TSE 2023,Driving the Technology Value Stream by Analyzing App Reviews,"An emerging feature of mobile application software is the need to quickly produce new versions to solve problems that emerged in previous versions. This helps adapt to changing user needs and preferences. In a continuous software development process, the user reviews collected by the apps themselves can play a crucial role to detect which components need to be reworked. This paper proposes a novel framework that enables software companies to drive their technology value stream based on the feedback (or reviews) provided by the end-users of an application. The proposed end-to-end framework exploits different Natural Language Processing (NLP) tasks to best understand the needs and goals of the end users. We also provide a thorough and in-depth analysis of the framework, the performance of each of the modules, and the overall contribution in driving the technology value stream. An analysis of reviews with sixteen popular Android Play Store applications from various genres over a long period of time provides encouraging evidence of the effectiveness of the proposed approach.","Continuous software development,technology value stream,NLP,app reviews",通过分析应用评论推动技术价值流,移动应用软件的一个新兴功能是需要快速生成新版本，以解决以前版本中出现的问题。这有助于适应不断变化的用户需求和偏好。在持续的软件开发过程中，应用程序自己收集的用户评论可以在检测哪些组件需要返工方面发挥关键作用。本文提出了一种新的框架，使软件公司能够根据应用程序最终用户提供的反馈（或评论）来推动其技术价值流。所提出的端到端框架利用不同的自然语言处理（NLP）任务来最好地理解最终用户的需求和目标。我们还对框架、每个模块的性能以及在推动技术价值流方面的总体贡献进行了全面深入的分析。在很长一段时间内，对来自不同类型的16个流行的Android Play Store应用程序的评论进行了分析，为所提出的方法的有效性提供了令人鼓舞的证据。,持续的软件开发，技术价值流，NLP，应用程序评论,,,
CJ4MVM2Z,2023,https://doi.org/10.1109/TSE.2022.3227418,TSE 2023,InterEvo-TR: Interactive Evolutionary Test Generation With Readability Assessment,"Automated test case generation has proven to be useful to reduce the usually high expenses of software testing. However, several studies have also noted the skepticism of testers regarding the comprehension of generated test suites when compared to manually designed ones. This fact suggests that involving testers in the test generation process could be helpful to increase their acceptance of automatically-produced test suites. In this paper, we propose incorporating interactive readability assessments made by a tester into EvoSuite, a widely-known evolutionary test generation tool. Our approach, InterEvo-TR, interacts with the tester at different moments during the search and shows different test cases covering the same coverage target for their subjective evaluation. The design of such an interactive approach involves a schedule of interaction, a method to diversify the selected targets, a plan to save and handle the readability values, and some mechanisms to customize the level of engagement in the revision, among other aspects. To analyze the potential and practicability of our proposal, we conduct a controlled experiment in which 39 participants, including academics, professional developers, and student collaborators, interact with InterEvo-TR. Our results show that the strategy to select and present intermediate results is effective for the purpose of readability assessment. Furthermore, the participants’ actions and responses to a questionnaire allowed us to analyze the aspects influencing test code readability and the benefits and limitations of an interactive approach in the context of test case generation, paving the way for future developments based on interactivity.","Evolutionary computing and genetic algorithms,interactive search-based software engineering,readability,testing tools",InterEvo TR：具有可读性评估的交互式进化测试生成,自动化测试用例生成已被证明有助于减少通常高昂的软件测试费用。然而，一些研究也注意到，与手动设计的测试套件相比，测试人员对生成的测试套件的理解持怀疑态度。这一事实表明，让测试人员参与测试生成过程可能有助于提高他们对自动生成的测试套件的接受度。在本文中，我们建议将测试人员进行的交互式可读性评估纳入EvoSuite，这是一种广为人知的进化测试生成工具。我们的方法InterEvo TR在搜索过程中的不同时刻与测试人员进行交互，并显示覆盖相同覆盖目标的不同测试用例，以进行主观评估。这种互动方法的设计包括互动时间表、使选定目标多样化的方法、保存和处理可读性值的计划，以及定制修订参与程度的一些机制等。为了分析我们提案的潜力和实用性，我们进行了一项对照实验，39名参与者，包括学者、专业开发人员和学生合作者，与InterEvo-TR互动。我们的结果表明，选择和呈现中间结果的策略对于可读性评估是有效的。此外，参与者的行为和对问卷的回答使我们能够分析影响测试代码可读性的方面，以及在测试用例生成的背景下交互式方法的好处和局限性，为未来基于交互性的开发铺平道路。,进化计算和遗传算法，基于交互式搜索的软件工程，可读性，测试工具,,,
4XJT5MR7,2023,https://doi.org/10.1109/TSE.2022.3206427,TSE 2023,A Declarative Metamorphic Testing Framework for Autonomous Driving,"Autonomous driving has gained much attention from both industry and academia. Currently, Deep Neural Networks (DNNs) are widely used for perception and control in autonomous driving. However, several fatal accidents caused by autonomous vehicles have raised serious safety concerns about autonomous driving models. Some recent studies have successfully used the metamorphic testing technique to detect thousands of potential issues in some popularly used autonomous driving models. However, prior study is limited to a small set of metamorphic relations, which do not reflect rich, real-world traffic scenarios and are also not customizable. This paper presents a novel declarative rule-based metamorphic testing framework called RMT. RMT provides a rule template with natural language syntax, allowing users to flexibly specify an enriched set of testing scenarios based on real-world traffic rules and domain knowledge. RMT automatically parses human-written rules to metamorphic relations using an NLP-based rule parser referring to an ontology list and generates test cases with a variety of image transformation engines. We evaluated RMT on three autonomous driving models. With an enriched set of metamorphic relations, RMT detected a significant number of abnormal model predictions that were not detected by prior work. Through a large-scale human study on Amazon Mechanical Turk, we further confirmed the authenticity of test cases generated by RMT and the validity of detected abnormal model predictions.","Metamorphic testing,Autonomous driving,testing",一种用于自动驾驶的声明性变形测试框架,自动驾驶已经引起了业界和学术界的广泛关注。目前，深度神经网络（DNN）被广泛用于自动驾驶中的感知和控制。然而，几起由自动驾驶汽车引发的致命事故引发了人们对自动驾驶车型的严重安全担忧。最近的一些研究已经成功地使用变形测试技术来检测一些常用的自动驾驶模型中的数千个潜在问题。然而，先前的研究仅限于一小部分变形关系，这些关系不能反映丰富的现实世界交通场景，也不可定制。本文提出了一种新的基于规则的声明性变形测试框架RMT。RMT提供了一个具有自然语言语法的规则模板，允许用户根据真实世界的流量规则和领域知识灵活地指定一组丰富的测试场景。RMT使用基于NLP的规则解析器参考本体列表，自动将人工编写的规则解析为变形关系，并使用各种图像转换引擎生成测试用例。我们对三种自动驾驶车型的RMT进行了评估。通过一组丰富的变质关系，RMT检测到了大量先前工作没有检测到的异常模型预测。通过对Amazon Mechanical Turk的大规模人体研究，我们进一步证实了RMT生成的测试用例的真实性和检测到的异常模型预测的有效性。,变形测试，自动驾驶，测试,,,
TZ9YT7UY,2023,https://doi.org/10.1109/TSE.2022.3209625,TSE 2023,Generating Class-Level Integration Tests Using Call Site Information,"Search-based approaches have been used in the literature to automate the process of creating unit test cases. However, related work has shown that generated tests with high code coverage could be ineffective, i.e., they may not detect all faults or kill all injected mutants. In this paper, we propose Cling, an integration-level test case generation approach that exploits how a pair of classes, the caller and the callee, interact with each other through method calls. In particular, Cling generates integration-level test cases that maximize the Coupled Branches Criterion (CBC). Coupled branches are pairs of branches containing a branch of the caller and a branch of the callee such that an integration test that exercises the former also exercises the latter. CBC is a novel integration-level coverage criterion, measuring the degree to which a test suite exercises the interactions between a caller and its callee classes. We implemented Cling and evaluated the approach on 140 pairs of classes from five different open-source Java projects. Our results show that (1) Cling generates test suites with high CBC coverage, thanks to the definition of the test suite generation as a many-objectives problem where each couple of branches is an independent objective; (2) such generated suites trigger different class interactions and can kill on average 7.7% (with a maximum of 50%) of mutants that are not detected by tests generated randomly or at the unit level; (3) Cling can detect integration faults coming from wrong assumptions about the usage of the callee class (25 for our subject systems) that remain undetected when using automatically generated random and unit-level test suites.","CLING,class integration testing,coverage criteria,search-based software testing,test adequacy",使用调用站点信息生成类级集成测试,文献中已经使用了基于搜索的方法来自动化创建单元测试用例的过程。然而，相关工作表明，具有高代码覆盖率的生成测试可能是无效的，即，它们可能无法检测到所有故障或杀死所有注入的突变体。在本文中，我们提出了Cling，这是一种集成级测试用例生成方法，它利用了一对类（调用者和被调用者）如何通过方法调用相互交互。特别是，Cling生成集成级测试用例，以最大化耦合分支标准（CBC）。耦合分支是包含调用方分支和被调用方分支的成对分支，因此，练习前者的集成测试也练习后者。CBC是一种新的集成级覆盖标准，衡量测试套件在调用方和被调用方类之间进行交互的程度。我们实现了Cling，并在来自五个不同开源Java项目的140对类上评估了该方法。我们的结果表明：（1）Cling生成具有高CBC覆盖率的测试套件，这要归功于将测试套件生成定义为多目标问题，其中每对分支都是一个独立的目标；（2） 这样生成的套件触发不同类别的相互作用，并且可以杀死平均7.7%（最大50%）的突变体，这些突变体不是通过随机生成或在单位水平上生成的测试检测到的；（3） Cling可以检测来自对被调用者类（对于我们的主题系统为25）的使用的错误假设的集成故障，这些错误假设在使用自动生成的随机和单元级测试套件时仍然未被检测到。,CLING，类集成测试，覆盖率标准，基于搜索的软件测试，测试充分性,,,
XRCV95K9,2023,https://doi.org/10.1109/TSE.2022.3183955,TSE 2023,DeepMerge: Learning to Merge Programs,"In collaborative software development, program merging is the mechanism to integrate changes from multiple programmers. Merge algorithms in modern version control systems report a conflict when changes interfere textually. Merge conflicts require manual intervention and frequently stall modern continuous integration pipelines. Prior work found that, although costly, a large majority of resolutions involve re-arranging text without writing any new code. Inspired by this observation we propose the first data-driven approach to resolve merge conflicts with a machine learning model. We realize our approach in a tool DeepMerge that uses a novel combination of (i) an edit-aware embedding of merge inputs and (ii) a variation of pointer networks, to construct resolutions from input segments. We also propose an algorithm to localize manual resolutions in a resolved file and employ it to curate a ground-truth dataset comprising 8,719 non-trivial resolutions in JavaScript programs. Our evaluation shows that, on a held out test set, DeepMerge can predict correct resolutions for 37% of non-trivial merges, compared to only 4% by a state-of-the-art semistructured merge technique. Furthermore, on the subset of merges with upto 3 lines (comprising 24% of the total dataset), DeepMerge can predict correct resolutions with 78% accuracy.","Merge conflicts,conflict resolution,software maintenance,software tools",DeepMerge：学习合并程序,在协同软件开发中，程序合并是集成来自多个程序员的更改的机制。现代版本控制系统中的合并算法在更改对文本产生干扰时会报告冲突。合并冲突需要手动干预，并且经常使现代连续集成管道停滞。先前的工作发现，尽管成本高昂，但大多数解决方案都涉及在不编写任何新代码的情况下重新排列文本。受这一观察结果的启发，我们提出了第一种用机器学习模型解决合并冲突的数据驱动方法。我们在工具DeepMerge中实现了我们的方法，该工具使用（i）合并输入的编辑感知嵌入和（ii）指针网络的变体的新颖组合，从输入段构建分辨率。我们还提出了一种算法，将手动分辨率定位在已解析的文件中，并使用它来策划一个基本事实数据集，该数据集包括JavaScript程序中的8719个非平凡分辨率。我们的评估表明，在一个搁置的测试集上，DeepMerge可以预测37%的非平凡合并的正确分辨率，而最先进的半结构合并技术只能预测4%。此外，在多达3行的合并子集（占总数据集的24%）上，DeepMerge可以预测78%的正确分辨率。,合并冲突，冲突解决，软件维护，软件工具,,,
SGD94EEI,2023,https://doi.org/10.1109/TSE.2022.3163969,TSE 2023,A Large-Scale Analysis of IoT Firmware Version Distribution in the Wild,"This paper examines the up-to-dateness of installed firmware versions of Internet of Things devices accessible via public Internet. It takes a novel approach to identify versions based on the source code of their web interfaces. It analyzes data sets of 1.06m devices collected using the IoT search engine Censys and then maps the results against the latest version each manufacturer offers. A fully scalable and adaptive approach is developed by applying the SEMMA data mining process. This approach relies on three data artifacts: raw data from Censys, a mapping table with firmware versions, and a keyword search list. The results confirm the heterogeneity of connected IoT devices and show that only 2.45 percent of the IoT devices “in the wild” run the latest available firmware. Installed versions are 19.2 months old on average. This real-world evidence suggests that the updating processes and methods used by engineers so far are not sufficient to keep IoT devices up-to-date. This paper identifies and quantifies influencing factors and captures the global and diverse distribution of IoT devices. It finds manufacturer and device type influence the up-to-dateness of firmware, whereas the country in which the device is deployed is less significant.","Internet of Things,IoT,embedded systems,firmware,version,patch,update,up-to-dateness,fingerprinting",物联网固件版本分布的大规模分析,本文考察了可通过公共互联网访问的物联网设备的安装固件版本的最新性。它采用了一种新颖的方法，根据其web界面的源代码来识别版本。它分析了使用物联网搜索引擎Censys收集的106万台设备的数据集，然后将结果与每个制造商提供的最新版本进行映射。通过应用SEMMA数据挖掘过程，开发了一种完全可扩展和自适应的方法。这种方法依赖于三个数据工件：Censys的原始数据、带有固件版本的映射表和关键字搜索列表。结果证实了联网物联网设备的异质性，并显示只有2.45%的“野生”物联网设备运行最新的可用固件。安装的版本平均使用19.2个月。这一现实世界的证据表明，到目前为止，工程师使用的更新过程和方法不足以使物联网设备保持最新。本文识别并量化了影响因素，并捕捉到了物联网设备的全球和多样化分布。它发现制造商和设备类型会影响固件的最新性，而设备部署的国家则不那么重要。,物联网，物联网，嵌入式系统，固件，版本，补丁，更新，最新，指纹,,,
RLAWKARX,2023,https://doi.org/10.1109/TSE.2023.3289808,TSE 2023,Self-Admitted Technical Debt in Ethereum Smart Contracts: A Large-Scale Exploratory Study,"Programmable blockchain platforms such as Ethereum offer unique benefits to application development, including a decentralized infrastructure, tamper-proof transactions, and auditability. These benefits enable new types of applications that can bring competitive advantage to several business segments. Nonetheless, the pressure of time-to-market combined with relatively immature development technologies (e.g., the Solidity programming language), lack of high-quality training resources, and an unclear roadmap for Ethereum creates a context that favors the introduction of technical debt (e.g., code hacks, workarounds, and suboptimal implementations) into application code. In this paper, we study self-admitted technical debt (SATD) in smart contracts. SATD refers to technical debt that is explicitly acknowledged in the source code by developers via code comments. We extract 726 k real-world contracts from Ethereum and apply both quantitative and qualitative methods in order to (i) determine SATD prevalence, (ii) understand the relationship between code cloning and SATD prevalence, and (iii) uncover the different categories of SATD. Our findings reveal that, while SATD is not a widespread phenomenon (1.5% of real-world contracts contain SATD), SATD does occur in extremely relevant contracts (e.g., multi-million contracts). We also observed a strong connection between SATD prevalence and code cloning activities, leading us to conclude that the former cannot be reliably studied without taking the latter into consideration. Finally, we produced a taxonomy for SATD that consists of 6 major and 26 minor categories. We note that several minor categories are bound to the domain of blockchain and smart contracts, including gas-inefficient implementations and Solidity-induced workarounds. Based on our results, we derive a set of practical recommendations for contract developers and introduce open research questions to guide future research on the topic.","Self-admitted technical debt,SATD,maintenance,smart contracts,ethereum,blockchain",以太坊智能合约中自我承认的技术债务：一项大规模探索性研究,以太坊等可编程区块链平台为应用程序开发提供了独特的好处，包括去中心化的基础设施、防篡改交易和可审计性。这些优势使新类型的应用程序能够为多个业务部门带来竞争优势。尽管如此，上市时间的压力，加上相对不成熟的开发技术（如Solidity编程语言）、缺乏高质量的培训资源以及以太坊的路线图不明确，造成了一种倾向于在应用程序代码中引入技术债务（如代码破解、变通方法和次优实现）的环境。在本文中，我们研究了智能合约中的自承认技术债务（SATD）。SATD指的是开发人员通过代码注释在源代码中明确承认的技术债务。我们从以太坊中提取726k份真实世界的合同，并应用定量和定性方法，以（i）确定SATD流行率，（ii）了解代码克隆和SATD流行之间的关系，以及（iii）揭示SATD的不同类别。我们的研究结果表明，虽然SATD不是一种普遍现象（1.5%的现实世界合同包含SATD），但SATD确实发生在极其相关的合同中（例如，数百万份合同）。我们还观察到SATD流行率与代码克隆活动之间有着密切的联系，这使我们得出结论，如果不考虑后者，就无法可靠地研究前者。最后，我们提出了SATD的分类法，包括6个大类和26个小类。我们注意到，区块链和智能合约领域有几个小类别，包括天然气低效实现和Solidity引发的变通方法。基于我们的研究结果，我们为合同开发人员提出了一套实用的建议，并引入了开放的研究问题，以指导未来对该主题的研究。,自行承认的技术债务，SATD，维护，智能合约，以太坊，区块链,,,
LBHELU5N,2023,https://doi.org/10.1109/TSE.2022.3162985,TSE 2023,Static Profiling of Alloy Models,"Modeling of software-intensive systems using formal declarative modeling languages offers a means of managing software complexity through the use of abstraction and early identification of correctness issues by formal analysis. Alloy is one such language used for modeling systems early in the development process. Little work has been done to study the styles and techniques commonly used in Alloy models. We present the first static analysis study of Alloy models. We investigate research questions that examine a large corpus of 1,652 Alloy models. To evaluate these research questions, we create a methodology that leverages the power of ANTLR pattern matching and the query language XPath. Our research questions are split into two categories depending on their purpose. The Model Characteristics category aims to identify what language constructs are used commonly. Modeling Practices questions are considerably more complex and identify how modelers are using Alloy's constructs. We also evaluate our research questions on a subset of models from our corpus written by expert modelers. We compare the results of the expert corpus to the results obtained from the general corpus to gain insight into how expert modelers use the Alloy language. We draw conclusions from the findings of our research questions and present actionable items for educators, language and environment designers, and tool developers. Actionable items for educators are intended to highlight underutilized language constructs and features, and help student modelers avoid discouraged practices. Actionable items aimed at language designers present ways to improve the Alloy language by adding constructs or removing unused ones based on trends identified in our corpus of models. The actionable items aimed at environment designers address features to facilitate model creation. Actionable items for tool developers provide suggestions for back-end optimizations.","Declarative modeling,Alloy,static analysis",合金模型的静态轮廓,使用形式声明性建模语言对软件密集型系统进行建模，通过使用抽象和通过形式分析早期识别正确性问题，提供了一种管理软件复杂性的方法。Alloy就是这样一种语言，用于在开发过程的早期对系统进行建模。研究合金模型中常用的样式和技术的工作很少。我们首次对合金模型进行了静态分析研究。我们调查了大量1652合金模型的研究问题。为了评估这些研究问题，我们创建了一种利用ANTLR模式匹配和查询语言XPath的能力的方法。我们的研究问题根据其目的分为两类。模型特征类别旨在确定哪些语言结构是常用的。建模实践问题要复杂得多，并确定建模人员是如何使用Alloy的构造的。我们还评估了由专家建模师编写的语料库中的一个子集模型的研究问题。我们将专家语料库的结果与一般语料库的结果进行比较，以深入了解专家建模师如何使用Alloy语言。我们从研究问题的发现中得出结论，并为教育工作者、语言和环境设计师以及工具开发人员提供可操作的项目。针对教育工作者的可操作项目旨在突出未充分利用的语言结构和功能，并帮助学生建模师避免不鼓励的做法。针对语言设计者的可操作项目提供了改进Alloy语言的方法，方法是根据模型语料库中确定的趋势添加结构或删除未使用的结构。针对环境设计者的可操作项目解决了促进模型创建的功能。工具开发人员的可操作项为后端优化提供了建议。,声明性建模，合金，静态分析,,,
UX3ZID34,2023,https://doi.org/10.1109/TSE.2022.3228851,TSE 2023,Operation-Based Refactoring-Aware Merging: An Empirical Evaluation,"Dealing with merge conflicts in version control systems is a challenging task for software developers. Resolving merge conflicts is a time-consuming and error-prone process, which distracts developers from important tasks. Recent work shows that refactorings are often involved in merge conflicts and that refactoring-related conflicts tend to be larger, making them harder to resolve. In the literature, there are two refactoring-aware merging techniques that claim to automatically resolve refactoring-related conflicts; however, these two techniques have never been empirically compared. In this paper, we present RefMerge, a rejuvenated Java-based design and implementation of the first technique, which is an operation-based refactoring-aware merging algorithm. We compare RefMerge to Git and the state-of-the-art graph-based refactoring-aware merging tool, IntelliMerge, on 2,001 merge scenarios with refactoring-related conflicts from 20 open-source projects. We find that RefMerge resolves or reduces conflicts in 497 (25%) merge scenarios while increasing conflicting LOC in only 214 (11%) scenarios. On the other hand, we find that IntelliMerge resolves or reduces conflicts in 478 (24%) merge scenarios but increases conflicting LOC in 597 (30%) merge scenarios. We additionally conduct a qualitative analysis of the differences between the three merging algorithms and provide insights of the strengths and weaknesses of each tool. We find that while IntelliMerge does well with ordering and formatting conflicts, it struggles with class-level refactorings and scenarios with several refactorings. On the other hand, RefMerge is resilient to the number of refactorings in a merge scenario, but we find that RefMerge introduces conflicts when inverting move-related refactorings.","Conflict resolution,refactoring,software merging,revision control systems",基于操作的重构感知合并：一个实证评估,处理版本控制系统中的合并冲突对软件开发人员来说是一项具有挑战性的任务。解决合并冲突是一个耗时且容易出错的过程，这会分散开发人员对重要任务的注意力。最近的工作表明，重构经常涉及合并冲突，而且与重构相关的冲突往往更大，更难解决。在文献中，有两种支持重构的合并技术声称可以自动解决与重构相关的冲突；然而，这两种技术从未进行过实证比较。在本文中，我们提出了RefMerge，这是第一种技术的一种基于Java的设计和实现，这是一种基于操作的重构感知合并算法。我们将RefMerge与Git以及最先进的基于图的重构感知合并工具IntelliMerge进行了比较，比较了2001年20个开源项目中与重构相关冲突的合并场景。我们发现，RefMerge在497个（25%）合并场景中解决或减少了冲突，而在只有214个（11%）场景中增加了冲突LOC。另一方面，我们发现IntelliMerge在478（24%）个合并场景中解决或减少了冲突，但在597（30%）个合并方案中增加了冲突LOC。此外，我们对三种合并算法之间的差异进行了定性分析，并深入了解了每种工具的优势和劣势。我们发现，虽然IntelliMerge在排序和格式冲突方面做得很好，但它在类级重构和多次重构的场景中却很吃力。另一方面，RefMerge对合并场景中的重构数量具有弹性，但我们发现RefMerge在反转与移动相关的重构时会引入冲突。,冲突解决，重构，软件合并，修订控制系统,,,
H3HQRPEI,2023,https://doi.org/10.1109/TSE.2023.3279570,TSE 2023,Generalized Coverage Criteria for Combinatorial Sequence Testing,"We present a new model-based approach for testing systems that use sequences of actions and assertions as test vectors. Our solution includes a method for quantifying testing quality, a tool for generating high-quality test suites based on the coverage criteria we propose, and a framework for assessing risks. For testing quality, we propose a method that specifies generalized coverage criteria over sequences of actions, which extends previous approaches. Our publicly available tool demonstrates how to extract effective test suites from test plans based on these criteria. We also present a Bayesian approach for measuring the probabilities of bugs or risks, and show how this quantification can help achieve an informed balance between exploitation and exploration in testing. Finally, we provide an empirical evaluation demonstrating the effectiveness of our tool in finding bugs, assessing risks, and achieving coverage.","Bayesian risk-Reduction,behavioral programming,combinatorial test design,model-based testing,sequence testing,test coverage,test generation,test optimization",组合序列测试的广义覆盖准则,我们提出了一种新的基于模型的方法来测试系统，该方法使用动作和断言序列作为测试向量。我们的解决方案包括一种量化测试质量的方法，一种基于我们提出的覆盖标准生成高质量测试套件的工具，以及一个评估风险的框架。为了测试质量，我们提出了一种方法，该方法指定了动作序列的广义覆盖标准，这扩展了以前的方法。我们的公开工具演示了如何根据这些标准从测试计划中提取有效的测试套件。我们还提出了一种用于测量错误或风险概率的贝叶斯方法，并展示了这种量化如何有助于在测试中实现开发和探索之间的知情平衡。最后，我们提供了一个实证评估，证明了我们的工具在发现漏洞、评估风险和实现覆盖方面的有效性。,贝叶斯风险降低，行为编程，组合测试设计，基于模型的测试，序列测试，测试覆盖率，测试生成，测试优化,,,
9YWDIEKD,2023,https://doi.org/10.1109/TSE.2022.3216683,TSE 2023,What Petri Nets Oblige us to Say Comparing Approaches for Behavior Composition,"We identify and demonstrate a weakness of Petri Nets (PN) in specifying composite behavior of reactive systems. Specifically, we show how, when specifying multiple requirements in one PN model, modelers are obliged to specify mechanisms for combining these requirements. This yields, in many cases, over-specification and incorrect models. We demonstrate how some execution paths are missed, and some are generated unintentionally. To support this claim, we analyze PN models from the literature, identify the combination mechanisms, and demonstrate their effect on the correctness of the model. To address this problem, we propose to model the system behavior using behavioral programming (BP), a software development and modeling paradigm designed for seamless integration of independent requirements. Specifically, we demonstrate how the semantics of BP, which define how to interweave scenarios into a single model, allow for avoiding the over-specification. Additionally, while BP maintains the same mathematical properties as PN, it provides means for changing the model dynamically, thus increasing the agility of the specification. We compare BP and PN in quantitative and qualitative measures by analyzing the models, their generated execution paths, and the specification process. Finally, while BP is supported by tools that allow for applying formal methods and reasoning techniques to the model, it lacks the legacy of PN tools and algorithms. To address this issue, we propose semantics and a tool for translating BP models to PN and vice versa.","Petri nets,behavioral programming,linguistic relativity",Petri网让我们不得不说什么行为组合的比较方法,我们发现并证明了Petri网（PN）在指定反应系统的复合行为方面的一个弱点。具体来说，我们展示了当在一个PN模型中指定多个需求时，建模人员必须指定组合这些需求的机制。在许多情况下，这会导致过度规范和错误的模型。我们展示了一些执行路径是如何被遗漏的，还有一些是无意生成的。为了支持这一说法，我们分析了文献中的PN模型，确定了组合机制，并证明了它们对模型正确性的影响。为了解决这个问题，我们建议使用行为编程（BP）对系统行为进行建模，这是一种软件开发和建模范式，旨在无缝集成独立需求。具体来说，我们展示了BP的语义如何避免过度规范，BP定义了如何将场景交织成一个单一的模型。此外，虽然BP保持了与PN相同的数学特性，但它提供了动态更改模型的方法，从而提高了规范的灵活性。我们通过分析模型、生成的执行路径和规范过程，在定量和定性测量方面比较了BP和PN。最后，虽然BP得到了允许将形式化方法和推理技术应用于模型的工具的支持，但它缺乏PN工具和算法的遗留问题。为了解决这个问题，我们提出了将BP模型转换为PN的语义和工具，反之亦然。,Petri网，行为编程，语言相关性,,,
4MHPFUQK,2023,https://doi.org/10.1109/TSE.2023.3313645,TSE 2023,Fast Parametric Model Checking With Applications to Software Performability Analysis,"We present an efficient parametric model checking technique for the analysis of software performability, i.e., of the performance and dependability properties of software systems. The new parametric model checking (pMC) technique works by using a heuristic to automatically decompose a parametric discrete-time Markov chain (pDTMC) model of the software system under verification into fragments that can be analysed independently, yielding results that are then combined to establish the required software performability properties. Our fast parametric model checking (fPMC) technique enables the formal analysis of software systems modelled by pDTMCs that are too complex to be handled by existing pMC methods. Furthermore, for many pDTMCs that state-of-the-art parametric model checkers can analyse, fPMC produces solutions (i.e., algebraic formulae) that are simpler and much faster to evaluate. We show experimentally that adding fPMC to the existing repertoire of pMC methods improves the efficiency of parametric model checking significantly, and extends its applicability to software systems with more complex behaviour than currently possible.","Parametric model checking,software performability,nonfunctional software properties,Markov models",快速参数模型检验及其在软件性能分析中的应用,我们提出了一种有效的参数模型检查技术，用于分析软件的可执行性，即软件系统的性能和可靠性特性。新的参数模型检查（pMC）技术通过使用启发式方法将被验证软件系统的参数离散时间马尔可夫链（pDTMC）模型自动分解为可以独立分析的片段，产生结果，然后将其组合以建立所需的软件可执行性特性。我们的快速参数模型检查（fPCM）技术能够对由pDTMC建模的软件系统进行形式化分析，这些系统过于复杂，无法用现有的pMC方法处理。此外，对于最先进的参数模型检查器可以分析的许多pDTMC，fPMC产生更简单、更快评估的解决方案（即代数公式）。我们通过实验表明，在现有的pMC方法中添加fPMC显著提高了参数模型检查的效率，并将其适用性扩展到具有比目前更复杂行为的软件系统。,参数模型检查，软件性能，非功能软件属性，马尔可夫模型,,,
5G9YMFKG,2023,https://doi.org/10.1109/TSE.2023.3237247,TSE 2023,Test Report Generation for Android App Testing Via Heterogeneous Data Analysis,"The rising of the Android market demands higher quality assurance of Android applications (apps) to sharpen the competitive edge, and techniques for traditional software have problems adapting for mobile apps. Android apps often require testing on a large-scale device cluster, which produces a large amount of test reports consisting of heterogeneous data, e.g., hardware information, GUI screenshots, runtime logs. Such data are hard to merge to be unified analyzed, while they serve as an essential basis for bug inspection and fixing. Existing test report generation or analysis techniques can only handle testing data from different devices separately. They simply list all the information to app developers and have no further processing to summarize test reports. Besides, they neglect the inner connection of the heterogeneous data. Such techniques cannot improve the report reviewing effectiveness and efficiency, and they can hardly find the inner links and rules of the bug occurrence on different devices. As a result, developers still need to devote many efforts to inspect and fix bugs. In this paper, a large amount of test reports are investigated by the authors, as to construct a structured bug model to analyze heterogeneous data of the testing results. According to the investigation, we also define the Bug Inconsistency of testing results from multiple devices and build a novel bug taxonomy. In general, an automated approach is proposed to generate structured and comprehensible test reports from raw testing results from multiple devices. Based on the approach, a tool, namely BreGat, is implemented to evaluate the classification and deduplication capability of our approach. The experimental results of 30 Android apps on 20 devices show that BreGat can successfully cover 83% bug categories and exclude 76% duplicate bugs. Furthermore, a user study involving 16 developers shows that our test reports are more comprehensible and BreGat greatly improves the bug inspection efficiency compared to the state-of-the-art tool.","Android testing,GUI testing,test report generation",基于异构数据分析的Android应用测试报告生成,安卓市场的兴起要求安卓应用程序的质量保证更高，以提高竞争优势，而传统软件的技术在适应移动应用程序方面存在问题。安卓应用程序通常需要在大型设备集群上进行测试，该集群会生成大量由异构数据组成的测试报告，例如硬件信息、GUI屏幕截图、运行时日志。这些数据很难合并以进行统一分析，同时它们是错误检查和修复的重要基础。现有的测试报告生成或分析技术只能单独处理来自不同设备的测试数据。他们只是向应用程序开发人员列出所有信息，没有进一步的处理来总结测试报告。此外，他们忽略了异构数据之间的内在联系。这样的技术无法提高报告审查的有效性和效率，也很难找到错误在不同设备上发生的内在联系和规律。因此，开发人员仍然需要投入大量精力来检查和修复错误。在本文中，作者对大量的测试报告进行了研究，以构建一个结构化的bug模型来分析测试结果的异构数据。根据调查，我们还定义了多台设备测试结果的Bug不一致性，并建立了一个新的Bug分类法。通常，提出了一种自动化方法，从多个设备的原始测试结果生成结构化和可理解的测试报告。基于该方法，实现了一个工具，即BreGat，以评估我们方法的分类和重复数据消除能力。30款安卓应用在20台设备上的实验结果显示，BreGat可以成功覆盖83%的bug类别，排除76%的重复bug。此外，一项涉及16名开发人员的用户研究表明，我们的测试报告更容易理解，与最先进的工具相比，BreGat大大提高了错误检查效率。,Android测试，GUI测试，测试报告生成,,,
Z6MAKW2F,2023,https://doi.org/10.1109/TSE.2022.3201209,TSE 2023,"Flakify: A Black-Box, Language Model-Based Predictor for Flaky Tests","Software testing assures that code changes do not adversely affect existing functionality. However, a test case can be flaky, i.e., passing and failing across executions, even for the same version of the source code. Flaky test cases introduce overhead to software development as they can lead to unnecessary attempts to debug production or testing code. Besides rerunning test cases multiple times, which is time-consuming and computationally expensive, flaky test cases can be predicted using machine learning (ML) models, thus reducing the wasted cost of re-running and debugging these test cases. However, the state-of-the-art ML-based flaky test case predictors rely on pre-defined sets of features that are either project-specific, i.e., inapplicable to other projects, or require access to production code, which is not always available to software test engineers. Moreover, given the non-deterministic behavior of flaky test cases, it can be challenging to determine a complete set of features that could potentially be associated with test flakiness. Therefore, in this article, we propose Flakify, a black-box, language model-based predictor for flaky test cases. Flakify relies exclusively on the source code of test cases, thus not requiring to (a) access to production code (black-box), (b) rerun test cases, (c) pre-define features. To this end, we employed CodeBERT, a pre-trained language model, and fine-tuned it to predict flaky test cases using the source code of test cases. We evaluated Flakify on two publicly available datasets (FlakeFlagger and IDoFT) for flaky test cases and compared our technique with the FlakeFlagger approach, the best state-of-the-art ML-based, white-box predictor for flaky test cases, using two different evaluation procedures: (1) cross-validation and (2) per-project validation, i.e., prediction on new projects. Flakify achieved F1-scores of 79% and 73% on the FlakeFlagger dataset using cross-validation and per-project validation, respectively. Similarly, Flakify achieved F1-scores of 98% and 89% on the IDoFT dataset using the two validation procedures, respectively. Further, Flakify surpassed FlakeFlagger by 10 and 18 percentage points (pp) in terms of precision and recall, respectively, when evaluated on the FlakeFlagger dataset, thus reducing the cost bound to be wasted on unnecessarily debugging test cases and production code by the same percentages (corresponding to reduction rates of 25% and 64%). Flakify also achieved significantly higher prediction results when used to predict test cases on new projects, suggesting better generalizability over FlakeFlagger. Our results further show that a black-box version of FlakeFlagger is not a viable option for predicting flaky test cases.","Flaky tests,software testing,black-box testing,natural language processing,CodeBERT",Flakify：一个用于Flaky测试的基于语言模型的黑盒预测器,软件测试确保代码更改不会对现有功能产生不利影响。然而，测试用例可能是不稳定的，即，即使对于同一版本的源代码，也会在执行之间通过和失败。缺陷测试用例会给软件开发带来开销，因为它们可能导致不必要的调试生产或测试代码的尝试。除了多次重新运行测试用例（耗时且计算昂贵）之外，还可以使用机器学习（ML）模型预测片状测试用例，从而减少重新运行和调试这些测试用例的浪费成本。然而，最先进的基于ML的薄片测试用例预测器依赖于预定义的功能集，这些功能集要么是特定于项目的，即不适用于其他项目，要么需要访问生产代码，而软件测试工程师并不总是可以使用这些代码。此外，考虑到片状测试用例的非确定性行为，确定可能与测试片状性相关的一整套特征可能具有挑战性。因此，在这篇文章中，我们提出了Flakify，一个用于薄片测试用例的黑匣子、基于语言模型的预测器。Flakify完全依赖于测试用例的源代码，因此不需要（a）访问生产代码（黑匣子），（b）重新运行测试用例，（c）预定义特性。为此，我们使用了CodeBERT，一个预先训练的语言模型，并使用测试用例的源代码对其进行了微调，以预测不稳定的测试用例。我们在两个公开可用的数据集（FlakeFlagger和IDoFT）上对薄片测试用例的Flakify进行了评估，并使用两种不同的评估程序将我们的技术与FlakeFlager方法进行了比较，后者是最先进的基于ML的薄片测试用例白盒预测器：（1）交叉验证和（2）每个项目的验证，即对新项目的预测。使用交叉验证和每个项目验证，Flakify在FlakeFlagger数据集上的F1得分分别为79%和73%。同样，使用两种验证程序，Flakify在IDoFT数据集上的F1得分分别为98%和89%。此外，在FlakeFlagger数据集上进行评估时，Flakify在精度和召回率方面分别超过了FlakeFlager 10和18个百分点，从而将不必要地调试测试用例和生产代码的成本降低了相同的百分比（对应于25%和64%的降低率）。当用于预测新项目的测试用例时，Flakify也获得了显著更高的预测结果，这表明它比FlakeFlagger具有更好的可推广性。我们的结果进一步表明，FlakeFlagger的黑匣子版本不是预测片状测试用例的可行选择。,Flaky测试，软件测试，黑盒测试，自然语言处理，CodeBERT,,,
3UMJF2RF,2023,https://doi.org/10.1109/TSE.2022.3174028,TSE 2023,Enhancing Mobile App Bug Reporting via Real-Time Understanding of Reproduction Steps,"One of the primary mechanisms by which developers receive feedback about in-field failures of software from users is through bug reports. Unfortunately, the quality of manually written bug reports can vary widely due to the effort required to include essential pieces of information, such as detailed reproduction steps (S2Rs). Despite the difficulty faced by reporters, few existing bug reporting systems attempt to offer automated assistance to users in crafting easily readable, and conveniently reproducible bug reports. To address the need for proactive bug reporting systems that actively aid the user in capturing crucial information, we introduce a novel bug reporting approach called EBug. EBug assists reporters in writing S2Rs for mobile applications by analyzing natural language information entered by reporters in real-time, and linking this data to information extracted via a combination of static and dynamic program analyses. As reporters write S2Rs, EBug is capable of automatically suggesting potential future steps using predictive models trained on realistic app usages. To evaluate EBug, we performed two user studies based on 20 failures from 11 real-world apps. The empirical studies involved ten participants that submitted ten bug reports each and ten developers that reproduced the submitted bug reports. In the studies, we found that reporters were able to construct bug reports 31% faster with EBug as compared to the state-of-the-art bug reporting system used as a baseline. EBug's reports were also more reproducible with respect to the ones generated with the baseline. Furthermore, we compared EBug's prediction models to other predictive modeling approaches and found that, overall, the predictive models of our approach outperformed the baseline approaches. Our results are promising and demonstrate the feasibility and potential benefits provided by proactively assistive bug reporting systems.","Bug reporting,language modeling,mobile apps,natural language processing",通过实时了解复制步骤增强移动应用程序错误报告,开发人员从用户那里接收关于软件现场故障的反馈的主要机制之一是通过错误报告。不幸的是，手动编写的错误报告的质量可能会有很大的差异，因为需要努力包括重要的信息，例如详细的复制步骤（S2R）。尽管报告者面临困难，但很少有现有的错误报告系统试图为用户提供自动化帮助，帮助他们制作易读、可方便复制的错误报告。为了满足对主动帮助用户捕获关键信息的主动错误报告系统的需求，我们引入了一种称为EBug的新型错误报告方法。EBug通过实时分析记者输入的自然语言信息，并将这些数据与通过静态和动态程序分析相结合提取的信息链接，帮助记者为移动应用程序编写S2R。当记者撰写S2R时，EBug能够使用根据实际应用程序使用情况训练的预测模型自动建议未来的潜在步骤。为了评估EBug，我们基于11个真实世界应用程序的20个失败进行了两项用户研究。实证研究涉及10名参与者，他们每人提交了10份错误报告，10名开发人员复制了提交的错误报告。在研究中，我们发现，与用作基线的最先进的错误报告系统相比，报告者使用EBug构建错误报告的速度快31%。EBug的报告也比使用基线生成的报告更具可重复性。此外，我们将EBug的预测模型与其他预测建模方法进行了比较，发现总体而言，我们方法的预测模型优于基线方法。我们的结果很有希望，并证明了主动辅助错误报告系统的可行性和潜在好处。,错误报告，语言建模，移动应用程序，自然语言处理,,,
4DS5L7XL,2023,https://doi.org/10.1109/TSE.2022.3170087,TSE 2023,Applying Human Values Theory to Software Engineering Practice: Lessons and Implications,"The study of human values in software engineering (SE) is increasingly recognised as a fundamental human-centric issue of SE decision making. However, values studies in SE still face a number of issues, including the difficulty of eliciting values in a systematic and structured way, the challenges of measuring and tracking values over time, and the lack of practice-based understanding of values among software practitioners. This paper aims to help address these issues by: 1) outlining a research framework that supports a systematic approach to values elicitation, analysis, and understanding; 2) introducing tools and techniques that help elicit and measure values during SE decision making processes in a systematic way; and 3) applying such tools to a month-long research sprint co-designed with an industry partner and conducted with 27 software practitioners. The case study builds on lessons from an earlier pilot (12 participants) and combines in-situ observations with the use of two values-informed tools: the Values Q-Sort (V-QS), and the Values-Retro. The V-QS adapts instruments from values research to the SE context, the Values-Retro adapts existing SE techniques to values theory. We distil implications for research and practice in ten lessons learned.","Software engineering,human values in software practice,human-centric software engineering,case studies of practice,ethics,non-functional requirements,responsible innovations,agile retrospectives,mixed-methods",人的价值理论在软件工程实践中的应用：经验与启示,软件工程中的人的价值观研究越来越被认为是软件工程决策中一个以人为中心的基本问题。然而，SE中的价值观研究仍然面临着许多问题，包括难以以系统和结构化的方式引出价值观，随着时间的推移测量和跟踪价值观的挑战，以及软件从业者对价值观缺乏基于实践的理解。本文旨在通过以下方式帮助解决这些问题：1）概述一个研究框架，该框架支持价值观启发、分析和理解的系统方法；2） 引入有助于在SE决策过程中以系统的方式引出和衡量价值的工具和技术；以及3）将这些工具应用于与行业合作伙伴共同设计并与27名软件从业者进行的为期一个月的研究sprint。该案例研究以早期试点（12名参与者）的经验教训为基础，将现场观察与两种价值知情工具的使用相结合：价值Q排序（V-QS）和价值回溯。V-QS将价值观研究的工具适应SE环境，而values Retro则将现有的SE技术适应价值观理论。我们从十个经验教训中提炼出对研究和实践的启示。,软件工程，软件实践中的人的价值观，以人为中心的软件工程，实践案例研究，伦理，非功能需求，负责任的创新，敏捷回顾，混合方法,,,
ZEZL66BD,2023,https://doi.org/10.1109/TSE.2023.3244123,TSE 2023,Dependent or Not: Detecting and Understanding Collections of Refactorings,"Refactoring is a program transformation to improve the internal structure of a program while preserving its external behavior. Developers frequently apply multiple refactorings that depend on each other to achieve goals such as improving code reusability. Although manually applying a sequence of dependent refactorings is a common practice, existing refactoring recommendation tools treat refactorings in isolation without revealing the dependencies among them to developers. One reason is that these relationships among refactorings are poorly understood. Current approaches treat refactoring recommendations as a strictly ordered sequence limiting developers’ ability to understand, validate, and apply recommended refactorings. To address this gap, this paper describes a theory for reasoning about collections of refactorings through defining an ordering dependency relation among refactorings and organizing collection of refactorings as a set of refactoring graphs. We propose an algorithm for identifying refactoring dependencies and illustrate these concepts with a tool for visualizing such refactoring dependencies and refactoring graphs. Our validation results demonstrate that 43% of the 1,457,873 recommended refactorings from 9,595 projects that we studied are part of dependent refactoring graphs. Furthermore, refactorings are not only commonly involved in dependent relations, but also when applied, dependent refactoring graphs improve all of the quality attribute metrics in our experiments more than individual refactorings.","Dependency,refactoring,search based software engineering",依赖与否：检测和理解重构集合,重构是一种程序转换，用于改进程序的内部结构，同时保留其外部行为。开发人员经常应用相互依赖的多个重构来实现诸如提高代码可重用性之类的目标。尽管手动应用一系列依赖重构是一种常见的做法，但现有的重构推荐工具将重构孤立地处理，而不会向开发人员揭示它们之间的依赖关系。其中一个原因是人们对重构之间的这些关系知之甚少。目前的方法将重构建议视为一个严格有序的序列，限制了开发人员理解、验证和应用推荐重构的能力。为了解决这一差距，本文描述了一种通过定义重构之间的排序依赖关系并将重构集合组织为一组重构图来推理重构集合的理论。我们提出了一种识别重构依赖关系的算法，并用一个可视化重构依赖关系和重构图的工具来说明这些概念。我们的验证结果表明，在我们研究的9595个项目中，1457873个推荐重构中，43%是依赖重构图的一部分。此外，重构不仅通常涉及依赖关系，而且在应用时，依赖重构图比单独的重构更能改善我们实验中的所有质量属性指标。,依赖性，重构，基于搜索的软件工程,,,
Q96PEVTQ,2023,https://doi.org/10.1109/TSE.2022.3149586,TSE 2023,"Impact of Usability Mechanisms: A Family of Experiments on Efficiency, Effectiveness and User Satisfaction","Context: The usability software quality characteristic aims to improve system user performance. In a previous study, we found evidence of the impact of a set of usability features from the viewpoint of users in terms of efficiency, effectiveness and satisfaction. However, the impact level appears to depend on the usability feature and suggest priorities with respect to their implementation depending on how they promote user performance. Objectives: We use a family of three experiments to increase the precision and generalization of the results in the baseline experiment and provide findings regarding the impact on user performance of the Abort Operation, Progress Feedback and Preferences usability mechanisms. Method: We conduct two replications of the baseline experiment in academic settings. We analyse the data of 366 experimental subjects and apply aggregation (meta-analysis) procedures. Results: We find that the Abort Operation and Preferences usability mechanisms appear to improve system usability a great deal with respect to efficiency, effectiveness and user satisfaction. Conclusions: We find that the family of experiments further corroborates the results of the baseline experiment. Most of the results are statistically significant, and, because of the large number of experimental subjects, the evidence that we gathered in the replications is sufficient to outweigh other experiments.","Usability mechanism,efficiency,effectiveness,satisfaction,experimental software engineering,family of experiments",可用性机制的影响：一系列关于效率、有效性和用户满意度的实验,上下文：可用性软件的质量特征旨在提高系统用户的性能。在之前的一项研究中，我们从用户的角度发现了一组可用性特征在效率、有效性和满意度方面的影响。然而，影响级别似乎取决于可用性特性，并根据其如何提高用户性能来建议其实现的优先级。目的：我们使用三个实验家族来提高基线实验结果的准确性和通用性，并提供关于中止操作、进度反馈和偏好可用性机制对用户性能的影响的结果。方法：我们在学术环境中进行了两次基线实验的复制。我们分析了366名实验受试者的数据，并应用汇总（荟萃分析）程序。结果：我们发现中止操作和偏好可用性机制在效率、有效性和用户满意度方面似乎大大提高了系统的可用性。结论：我们发现实验家族进一步证实了基线实验的结果。大多数结果都具有统计学意义，而且由于实验对象数量众多，我们在复制中收集的证据足以超过其他实验。,可用性机制，效率，有效性，满意度，实验软件工程，实验家族,,,
NVH29537,2023,https://doi.org/10.1109/TSE.2023.3313875,TSE 2023,A Grounded Theory of Cross-Community SECOs: Feedback Diversity Versus Synchronization,"Despite their proliferation, growing sustainable software ecosystems (SECOs) remains a substantial challenge. One approach to mitigate this challenge is by collecting and integrating feedback from distributors (distros) and end-users of the SECO releases into future SECO releases, tools, or policies. This paper performs a socio-technical analysis of cross-community collaboration in the OpenStack SECO, which consists of the upstream OpenStack project and 21 distribution (distro) communities. First, we followed Masood et al.'s adaptation of Strauss-Corbinian grounded theory methodology for socio-technical contexts on data from an open-ended unstructured interview, a survey, focus groups, and 384 mailing list threads to investigate how SECOs manage to sustain cross-community collaboration. Our theory has 15 constructs divided into four categories: diverse feedback types and mechanisms (2), characteristics of feedback (2), challenges (7), and the benefits (4) of cross-community collaboration. We then empirically study the salient aspects of the theory, i.e., diversity and synchronization, among 21 OpenStack distros. We empirically mined feedback that distros contribute to upstream, i.e., 140,261 mailing list threads, 142,914 bugs reported, 65,179 bugs resolved, and 4,349 new features. Then, we use influence maximization social network analysis to model the synchronization of feedback in the OpenStack SECO. Our results suggest that distros contribute substantially towards the sustainability of the SECO in the form of 25.6% of new features, 30.7% of emails, 44.3% of bug reports, and 30.7% of bug fixes. Finally, we found evidence of distros playing different roles in a SECO, with nine distros contributing all four types of feedback in equal proportions, while 12 distros specialize in one type of feedback. Distros that are influential in propagating a given type of feedback to the SECO community are not necessarily specialized in that feedback type.","Grounded theory,software ecosystem,cross-community,feedback diversity,sustainability,influence maximization",跨社区SECO的基础理论：反馈多样性与同步性,尽管可持续软件生态系统（SECO）数量激增，但其发展仍然是一个巨大的挑战。缓解这一挑战的一种方法是收集来自SECO发布的分销商（发行版）和最终用户的反馈，并将其集成到未来的SECO发布、工具或政策中。本文对OpenStack SECO中的跨社区协作进行了社会技术分析，该组织由上游OpenStack项目和21个分发（distro）社区组成。首先，我们遵循Masood等人对Strauss-Corbinian基于社会技术背景的理论方法的改编，从开放式非结构化访谈、调查、焦点小组和384个邮件列表线程中获得数据，以调查SECO如何管理维持跨社区合作。我们的理论有15个结构，分为四类：不同的反馈类型和机制（2），反馈的特征（2）、挑战（7）和跨社区合作的好处（4）。然后，我们在21个OpenStack发行版中实证研究了该理论的显著方面，即多样性和同步性。我们根据经验挖掘了发行版对上游的贡献，即140261个邮件列表线程，142914个错误报告，65179个错误解决，4349个新功能。然后，我们使用影响力最大化社交网络分析来对OpenStack SECO中的反馈同步进行建模。我们的研究结果表明，发行版以25.6%的新功能、30.7%的电子邮件、44.3%的错误报告和30.7%的错误修复的形式对SECO的可持续性做出了重大贡献。最后，我们发现了发行版在SECO中扮演不同角色的证据，9个发行版以同等比例贡献了所有四种类型的反馈，而12个发行版专门负责一种类型的回馈。在向SECO社区传播特定类型反馈方面有影响力的Distros不一定专门从事该反馈类型。,扎根理论，软件生态系统，跨社区，反馈多样性，可持续性，影响力最大化,,,
IFGRBT8F,2023,https://doi.org/10.1109/TSE.2023.3305244,TSE 2023,VulExplainer: A Transformer-Based Hierarchical Distillation for Explaining Vulnerability Types,"Deep learning-based vulnerability prediction approaches are proposed to help under-resourced security practitioners to detect vulnerable functions. However, security practitioners still do not know what type of vulnerabilities correspond to a given prediction (aka CWE-ID). Thus, a novel approach to explain the type of vulnerabilities for a given prediction is imperative. In this paper, we propose VulExplainer, an approach to explain the type of vulnerabilities. We represent VulExplainer as a vulnerability classification task. However, vulnerabilities have diverse characteristics (i.e., CWE-IDs) and the number of labeled samples in each CWE-ID is highly imbalanced (known as a highly imbalanced multi-class classification problem), which often lead to inaccurate predictions. Thus, we introduce a Transformer-based hierarchical distillation for software vulnerability classification in order to address the highly imbalanced types of software vulnerabilities. Specifically, we split a complex label distribution into sub-distributions based on CWE abstract types (i.e., categorizations that group similar CWE-IDs). Thus, similar CWE-IDs can be grouped and each group will have a more balanced label distribution. We learn TextCNN teachers on each of the simplified distributions respectively, however, they only perform well in their group. Thus, we build a transformer student model to generalize the performance of TextCNN teachers through our hierarchical knowledge distillation framework. Through an extensive evaluation using the real-world 8,636 vulnerabilities, our approach outperforms all of the baselines by 5%–29%. The results also demonstrate that our approach can be applied to Transformer-based architectures such as CodeBERT, GraphCodeBERT, and CodeGPT. Moreover, our method maintains compatibility with any Transformer-based model without requiring any architectural modifications but only adds a special distillation token to the input. These results highlight our significant contributions towards the fundamental and practical problem of explaining software vulnerability.","Software vulnerability,software security",VulExplainer：一种用于解释漏洞类型的基于转换器的分层蒸馏,提出了基于深度学习的漏洞预测方法，以帮助资源不足的安全从业者检测漏洞功能。然而，安全从业者仍然不知道什么类型的漏洞对应于给定的预测（也称为CWE-ID）。因此，必须采用一种新的方法来解释给定预测的漏洞类型。在本文中，我们提出了VulExplainer，一种解释漏洞类型的方法。我们将VulExplainer表示为一个漏洞分类任务。然而，漏洞具有不同的特征（即CWE-ID），并且每个CWE-ID中的标记样本数量高度不平衡（称为高度不平衡的多类分类问题），这往往导致预测不准确。因此，我们引入了一种基于Transformer的软件漏洞分类分层蒸馏，以解决高度不平衡的软件漏洞类型。具体来说，我们根据CWE抽象类型（即对相似CWE ID进行分组的分类）将复杂的标签分布拆分为子分布。因此，可以对类似的CWE ID进行分组，并且每个组将具有更平衡的标签分布。我们分别在每个简化分布上学习TextCNN教师，然而，他们只在小组中表现良好。因此，我们建立了一个转换学生模型，通过我们的分层知识提取框架来概括TextCNN教师的表现。通过使用真实世界中的8636个漏洞进行广泛评估，我们的方法比所有基线都好5%-29%。结果还表明，我们的方法可以应用于基于Transformer的体系结构，如CodeBERT、GraphCodeBERT和CodeGPT。此外，我们的方法在不需要任何架构修改的情况下保持与任何基于Transformer的模型的兼容性，但只向输入添加了一个特殊的蒸馏令牌。这些结果突出了我们对解释软件漏洞这一基本和实际问题的重大贡献。,软件漏洞，软件安全,,,
DHQSITAN,2023,https://doi.org/10.1109/TSE.2022.3158252,TSE 2023,GPT2SP: A Transformer-Based Agile Story Point Estimation Approach,"Story point estimation is a task to estimate the overall effort required to fully implement a product backlog item. Various estimation approaches (e.g., Planning Poker, Analogy, and expert judgment) are widely-used, yet they are still inaccurate and may be subjective, leading to ineffective sprint planning. Recent work proposed Deep-SE, a deep learning-based Agile story point estimation approach, yet it is still inaccurate, not transferable to other projects, and not interpretable. In this paper, we propose GPT2SP, a Transformer-based Agile Story Point Estimation approach. Our GPT2SP employs a GPT-2 pre-trained language model with a GPT-2 Transformer-based architecture, allowing our GPT2SP models to better capture the relationship among words while considering the context surrounding a given word and its position in the sequence and be transferable to other projects, while being interpretable. Through an extensive evaluation on 23,313 issues that span across 16 open-source software projects with 10 existing baseline approaches for within- and cross-project scenarios, our results show that our GPT2SP approach achieves a median MAE of 1.16, which is (1) 34%-57% more accurate than existing baseline approaches for within-project estimations; (2) 39%-49% more accurate than existing baseline approaches for cross-project estimations. The ablation study also shows that the GPT-2 architecture used in our approach substantially improves Deep-SE by 6%-47%, highlighting the significant advancement of the AI for Agile story point estimation. Finally, we develop a proof-of-concept tool to help practitioners better understand the most important words that contributed to the story point estimation of the given issue with the best supporting examples from past estimates. Our survey study with 16 Agile practitioners shows that the story point estimation task is perceived as an extremely challenging task. In addition, our AI-based story point estimation with explanations is perceived as more useful and trustworthy than without explanations, highlighting the practical need of our Explainable AI-based story point estimation approach.","Agile story point estimation,AI for SE,explainable AI",GPT2SP：一种基于Transformer的敏捷故事点估计方法,故事点估计是一项任务，用于估计完全实现产品积压项目所需的总体工作量。各种估计方法（例如，Planning Poker、Analogy和专家判断）被广泛使用，但它们仍然不准确，可能是主观的，导致短跑计划无效。最近的工作提出了Deep SE，这是一种基于深度学习的敏捷故事点估计方法，但它仍然不准确，不能转移到其他项目，也不可解释。在本文中，我们提出了GPT2SP，一种基于Transformer的敏捷故事点估计方法。我们的GPT2SP采用了GPT-2预训练语言模型和基于GPT-2 Transformer的架构，使我们的GPT2SP模型能够更好地捕捉单词之间的关系，同时考虑给定单词周围的上下文及其在序列中的位置，并可转移到其他项目，同时具有可解释性。通过对跨越16个开源软件项目的23313个问题的广泛评估，以及针对项目内和跨项目场景的10种现有基线方法，我们的结果表明，我们的GPT2SP方法实现了1.16的中值MAE，这（1）对于项目内估计，比现有基线方法准确34%-57%；（2） 跨项目估计的准确率比现有的基线方法高39%-49%。消融研究还表明，我们方法中使用的GPT-2架构将Deep SE显著提高了6%-47%，突出了AI在敏捷故事点估计方面的显著进步。最后，我们开发了一个概念验证工具，通过过去估计的最佳支持示例，帮助从业者更好地理解对给定问题的故事点估计有贡献的最重要的单词。我们对16名敏捷从业者的调查研究表明，故事点估计任务被认为是一项极具挑战性的任务。此外，我们基于人工智能的有解释的故事点估计被认为比没有解释的更有用、更值得信赖，这突出了我们基于可解释人工智能的故事点评估方法的实际需求。,敏捷的故事点估计，AI for SE，可解释的AI,,,
IPPW7MBL,2023,https://doi.org/10.1109/TSE.2022.3163576,TSE 2023,"On the Variability of Software Engineering Needs for Deep Learning: Stages, Trends, and Application Types","The wide use of Deep learning (DL) has not been followed by the corresponding advances in software engineering (SE) for DL. Research shows that developers writing DL software have specific development stages (i.e., SE4DL stages) and face new DL-specific problems. Despite substantial research, it is unclear how DL developers’ SE needs for DL vary over stages, application types, or if they change over time. To help focus research and development efforts on DL-development challenges, we analyze 92,830 Stack Overflow (SO) questions and 227,756 READMEs of public repositories related to DL. Latent Dirichlet Allocation (LDA) reveals 27 topics for the SO questions where 19 (70.4%) topics mainly relate to a single SE4DL stage, and eight topics span multiple stages. Most questions concern Data Preparation and Model Setup stages. The relative rates of questions for 11 topics have increased, for eight topics decreased over time. Questions for the former 11 topics had a lower percentage of accepting an answer than the remaining questions. LDA on README files reveals 16 distinct application types for the 227k repositories. We apply the LDA model fitted on READMEs to the 92,830 SO questions and find that 27% of the questions are related to the 16 DL application types. The most asked question topic varies across application types, with half primarily relating to the second and third stages. Specifically, developers ask the most questions about topics primarily relating to Data Preparation (2nd) stage for four mature application types such as ${{\sf Image\ Segmentation}}$, and topics primarily relating to Model Setup (3rd) stage for four application types concerning emerging methods such as ${{\sf Transfer\ Learning}}$. Based on our findings, we distill several actionable insights for SE4DL research, practice, and education, such as better support for using trained models, application-type specific tools, and teaching materials.","Software engineering needs for deep learning,mining software repositories,topic modeling,stack overflow",论软件工程对深度学习需求的可变性：阶段、趋势和应用类型,深度学习（DL）的广泛使用并没有伴随着DL软件工程（SE）的相应进步。研究表明，编写DL软件的开发人员有特定的开发阶段（即SE4DL阶段），并面临新的DL特定问题。尽管进行了大量的研究，但尚不清楚DL开发人员对DL的SE需求如何随着阶段、应用程序类型而变化，或者是否会随着时间的推移而变化。为了帮助将研发工作集中在DL开发挑战上，我们分析了92830个堆栈溢出（SO）问题和227756个与DL相关的公共存储库的自述文件。潜在狄利克雷分配（LDA）揭示了SO问题的27个主题，其中19个（70.4%）主题主要与单个SE4DL阶段有关，8个主题跨越多个阶段。大多数问题涉及数据准备和模型设置阶段。随着时间的推移，11个主题的相对提问率有所上升，8个主题的提问率有所下降。前11个主题的问题接受答案的比例低于其余问题。README文件的LDA揭示了227k存储库的16种不同的应用程序类型。我们将适用于README的LDA模型应用于92830个SO问题，发现27%的问题与16种DL应用程序类型有关。提问最多的问题主题因应用程序类型而异，其中一半主要与第二和第三阶段有关。具体而言，开发人员提出的问题最多的是四种成熟应用程序类型（如$｛\sf Image\ Segmentation｝｝$）的主要与数据准备（第二）阶段有关的主题，以及四种新兴方法（如$｝\sf Transfer \ Learning｝$。基于我们的发现，我们为SE4DL的研究、实践和教育提取了一些可操作的见解，例如更好地支持使用经过训练的模型、特定于应用程序类型的工具和教材。,软件工程需要深度学习，挖掘软件存储库，主题建模，堆栈溢出,,,
JRKBCKXG,2023,https://doi.org/10.1109/TSE.2022.3140510,TSE 2023,Cerebro: Static Subsuming Mutant Selection,"Mutation testing research has indicated that a major part of its application cost is due to the large number of low utility mutants that it introduces. Although previous research has identified this issue, no previous study has proposed any effective solution to the problem. Thus, it remains unclear how to mutate and test a given piece of code in a best effort way, i.e., achieving a good trade-off between invested effort and test effectiveness. To achieve this, we propose Cerebro, a machine learning approach that statically selects subsuming mutants, i.e., the set of mutants that resides on the top of the subsumption hierarchy, based on the mutants’ surrounding code context. We evaluate Cerebro using 48 and 10 programs written in C and Java, respectively, and demonstrate that it preserves the mutation testing benefits while limiting application cost, i.e., reduces all cost application factors such as equivalent mutants, mutant executions, and the mutants requiring analysis. We demonstrate that Cerebro has strong inter-project prediction ability, which is significantly higher than two baseline methods, i.e., supervised learning on features proposed by state-of-the-art, and random mutant selection. More importantly, our results show that Cerebro’s selected mutants lead to strong tests that are respectively capable of killing 2 times higher than the number of subsuming mutants killed by the baselines when selecting the same number of mutants. At the same time, Cerebro reduces the cost-related factors, as it selects, on average, 68% fewer equivalent mutants, while requiring 90% fewer test executions than the baselines.","Mutant,mutation,mutation testing,subsuming mutant,mutant prediction,static selection,static mutant selection,static subsuming mutant selection,static subsuming mutant prediction,encoder-decoder,machine translation,tf-seq2seq",Cerebro：静态亚发光突变体选择,突变测试研究表明，其应用成本的很大一部分是由于其引入了大量低效用突变体。尽管之前的研究已经发现了这个问题，但之前的研究没有提出任何有效的解决方案。因此，目前尚不清楚如何以尽最大努力的方式对给定的代码进行变异和测试，即在投入的精力和测试有效性之间实现良好的权衡。为了实现这一点，我们提出了Cerebro，这是一种机器学习方法，它基于突变体周围的代码上下文静态地选择包容突变体，即位于包容层次结构顶部的一组突变体。我们分别使用用C和Java编写的48个和10个程序对Cerebro进行了评估，并证明它在限制应用成本的同时保留了突变测试的优势，即减少了所有成本应用因素，如等效突变体、突变体执行和需要分析的突变体。我们证明，Cerebro具有强大的项目间预测能力，显著高于两种基线方法，即最先进的特征监督学习和随机变异选择。更重要的是，我们的结果表明，Cerebro选择的突变体导致了强有力的测试，当选择相同数量的突变体时，其杀伤能力分别是基线杀死的包含突变体数量的2倍。与此同时，Cerebro减少了成本相关因素，因为它选择的等效突变体平均减少了68%，同时所需的测试执行比基线减少了90%。,突变体，突变，突变测试，包含突变体，突变体预测，静态选择，静态突变体选择，静态包含突变体选择，静止包含突变体预测，编码器-解码器，机器翻译，tf-seq2seq,,,
EUMEUHEQ,2023,https://doi.org/10.1109/TSE.2022.3216879,TSE 2023,Leveraging Android Automated Testing to Assist Crowdsourced Testing,"Crowdsourced testing is an emerging trend in mobile application testing. The openness of crowdsourced testing provides a promising way to conduct large-scale and user-oriented testing scenarios on various mobile devices, while it also brings a problem, i.e., crowdworkers with different levels of testing experience severely threaten the quality of crowdsourced testing. Currently, many approaches have been proposed and studied to improve crowdsourced testing. However, these approaches do not fundamentally improve the ability of crowdworkers. In essence, the low-quality crowdsourced testing is caused by crowdworkers who are unfamiliar with the App Under Test (AUT) and do not know which part of the AUT should be tested. To address this problem, we propose a testing assistance approach, which leverages Android automated testing (i.e., dynamic and static analysis) to improve crowdsourced testing. Our approach constructs an Annotated Window Transition Graph (AWTG) model for the AUT by merging dynamic and static analysis results. Based on the AWTG model, our approach implements a testing assistance pipeline that provides the test task extraction, test task recommendation, and test task guidance to assist crowdworkers in testing the AUT. We experimentally evaluate our approach on real-world AUTs. The quantitative results demonstrate that our approach can effectively and efficiently assist crowdsourced testing. Besides, the qualitative results from a user study confirm the usefulness of our approach.","Crowdsourced testing,dynamic analysis,static analysis,test recommendation,test assistance",利用Android自动测试来辅助众包测试,众包测试是移动应用程序测试的一个新兴趋势。众包测试的开放性为在各种移动设备上进行大规模、面向用户的测试场景提供了一种很有前途的方式，同时也带来了一个问题，即具有不同测试经验的众包工作者严重威胁着众包测试质量。目前，已经提出并研究了许多方法来改进众包测试。然而，这些方法并没有从根本上提高众包工作者的能力。从本质上讲，低质量的众包测试是由不熟悉测试中的应用程序（AUT）的众包工作者造成的，他们不知道应该测试AUT的哪一部分。为了解决这个问题，我们提出了一种测试辅助方法，该方法利用Android自动测试（即动态和静态分析）来改进众包测试。我们的方法通过合并动态和静态分析结果，为AUT构建了一个带注释的窗口转换图（AWTG）模型。基于AWTG模型，我们的方法实现了一个测试辅助管道，该管道提供测试任务提取、测试任务推荐和测试任务指导，以帮助众包工作者测试AUT。我们在真实世界的AUT上对我们的方法进行了实验评估。定量结果表明，我们的方法可以有效地帮助众包测试。此外，用户研究的定性结果证实了我们方法的有用性。,众包测试，动态分析，静态分析，测试推荐，测试辅助,,,
Z3P8MDHK,2023,https://doi.org/10.1109/TSE.2023.3305052,TSE 2023,Human-in-the-Loop Automatic Program Repair,"learn2fix is a human-in-the-loop interactive program repair technique, which can be applied when no bug oracle—except the user who is reporting the bug—is available. This approach incrementally learns the condition under which the bug is observed by systematic negotiation with the user. In this process, learn2fix generates alternative test inputs and sends some of those to the user for obtaining their labels. A limited query budget is assigned to the user for this task. A query is a Yes/No question: “When executing this alternative test input, the program under test produces the following output; is the bug observed?”. Using the labelled test inputs, learn2fix incrementally learns an automatic bug oracle to predict the user's response. A classification algorithm in machine learning is used for this task. Our key challenge is to maximise the oracle's accuracy in predicting the tests that expose the bug given a practical, small budget of queries. After learning the automatic oracle, an existing program repair tool attempts to repair the bug using the alternative tests that the user has labelled. Our experiments demonstrate that learn2fix trains a sufficiently accurate automatic oracle with a reasonably low labelling effort (lt. 20 queries), and the oracles represented by interpolation-based classifiers produce more accurate predictions than those represented by approximation-based classifiers. Given the user-labelled test inputs, generated using the interpolation-based approach, the GenProg and Angelix automatic program repair tools produce patches that pass a much larger proportion of validation tests than the manually constructed test suites provided by the repair benchmark.","Automated test oracles,semi-automatic program repair,classification algorithms,active machine learning",人在环自动程序修复,learn2fix是一种人在循环的交互式程序修复技术，当除了报告错误的用户之外没有错误预言机可用时，可以应用它。这种方法通过与用户进行系统协商，逐步了解观察到错误的条件。在这个过程中，learn2fix生成替代测试输入，并将其中一些输入发送给用户，以获得他们的标签。为该任务分配给用户的查询预算有限。查询是一个“是/否”问题：“当执行这个替代测试输入时，被测试的程序会产生以下输出；是否观察到错误？”。使用标记的测试输入，learn2fix逐步学习一个自动bug预言器来预测用户的响应。机器学习中的分类算法被用于该任务。我们面临的关键挑战是，在实际、少量的查询预算下，最大限度地提高oracle预测暴露错误的测试的准确性。在学习了自动预言机之后，现有的程序修复工具会尝试使用用户标记的替代测试来修复错误。我们的实验表明，learn2fix以相当低的标记工作量（lt.20查询）训练了一个足够准确的自动预言机，并且基于插值的分类器所表示的预言机比基于近似的分类器所代表的预言机产生更准确的预测。给定使用基于插值的方法生成的用户标记的测试输入，GenProg和Angelix自动程序修复工具生成的补丁通过的验证测试比例比修复基准提供的手动构建的测试套件大得多。,自动化测试预言机，半自动程序修复，分类算法，主动机器学习,,,
5P7JDDYH,2023,https://doi.org/10.1109/TSE.2022.3222160,TSE 2023,Studying the Interplay Between the Durations and Breakages of Continuous Integration Builds,"The Continuous Integration (CI) practice allows developers to build software projects automatically and more frequently. However, CI builds may undergo long build durations or frequent build breakages, which we refer to as build performance. Both long durations and frequent breakages of CI builds can impede developers from engaging in other development activities. Prior research has conducted independent studies on build durations or build breakages. However, there is little attention to the possible interplay between reducing build durations and build breakages. In particular, it is unclear from prior studies (i) whether and how build performance is influenced by the context of projects; (ii) whether the actions to reduce build durations would reduce or increase build breakages; and (iii) whether fixing build breakages would lead to longer or faster builds. It is important for developers to understand the practices that make both timely and passing CI builds. In this paper, we conduct experimental and survey studies on the practices that can have dual or inverse associations with two build performance measures: build durations and build breakages. To this end, we extend an existing dataset called TravisTorrent to exclude inactive projects and collect recent builds of active projects. As a result, we study 924,616 CI builds from 588 GitHub projects that are linked with Travis CI. In addition, we survey developers who contributed to the projects in our dataset to get their feedback on our experimental observations. First, we investigate project-level metrics and find that project characteristics have a significant association with build durations and breakages. In addition, we investigate how build-level metrics are associated with both build durations and breakages and observe an evident interplay between them. In particular, we observe that actions to fix build breakages (e.g., retrying or waiting for build commands) not only increase build durations but also do not guarantee passing builds. We also find that improving the build performance of a project is dependent on the current build durations and breakages of that project. Furthermore, we analyze how build performance changes over time and observe nearly a third of projects in which one performance measure is sacrificed in favor of the other, especially when not possible to achieve both together. The majority of our experimental observations are confirmed by survey results, which provide useful insights though some survey responses disagree with some of our experimental observations. Our work (a) provides developers with development and building practices to maintain timely and passing CI builds, and (b) encourages researchers to highlight any potential dual or inverse side effects when reporting actionable findings about CI builds.","Continuous integration (CI),build performance,build duration,build breakage,empirical software engineering,mining software repositories,questionnaire survey",持续集成建筑的持续与中断之间的互动研究,持续集成（CI）实践允许开发人员更频繁地自动构建软件项目。但是，CI构建可能会经历较长的构建持续时间或频繁的构建中断，我们称之为构建性能。CI构建的持续时间长和频繁中断都会阻碍开发人员参与其他开发活动。先前的研究对构建持续时间或构建破损进行了独立研究。然而，很少关注减少构建持续时间和构建破坏之间可能的相互作用。特别是，从先前的研究来看，尚不清楚（i）构建性能是否以及如何受到项目背景的影响；（ii）减少建造持续时间的行动是否会减少或增加建造破损；以及（iii）修复构建破损是否会导致更长或更快的构建。对于开发人员来说，了解及时构建和传递CI构建的实践非常重要。在本文中，我们对与两个构建性能指标（构建持续时间和构建破损）具有双重或反向关联的实践进行了实验和调查研究。为此，我们扩展了一个名为TravisTorrent的现有数据集，以排除非活动项目，并收集活动项目的最新构建。因此，我们研究了588个与Travis CI相关的GitHub项目中的924616个CI构建。此外，我们还调查了数据集中为项目做出贡献的开发人员，以获得他们对我们实验观察的反馈。首先，我们研究了项目级别的指标，发现项目特征与构建持续时间和破坏有着显著的关联。此外，我们还研究了构建级别指标如何与构建持续时间和中断相关联，并观察到它们之间的明显相互作用。特别是，我们观察到修复构建中断的操作（例如，重试或等待构建命令）不仅增加了构建持续时间，而且不能保证通过构建。我们还发现，提高项目的构建性能取决于该项目的当前构建持续时间和中断时间。此外，我们分析了构建性能如何随着时间的推移而变化，并观察到近三分之一的项目牺牲了一个性能指标来支持另一个，尤其是在不可能同时实现这两个指标的情况下。我们的大多数实验观察结果都得到了调查结果的证实，这些结果提供了有用的见解，尽管一些调查结果与我们的一些实验观察结果不一致。我们的工作（a）为开发人员提供开发和构建实践，以保持及时和通过的CI构建，以及（b）鼓励研究人员在报告有关CI构建的可操作发现时，强调任何潜在的双重或反向副作用。,持续集成（CI），构建性能，构建持续时间，构建破坏，经验软件工程，挖掘软件存储库，问卷调查,,,
LBLHSD2M,2023,https://doi.org/10.1109/TSE.2023.3269081,TSE 2023,Behavior Trees and State Machines in Robotics Applications,"Autonomous robots combine skills to form increasingly complex behaviors, called missions. While skills are often programmed at a relatively low abstraction level, their coordination is architecturally separated and often expressed in higher-level languages or frameworks. State machines have been the go-to language to model behavior for decades, but recently, behavior trees have gained attention among roboticists. Originally designed to model autonomous actors in computer games, behavior trees offer an extensible tree-based representation of missions and are claimed to support modular design and code reuse. Although several implementations of behavior trees are in use, little is known about their usage and scope in the real world. How do concepts offered by behavior trees relate to traditional languages, such as state machines? How are concepts in behavior trees and state machines used in actual applications? This paper is a study of the key language concepts in behavior trees as realized in domain-specific languages (DSLs), internal and external DSLs offered as libraries, and their use in open-source robotic applications supported by the Robot Operating System (ROS). We analyze behavior-tree DSLs and compare them to the standard language for behavior models in robotics: state machines. We identify DSLs for both behavior-modeling languages, and we analyze five in-depth. We mine open-source repositories for robotic applications that use the analyzed DSLs and analyze their usage. We identify similarities between behavior trees and state machines in terms of language design and the concepts offered to accommodate the needs of the robotics domain. We observed that the usage of behavior-tree DSLs in open-source projects is increasing rapidly. We observed similar usage patterns at model structure and at code reuse in the behavior-tree and state-machine models within the mined open-source projects. We contribute all extracted models as a dataset, hoping to inspire the community to use and further develop behavior trees, associated tools, and analysis techniques.","Behavior trees,exploratory empirical study,robotics applications,state machines,usage patterns",行为树和状态机在机器人应用中的应用,自主机器人结合技能形成越来越复杂的行为，称为任务。虽然技能通常在相对较低的抽象级别进行编程，但它们的协调在架构上是分离的，并且通常用更高级别的语言或框架来表达。几十年来，状态机一直是建模行为的首选语言，但最近，行为树在机器人学家中引起了关注。行为树最初是为计算机游戏中的自主参与者建模而设计的，它提供了一种可扩展的基于树的任务表示，并声称支持模块化设计和代码重用。尽管有几种行为树的实现正在使用中，但人们对它们在现实世界中的用法和范围知之甚少。行为树提供的概念如何与传统语言（如状态机）相关？行为树和状态机中的概念是如何在实际应用中使用的？本文研究了行为树中的关键语言概念，如领域特定语言（DSL）、作为库提供的内部和外部DSL，以及它们在机器人操作系统（ROS）支持的开源机器人应用程序中的使用。我们分析了行为树DSL，并将其与机器人行为模型的标准语言：状态机进行了比较。我们确定了两种行为建模语言的DSL，并深入分析了五种。我们为使用分析的DSL的机器人应用程序挖掘开源存储库，并分析它们的使用情况。我们在语言设计和为满足机器人领域的需求而提供的概念方面确定了行为树和状态机之间的相似之处。我们观察到，行为树DSL在开源项目中的使用正在迅速增加。在挖掘的开源项目中，我们在行为树和状态机模型的模型结构和代码重用方面观察到了类似的使用模式。我们将所有提取的模型作为一个数据集，希望能激励社区使用和进一步开发行为树、相关工具和分析技术。,行为树，探索性实证研究，机器人应用，状态机，使用模式,,,
MU8QJQ9M,2023,https://doi.org/10.1109/TSE.2022.3220740,TSE 2023,A Comprehensive Investigation of the Impact of Class Overlap on Software Defect Prediction,"Software Defect Prediction (SDP) is one of the most vital and cost-efficient operations to ensure the software quality. However, there exists the phenomenon of class overlap in the SDP datasets (i.e., defective and non-defective modules are similar in terms of values of metrics), which hinders the performance as well as the use of SDP models. Even though efforts have been made to investigate the impact of removing overlapping technique on the performance of SDP, many open issues are still challenging yet unknown. Therefore, we conduct an empirical study to comprehensively investigate the impact of class overlap on SDP. Specifically, we first propose an overlapping instances identification approach by analyzing the class distribution in the local neighborhood of a given instance. We then investigate the impact of class overlap and two common overlapping instance handling techniques on the performance and the interpretation of seven representative SDP models. Through an extensive case study on 230 diversity datasets, we observe that: i) 70.0% of SDP datasets contain overlapping instances; ii) different levels of class overlap have different impacts on the performance of SDP models; iii) class overlap affects the rank of the important feature list of SDP models, particularly the feature lists at the top 2 and top 3 ranks; IV) Class overlap handling techniques could statistically significantly improve the performance of SDP models trained on datasets with over 12.5% overlap ratios. We suggest that future work should apply our KNN method to identify the overlap ratios of datasets before building SDP models.","Class overlap,data quality,k-nearest neighbourhood,local analysis,software defect prediction,software metrics",类重叠对软件缺陷预测影响的综合研究,软件缺陷预测（SDP）是保证软件质量的最重要、最具成本效益的操作之一。然而，SDP数据集中存在类重叠现象（即，缺陷模块和无缺陷模块在度量值方面相似），这阻碍了SDP模型的性能和使用。尽管已经努力研究消除重叠技术对SDP性能的影响，但许多悬而未决的问题仍然具有挑战性，但尚不清楚。因此，我们进行了一项实证研究，以全面调查阶级重叠对SDP的影响。具体来说，我们首先通过分析给定实例的局部邻域中的类分布，提出了一种重叠实例识别方法。然后，我们研究了类重叠和两种常见的重叠实例处理技术对七个有代表性的SDP模型的性能和解释的影响。通过对230个多样性数据集的广泛案例研究，我们观察到：i）70.0%的SDP数据集包含重叠实例；ii）不同级别的类重叠对SDP模型的性能有不同的影响；iii）类重叠影响SDP模型的重要特征列表的秩，特别是前2和前3秩的特征列表；IV） 类重叠处理技术可以在统计上显著提高在重叠率超过12.5%的数据集上训练的SDP模型的性能。我们建议，在构建SDP模型之前，未来的工作应该应用我们的KNN方法来识别数据集的重叠率。,类重叠，数据质量，k近邻，局部分析，软件缺陷预测，软件度量,,,
BTVB3RHI,2023,https://doi.org/10.1109/TSE.2022.3215289,TSE 2023,Almost Rerere: Learning to Resolve Conflicts in Distributed Projects,"The concurrent development of applications requires reconciling conflicting code updates by different developers. Recent research on the nature of merge conflicts in open source projects shows that a significant fraction of merge conflicts have limited size (one or two lines of code) and are resolved with simple strategies that use code present in the merged versions. Thus the opportunity arises of supporting the resolution of merge conflicts automatically by learning the way in which developers fix them. In this paper we propose a framework for automating the resolution of merge conflicts which learns from the resolutions made by developers and encodes such knowledge into conflict resolution rules applicable to conflicts not seen before. The proposed approach is text-based, does not depend on the programming languages of the merged files and exploits a well-known and general language (search and replacement regular expressions) to encode the conflict resolution rules. Evaluation results on 14,872 conflicts from 25 projects show that the system can synthesize a resolution for $\approx$ 49% of the conflicts occurred during the merge process ($\approx$ 89% if one considers conflicts that have at least one similar conflict in the data set) and can reproduce exactly the same solution that human developers have applied in $\approx$ 55% of the cases ($\approx$ 62% for single line conflicts).","Automatic conflict resolution,GIT,code integration",几乎重读：学习解决分布式项目中的冲突,应用程序的并发开发需要协调不同开发人员的冲突代码更新。最近对开源项目中合并冲突性质的研究表明，相当一部分合并冲突的大小有限（一两行代码），可以通过使用合并版本中存在的代码的简单策略来解决。因此，通过学习开发人员解决合并冲突的方法，可以自动支持解决合并冲突。在本文中，我们提出了一个自动解决合并冲突的框架，该框架从开发人员做出的解决方案中学习，并将这些知识编码为适用于以前从未见过的冲突的冲突解决规则。所提出的方法是基于文本的，不依赖于合并文件的编程语言，并利用一种众所周知的通用语言（搜索和替换正则表达式）来编码冲突解决规则。对来自25个项目的14872个冲突的评估结果表明，该系统可以为合并过程中发生的冲突综合解决约49%的冲突（如果考虑到数据集中至少有一个类似冲突的冲突，则为约89%），并且可以再现人类开发人员在约55%的情况下应用的完全相同的解决方案（对于单行冲突，$\约为$62%）。,自动冲突解决，GIT，代码集成,,,
35ZN4A4Q,2023,https://doi.org/10.1109/TSE.2023.3251858,TSE 2023,Verification of Fuzzy Decision Trees,"In recent years, there have been major strides in the safety verification of machine learning models such as neural networks and tree ensembles. However, fuzzy decision trees (FDT), also called soft or differentiable decision trees, are yet unstudied in the context of verification. They present unique verification challenges resulting from multiplications of input values; in the simplest case with a piecewise-linear splitting function, an FDT is piecewise-polynomial with degree up to the depth of the tree. We propose an abstraction-refinement algorithm for verification of properties of FDTs. We show that the problem is NP-Complete, like many other machine learning verification problems, and that our algorithm is complete in a finite precision setting. We benchmark on a selection of public data sets against an off-the-shelf SMT solver and a baseline variation of our algorithm that uses a refinement strategy from similar methods for neural network verification, finding the proposed method to be the fastest. Code for our algorithm along with our experiments and demos are available on GitHub at https://github.com/autonlab/fdt_verification.","Decision tree,formal methods,fuzzy set,robustness,verification",模糊决策树的验证,近年来，在神经网络和树集合等机器学习模型的安全性验证方面取得了重大进展。然而，模糊决策树（FDT），也称为软决策树或可微决策树，在验证的背景下尚未进行研究。由于输入值的乘法运算，它们带来了独特的验证挑战；在分段线性分裂函数的最简单情况下，FDT是阶数达到树深度的分段多项式。我们提出了一种用于验证FDTs性质的抽象精化算法。我们证明了这个问题是NP完全的，就像许多其他机器学习验证问题一样，并且我们的算法在有限精度设置中是完整的。我们以一组公共数据集为基准，对照现成的SMT求解器和我们算法的基线变体，该算法使用类似方法的细化策略进行神经网络验证，发现所提出的方法是最快的。我们的算法代码以及我们的实验和演示可在GitHub上获得，网址为https://github.com/autonlab/fdt_verification.,决策树，形式化方法，模糊集，鲁棒性，验证,,,
J99DSNF8,2023,https://doi.org/10.1109/TSE.2023.3291003,TSE 2023,Dealing With Data Challenges When Delivering Data-Intensive Software Solutions,"The predicted increase in demand for data-intensive solution development is driving the need for software, data, and domain experts to effectively collaborate in multi-disciplinary data-intensive software teams (MDSTs). We conducted a socio-technical grounded theory study through interviews with 24 practitioners in MDSTs to better understand the challenges these teams face when delivering data-intensive software solutions. The interviews provided perspectives across different types of roles including domain, data and software experts, and covered different organisational levels from team members, team managers to executive leaders. We found that the key concern for these teams is dealing with data-related challenges. In this article, we present a theory of dealing with data challenges that explains the challenges faced by MDSTs including gaining access to data, aligning data, understanding data, and resolving data quality issues; the context in and condition under which these challenges occur, the causes that lead to the challenges, and the related consequences such as having to conduct remediation activities, inability to achieve expected outcomes and lack of trust in the delivered solutions. We also identified contingencies or strategies applied to address the challenges including high-level strategic approaches such as implementing data governance, implementing new tools and techniques such as data quality visualisation and monitoring tools, as well as building stronger teams by focusing on people dynamics, communication skill development and cross-skilling. Our findings have direct implications for practitioners and researchers to better understand the landscape of data challenges and how to deal with them.","Data challenges,data-intensive solutions,multi-disciplinary teams,socio-technical grounded theory method",提供数据密集型软件解决方案时应对数据挑战,预计对数据密集型解决方案开发的需求将增加，这推动了软件、数据和领域专家在多学科数据密集型软件团队（MDST）中进行有效协作的需求。我们通过采访24名MDST从业者进行了一项基于社会技术的理论研究，以更好地了解这些团队在提供数据密集型软件解决方案时面临的挑战。采访提供了不同类型角色的视角，包括领域、数据和软件专家，并涵盖了从团队成员、团队经理到高管的不同组织级别。我们发现，这些团队的主要关注点是处理与数据相关的挑战。在这篇文章中，我们提出了一个处理数据挑战的理论，解释了MDST面临的挑战，包括访问数据、对齐数据、理解数据和解决数据质量问题；这些挑战发生的背景和条件，导致这些挑战的原因，以及相关后果，如必须开展补救活动、无法实现预期结果以及对所提供的解决方案缺乏信任。我们还确定了应对挑战的突发事件或策略，包括实施数据治理等高级别战略方法，实施数据质量可视化和监控工具等新工具和技术，以及通过关注人员动态、沟通技能发展和交叉技能来建立更强大的团队。我们的发现对从业者和研究人员更好地了解数据挑战的前景以及如何应对这些挑战具有直接意义。,数据挑战，数据密集型解决方案，多学科团队，基于社会技术的理论方法,,,
5XF69E7U,2023,https://doi.org/10.1109/TSE.2022.3218859,TSE 2023,DiffSearch: A Scalable and Precise Search Engine for Code Changes,"The source code of successful projects is evolving all the time, resulting in hundreds of thousands of code changes stored in source code repositories. This wealth of data can be useful, e.g., to find changes similar to a planned code change or examples of recurring code improvements. This paper presents DiffSearch, a search engine that, given a query that describes a code change, returns a set of changes that match the query. The approach is enabled by three key contributions. First, we present a query language that extends the underlying programming language with wildcards and placeholders, providing an intuitive way of formulating queries that is easy to adapt to different programming languages. Second, to ensure scalability, the approach indexes code changes in a one-time preprocessing step, mapping them into a feature space, and then performs an efficient search in the feature space for each query. Third, to guarantee precision, i.e., that any returned code change indeed matches the given query, we present a tree-based matching algorithm that checks whether a query can be expanded to a concrete code change. We present implementations for Java, JavaScript, and Python, and show that the approach responds within seconds to queries across one million code changes, has a recall of 80.7% for Java, 89.6% for Python, and 90.4% for JavaScript, enables users to find relevant code changes more effectively than a regular expression-based search and GitHub's search feature, and is helpful for gathering a large-scale dataset of real-world bug fixes.","Software engineering,program analysis,software maintenance",DiffSearch：一个可扩展且精确的代码更改搜索引擎,成功项目的源代码一直在发展，导致源代码存储库中存储了数十万个代码更改。这些丰富的数据可能很有用，例如，可以找到类似于计划代码更改的更改或重复代码改进的示例。本文介绍了DiffSearch，一个搜索引擎，它在给定描述代码更改的查询时，返回一组与查询匹配的更改。这一方法有三个关键贡献。首先，我们提出了一种查询语言，该语言使用通配符和占位符扩展了底层编程语言，提供了一种直观的查询形式，易于适应不同的编程语言。其次，为了确保可伸缩性，该方法在一次性预处理步骤中对代码更改进行索引，将它们映射到特征空间中，然后在特征空间中为每个查询执行高效搜索。第三，为了保证精度，即任何返回的代码更改都确实与给定的查询匹配，我们提出了一种基于树的匹配算法，该算法检查查询是否可以扩展为具体的代码更改。我们展示了Java、JavaScript和Python的实现，并表明该方法在几秒钟内对一百万个代码更改的查询做出响应，Java的召回率为80.7%，Python为89.6%，JavaScript为90.4%，使用户能够比基于正则表达式的搜索和GitHub的搜索功能更有效地找到相关的代码更改，并且有助于收集真实世界错误修复的大规模数据集。,软件工程，程序分析，软件维护,,,
SHIXN6Q2,2023,https://doi.org/10.1109/TSE.2022.3175660,TSE 2023,An Actionable Framework for Understanding and Improving Developer Experience,"Developer experience is an important concern for software organizations as enhancing developer experience improves productivity, satisfaction, engagement and retention. We set out to understand what affects developer experience through semi-structured interviews with 21 developers from industry, which we transcribed and iteratively coded. Our findings elucidate factors that affect developer experience and characteristics that influence their respective importance to individual developers. We also identify strategies employed by individuals and teams to improve developer experience and the barriers that stand in their way. Lastly, we describe the coping mechanisms of developers when developer experience cannot be sufficiently improved. Our findings result in the DX Framework, an actionable conceptual framework for understanding and improving developer experience. The DX Framework provides a go-to reference for organizations that want to enable more productive and effective work environments for their developers.","Developer experience,grounded theory,development practices,satisfaction,productivity",理解和改善开发人员体验的可操作框架,开发人员体验是软件组织关注的一个重要问题，因为增强开发人员体验可以提高生产力、满意度、参与度和保留率。我们开始通过对来自行业的21名开发人员的半结构化访谈来了解是什么影响了开发人员的体验，我们对这些访谈进行了转录和迭代编码。我们的研究结果阐明了影响开发人员体验的因素以及影响其对单个开发人员的重要性的特征。我们还确定了个人和团队为改善开发人员体验而采用的策略以及阻碍他们的障碍。最后，我们描述了当开发人员体验不能得到充分改善时，开发人员的应对机制。我们的研究结果产生了DX框架，这是一个可操作的概念框架，用于理解和改善开发人员体验。DX框架为那些希望为开发人员提供更高效的工作环境的组织提供了一个参考。,开发人员经验，基础理论，开发实践，满意度，生产力,,,
NE3IQURM,2023,https://doi.org/10.1109/TSE.2023.3241299,TSE 2023,TrinityRCL: Multi-Granular and Code-Level Root Cause Localization Using Multiple Types of Telemetry Data in Microservice Systems,"The microservice architecture has been commonly adopted by large scale software systems exemplified by a wide range of online services. Service monitoring through anomaly detection and root cause analysis (RCA) is crucial for these microservice systems to provide stable and continued services. However, compared with monolithic systems, software systems based on the layered microservice architecture are inherently complex and commonly involve entities at different levels of granularity. Therefore, for effective service monitoring, these systems have a special requirement of multi-granular RCA. Furthermore, as a large proportion of anomalies in microservice systems pertain to problematic code, to timely troubleshoot these anomalies, these systems have another special requirement of RCA at the finest code-level. Microservice systems rely on telemetry data to perform service monitoring and RCA of service anomalies. The majority of existing RCA approaches are only based on a single type of telemetry data and as a result can only support uni-granular RCA at either application-level or service-level. Although there are attempts to combine metric and tracing data in RCA, their objective is to improve RCA's efficiency or accuracy rather than to support multi-granular RCA. In this article, we propose a new RCA solution TrinityRCL that is able to localize the root causes of anomalies at multiple levels of granularity including application-level, service-level, host-level, and metric-level, with the unique capability of code-level localization by harnessing all three types of telemetry data to construct a causal graph representing the intricate, dynamic, and nondeterministic relationships among the various entities related to the anomalies. By implementing and deploying TrinityRCL in a real production environment, we evaluate TrinityRCL against two baseline methods and the results show that TrinityRCL has a significant performance advantage in terms of accuracy at the same level of granularity with comparable efficiency and is particularly effective to support large-scale systems with massive telemetry data.","Root cause,telemetry data,microservices",TrinityRCL：微服务系统中使用多种类型遥测数据的多粒度和代码级根本原因定位,微服务架构已被大规模软件系统普遍采用，例如广泛的在线服务。通过异常检测和根本原因分析（RCA）进行服务监控对于这些微服务系统提供稳定和持续的服务至关重要。然而，与单片系统相比，基于分层微服务架构的软件系统本质上很复杂，通常涉及不同粒度级别的实体。因此，为了进行有效的服务监控，这些系统对多粒度RCA有着特殊的要求。此外，由于微服务系统中很大一部分异常与有问题的代码有关，为了及时排除这些异常，这些系统对最佳代码级别的RCA有另一个特殊要求。微服务系统依靠遥测数据来执行服务监控和服务异常的RCA。大多数现有的RCA方法仅基于单一类型的遥测数据，因此只能在应用程序级别或服务级别支持单粒度RCA。尽管有人试图在RCA中结合度量数据和跟踪数据，但他们的目标是提高RCA的效率或准确性，而不是支持多粒度RCA。在本文中，我们提出了一种新的RCA解决方案TrinityRCL，它能够在多个粒度级别（包括应用程序级别、服务级别、主机级别和度量级别）定位异常的根本原因，并通过利用所有三种类型的遥测数据来构建一个因果图，以及与异常相关的各种实体之间的不确定性关系。通过在实际生产环境中实施和部署TrinityRCL，我们对照两种基线方法对TrinityRC L进行了评估，结果表明，Trinity RCL在相同粒度级别的精度方面具有显著的性能优势，具有相当的效率，并且特别有效地支持具有大量遥测数据的大型系统。,根本原因，遥测数据，微服务,,,
LKSYTVL2,2023,https://doi.org/10.1109/TSE.2022.3166924,TSE 2023,Logging Practices in Software Engineering: A Systematic Mapping Study,"Background: Logging practices provide the ability to record valuable runtime information of software systems to support operations tasks such as service monitoring and troubleshooting. However, current logging practices face common challenges. On the one hand, although the importance of logging practices has been broadly recognized, most of them are still conducted in an arbitrary or ad-hoc manner, ending up with questionable or inadequate support to perform these tasks. On the other hand, considerable research effort has been carried out on logging practices, however, few of the proposed techniques or methods have been widely adopted in industry. Objective:This study aims to establish a comprehensive understanding of the research state of logging practices, with a focus on unveiling possible problems and gaps which further shed light on the potential future research directions. Method:We carried out a systematic mapping study on logging practices with 56 primary studies. Results:This study provides a holistic report of the existing research on logging practices by systematically synthesizing and analyzing the focus and inter-relationship of the existing research in terms of issues, research topics and solution approaches. Using 3W1H—Why to log, Where to log, What to log and How well is the logging—as the categorization standard, we find that: (1) the best known issues in logging practices have been repeatedly investigated; (2) the issues are often studied separately without considering their intricate relationships; (3) the Where and What questions have attracted the majority of research attention while little research effort has been made on the Why and How well questions; and (4) the relationships between issues, research topics, and approaches regarding logging practices appear many-to-many, which indicates a lack of profound understanding of the issues in practice and how they should be appropriately tackled. Conclusions:This study indicates a need to advance the state of research on logging practices. For example, more research effort should be invested on why to log to set the anchor of logging practices as well as on how well is the logging to close the loop. In addition, a holistic process perspective should be taken into account in both the research and the adoption related to logging practices.","Logging practices,log,systematic mapping,empirical study",软件工程中的测井实践：系统制图研究,背景：日志记录实践提供了记录软件系统的宝贵运行时信息的能力，以支持操作任务，如服务监控和故障排除。然而，当前的日志记录实践面临着共同的挑战。一方面，尽管日志记录实践的重要性已得到广泛认可，但大多数日志记录实践仍以武断或临时的方式进行，最终导致执行这些任务的支持有问题或不足。另一方面，对测井实践进行了大量的研究，然而，所提出的技术或方法很少在工业中被广泛采用。目的：本研究旨在全面了解测井实践的研究现状，重点揭示可能存在的问题和差距，进一步阐明未来潜在的研究方向。方法：我们对测井实践进行了系统的测绘研究，共进行了56项初步研究。结果：本研究从问题、研究主题和解决方法等方面系统地综合和分析了现有研究的重点和相互关系，为现有的测井实践研究提供了一份全面的报告。以3W1H——为什么记录、在哪里记录、记录什么以及记录效果如何——作为分类标准，我们发现：（1）日志实践中最著名的问题已经被反复研究；（2） 这些问题往往是单独研究的，而不考虑它们之间错综复杂的关系；（3） 在哪里和什么问题引起了大多数研究的关注，而在为什么和效果如何的问题上却很少进行研究；（4）与伐木实践有关的问题、研究主题和方法之间的关系似乎是多对多的，这表明人们对实践中的问题以及如何适当解决这些问题缺乏深刻的理解。结论：本研究表明有必要提高测井实践的研究水平。例如，应该投入更多的研究精力，研究为什么要进行日志记录，以确定日志记录实践的锚，以及日志记录结束循环的效果如何。此外，在与日志记录实践相关的研究和采用中，应考虑到整体过程的观点。,测井实践，测井，系统测绘，实证研究,,,
AL87LVK6,2023,https://doi.org/10.1109/TSE.2023.3308392,TSE 2023,An Empirical Study on the Effectiveness of Privacy Indicators,"The increasing diffusion of mobile devices and their integration with sophisticated hardware and software components has promoted the development of numerous applications in which developers find new ingenious ways to exploit the possibilities offered by the access to resources such as cameras, biometric sensors, and GPS receivers. As a result, we are increasingly used to seeing applications that make extensive use of sensitive resources, potentially dangerous for our privacy. To address this problem, the latest approach to support user awareness in terms of privacy is represented by the Privacy Indicators (PI), a software solution implemented by the operating system to provide a visual stimulus to inform users whenever a dangerous resource is exploited by the app. However, the effectiveness of this approach has not been assessed yet. In this article, we present the result of a study on the effectiveness of using the PI to inform the user every time an app accesses the mobile device camera or microphone. We have chosen these two resources as the PI are currently implemented only for a very limited number of permissions. The controlled experiment involved 122 Android users who were asked to complete a series of tasks on their smartphone through prototypes using the involved resources in an explicit and latent way. Although the PI mechanism is very similar between Android and iOS, we have decided to focus on the former due to its greater diffusion. The results show no significant correlation between the use of PI and the detection of the resource being used by the app, suggesting that the effectiveness of PI in improving sensitive-related resources usage awareness, as currently implemented, is still unsatisfactory. In order to understand if the problem was due to the specific implementation of the PI, we implemented an enhanced version and compared it with the standard one. The results confirmed that an implementation that makes the indicators more visible and that is clearer in highlighting the fact that the app is accessing a resource improves resources usage awareness.","Android permissions,privacy,empirical evaluation",隐私指标有效性的实证研究,移动设备的日益普及及其与复杂硬件和软件组件的集成促进了许多应用程序的发展，在这些应用程序中，开发人员找到了新的巧妙方法来利用相机、生物传感器和GPS接收器等资源所提供的可能性。因此，我们越来越习惯于看到大量使用敏感资源的应用程序，这对我们的隐私有潜在的危险。为了解决这个问题，在隐私方面支持用户意识的最新方法是隐私指标（PI），这是一种由操作系统实现的软件解决方案，每当应用程序利用危险资源时，它会提供视觉刺激来通知用户。然而，这种方法的有效性尚未得到评估。在本文中，我们介绍了一项关于每次应用程序访问移动设备摄像头或麦克风时使用PI通知用户的有效性的研究结果。我们选择了这两种资源，因为PI目前只针对数量非常有限的权限实现。这项受控实验涉及122名安卓用户，他们被要求通过原型以明确和潜在的方式使用相关资源在智能手机上完成一系列任务。尽管Android和iOS的PI机制非常相似，但由于其更大的扩散性，我们决定专注于前者。结果显示，PI的使用与应用程序正在使用的资源的检测之间没有显著的相关性，这表明目前实施的PI在提高敏感相关资源使用意识方面的有效性仍然不令人满意。为了了解问题是否是由PI的具体实现引起的，我们实现了一个增强版本，并将其与标准版本进行了比较。结果证实，一个使指标更可见的实现，以及在突出应用程序正在访问资源的事实时更清晰的实现，可以提高资源使用意识。,Android权限，隐私，经验评估,,,
83IBM7SH,2023,https://doi.org/10.1109/TSE.2023.3313989,TSE 2023,An Easy Data Augmentation Approach for Application Reviews Event Inference,"Application review event inference aims to assess the effectiveness of application problems in response to user actions, which enables application developers to promptly discover and address potential issues in various applications, thereby improving their development and maintenance efficiency. Despite the development of event inference models for app reviews, which extract them as user action and app problem events and establish a relationship model between events and inference labels, the accuracy of these models is constrained due to limitations in labeling and characterizing noise and the lack of robustness and generalization. To address this challenge, we propose a model called Easy Data Augmentation for Application Reviews Event Inference (short for EDA-AREI), which comprises a denoising component, data augmentation component, and event inference prediction component. Specifically, the denoising component identifies labels and characterizes noisy data to enhance dataset quality, the data augmentation component replaces non-stop words with synonyms to increase textual diversity, and the event inference and prediction component reconstructs the classifier using denoised and augmented data. Experimental results on six datasets of one-star app reviews in the Apple App Store demonstrate that the EDA-AREI method achieves an Accuracy of 71.19%, 79.14%, 69.05%, 69.02%, 68.24% and 68.48%, respectively, representing an improvement of 0.83%–2.09% compared to state-of-the-art models. Regarding the F1-score, EDA-AREI achieves values of 71.30%, 69.93%, and 68.76% on the threshold_0.5, k-means_2, and random datasets, respectively, outperforming state-of-the-art models by 1.89%–4.02%. Furthermore, EDA-AREI achieves AUC values of 75.66% and 73.37% on the threshold_0.5 and k-means_2 datasets, respectively. As a result, EDA-AREI demonstrates substantial improvements in Accuracy, as well as enhanced F1-score and AUC across most datasets, thereby enhancing the model's accuracy and robustness in identifying related action-problem pairs.","Application reviews event inference,data augmentation,Confident Learning",一种用于应用程序评论事件推断的简单数据增强方法,应用程序审查事件推理旨在评估应用程序问题对用户行为的有效性，使应用程序开发人员能够及时发现和解决各种应用程序中的潜在问题，从而提高其开发和维护效率。尽管开发了应用程序评论的事件推理模型，将其提取为用户行为和应用程序问题事件，并在事件和推理标签之间建立关系模型，但由于标记和表征噪声的局限性以及缺乏鲁棒性和通用性，这些模型的准确性受到限制。为了应对这一挑战，我们提出了一种称为“应用审查事件推断的简单数据增强”（EDA-AREI的缩写）的模型，该模型包括去噪组件、数据增强组件和事件推断预测组件。具体而言，去噪组件识别标签并表征有噪声的数据以提高数据集质量，数据增强组件用同义词替换不间断的单词以增加文本多样性，事件推理和预测组件使用去噪和增强的数据重建分类器。在苹果应用商店的六个一星应用评论数据集上的实验结果表明，EDA-AREI方法的准确率分别为71.19%、79.14%、69.05%、69.02%、68.24%和68.48%，与最先进的模型相比提高了0.83%-2.09%。关于F1得分，EDA-AREI在threshold_0.5、k-means_2和随机数据集上分别获得71.30%、69.93%和68.76%的值，比最先进的模型高1.89%–4.02%。此外，EDA-ARE在threshold_5和k-means_ 2数据集上的AUC值分别为75.66%和73.37%。因此，EDA-AREI证明了准确性的显著提高，以及大多数数据集的F1分数和AUC的提高，从而增强了模型在识别相关行动问题对方面的准确性和稳健性。,应用程序审查事件推理，数据扩充，自信学习,,,
X3GABXZ4,2023,https://doi.org/10.1109/TSE.2023.3306461,TSE 2023,An Effective Approach to High Strength Covering Array Generation in Combinatorial Testing,"Combinatorial testing (CT) is an effective testing method that can detect failures caused by the interaction of parameters of the software under test (SUT). With the increasing complexity of SUT and the parameters involved, the variable strength test suite supporting high strength interaction is challenging in a practical testing scenario. This paper presents a multi-learning-based quantum particle swarm optimization (IQIPSO) for high and variable strength covering array generation (VSCAG). Specifically, a specially designed data structure and several combination location methods are proposed to support and speed up the high-strength VSCAG. Besides, multi-learning strategies, including Lamarckian and Baldwinian learning, are applied to IQIPSO to address the premature convergence leading to a large test suite size. Studies for parameter settings of IQIPSO are presented systematically. The IQIPSO method successfully builds test suites where strength is up to 15 and totally reports 13 new best test suite size records. Extensive experiments demonstrate that IQIPSO tends to outperform most other existing methods.","Combinatorial testing (CT),high strength covering array generation,variable strength covering array generation (VSCAG),multi-learning strategies,quantum particle swarm optimization (QPSO)",组合测试中高强度覆盖阵列生成的一种有效方法,组合测试（CT）是一种有效的测试方法，可以检测被测软件（SUT）参数交互引起的故障。随着SUT和所涉及参数的复杂性不断增加，支持高强度交互的可变强度测试套件在实际测试场景中具有挑战性。本文提出了一种基于多学习的量子粒子群优化算法（IQIPSO），用于高强度和可变强度覆盖阵列的生成。具体而言，提出了一种专门设计的数据结构和几种组合定位方法来支持和加速高强度VSTAG。此外，还将包括拉马克学习和鲍德温学习在内的多种学习策略应用于IQIPSO，以解决导致测试套件规模过大的过早收敛问题。系统地介绍了IQIPSO的参数设置研究。IQIPSO方法成功构建了强度高达15的测试套件，并总共报告了13个新的最佳测试套件大小记录。大量实验表明，IQIPSO往往优于大多数其他现有方法。,组合测试（CT），高强度覆盖阵列生成，可变强度覆盖阵列（VSTAG），多学习策略，量子粒子群优化（QPSO）,,,
Z7RJ7XM4,2023,https://doi.org/10.1109/TSE.2022.3232623,TSE 2023,Software Pipelining for Quantum Loop Programs,"We propose a method for performing software pipelining on quantum for-loop programs to exploit parallelism in and across iterations. We redefined concepts useful in program optimization, including array aliasing, instruction dependency, and resource conflict required in optimizing quantum programs. Using these concepts, we present a software pipelining framework exploiting instruction-level parallelism in quantum loop programs. This method is further enhanced with several improvements to reduce total gate count and program depth. The optimization method is then evaluated on some popular quantum algorithms like Grover and QAOA, and compared under different configurations and with several baseline compilers. The evaluation results show that our approach can schedule loop programs with depth close to the depth of the entire loop unrolling while generating smaller code sizes and consuming much less time. This is the first step towards optimization of a quantum program with such loop control flow, as far as we know.","Quantum program scheduling,quantum program compilation",量子循环程序的软件流水线,我们提出了一种在量子for循环程序上执行软件流水线的方法，以利用迭代中和迭代之间的并行性。我们重新定义了程序优化中有用的概念，包括优化量子程序所需的数组别名、指令依赖性和资源冲突。利用这些概念，我们提出了一个在量子循环程序中利用指令级并行性的软件流水线框架。该方法通过若干改进得到进一步增强，以减少总门数和程序深度。然后，在Grover和QAOA等流行的量子算法上对优化方法进行了评估，并在不同配置下与几种基线编译器进行了比较。评估结果表明，我们的方法可以调度深度接近整个循环展开深度的循环程序，同时生成更小的代码大小和消耗更少的时间。据我们所知，这是用这种环路控制流优化量子程序的第一步。,量子程序调度，量子程序编译,,,
SN6DHAJN,2023,https://doi.org/10.1109/TSE.2023.3236582,TSE 2023,Mixed Signals: Analyzing Software Attribution Challenges in the Android Ecosystem,"The ability to identify the author responsible for a given software object is critical for many research studies and for enhancing software transparency and accountability. However, as opposed to other application markets like Apple's iOS App Store, attribution in the Android ecosystem is known to be hard. Prior research has leveraged market metadata and signing certificates to identify software authors without questioning the validity and accuracy of these attribution signals. However, Android application (app) authors can, either intentionally or by mistake, hide their true identity due to: (1) the lack of policy enforcement by markets to ensure the accuracy and correctness of the information disclosed by developers in their market profiles during the app release process, and (2) the use of self-signed certificates for signing apps instead of certificates issued by trusted CAs. In this paper, we perform the first empirical analysis of the availability, volatility and overall aptness of publicly available market and app metadata for author attribution in Android markets. To that end, we analyze a dataset of over 2.5 million market entries and apps extracted from five Android markets for over two years. Our results show that widely used attribution signals are often missing from market profiles and that they change over time. We also invalidate the general belief about the validity of signing certificates for author attribution. For instance, we find that apps from different authors share signing certificates due to the proliferation of app building frameworks and software factories. Finally, we introduce the concept of an attribution graph and we apply it to evaluate the validity of existing attribution signals on the Google Play Store. Our results confirm that the lack of control over publicly available signals can confuse automatic attribution processes.","Android,attribution,attribution graph,mobile apps",混合信号：分析安卓生态系统中的软件归因挑战,识别对给定软件对象负责的作者的能力对于许多研究以及增强软件透明度和问责制至关重要。然而，与苹果iOS应用商店等其他应用市场不同，安卓生态系统中的归属是很难的。先前的研究利用市场元数据和签名证书来识别软件作者，而不质疑这些归因信号的有效性和准确性。然而，安卓应用程序的作者可能有意或错误地隐藏自己的真实身份，原因是：（1）市场缺乏政策执行，无法确保开发者在应用程序发布过程中在其市场档案中披露的信息的准确性和正确性，以及（2）使用自签名证书来签名应用程序，而不是由可信CA颁发的证书。在本文中，我们首次对安卓市场中公开的市场和应用元数据的可用性、波动性和整体适配性进行了实证分析。为此，我们分析了两年多来从五个安卓市场提取的250多万个市场条目和应用程序的数据集。我们的研究结果表明，市场概况中经常缺少广泛使用的归因信号，而且这些信号会随着时间的推移而变化。我们还否定了关于作者署名签名证书有效性的普遍看法。例如，我们发现，由于应用程序构建框架和软件工厂的激增，来自不同作者的应用程序共享签名证书。最后，我们引入了归因图的概念，并将其应用于评估Google Play商店上现有归因信号的有效性。我们的研究结果证实，对公开可用信号缺乏控制可能会混淆自动归因过程。,Android，归因，归因图，移动应用程序,,,
7SWLW82F,2023,https://doi.org/10.1109/TSE.2023.3278129,TSE 2023,Automating Dependency Updates in Practice: An Exploratory Study on GitHub Dependabot,"Dependency management bots automatically open pull requests to update software dependencies on behalf of developers. Early research shows that developers are suspicious of updates performed by dependency management bots and feel tired of overwhelming notifications from these bots. Despite this, dependency management bots are becoming increasingly popular. Such contrast motivates us to investigate Dependabot, currently the most visible bot on GitHub, to reveal the effectiveness and limitations of state-of-art dependency management bots. We use exploratory data analysis and a developer survey to evaluate the effectiveness of Dependabot in keeping dependencies up-to-date, interacting with developers, reducing update suspicion, and reducing notification fatigue. We obtain mixed findings. On the positive side, projects do reduce technical lag after Dependabot adoption and developers are highly receptive to its pull requests. On the negative side, its compatibility scores are too scarce to be effective in reducing update suspicion; developers tend to configure Dependabot toward reducing the number of notifications; and 11.3% of projects have deprecated Dependabot in favor of other alternatives. The survey confirms our findings and provides insights into the key missing features of Dependabot. Based on our findings, we derive and summarize the key characteristics of an ideal dependency management bot which can be grouped into four dimensions: configurability, autonomy, transparency, and self-adaptability.","Dependabot,dependency management,mining software repositories,software engineering bot",在实践中实现依赖关系的自动更新——对GitHub可靠度的探索性研究,依赖关系管理机器人会自动打开拉取请求，以代表开发人员更新软件依赖关系。早期研究表明，开发人员对依赖管理机器人执行的更新持怀疑态度，并对这些机器人发出的铺天盖地的通知感到厌倦。尽管如此，依赖管理机器人正变得越来越受欢迎。这种对比促使我们调查目前GitHub上最显眼的机器人——可靠，以揭示最先进的依赖管理机器人的有效性和局限性。我们使用探索性数据分析和开发人员调查来评估可靠在保持依赖关系最新、与开发人员互动、减少更新怀疑和减少通知疲劳方面的有效性。我们得到了喜忧参半的结果。从积极的方面来看，在采用可靠技术后，项目确实减少了技术滞后，开发人员非常容易接受其拉取请求。消极的一面是，它的兼容性分数太少，无法有效减少更新怀疑；开发人员倾向于将可靠配置为减少通知数量；11.3%的项目不赞成使用可靠产品，而赞成使用其他替代产品。该调查证实了我们的发现，并深入了解了可靠的关键缺失功能。基于我们的发现，我们推导并总结了理想的依赖管理机器人的关键特征，该机器人可以分为四个维度：可配置性、自主性、透明度和自适应性。,可靠，依赖关系管理，挖掘软件存储库，软件工程机器人,,,
MHUJSVD2,2023,https://doi.org/10.1109/TSE.2022.3162236,TSE 2023,Selecting Context-Sensitivity Modularly for Accelerating Object-Sensitive Pointer Analysis,"Object-sensitive pointer analysis (denoted kobj under $k$-limiting) for an object-oriented program can be accelerated if context-sensitivity can be selectively applied to only some precision-critical variables/objects in a program. Existing pre-analyses for making such selections, which are performed as whole-program analyses to a program, are developed based on two broad approaches. One approach preserves the precision of object-sensitive pointer analysis but achieves limited speedups by reasoning about all the possible value flows in the program conservatively, while the other approach achieves greater speedups but sacrifices precision (often unduly) by examining only some but not all the value flows in the program heuristically. In this paper, we introduce a new pre-analysis approach, Turner$^{\mathcal{m}}$ (where $\mathcal {m}$ stands for modularity), that represents a sweet spot between these two existing ones, as it is designed to enable kobj to run significantly faster than the former approach and achieve significantly better precision than the latter approach. Turner$^{\mathcal{m}}$ is simple, lightweight yet effective due to two novel aspects in its design. First, we exploit a key observation that some precision-uncritical objects in the program can be approximated based on the object-containment relationship pre-established (from Andersen's analysis). In practice, this approximation introduces only a small degree of imprecision into kobj. Second, leveraging this initial approximation, we apply a novel object reachability analysis to the program by pre-analyzing its methods according to a reverse topological order of its call graph. When pre-analyzing each method, we make use of a simple DFA (Deterministic Finite Automaton) to reason about object reachability intra-procedurally from its entry to its exit along all the possible value flows established by its statements to identify its precision-critical variables/objects. In practice, this new modular object reachability analysis, which runs linearly in terms of the number of statements in the program, introduces again only a small loss of precision into kobj. We have validated Turner$^{\mathcal{m}}$ with an open-source implementation in Soot (already publicly available) against the state of the art by using a set of 12 widely used Java benchmarks and applications.","Object-sensitive pointer analysis,CFL reachability,object containment,modular static analysis",模块化选择上下文敏感度加速对象敏感指针分析,如果上下文敏感性可以选择性地仅应用于程序中的一些精度关键变量/对象，则可以加速面向对象程序的对象敏感指针分析（在$k$限制下表示为kobj）。现有的用于进行此类选择的预分析是作为对程序的整体程序分析进行的，基于两种广泛的方法进行开发。一种方法保留了对象敏感指针分析的精度，但通过保守地推理程序中所有可能的值流来实现有限的加速，而另一种方法通过启发式地只检查程序中的部分而不是全部值流来获得更大的加速，但牺牲了精度（通常是不适当的）。在本文中，我们介绍了一种新的预分析方法Turner$^｛\mathcal｛m｝｝$（其中$\mathcal{m｝$代表模块化），它代表了这两种现有方法之间的一个最佳点，因为它的设计目的是使kobj比前一种方法运行得更快，并实现比后一种方法更好的精度。Turner$^｛\mathcal｛m｝｝$由于其设计中的两个新颖方面而简单、轻便但有效。首先，我们利用了一个关键的观察结果，即程序中的一些精度不高的对象可以基于预先建立的对象包含关系（来自Andersen的分析）进行近似。在实践中，这种近似只在kobj中引入了很小程度的不精确性。其次，利用这种初始近似，我们通过根据调用图的反向拓扑顺序预先分析程序的方法，将一种新的对象可达性分析应用于程序。在预分析每种方法时，我们使用一个简单的DFA（确定性有限自动机）来推理对象的可达性，从其入口到出口，沿着其语句建立的所有可能的值流，以识别其精度关键变量/对象。在实践中，这种新的模块化对象可达性分析根据程序中的语句数量线性运行，再次在kobj中只引入了很小的精度损失。我们已经通过使用一组12个广泛使用的Java基准测试和应用程序，在Soot中使用开源实现（已经公开）验证了Turner$^｛\mathcal｛m｝｝$与现有技术的对比。,对象敏感指针分析，CFL可达性，对象包容，模块静态分析,,,
2UGPUCXU,2023,https://doi.org/10.1109/TSE.2022.3158543,TSE 2023,Quality Evaluation of Modern Code Reviews Through Intelligent Biometric Program Comprehension,"Code review is an essential practice in software engineering to spot code defects in the early stages of software development. Modern code reviews (e.g., acceptance or rejection of pull requests with Git) have become less formal than classic Fagan's inspections, lightweight, and more reliant on individuals (i.e., reviewers). However, reviewers may encounter mentally demanding challenges during the code review, such as code comprehension difficulties or distractions that might affect the code review quality. This work proposes a novel approach that evaluates the quality of code reviews in terms of bug-finding effectiveness and provides the reviewers with a clear message of whether the review should be repeated, indicating the code regions that may not have been well-reviewed. The proposed approach utilizes biometric information collected from the reviewer during the review process using non-intrusive biofeedback devices (e.g., smartwatches). Biometric measures such as Heart Rate Variability (HRV) and task-evoked pupillary response are captured as a surrogate of the cognitive state of the reviewer (e.g., mental workload) and inexpensive desktop eye-trackers compatible with the software development settings. This work uses Artificial Intelligence techniques to predict the cognitive load from the extracted biomarkers and classify each code region according to a set of features. The final evaluation considers various factors such as code complexity, time of the code review, the experience level of the reviewer, and other factors. Our experimental results show the approach could predict the review quality with 87.77%±4.65 accuracy and a Spearman correlation coefficient of 0.85 (p-value < 0.001) between the predicted and the actual review performance. This evaluation validates the cognitive load measurement using electroencephalography (EEG) signals as ground truth for the HRV and pupil signals.","Artificial Intelligence,Biometrics,Code inspections and walkthroughs,Human factors",基于智能生物识别程序理解的现代代码评审质量评估,代码评审是软件工程中的一项重要实践，用于在软件开发的早期阶段发现代码缺陷。现代的代码审查（例如，使用Git接受或拒绝拉取请求）已经变得不像经典的Fagan检查那样正式、轻量级，并且更依赖于个人（即审查人员）。然而，评审人员在代码评审过程中可能会遇到心理上要求很高的挑战，例如代码理解困难或可能影响代码评审质量的干扰。这项工作提出了一种新的方法，根据错误发现的有效性来评估代码审查的质量，并向审查人员提供是否应该重复审查的明确信息，指出可能没有得到很好审查的代码区域。所提出的方法利用在审查过程中使用非侵入性生物反馈设备（例如智能手表）从审查者收集的生物特征信息。生物特征测量，如心率变异性（HRV）和任务诱发的瞳孔反应，被捕获作为审查者认知状态（如精神工作量）的替代品，以及与软件开发设置兼容的廉价台式眼动仪。这项工作使用人工智能技术从提取的生物标志物中预测认知负荷，并根据一组特征对每个代码区域进行分类。最终评估考虑了各种因素，如代码复杂性、代码审查时间、审查人员的经验水平和其他因素。我们的实验结果表明，该方法可以预测复习质量，准确率为87.77%±4.65，预测成绩与实际复习成绩之间的Spearman相关系数为0.85（p值<0.001）。该评估验证了使用脑电图（EEG）信号作为HRV和瞳孔信号的基本事实的认知负荷测量。,人工智能，生物识别，代码检查和演练，人为因素,,,
QPSJTFXB,2023,https://doi.org/10.1109/TSE.2022.3148539,TSE 2023,The Human Side of Software Engineering Teams: An Investigation of Contemporary Challenges,"Context: There have been numerous recent calls for research on the human side of software engineering and its impact on various factors such as productivity, developer happiness and project success. An analysis of which challenges in software engineering teams are most frequent is still missing. As teams are more international, it is more frequent that their members have different human values as well as different communication habits. Additionally, virtual team setups (working geographically separated, remote communication using digital tools and frequently changing team members) are increasingly prevalent. Objective: We aim to provide a starting point for a theory about contemporary human challenges in teams and their causes in software engineering. To do so, we look to establish a reusable set of challenges and start out by investigating the effect of team virtualization. Virtual teams often use digital communication and consist of members with different nationalities that may have more divergent human values due to cultural differences compared to single nationality teams. Method: We designed a survey instrument and asked respondents to assess the frequency and criticality of a set of challenges, separated in context ”within teams” as well as ”between teams and clients”, compiled from previous empirical work, blog posts, and pilot survey feedback. For the team challenges, we asked if mitigation measures were already in place to tackle the challenge. Respondents were also asked to provide information about their team setup. The survey included the Personal Value Questionnaire to measure Schwartz human values. Finally, respondents were asked if there were additional challenges at their workplace. The survey was first piloted and then distributed to professionals working in software engineering teams via social networking sites and personal business networks. Result: In this article, we report on the results obtained from 192 respondents. We present a set of challenges that takes the survey feedback into account and introduce two categories of challenges; ”interpersonal” and ”intrapersonal”. We found no evidence for links between human values and challenges. We found some significant links between the number of distinct nationalities in a team and certain challenges, with less frequent and critical challenges occurring if 2-3 different nationalities were present compared to a team having members of just one nationality or more than three. A higher degree of virtualization seems to increase the frequency of some human challenges, which warrants further research about how to improve working processes when teams work from remote or in a distributed fashion. Conclusion: We present a set of human challenges in software engineering that can be used for further research on causes and mitigation measures, which serves as our starting point for a theory about causes of contemporary human challenges in software engineering teams. We report on evidence that a higher degree of virtualization of teams leads to an increase of certain challenges. This warrants further research to gather more evidence and test countermeasures, such as whether the employment of virtual reality software incorporating facial expressions and movements can help establish a less detached way of communication.","Software engineering,human challenges,virtual teams,human values,diversity,survey research",软件工程团队人性化的一面：对当代挑战的调查,背景：最近有很多人呼吁研究软件工程的人性方面及其对生产力、开发人员幸福感和项目成功等各种因素的影响。对软件工程团队中哪些挑战最常见的分析仍然缺失。随着团队的国际化，他们的成员往往有不同的人类价值观以及不同的沟通习惯。此外，虚拟团队设置（在地理上分开工作、使用数字工具进行远程通信以及频繁更换团队成员）越来越普遍。目的：我们旨在为软件工程中团队中的当代人类挑战及其原因的理论提供一个起点。为此，我们希望建立一套可重复使用的挑战，并从研究团队虚拟化的效果开始。虚拟团队通常使用数字通信，由不同国籍的成员组成，与单一国籍的团队相比，由于文化差异，这些成员可能具有更不同的人类价值观。方法：我们设计了一个调查工具，并要求受访者评估一系列挑战的频率和关键性，这些挑战在“团队内部”以及“团队和客户之间”的背景下是分开的，这些挑战是根据以前的实证工作、博客文章和试点调查反馈汇编而成的。对于团队挑战，我们询问是否已经采取了缓解措施来应对挑战。受访者还被要求提供有关其团队设置的信息。该调查包括测量施瓦茨人类价值观的个人价值问卷。最后，受访者被问及工作场所是否存在其他挑战。该调查首先进行了试点，然后通过社交网站和个人商业网络分发给软件工程团队的专业人员。结果：在本文中，我们报告了192名受访者的调查结果。我们提出了一系列将调查反馈考虑在内的挑战，并引入了两类挑战；”人际”和“内在”。我们没有发现人类价值观和挑战之间存在联系的证据。我们发现，团队中不同国籍的人数与某些挑战之间存在一些重要联系，与只有一个或三个以上国籍的团队相比，如果有2-3个不同国籍的团队成员，则会出现较少的重大挑战。更高程度的虚拟化似乎会增加一些人为挑战的频率，这就需要进一步研究团队在远程或分布式工作时如何改进工作流程。结论：我们提出了一组软件工程中的人类挑战，可用于进一步研究原因和缓解措施，这是我们在软件工程团队中建立当代人类挑战原因理论的起点。我们报告的证据表明，团队的高度虚拟化会导致某些挑战的增加。这就需要进行进一步的研究，以收集更多的证据并测试对策，例如使用包含面部表情和动作的虚拟现实软件是否有助于建立一种不那么超然的沟通方式。,软件工程，人类挑战，虚拟团队，人类价值观，多样性，调查研究,,,
KLQ87IF3,2023,https://doi.org/10.1109/TSE.2022.3171404,TSE 2023,We're Not Gonna Break It! Consistency-Preserving Operators for Efficient Product Line Configuration,"When configuring a software product line, finding a good trade-off between multiple orthogonal quality concerns is a challenging multi-objective optimisation problem. State-of-the-art solutions based on search-based techniques create invalid configurations in intermediate steps, requiring additional repair actions that reduce the efficiency of the search. In this work, we introduce consistency-preserving configuration operators (CPCOs)—genetic operators that maintain valid configurations throughout the entire search. CPCOs bundle coherent sets of changes: the activation or deactivation of a particular feature together with other (de)activations that are needed to preserve validity. In our evaluation, our instantiation of the IBEA algorithm with CPCOs outperforms two state-of-the-art tools for optimal product line configuration in terms of both speed and solution quality. The improvements are especially pronounced in large product lines with thousands of features.","Software product lines,feature model configuration,search-based software engineering",我们不会打破它！高效产品线配置的保一致性算子,在配置软件产品线时，在多个正交质量问题之间找到良好的权衡是一个具有挑战性的多目标优化问题。基于基于搜索的技术的现有技术解决方案在中间步骤中创建无效配置，需要降低搜索效率的额外修复操作。在这项工作中，我们引入了保持一致性的配置算子（CPCO）——在整个搜索过程中保持有效配置的遗传算子。CPCO将连贯的变化集捆绑在一起：将特定功能的激活或去激活与保持有效性所需的其他（去）激活一起。在我们的评估中，我们用CPCO实例化的IBEA算法在速度和解决方案质量方面都优于两种最先进的优化产品线配置工具。这些改进在具有数千个功能的大型产品线中尤为明显。,软件产品线，功能模型配置，基于搜索的软件工程,,,
AJRK6ME9,2023,https://doi.org/10.1109/TSE.2023.3275655,TSE 2023,Can We Trust the Phone Vendors? Comprehensive Security Measurements on the Android Firmware Ecosystem,"Android is the most popular smartphone platform with over 85% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make customized products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous works focus on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem security and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale comprehensive measurement of the Android firmware ecosystem security. Our study is based on 8,325 firmware images from 153 vendors and 813 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabilities, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, AndScanner+, to complete firmware crawling, firmware parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, several interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android firmware images, say 31.4% and 5.6% of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In addition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities. There are 46 new vulnerabilities found by AndScanner+, 36 of which have been assigned CVE/CNVD IDs. This study provides much new knowledge of the Android firmware ecosystem with a deep understanding of software engineering security practices.","Android firmware ecosystem,security measurements,security patches,pre-installed apps",我们可以信任电话供应商吗？安卓固件生态系统的全面安全措施,安卓是最受欢迎的智能手机平台，市场份额超过85%。它的成功建立在开放的基础上，手机供应商可以利用Android源代码制作具有独特软件/硬件功能的定制产品。另一方面，安卓系统的碎片化和定制化也带来了许多安全风险，引起了研究人员的关注。为了调查定制的安卓固件的安全性，人们付出了许多努力。然而，以前的大多数工作都集中在设计高效的分析工具或分析固件的特定方面。目前还缺乏对安卓固件生态系统安全的全景，以及基于大规模固件数据集的相应理解。在这项工作中，我们对安卓固件生态系统的安全性进行了大规模的全面测量。我们的研究基于来自153家供应商的8325个固件映像和813个与安卓相关的CVE，这是有史以来用于安全测量的最大的安卓固件数据集。特别是，我们的研究遵循了一系列研究问题，包括漏洞、补丁、安全更新和预装应用程序。为了自动化分析过程，我们设计了一个框架AndScanner+，用于完成固件爬网、固件解析、补丁分析和应用程序分析。通过大量的数据分析和案例探索，得出了一些有趣的发现。例如，补丁延迟和丢失问题在Android固件映像中普遍存在，分别占所有映像的31.4%和5.6%。几款手机的最新图片中仍然包含易受攻击的预装应用程序，甚至相应的漏洞也已公开披露。除了数据测量外，我们还通过案例研究探讨了这些安全威胁背后的原因，并证明发现的安全威胁可以转化为可利用的漏洞。AndScanner+发现了46个新漏洞，其中36个已分配CVE/CNVD ID。这项研究为Android固件生态系统提供了许多新的知识，并对软件工程安全实践有了深入的了解。,安卓固件生态系统，安全措施，安全补丁，预装应用程序,,,
WLB87HGE,2023,https://doi.org/10.1109/TSE.2023.3295801,TSE 2023,Code2Img: Tree-Based Image Transformation for Scalable Code Clone Detection,"Code clone detection is an active research domain of software engineering. There are two core demands for clone detection: scalable detection and complicated clone detection. For scalable detection, existing approaches treat the source code as a text or token sequence and then calculate their similarity. However, the text-based and token-based approaches are difficult to detect complicated clone types due to the lack of consideration of code structure. The methods based on intermediate representations of code can effectively achieve complex clone types detection but are limited by the complexity of representations to be scalable. In this paper, we propose Code2Img, a tree-based code clone detector, which satisfies scalability while detecting complicated clones effectively. Given the source code, we first perform clone filtering by the inverted index to locate the suspected clones. For each suspected clone, we create the adjacency image based on the adjacency matrix of the normalized abstract syntax tree (AST). Then we design an image encoder to highlight the structural details further and refine pixels of the image. Specifically, we employ the Markov model to encode the adjacency image into a state probability image and remove its useless pixels. By this, the original complex tree can be transformed into a one-dimensional vector while preserving the structural feature of the AST. Finally, we detect clones by calculating the Jaccard Similarity of these vectors. We conduct comparative evaluations on effectiveness and scalability with eight other state-of-the-art clone detectors (SourcererCC, NIL, LVMapper, Nicad, Siamese, CCAligner, Deckard, and Yang2018). The experimental results show that Code2Img achieves the best performance among all the comparative tools in terms of both detection effectiveness and scalability. It indicates that Code2Img can be applicable to scalable complicated clone detection.","Code clone,clone detection,scalability",Code2Img:用于可扩展代码克隆检测的基于树的图像转换,代码克隆检测是软件工程中一个活跃的研究领域。克隆检测有两个核心需求：可扩展检测和复杂的克隆检测。对于可伸缩检测，现有的方法将源代码视为文本或令牌序列，然后计算它们的相似性。然而，由于缺乏对代码结构的考虑，基于文本和基于令牌的方法很难检测复杂的克隆类型。基于代码的中间表示的方法可以有效地实现复杂的克隆类型检测，但由于表示的复杂性而受到可扩展性的限制。在本文中，我们提出了一种基于树的代码克隆检测器Code2Img，它在有效检测复杂克隆的同时满足可扩展性。给定源代码，我们首先通过反向索引执行克隆筛选，以定位可疑的克隆。对于每个可疑的克隆，我们基于归一化抽象语法树（AST）的邻接矩阵创建邻接图像。然后，我们设计了一个图像编码器，以进一步突出结构细节并细化图像的像素。具体来说，我们使用马尔可夫模型将相邻图像编码为状态概率图像，并去除其无用像素。通过这种方式，可以将原始复杂树转换为一维向量，同时保留AST的结构特征。最后，我们通过计算这些向量的Jaccard相似度来检测克隆。我们与其他八种最先进的克隆检测器（SourcererCC、NIL、LVMapper、Nicad、Siamese、CCAligner、Deckard和Yang2018）对有效性和可扩展性进行了比较评估。实验结果表明，Code2Img在检测有效性和可扩展性方面都是所有比较工具中性能最好的。这表明Code2Ig可以应用于可扩展的复杂克隆检测。,代码克隆，克隆检测，可扩展性,,,
KZT55YWL,2023,https://doi.org/10.1109/TSE.2023.3309610,TSE 2023,"scenoRITA: Generating Diverse, Fully Mutable, Test Scenarios for Autonomous Vehicle Planning","Autonomous Vehicles (AVs) leverage advanced sensing and networking technologies (e.g., camera, LiDAR, RADAR, GPS, DSRC, 5G, etc.) to enable safe and efficient driving without human drivers. Although still in its infancy, AV technology is becoming increasingly common and could radically transform our transportation system and by extension, our economy and society. As a result, there is tremendous global enthusiasm for research, development, and deployment of AVs, e.g., self-driving taxis and trucks from Waymo and Baidu. The current practice for testing AVs uses virtual tests—where AVs are tested in software simulations—since they offer a more efficient and safer alternative compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, systematically creating valid and effective tests for AV software remains a major challenge. To address this challenge, we introduce scenoRITA, a test generation approach for AVs that uses an evolutionary algorithm with (1) a novel gene representation that allows obstacles to be fully mutable, hence, resulting in more reported violations and more diverse scenarios, (2) 5 test oracles to determine both safety and motion sickness-inducing violations and (3) a novel technique to identify and eliminate duplicate tests. Our extensive evaluation shows that scenoRITA can produce test scenarios that are more effective in revealing ADS bugs and more diverse in covering different parts of the map compared to other state-of-the-art test generation approaches.","Embedded/cyber-physical systems,search-based software engineering,software testing",场景RITA：为自动驾驶汽车规划生成多样化、完全可变的测试场景,自动驾驶汽车（AV）利用先进的传感和网络技术（如摄像头、激光雷达、雷达、GPS、DSRC、5G等），实现无人驾驶的安全高效驾驶。尽管AV技术仍处于起步阶段，但它正变得越来越普遍，并可能从根本上改变我们的交通系统，进而改变我们的经济和社会。因此，全球对电动汽车的研发和部署充满热情，例如Waymo和百度的自动驾驶出租车和卡车。目前测试AV的做法使用虚拟测试——在软件模拟中测试AV——因为与现场操作测试相比，它们提供了更高效、更安全的替代方案。具体来说，基于搜索的方法用于查找特别关键的情况。这些方法提供了一个自动生成测试的机会；然而，系统地为AV软件创建有效的测试仍然是一个重大挑战。为了应对这一挑战，我们引入了scenoRITA，这是一种AVs的测试生成方法，它使用了一种进化算法，该算法具有（1）一种新的基因表示，允许障碍物完全可变，从而导致更多报告的违规行为和更多样的场景，（2）5个测试神谕，以确定安全性和引发晕动病的违规行为；（3）一种识别和消除重复测试的新技术。我们的广泛评估表明，与其他最先进的测试生成方法相比，scenoRITA可以生成更有效地揭示ADS错误的测试场景，并且在覆盖地图的不同部分方面更具多样性。,嵌入式/网络物理系统，基于搜索的软件工程，软件测试,,,
KYW5ZWS3,2023,https://doi.org/10.1109/TSE.2022.3215628,TSE 2023,Scalably Detecting Third-Party Android Libraries With Two-Stage Bloom Filtering,"Third-party library (TPL) detection is important for Android app security analysis nowadays. Unfortunately, the existing techniques often suffer from poor scalability. In some situations, the detection time cost is even unacceptable. Although a few existing methods run relatively fast, they cannot provide enough effectiveness, especially for non-structure-preserving obfuscated apps, e.g., repackaged and flattened. In this paper, we treat TPLs detection as a set inclusion problem to effectively and efficiently analyze obfuscated apps, and develop a scalable two-stage detection approach, Libloom. Specifically, the package and class signatures are encoded into two levels of Bloom filters respectively. At the first stage, the package filters are used to identify a limited number of candidate TPLs via set overlapping measurement to avoid unnecessary class-level set analysis. Subsequently, with the class filters, a similarity score is computed between the query app and each candidate to detect the integrated TPLs, and a novel entropy-based metric is presented to specially handle the repackaged and flattened apps. We have evaluated Libloom on some large-scale benchmarks involving tens of thousands of TPL instances. The experiment results demonstrate that Libloom outperforms state-of-the-art tools in both effectiveness and efficiency. Especially, the proposed two-stage method can run about ten times faster than the straightforward class-level analysis on flattened apps, and without loss of accuracy.","Third-party library,non-structure-preserving obfuscation,set inclusion,bloom filter,entropy",使用两阶段Bloom过滤可扩展地检测第三方安卓库,第三方库（TPL）检测是当今安卓应用程序安全分析的重要内容。不幸的是，现有技术的可扩展性往往较差。在某些情况下，检测时间成本甚至是不可接受的。尽管一些现有的方法运行速度相对较快，但它们无法提供足够的有效性，尤其是对于非结构保护的模糊应用程序，例如重新打包和扁平化。在本文中，我们将TPL检测视为一个集包含问题，以有效、高效地分析混淆的应用程序，并开发了一种可扩展的两阶段检测方法Libloom。具体来说，包和类签名分别被编码到两个级别的Bloom过滤器中。在第一阶段，包滤波器用于通过集合重叠测量来识别有限数量的候选TPL，以避免不必要的类级集合分析。随后，使用类过滤器，计算查询应用程序和每个候选程序之间的相似性得分，以检测集成的TPL，并提出了一种新的基于熵的度量，以专门处理重新打包和扁平化的应用程序。我们已经在涉及数万个TPL实例的一些大型基准上评估了Libloom。实验结果表明，Libloom在有效性和效率方面都优于最先进的工具。特别是，所提出的两阶段方法可以比在扁平应用程序上直接进行类级分析快十倍左右，而且不会损失准确性。,第三方库，非保结构模糊处理，集合包含，布隆过滤器，熵,,,
CJSRAYBE,2023,https://doi.org/10.1109/TSE.2022.3140868,TSE 2023,The Secret Life of Software Vulnerabilities: A Large-Scale Empirical Study,"Software vulnerabilities are weaknesses in source code that can be potentially exploited to cause loss or harm. While researchers have been devising a number of methods to deal with vulnerabilities, there is still a noticeable lack of knowledge on their software engineering life cycle, for example how vulnerabilities are introduced and removed by developers. This information can be exploited to design more effective methods for vulnerability prevention and detection, as well as to understand the granularity at which these methods should aim. To investigate the life cycle of known software vulnerabilities, we focus on how, when, and under which circumstances the contributions to the introduction of vulnerabilities in software projects are made, as well as how long, and how they are removed. We consider 3,663 vulnerabilities with public patches from the National Vulnerability Database—pertaining to 1,096 open-source software projects on GitHub—and define an eight-step process involving both automated parts (e.g., using a procedure based on the SZZ algorithm to find the vulnerability-contributing commits) and manual analyses (e.g., how vulnerabilities were fixed). The investigated vulnerabilities can be classified in 144 categories, take on average at least 4 contributing commits before being introduced, and half of them remain unfixed for at least more than one year. Most of the contributions are done by developers with high workload, often when doing maintenance activities, and removed mostly with the addition of new source code aiming at implementing further checks on inputs. We conclude by distilling practical implications on how vulnerability detectors should work to assist developers in timely identifying these issues.","Software vulnerabilities,mining software repositories,empirical software engineering",软件漏洞的秘密生命：一项大规模的实证研究,软件漏洞是源代码中的弱点，可能被利用来造成损失或伤害。尽管研究人员一直在设计一些处理漏洞的方法，但对其软件工程生命周期的了解仍然明显不足，例如开发人员如何引入和消除漏洞。可以利用这些信息来设计更有效的漏洞预防和检测方法，并了解这些方法的目标粒度。为了调查已知软件漏洞的生命周期，我们重点关注在软件项目中引入漏洞的方式、时间和情况，以及消除漏洞的时间和方式。我们考虑了国家漏洞数据库中的3663个带有公共补丁的漏洞，涉及GitHub上的1096个开源软件项目，并定义了一个八步流程，包括自动化部分（例如，使用基于SZZ算法的程序来查找导致提交的漏洞）和手动分析（例如，如何修复漏洞）。被调查的漏洞可分为144类，在引入之前平均至少需要4次提交，其中一半的漏洞在至少一年以上的时间内仍未修复。大多数贡献都是由工作量大的开发人员完成的，通常是在进行维护活动时，并且主要通过添加新的源代码来删除，目的是对输入进行进一步的检查。最后，我们提炼了漏洞检测器应该如何工作的实际含义，以帮助开发人员及时识别这些问题。,软件漏洞，挖掘软件存储库，经验软件工程,,,
9CG9HPSB,2023,https://doi.org/10.1109/TSE.2022.3181010,TSE 2023,Open or Sneaky? Fast or Slow? Light or Heavy?: Investigating Security Releases of Open Source Packages,"Vulnerabilities in open source packages can be a security risk for the downstream client projects. When a new vulnerability is discovered, a package should quickly release a fix in a new version, referred to as a security release in this study. The security release should be well-documented and require minimal migration effort to facilitate fast adoption by the clients. However, to what extent the open source packages follow these recommendations is not known. In this paper, we study (1) the time lag between fix and release; (2) how security fixes are documented in the release notes; (3) code change characteristics (size and semantic versioning) of the release; and (4) the time lag between the release and an advisory publication for security releases over a dataset of 4,377 security advisories across seven package ecosystems. We find that the median security release becomes available within 4 days of the corresponding fix and contains 131 lines of code (LOC) change. However, one-fourth of the releases in our data set still came at least 20 days after the fix was made.Further, we find that 61.5% of the security releases come with a release note that documents the corresponding security fix. Still, Snyk and NVD, two popular databases, take a median of 17 days (from the release) to publish a security advisory, possibly resulting in delayed notifications to the client projects. We also find that security releases may contain breaking change(s) as 13.2% indicated backward incompatibility through semantic versioning, while 6.4% mentioned breaking change(s) in the release notes. Based on our findings, we point out areas for future work, such as private fork for security fixes and standardized practice for announcing security releases.","Empirical study,open source security,supply chain security",打开还是偷偷？快还是慢？轻还是重？：调查开源软件包的安全发布,开源软件包中的漏洞可能是下游客户端项目的安全风险。当发现新的漏洞时，软件包应该在新版本中快速发布修复程序，在本研究中称为安全发布。安全发布应该有充分的文档记录，并且需要最少的迁移工作来促进客户端的快速采用。然而，开源软件包在多大程度上遵循了这些建议尚不清楚。在本文中，我们研究了（1）固定和释放之间的时间滞后；（2） 如何在发行说明中记录安全修复；（3） 发布的代码更改特性（大小和语义版本控制）；以及（4）在七个一揽子生态系统的4377份安全咨询数据集上，发布与安全发布咨询出版物之间的时间滞后。我们发现，中位安全发布在相应修复的4天内可用，并包含131行代码（LOC）更改。然而，我们数据集中四分之一的发布仍然是在修复后至少20天发布的。此外，我们发现61.5%的安全发布附带一个记录相应安全修复的发布说明。尽管如此，Snyk和NVD这两个流行的数据库（从发布之日起）发布安全咨询的平均时间为17天，这可能会导致向客户端项目的通知延迟。我们还发现，安全发布可能包含破坏性更改，13.2%的人通过语义版本控制表示向后不兼容，6.4%的人在发布说明中提到了破坏性更改。根据我们的发现，我们指出了未来工作的领域，例如安全修复的私有分支和发布安全发布的标准化实践。,实证研究，开源安全，供应链安全,,,
XUV9BF6Q,2023,https://doi.org/10.1109/TSE.2023.3290237,TSE 2023,"Privacy Engineering in the Wild: Understanding the Practitioners' Mindset, Organizational Aspects, and Current Practices","Privacy engineering, as an emerging field of research and practice, comprises the technical capabilities and management processes needed to implement, deploy, and operate privacy features and controls in working systems. For that, software practitioners and other stakeholders in software companies need to work cooperatively toward building privacy-preserving businesses and engineering solutions. Significant research has been done to understand the software practitioners’ perceptions of information privacy, but more emphasis should be given to the uptake of concrete privacy engineering components. This research delves into the software practitioners’ perspectives and mindset, organizational aspects, and current practices on privacy and its engineering processes. A total of 30 practitioners from nine countries and backgrounds were interviewed, sharing their experiences and voicing their opinions on a broad range of privacy topics. The thematic analysis methodology was adopted to code the interview data qualitatively and construct a rich and nuanced thematic framework. As a result, we identified three critical interconnected themes that compose our thematic framework for privacy engineering “in the wild”: (1) personal privacy mindset and stance, categorised into practitioners’ privacy knowledge, attitudes and behaviours; (2) organizational privacy aspects, such as decision-power and positive and negative examples of privacy climate; and, (3) privacy engineering practices, such as procedures and controls concretely used in the industry. Among the main findings, this study provides many insights about the state-of-the-practice of privacy engineering, pointing to a positive influence of privacy laws (e.g., EU General Data Protection Regulation) on practitioners’ behaviours and organizations’ cultures. Aspects such as organizational privacy culture and climate were also confirmed to have a powerful influence on the practitioners’ privacy behaviours. A conducive environment for privacy engineering needs to be created, aligning the privacy values of practitioners and their organizations, with particular attention to the leaders and top management's commitment to privacy. Organizations can also facilitate education and awareness training for software practitioners on existing privacy engineering theories, methods and tools that have already been proven effective.","Privacy,security,data protection,privacy engineering,privacy by design,software engineering,qualitative research",野外隐私工程：了解从业者的心态、组织方面和当前实践,隐私工程作为一个新兴的研究和实践领域，包括在工作系统中实施、部署和操作隐私功能和控制所需的技术能力和管理流程。为此，软件从业者和软件公司的其他利益相关者需要合作构建保护隐私的业务和工程解决方案。已经进行了大量的研究来了解软件从业者对信息隐私的看法，但应该更加重视具体的隐私工程组件。这项研究深入探讨了软件从业者对隐私及其工程过程的看法和心态、组织方面以及当前的实践。共有来自九个国家和背景的30名从业者接受了采访，分享了他们的经验，并就广泛的隐私话题发表了意见。采用主题分析方法对访谈数据进行定性编码，构建了一个丰富细致的主题框架。因此，我们确定了三个关键的相互关联的主题，这些主题构成了我们“野外”隐私工程的主题框架：（1）个人隐私心态和立场，分为从业者的隐私知识、态度和行为；（2） 组织隐私方面，如决策权和隐私氛围的正面和负面例子；以及（3）隐私工程实践，例如行业中具体使用的程序和控制。在主要发现中，本研究提供了许多关于隐私工程实践状况的见解，指出隐私法（如《欧盟通用数据保护条例》）对从业者的行为和组织文化产生了积极影响。组织隐私文化和氛围等方面也被证实对从业者的隐私行为有着强大的影响。需要为隐私工程创造一个有利的环境，使从业者及其组织的隐私价值观保持一致，并特别关注领导者和最高管理层对隐私的承诺。组织还可以促进软件从业者对现有隐私工程理论、方法和工具的教育和意识培训，这些理论、方法、工具已经被证明是有效的。,隐私，安全，数据保护，隐私工程，设计隐私，软件工程，定性研究,,,
FICFNNDG,2023,https://doi.org/10.1109/TSE.2023.3307243,TSE 2023,ADPTriage: Approximate Dynamic Programming for Bug Triage,"Bug triaging is a critical task in any software development project. It entails triagers going over a list of open bugs, deciding whether each is required to be addressed, and, if so, which developer should fix it. However, the manual bug assignment in Issue Tracking Systems (ITS) offers only a limited solution and might easily fail when triagers are required to handle a large number of bug reports. During the automated assignment, there are multiple sources of uncertainties in the ITS, which should be addressed meticulously. In this study, we develop a Markov decision process (MDP) model for an online bug triage problem. In addition to an optimization-based myopic technique, we provide an ADP-based bug triage solution, called ADPTriage, which has the ability to reflect the downstream uncertainty in the bug arrivals and developers’ timetables. Specifically, without placing any limits on the underlying stochastic process, this technique enables real-time decision-making on bug assignments while taking into consideration developers’ expertise, bug type, and bug fixing time. Our result shows a significant improvement over the myopic approach in terms of assignment accuracy and fixing time. We also demonstrate the empirical convergence of the model and conduct sensitivity analysis with various model parameters. Accordingly, this work constitutes a significant step forward in addressing the uncertainty in bug triage.","Software engineering,bug triage,Reinforcement Learning,Approximate Dynamic Programming,software quality",ADPTriage:错误分类的近似动态规划,Bug测试是任何软件开发项目中的一项关键任务。这需要测试人员查看一份打开的错误列表，决定是否需要解决每个错误，如果需要，由哪个开发人员进行修复。然而，问题跟踪系统（ITS）中的手动错误分配只提供了有限的解决方案，当测试人员需要处理大量错误报告时，可能很容易失败。在自动分配过程中，ITS中存在多种不确定性来源，应仔细处理。在这项研究中，我们开发了一个在线错误分类问题的马尔可夫决策过程（MDP）模型。除了基于优化的短视技术外，我们还提供了一种基于ADP的错误分类解决方案，称为ADPTriage，它能够反映错误到达和开发人员时间表中的下游不确定性。具体来说，在不限制底层随机过程的情况下，该技术能够在考虑开发人员的专业知识、错误类型和错误修复时间的同时，对错误分配进行实时决策。我们的研究结果表明，在任务准确性和固定时间方面，与近视方法相比有了显著的改进。我们还证明了模型的经验收敛性，并对各种模型参数进行了敏感性分析。因此，这项工作在解决错误分类的不确定性方面迈出了重要的一步。,软件工程，错误分类，强化学习，近似动态编程，软件质量,,,
YI76RBBZ,2023,https://doi.org/10.1109/TSE.2022.3188005,TSE 2023,Automated Generation and Evaluation of JMH Microbenchmark Suites From Unit Tests,"Performance is a crucial non-functional requirement of many software systems. Despite the widespread use of performance testing, developers still struggle to construct and evaluate the quality of performance tests. To address these two major challenges, we implement a framework, dubbed ju2jmh, to automatically generate performance microbenchmarks from JUnit tests and use mutation testing to study the quality of generated microbenchmarks. Specifically, we compare our ju2jmh generated benchmarks to manually written JMH benchmarks and to automatically generated JMH benchmarks using the AutoJMH framework, as well as directly measuring system performance with JUnit tests. For this purpose, we have conducted a study on three subjects (Rxjava, Eclipse-collections, and Zipkin) with $\sim$454K source lines of code (SLOC), 2,417 JMH benchmarks (including manually written and generated AutoJMH benchmarks) and 35,084 JUnit tests. Our results show that the ju2jmh generated JMH benchmarks consistently outperform using the execution time and throughput of JUnit tests as a proxy of performance and JMH benchmarks automatically generated using the AutoJMH framework while being comparable to JMH benchmarks manually written by developers in terms of tests’ stability and ability to detect performance bugs. Nevertheless, ju2jmh benchmarks are able to cover more of the software applications than manually written JMH benchmarks during the microbenchmark execution. Furthermore, ju2jmh benchmarks are generated automatically, while manually written JMH benchmarks require many hours of hard work and attention; therefore our study can reduce developers’ effort to construct microbenchmarks. In addition, we identify three factors (too low test workload, unstable tests and limited mutant coverage) that affect a benchmark's ability to detect performance bugs. To the best of our knowledge, this is the first study aimed at assisting developers in fully automated microbenchmark creation and assessing microbenchmark quality for performance testing.","Performance,performance testing,performance microbenchmarking,JMH,performance mutation testing",从单元测试中自动生成和评估JMH微基准套件,性能是许多软件系统的关键非功能性需求。尽管性能测试被广泛使用，但开发人员仍然难以构建和评估性能测试的质量。为了解决这两个主要挑战，我们实现了一个名为ju2jmh的框架，从JUnit测试中自动生成性能微基准，并使用突变测试来研究生成的微基准的质量。具体来说，我们将ju2jmh生成的基准测试与手动编写的JMH基准测试、使用AutoJMH框架自动生成的JMH标准测试以及使用JUnit测试直接测量系统性能进行了比较。为此，我们对三个主题（Rxjava、Eclipse集合和Zipkin）进行了一项研究，其中包括$\sim 454K源代码行（SLOC）、2417个JMH基准测试（包括手动编写和生成的AutoJMH基准）和35084个JUnit测试。我们的结果表明，ju2jmh生成的JMH基准测试始终优于使用JUnit测试的执行时间和吞吐量作为性能代理的JMH，以及使用AutoJMH框架自动生成的JMH基准测试，同时在测试的稳定性和检测性能错误的能力方面与开发人员手动编写的JMH标准测试相当。尽管如此，在微基准测试执行期间，ju2jmh基准测试能够覆盖比手动编写的JMH基准测试更多的软件应用程序。此外，ju2jmh基准测试是自动生成的，而手动编写的JMH基准测试需要许多小时的艰苦工作和关注；因此，我们的研究可以减少开发人员构建微基准的工作量。此外，我们确定了影响基准测试检测性能缺陷能力的三个因素（测试工作量过低、测试不稳定和变异覆盖率有限）。据我们所知，这是第一项旨在帮助开发人员全自动创建微基准点并评估性能测试微基准点质量的研究。,性能，性能测试，性能微标记，JMH，性能突变测试,,,
8QCRU7SU,2023,https://doi.org/10.1109/TSE.2023.3269500,TSE 2023,\textdollar{\textbackslashtext{FS}{3}}_{\textbackslashtext{change}}\textdollarFS3change: A Scalable Method for Change Pattern Mining,"Mining change patterns can give unique understanding on the evolution of dynamically changing systems like social relation graphs, weblinks, hardware descriptions and models. A more recent focus is source code change pattern mining that may qualitatively justify expected or uncover unexpected patterns. These patterns then offer a basis, e.g., for program language evolution or auto-completion support. We present a change pattern mining method that greatly expands the limits of input data and pattern complexity, over existing methods. We propose scalability solutions on conceptual and algorithmic level, thereby evolving the state-of-the-art sampling-based frequent subgraph mining method FS3, resulting in 75% reduction in memory consumption and a speedup of 6500 for a large scale dataset. Patterns can have 100,000 s of occurrences for which manual review is impossible and may lead to misinterpretation. We propose the novel content track approach for interactively exploring pattern contents in context, based on marginal distributions. We evaluate our approach by mining 1,000 open source projects contributing a total of 558 million changes and 2 billion contextual connections among them, thereby, demonstrating its scalability. A manual interpretation of 19 patterns shows sensible mined patterns allowing to deduct implications for language design and demonstrating the soundness of the approach.","Auto-completion,change patterns,code changes,data mining,frequent graph mining,scalability",\textdollar｛\textrashtext｛FS｝｛3｝｝_｛\txtrashttext｛change｝\textdollarFS3change：一种可扩展的变化模式挖掘方法,挖掘变化模式可以对动态变化系统（如社会关系图、网络链接、硬件描述和模型）的演变提供独特的理解。最近的一个焦点是源代码更改模式挖掘，它可以定性地证明预期的模式或揭示意外的模式。然后，这些模式为程序语言进化或自动完成支持提供了基础。我们提出了一种变化模式挖掘方法，与现有方法相比，该方法大大扩展了输入数据和模式复杂性的限制。我们在概念和算法层面提出了可扩展性解决方案，从而发展了最先进的基于采样的频繁子图挖掘方法FS3，从而使大规模数据集的内存消耗减少了75%，速度提高了6500。模式可能有100000 s的事件，手动审查是不可能的，并可能导致误解。我们提出了一种新的基于边际分布的内容跟踪方法，用于在上下文中交互式地探索模式内容。我们通过挖掘1000个开源项目来评估我们的方法，这些项目总共贡献了5.58亿个更改和20亿个上下文连接，从而证明了其可扩展性。对19种模式的手动解释显示了合理的挖掘模式，可以推断出语言设计的含义，并证明该方法的合理性。,自动完成，更改模式，代码更改，数据挖掘，频繁图形挖掘，可扩展性,,,
I4YYWV4N,2023,https://doi.org/10.1109/TSE.2022.3178945,TSE 2023,Learning to Predict User-Defined Types,"TypeScript is a widely adopted gradual typed language where developers can optionally type variables, functions, parameters and more. Probabilistic type inference approaches with ML (machine learning) work well especially for commonly occurring types such as boolean, number, and string. TypeScript permits a wide range of types including developer defined class names and type interfaces. These developer defined types, termed user-defined types, can be written within the realm of language naming conventions. The set of user-defined types is boundless and existing bounded type guessing approaches are an imperfect solution. Existing works either under perform in user-defined types or ignore user-defined types altogether. This work leverages a BERT-style pre-trained model, with multi-task learning objectives, to learn how to type user-defined classes and interfaces. Thus we present DiverseTyper, a solution that explores the diverse set of user-defined types by uniquely aligning classes and interfaces declarations to the places in which they are used. DiverseTyper surpasses all existing works including those that model user-defined types.","Multi-Task learning,representation learning,transfer learning,type inference.",学习预测用户定义的类型,TypeScript是一种广泛采用的渐进式类型语言，开发人员可以选择键入变量、函数、参数等。ML（机器学习）的概率类型推理方法工作得很好，尤其是对于常见的类型，如布尔值、数字和字符串。TypeScript允许多种类型，包括开发人员定义的类名和类型接口。这些开发人员定义的类型，称为用户定义类型，可以在语言命名约定的范围内编写。用户定义的类型集是无限的，现有的有界类型猜测方法是不完美的解决方案。现有作品要么在用户定义类型中执行不足，要么完全忽略用户定义类型。这项工作利用BERT风格的预训练模型，具有多任务学习目标，来学习如何键入用户定义的类和接口。因此，我们提出了DiversType，这是一个解决方案，通过将类和接口声明与使用它们的位置唯一地对齐，来探索用户定义类型的不同集合。DiversType超越了所有现有作品，包括那些为用户定义类型建模的作品。,多任务学习，表征学习，迁移学习，类型推理。,,,
G7VF7BVH,2023,https://doi.org/10.1109/TSE.2022.3187811,TSE 2023,A Comprehensive Study on ARM Disassembly Tools,"Embedded devices are becoming ubiquitous, and ARM is becoming the dominant architecture for them. Meanwhile, there is a pressing need to perform security assessments for these devices. Due to different types of peripherals, emulating the software, i.e., firmware, of these devices in scale is challenging. Therefore, static analysis is still widely used. Existing works usually leverage off-the-shelf tools to disassemble stripped ARM binaries and (implicitly) assume that reliably disassembling binaries is a solved problem. However, whether this assumption really holds is unknown. In this paper, we conduct the first comprehensive study on ARM disassembly tools. Specifically, we build 1,896 ARM binaries (including 248 obfuscated ones) with different compilers, compiling options, and obfuscation methods. We then evaluate them using eight state-of-the-art ARM disassembly tools (including both commercial and noncommercial ones) in three different versions on their capabilities to locate instruction boundary, function boundary, and function signature. Instruction and function boundary are two fundamental primitives that the other primitives are built upon while function signature is significant for control flow integrity (CFI) techniques. Our work reveals some observations that have not been systematically summarized and/or confirmed. For instance, we find that the existence of both ARM and Thumb instruction sets, and the reuse of the BL instruction for both function calls and branches bring serious challenges to disassembly tools. Our evaluation sheds light on the limitations of state-of-the-art disassembly tools and points out potential directions for improvement.","ARM,disassembly,reverse engineering",ARM拆卸工具的综合研究,嵌入式设备正在变得无处不在，ARM正在成为它们的主导架构。与此同时，迫切需要对这些设备进行安全评估。由于外围设备的类型不同，大规模模拟这些设备的软件（即固件）具有挑战性。因此，静态分析仍然被广泛使用。现有的工作通常利用现成的工具来反汇编剥离的ARM二进制文件，并（隐含地）假设可靠地反汇编二进制文件是一个已解决的问题。然而，这个假设是否真的成立还不得而知。在本文中，我们对ARM拆卸工具进行了首次全面的研究。具体来说，我们使用不同的编译器、编译选项和模糊处理方法构建了1896个ARM二进制文件（包括248个模糊处理的二进制文件）。然后，我们使用八种最先进的ARM反汇编工具（包括商业和非商业反汇编工具）在三个不同版本中评估它们定位指令边界、函数边界和函数签名的能力。指令和函数边界是两个基本基元，其他基元是建立在这两个基元之上的，而函数签名对于控制流完整性（CFI）技术来说是重要的。我们的工作揭示了一些尚未得到系统总结和/或证实的观察结果。例如，我们发现ARM和Thumb指令集的存在，以及BL指令对函数调用和分支的重用，给反汇编工具带来了严重的挑战。我们的评估揭示了最先进的拆卸工具的局限性，并指出了潜在的改进方向。,ARM，拆卸，逆向工程,,,
ZMIGPGGS,2023,https://doi.org/10.1109/TSE.2023.3235942,TSE 2023,DupHunter: Detecting Duplicate Pull Requests in Fork-Based Development,"The emergence of numerous fork-based development platforms facilitates the development of Open-Source Software (OSS) projects. Developers across the world can fork software projects and submit their Pull Requests (PRs) to the projects. However, as the number of forks increases, numerous duplicate PRs might be submitted. These duplicate PRs may cause extra code review workload and frustrate developers working on the projects. To detect duplicate PRs, many approaches have been proposed, which analyze the similarity of different elements in PRs. However, previous approaches still suffer from unsatisfied detection accuracy due to two challenges. That is, they ignore the syntactic structural information of text elements in PRs and lack the joint reasoning between different elements of two PRs. In this study, we propose an automated duplicate PRs detector named DupHunter (Duplicate PRs Hunter), which includes a graph embedding component and a duplicate PRs detection component to address the above challenges. The graph embedding component uses a feature graph to represent a PR. It encodes the syntactic structure and semantics of text elements (e.g., the title and the description), as well as the knowledge of non-text elements (e.g., the submission time), to address the syntactic structural information challenge. The duplicate PRs detection component tackles the joint reasoning challenge using a graph matching network, which enables the information exchange and matching across different elements of two feature graphs with an attention coefficient mechanism. Experiments on 26 open-source projects show that DupHunter achieves an average F1-score@1 value of 0.650, significantly outperforming the state-of-the-art approaches by 3.2% to 48.1%. DupHunter can accurately detect duplicate PRs, with an average Precision@1 value of 0.922 and an average Recall@1 value of 0.502.","Duplicate pull requests detection,fork-based development,open source,graph embeddings,text processing",DupHunter：在基于Fork的开发中检测重复的拉取请求,大量基于fork的开发平台的出现促进了开源软件（OSS）项目的开发。世界各地的开发人员可以分叉软件项目，并向项目提交他们的Pull Request（PR）。然而，随着分叉数量的增加，可能会提交大量重复的PR。这些重复的PR可能会导致额外的代码审查工作量，并使从事项目的开发人员感到沮丧。为了检测重复的PR，已经提出了许多方法，分析PR中不同元素的相似性。然而，由于两个挑战，以前的方法仍然存在检测精度不令人满意的问题。也就是说，它们忽略了PR中文本元素的句法结构信息，缺乏两个PR中不同元素之间的联合推理。在本研究中，我们提出了一种名为DupHunter（duplicate PRs Hunter）的自动重复PRs检测器，该检测器包括一个图嵌入组件和一个重复PRs检测组件，以解决上述挑战。图嵌入组件使用特征图来表示PR。它对文本元素的句法结构和语义（例如标题和描述）以及非文本元素的知识（例如提交时间）进行编码，以解决句法结构信息挑战。重复PRs检测组件使用图匹配网络来解决联合推理挑战，该网络能够通过注意力系数机制在两个特征图的不同元素之间进行信息交换和匹配。对26个开源项目的实验表明，DupHunter实现了平均F1-score@1值为0.650，显著优于最先进的方法3.2%至48.1%。DupHunter可以准确检测重复PR，平均Precision@1值0.922和平均值Recall@1值0.502。,重复拉取请求检测，基于fork的开发，开源，图形嵌入，文本处理,,,
TRDDAGRT,2023,https://doi.org/10.1109/TSE.2022.3177713,TSE 2023,BugBuilder: An Automated Approach to Building Bug Repository,"Bug-related research, e.g., fault localization, program repair, and software testing, relies heavily on high-quality and large-scale software bug repositories. The importance of such repositories is twofold. On one side, real-world bugs and their associated patches may inspire novel approaches for finding, locating, and repairing software bugs. On the other side, the real-world bugs and their patches are indispensable for rigorous and meaningful evaluation of approaches to software testing, fault localization, and program repair. To this end, a number of software bug repositories, e.g., iBUGS and Defects4J, have been constructed recently by mining version control systems and bug tracking systems. However, fully automated construction of bug repositories by simply taking bug-fixing commits from version control systems often results in inaccurate patches that contain many bug-irrelevant changes. Although we may request experts or developers to manually exclude the bug-irrelevant changes (as the authors of Defects4J did), such extensive human intervention makes it difficult to build large-scale bug repositories. To this end, in this paper, we propose an automatic approach, called BugBuilder, to construct bug repositories from version control systems. Different from existing approaches, it automatically extracts complete and concise bug-fixing patches and excludes bug-irrelevant changes. It first detects and excludes software refactorings involved in bug-fixing commits. BugBuilder then enumerates all subsets of the remaining part, and discards invalid subsets by compilation and software testing. If exactly a single subset survives the validation, this subset is taken as the complete and concise bug-fixing patch for the associated bug. In case multiple subsets survive, BugBuilder employs a sequence of heuristics to select the most likely one. Evaluation results on 809 real-world bug-fixing commits in Defects4J suggest that BugBuilder successfully extracted complete and concise bug-fixing patches from forty-three percent of the bug-fixing commits, and its precision (99%) was even higher than human experts. We also built a bug repository, called GrowingBugs, with the proposed approach. The resulting repository serves as evidence of the usefulness of the proposed approach, as well as a publicly available benchmark for bug-related research.","Bug,defect,testing,patch,repository,dataset,refactoring",BugBuilder:一种构建Bug库的自动化方法,与Bug相关的研究，例如故障定位、程序修复和软件测试，在很大程度上依赖于高质量和大规模的软件Bug存储库。这种储存库的重要性是双重的。一方面，现实世界中的漏洞及其相关补丁可能会激发寻找、定位和修复软件漏洞的新方法。另一方面，真实世界的bug及其补丁对于软件测试、故障定位和程序修复方法的严格和有意义的评估是必不可少的。为此，最近通过挖掘版本控制系统和漏洞跟踪系统构建了许多软件漏洞库，例如iBUGS和Defects4J。然而，通过简单地从版本控制系统中进行错误修复提交来完全自动化地构建错误存储库，通常会导致不准确的补丁，其中包含许多与错误无关的更改。尽管我们可能会要求专家或开发人员手动排除与bug无关的更改（正如Defects4J的作者所做的那样），但如此广泛的人工干预使构建大规模的bug存储库变得困难。为此，在本文中，我们提出了一种称为BugBuilder的自动方法，用于从版本控制系统构建bug存储库。与现有方法不同，它自动提取完整简洁的bug修复补丁，并排除与bug无关的更改。它首先检测并排除错误修复提交中涉及的软件重构。BugBuilder然后枚举剩余部分的所有子集，并通过编译和软件测试丢弃无效子集。如果只有一个子集能够通过验证，则该子集将被视为相关错误的完整简洁的错误修复补丁。在多个子集存活的情况下，BugBuilder会采用一系列启发式方法来选择最有可能的子集。对Defects4J中809个真实世界错误修复提交的评估结果表明，BugBuilder成功地从43%的错误修复提交中提取了完整简洁的错误修复补丁，其精度（99%）甚至高于人类专家。我们还使用所提出的方法构建了一个名为GrowingBugs的bug存储库。由此产生的存储库是所提出方法有用性的证据，也是漏洞相关研究的公开基准。,Bug，缺陷，测试，补丁，存储库，数据集，重构,,,
GZEAJ4VU,2023,https://doi.org/10.1109/TSE.2022.3214859,TSE 2023,Towards Usable Neural Comment Generation via Code-Comment Linkage Interpretation: Method and Empirical Study,"Code comment is important to facilitate code comprehension for developers. Recent studies suggest to generate comments automatically with deep learning, in particular, based on neural machine translation models. However, such a promising Neural Comment Generation (NCG) technique suffers from unsatisfactory performance, as well as poor usability, i.e., developers cannot easily understand and modify the auto-generated comments. This paper suggests that a proper interpretation of how the comments are generated can significantly improve the usability of NCG approaches. We propose a novel model-independent framework, namely CCLink, to interpret the auto-generated comments. CCLink generates a set of code mutants and obtains their corresponding comments. Based on these data, several contribution mining algorithms are designed to infer the key elements in code that contributes to the generation of the key phrases in the comments. The links between code and its auto-generated comment can thus be constructed. This in turn allows CCLink to visualize the links as the comment interpretations to developers. It greatly facilitates manual verification and correction of the comments. We examine the performance of CCLink with different contribution mining algorithms, NCG approaches, and real-world datasets. We also conduct an empirical study on 32 experienced Java programmers to evaluate the effectiveness of CCLink. The results show that CCLink is promising in making NCG more usable with a proper interpretation of the auto-generated comments.","Code comment,neural comment generation,software usability",通过代码注释链接解释实现可用神经注释生成：方法和实证研究,代码注释对于促进开发人员理解代码非常重要。最近的研究建议通过深度学习自动生成评论，特别是基于神经机器翻译模型。然而，这种有前途的神经评论生成（NCG）技术存在性能不令人满意以及可用性差的问题，即开发人员无法轻松理解和修改自动生成的评论。本文认为，正确解释评论是如何生成的，可以显著提高NCG方法的可用性。我们提出了一个新的独立于模型的框架，即CCLink，来解释自动生成的评论。CCLink生成一组代码突变体并获得它们相应的注释。基于这些数据，设计了几种贡献挖掘算法来推断代码中有助于生成注释中关键短语的关键元素。因此，可以构建代码与其自动生成的注释之间的链接。这反过来又允许CCLink将链接可视化为对开发人员的注释解释。它极大地方便了对注释的手动验证和更正。我们使用不同的贡献挖掘算法、NCG方法和真实世界的数据集来检查CCLink的性能。我们还对32名经验丰富的Java程序员进行了实证研究，以评估CCLink的有效性。结果表明，CCLink有希望通过正确解释自动生成的评论，使NCG更容易使用。,代码注释，神经注释生成，软件可用性,,,
CHFRVK3U,2023,https://doi.org/10.1109/TSE.2022.3171288,TSE 2023,Evaluating the Impact of Possible Dependencies on Architecture-Level Maintainability,"Dependencies among software entities are the foundation for much of the research on software architecture analysis and architecture analysis tools. Dynamically typed languages, such as Python, JavaScript and Ruby, tolerate the lack of explicit type references, making certain dependencies indiscernible by a purely syntactic analysis of source code. We call these possible dependencies, in contrast with the explicit dependencies that are directly manifested in source code. We find that existing architecture analysis tools have not taken possible dependencies into consideration. An important question therefore is: to what extent will these missing possible dependencies impact architecture analysis?To answer this question, we conducted a study of 499 open-source Python projects, employing type inference techniques and type hint practices to discern possible dependencies. We investigated the consequences of possible dependencies in three software maintenance contexts, including capturing co-change relations recorded in revision history, measuring architectural maintainability, and detecting architecture anti-patterns that violate design principles and impact maintainability. Our study revealed that the impact of possible dependencies on architecture-level maintainability is substantial—higher than that of explicit dependencies. Our findings suggest that architecture analysis and tools should take into account, assess, and highlight the impacts of possible dependencies caused by dynamic typing.","Dynamic typing,possible dependency,software architecture,empirical study",评估可能的依赖关系对体系结构级可维护性的影响,软件实体之间的依赖关系是许多软件体系结构分析和体系结构分析工具研究的基础。Python、JavaScript和Ruby等动态类型语言容忍缺乏显式类型引用，使得某些依赖关系无法通过对源代码的纯语法分析来辨别。我们称这些可能的依赖关系，与直接在源代码中表现出来的显式依赖关系形成对比。我们发现现有的体系结构分析工具没有考虑到可能的依赖关系。因此，一个重要的问题是：这些缺失的可能依赖关系会在多大程度上影响体系结构分析？为了回答这个问题，我们对499个开源Python项目进行了研究，采用类型推理技术和类型提示实践来辨别可能的依赖关系。我们研究了三种软件维护环境中可能的依赖关系的后果，包括捕获修订历史中记录的协同更改关系，测量体系结构可维护性，以及检测违反设计原则并影响可维护性的体系结构反模式。我们的研究表明，可能的依赖关系对体系结构级可维护性的影响是巨大的——高于显式依赖关系。我们的研究结果表明，架构分析和工具应该考虑、评估并强调动态类型可能引起的依赖关系的影响。,动态类型，可能的依赖性，软件架构，实证研究,,,
PEQSXDBX,2023,https://doi.org/10.1109/TSE.2022.3212329,TSE 2023,Supporting Developers in Addressing Human-Centric Issues in Mobile Apps,"Failure to consider the characteristics, limitations, and abilities of diverse end-users during mobile app development may lead to problems for end-users, such as accessibility and usability issues. We refer to this class of problems as human-centric issues. Despite their importance, there is a limited understanding of the types of human-centric issues that are encountered by end-users and taken into account by the developers of mobile apps. In this paper, we examine what human-centric issues end-users report through Google App Store reviews, what human-centric issues are a topic of discussion for developers on GitHub, and whether end-users and developers discuss the same human-centric issues. We then investigate whether an automated tool might help detect such human-centric issues and whether developers would find such a tool useful. To do this, we conducted an empirical study by extracting and manually analysing a random sample of 1,200 app reviews and 1,200 issue comments from 12 diverse projects that exist on both Google App Store and GitHub. Our analysis led to a taxonomy of human-centric issues that characterises human-centric issues into three-high level categories: App Usage, Inclusiveness, and User Reaction. We then developed machine learning and deep learning models that are promising in automatically identifying and classifying human-centric issues from app reviews and developer discussions. A survey of mobile app developers shows that the automated detection of human-centric issues has practical applications. Guided by our findings, we highlight some implications and possible future work to further understand and better incorporate addressing human-centric issues into mobile app development.","Human-centric issues,GitHub repositories,Google Play Store,human aspects,machine learning,deep learning",支持开发人员解决移动应用程序中以人为中心的问题,在移动应用程序开发过程中，如果不考虑不同最终用户的特点、局限性和能力，可能会给最终用户带来问题，如可访问性和可用性问题。我们把这类问题称为以人为中心的问题。尽管这些问题很重要，但对最终用户遇到的、移动应用程序开发人员考虑到的以人为中心的问题类型的了解有限。在本文中，我们研究了最终用户通过谷歌应用商店评论报告的以人为中心的问题，GitHub上开发者讨论的主题是什么以人为中心问题，以及最终用户和开发者是否讨论了相同的以人为核心的问题。然后，我们研究自动化工具是否有助于检测这种以人为中心的问题，以及开发人员是否会发现这种工具有用。为此，我们进行了一项实证研究，从谷歌应用商店和GitHub上存在的12个不同项目中提取并手动分析了1200条应用评论和1200条问题评论的随机样本。我们的分析得出了以人为中心问题的分类法，将以人为中心的问题分为三个高级类别：应用程序使用、包容性和用户反应。然后，我们开发了机器学习和深度学习模型，这些模型有望从应用程序评论和开发者讨论中自动识别和分类以人为中心的问题。一项针对移动应用程序开发人员的调查显示，以人为中心的问题的自动检测具有实际应用。在我们的研究结果的指导下，我们强调了一些含义和未来可能的工作，以进一步理解并更好地将解决以人为中心的问题纳入移动应用程序开发。,以人为中心的问题，GitHub存储库，Google Play Store，人的方面，机器学习，深度学习,,,
GCRXLPUJ,2023,https://doi.org/10.1109/TSE.2022.3207428,TSE 2023,Code Cloning in Smart Contracts on the Ethereum Platform: An Extended Replication Study,"Smart contracts are programs deployed on blockchains that run upon meeting predetermined conditions. Once deployed, smart contracts are immutable, thus, defects in the deployed code cannot be fixed. As a consequence, software engineering anti-patterns, such as code cloning, pose a threat to code quality and security if unnoticed before deployment. In this paper, we report on the cloning practices of the Ethereum blockchain platform by analyzing 33,073 smart contracts amounting to over 4MLOC. Prior work reported an unusually high 79.2% of code clones in Ethereum smart contracts. We replicate this study at the conceptual level, i.e., we answer the same research questions by employing different methods. In particular, we analyze clones at the granularity of functions instead of code files, thereby providing a more fine-grained estimate of the clone ratio. Furthermore, we analyze more complex clone types, allowing for a richer analysis of cloning cases. To achieve this finer granularity of cloning analysis, we rely on the NiCad clone detection tool and extend it with support for Solidity, the programming language of the Ethereum platform. Our analysis shows that most findings of the original study hold at the finer granularity of our study as well; but also sheds light on some differences, and contributes new findings. Most notably, we report a 30.13% overall clone ratio, out of which 27.03% are exact duplicates. Our findings motivate improving the reuse mechanisms of Solidity, and in a broader context, of programming languages used for the development of smart contracts. Tool builders and language engineers can use this paper in the design and development of such reuse mechanisms. Business stakeholders can use this paper to better assess the security risks and technical outlooks of blockchain platforms.","Code cloning,Smart contracts,Ethereum,blockchain,empirical study",以太坊平台上智能合约中的代码克隆：一项扩展复制研究,智能合约是部署在区块链上的程序，在满足预定条件的情况下运行。一旦部署，智能合约是不可变的，因此，无法修复已部署代码中的缺陷。因此，软件工程反模式，如代码克隆，如果在部署前未被注意，就会对代码质量和安全性构成威胁。在本文中，我们通过分析33073个智能合约，总计超过4MLOC，报告了以太坊区块链平台的克隆实践。之前的工作报告称，以太坊智能合约中79.2%的代码克隆率异常高。我们在概念层面复制了这项研究，即我们通过使用不同的方法回答相同的研究问题。特别是，我们以函数的粒度而不是代码文件的粒度来分析克隆，从而提供对克隆比率的更细粒度的估计。此外，我们还分析了更复杂的克隆类型，从而可以对克隆案例进行更丰富的分析。为了实现这种更精细的克隆分析，我们依靠NiCad克隆检测工具，并通过支持以太坊平台的编程语言Solidity对其进行扩展。我们的分析表明，原始研究的大多数发现也符合我们研究的细粒度；但也揭示了一些差异，并提供了新的发现。最值得注意的是，我们报告了30.13%的总克隆率，其中27.03%是完全重复的。我们的发现激励了Solidity的重用机制的改进，以及在更广泛的背景下，用于开发智能合约的编程语言的重用机制。工具构建者和语言工程师可以将本文用于此类重用机制的设计和开发。商业利益相关者可以利用本文更好地评估区块链平台的安全风险和技术前景。,代码克隆，智能合约，以太坊，区块链，实证研究,,,
9AHDLQ5G,2023,https://doi.org/10.1109/TSE.2022.3187689,TSE 2023,Revisiting Binary Code Similarity Analysis Using Interpretable Feature Engineering and Lessons Learned,"Binary code similarity analysis (BCSA) is widely used for diverse security applications, including plagiarism detection, software license violation detection, and vulnerability discovery. Despite the surging research interest in BCSA, it is significantly challenging to perform new research in this field for several reasons. First, most existing approaches focus only on the end results, namely, increasing the success rate of BCSA, by adopting uninterpretable machine learning. Moreover, they utilize their own benchmark, sharing neither the source code nor the entire dataset. Finally, researchers often use different terminologies or even use the same technique without citing the previous literature properly, which makes it difficult to reproduce or extend previous work. To address these problems, we take a step back from the mainstream and contemplate fundamental research questions for BCSA. Why does a certain technique or a certain feature show better results than the others? Specifically, we conduct the first systematic study on the basic features used in BCSA by leveraging interpretable feature engineering on a large-scale benchmark. Our study reveals various useful insights on BCSA. For example, we show that a simple interpretable model with a few basic features can achieve a comparable result to that of recent deep learning-based approaches. Furthermore, we show that the way we compile binaries or the correctness of underlying binary analysis tools can significantly affect the performance of BCSA. Lastly, we make all our source code and benchmark public and suggest future directions in this field to help further research.","Binary code similarity analysis,similarity measures,feature evaluation and selection,benchmark",使用可解释特征工程对二进制代码相似性分析的改进和经验教训,二进制代码相似性分析（BCSA）广泛用于各种安全应用，包括剽窃检测、软件许可证违规检测和漏洞发现。尽管对BCSA的研究兴趣激增，但由于几个原因，在该领域进行新的研究具有重大挑战性。首先，大多数现有的方法只关注最终结果，即通过采用无法解释的机器学习来提高BCSA的成功率。此外，他们使用自己的基准测试，既不共享源代码，也不共享整个数据集。最后，研究人员经常使用不同的术语，甚至使用相同的技术，而没有正确引用以前的文献，这使得很难复制或扩展以前的工作。为了解决这些问题，我们从主流后退一步，思考BCSA的基本研究问题。为什么某项技术或某项功能比其他技术或功能显示出更好的结果？具体而言，我们通过在大规模基准上利用可解释特征工程，对BCSA中使用的基本特征进行了首次系统研究。我们的研究揭示了关于BCSA的各种有用见解。例如，我们表明，一个具有一些基本特征的简单可解释模型可以获得与最近基于深度学习的方法相当的结果。此外，我们还表明，编译二进制文件的方式或底层二进制分析工具的正确性会显著影响BCSA的性能。最后，我们公开了我们所有的源代码和基准测试，并提出了该领域的未来方向，以帮助进一步的研究。,二进制代码相似性分析，相似性度量，特征评估和选择，基准测试,,,
HNN5P8QH,2023,https://doi.org/10.1109/TSE.2022.3165938,TSE 2023,How Should Software Engineering Secondary Studies Include Grey Material?,"Context: Recent papers have proposed the use of grey literature (GL) and multivocal reviews. These papers have raised issues about the practices used for systematic reviews (SRs) in software engineering (SE) and suggested that there should be changes to the current SR guidelines. Objective: To investigate whether current SR guidelines need to be changed to support GL and multivocal reviews. Method: We discuss the definitions of GL and the importance of GL and of industry-based field studies in SE SRs. We identify properties of SRs that constrain the material used in SRs: a) the nature of primary studies; b) the requirements of SRs to be auditable, traceable, and reproducible; and explain why these requirements restrict the use of blogs in SRs. Results: SR guidelines have always considered GL as a possible source of primary studies and have never supported exclusion of field studies that incorporate the practitioners’ viewpoint. However, the concept of GL, which was meant to refer to documents that were not formally published, is now being extended to information from sources such as blogs/tweets/Q&A posts. Thus, it might seem that SRs do not make full use of GL because they do not include such information. However, the unit of analysis for an SR is the primary study. Thus, it is not the source but the type of information that is important. Any report describing a rigorous empirical evaluation is a candidate primary study. Whether it is actually included in an SR depends on the SR eligibility criteria. However, any study that cannot be guaranteed to be publicly available in the long term should not be used as a primary study in an SR. This does not prevent such information from being aggregated in surveys of social media and used in the context of evidence-based software engineering (EBSE). Conclusions: Current guidelines for SRs do not require extensions, but their scope needs to be better defined. SE researchers require guidelines for analysing social media posts (e.g., blogs, tweets, vlogs), but these should be based on qualitative primary (not secondary) study guidelines. SE researchers can use mixed-methods SRs and/or the fourth step of EBSE to incorporate findings from social media surveys with those from SRs and to develop industry-relevant recommendations.","Evidence-based software engineering,systematic reviews,systematic mapping studies,mixed-methods reviews,grey literature,multivocal reviews",软件工程二级课程应如何包含灰色材料？,背景：最近的论文提出了使用灰色文献（GL）和多声音评论。这些论文提出了关于软件工程（SE）中用于系统评审（SR）的实践的问题，并建议对当前的SR指南进行修改。目的：研究是否需要改变当前的SR指南，以支持GL和多声音审查。方法：我们讨论了GL的定义，以及GL和基于行业的实地研究在SE SR中的重要性。我们确定了限制SRs中使用的材料的SRs的性质：a）初步研究的性质；b） SR的可审核性、可追溯性和可重复性要求；并解释为什么这些要求限制了博客在SR中的使用。结果：SR指南一直认为GL是主要研究的可能来源，从未支持排除纳入从业者观点的实地研究。然而，GL的概念是指未正式发布的文件，现在正扩展到博客/推文/问答帖子等来源的信息。因此，SR似乎没有充分利用GL，因为它们不包括此类信息。然而，SR的分析单位是主要研究。因此，重要的不是信息的来源，而是信息的类型。任何描述严格实证评估的报告都是候选的初步研究。是否实际包含在SR中取决于SR资格标准。然而，任何不能保证长期公开的研究都不应被用作SR的主要研究。这并不妨碍这些信息在社交媒体调查中被汇总，并在循证软件工程（EBSE）的背景下使用。结论：目前的SR指南不需要扩展，但其范围需要更好地定义。SE研究人员需要分析社交媒体帖子（如博客、推文、vlog）的指南，但这些指南应基于定性的初级（而非次级）研究指南。SE研究人员可以使用混合方法SR和/或EBSE的第四步，将社交媒体调查的结果与SR的结果相结合，并制定与行业相关的建议。,基于证据的软件工程，系统综述，系统映射研究，混合方法综述，灰色文献，多语言综述,,,
67VHHCEI,2023,https://doi.org/10.1109/TSE.2022.3174092,TSE 2023,SEGRESS: Software Engineering Guidelines for REporting Secondary Studies,"Context: Several tertiary studies have criticized the reporting of software engineering secondary studies. Objective: Our objective is to identify guidelines for reporting software engineering (SE) secondary studies which would address problems observed in the reporting of software engineering systematic reviews (SRs). Method: We review the criticisms of SE secondary studies and identify the major areas of concern. We assess the PRISMA 2020 (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement as a possible solution to the need for SR reporting guidelines, based on its status as the reporting guideline recommended by the Cochrane Collaboration whose SR guidelines were a major input to the guidelines developed for SE. We report its advantages and limitations in the context of SE secondary studies. We also assess reporting guidelines for mapping studies and qualitative reviews, and compare their structure and content with that of PRISMA 2020. Results: Previous tertiary studies confirm that reports of secondary studies are of variable quality. However, ad hoc recommendations that amend reporting standards may result in unnecessary duplication of text. We confirm that the PRISMA 2020 statement addresses SE reporting problems, but is mainly oriented to quantitative reviews, mixed-methods reviews and meta-analyses. However, we show that the PRISMA 2020 item definitions can be extended to cover the information needed to report mapping studies and qualitative reviews. Conclusions: In this paper and its Supplementary Material, we present and illustrate an integrated set of guidelines called SEGRESS (Software Engineering Guidelines for REporting Secondary Studies), suitable for quantitative systematic reviews (building upon PRISMA 2020), mapping studies (PRISMA-ScR), and qualitative reviews (ENTREQ and RAMESES), that addresses reporting problems found in current SE SRs.","Evidence-based software engineering,reporting guidelines,systematic reviews,quality reviews,mapping studies,mixed-methods reviews,threats to validity,risk of bias,quality assessment,PRISMA 2020",SEGRESS：二级研究再报告的软件工程指南,背景：一些三级研究批评了软件工程二级研究的报道。目标：我们的目标是确定报告软件工程（SE）二级研究的指南，该指南将解决软件工程系统评审（SR）报告中观察到的问题。方法：我们回顾了对SE二级研究的批评，并确定了主要关注领域。我们评估了PRISMA 2020（系统评价和荟萃分析的首选报告项目）声明作为SR报告指南需求的可能解决方案，基于其作为Cochrane协作推荐的报告指南的地位，其SR指南是为SE制定的指南的主要输入。我们在SE二级研究的背景下报道了它的优势和局限性。我们还评估了制图研究和定性审查的报告指南，并将其结构和内容与2020年PRISMA的报告指南进行了比较。结果：先前的三级研究证实，二级研究的报告质量参差不齐。然而，修改报告标准的特设建议可能导致不必要的文本重复。我们确认，PRISMA 2020声明解决了SE报告问题，但主要面向定量审查、混合方法审查和荟萃分析。然而，我们表明，PRISMA 2020项目定义可以扩展，以涵盖报告映射研究和定性审查所需的信息。结论：在本文及其补充材料中，我们提出并说明了一套名为SEGRESS（二级研究再报告软件工程指南）的综合指南，适用于定量系统评审（基于PRISMA 2020）、制图研究（PRISMA ScR）和定性评审（ENTREQ和RAMESES），解决了当前SE SR中发现的报告问题。,基于证据的软件工程，报告指南，系统审查，质量审查，制图研究，混合方法审查，有效性威胁，偏见风险，质量评估，PRISMA 2020,,,
EBDQLZTT,2023,https://doi.org/10.1109/TSE.2023.3271065,TSE 2023,Self-Supervised Learning to Prove Equivalence Between Straight-Line Programs via Rewrite Rules,"We target the problem of automatically synthesizing proofs of semantic equivalence between two programs made of sequences of statements. We represent programs using abstract syntax trees (AST), where a given set of semantics-preserving rewrite rules can be applied on a specific AST pattern to generate a transformed and semantically equivalent program. In our system, two programs are equivalent if there exists a sequence of application of these rewrite rules that leads to rewriting one program into the other. We propose a neural network architecture based on a transformer model to generate proofs of equivalence between program pairs. The system outputs a sequence of rewrites, and the validity of the sequence is simply checked by verifying it can be applied. If no valid sequence is produced by the neural network, the system reports the programs as non-equivalent, ensuring by design no programs may be incorrectly reported as equivalent. Our system is fully implemented for one single grammar which can represent straight-line programs with function calls and multiple types. To efficiently train the system to generate such sequences, we develop an original incremental training technique, named self-supervised sample selection. We extensively study the effectiveness of this novel training approach on proofs of increasing complexity and length. Our system, $\mathsf {S4Eq}$, achieves 97% proof success on a curated dataset of 10,000 pairs of equivalent programs.","Machine learning,program equivalence,self-supervised learning,symbolic reasoning",自监督学习通过重写规则证明直线程序之间的等价性,我们的目标是自动合成两个由语句序列组成的程序之间语义等价的证明问题。我们使用抽象语法树（AST）来表示程序，其中给定的一组语义保留重写规则可以应用于特定的AST模式，以生成经过转换的语义等效程序。在我们的系统中，如果存在这些重写规则的应用序列，导致将一个程序重写到另一个程序，那么两个程序是等价的。我们提出了一种基于变换器模型的神经网络架构，以生成程序对之间的等价性证明。系统输出一个重写序列，通过验证该序列是否可以应用来简单地检查该序列的有效性。如果神经网络没有产生有效的序列，系统会将程序报告为非等效程序，确保通过设计不会将任何程序错误地报告为等效程序。我们的系统完全实现为一个语法，它可以用函数调用和多种类型表示直线程序。为了有效地训练系统生成这样的序列，我们开发了一种原始的增量训练技术，称为自监督样本选择。我们广泛研究了这种新的训练方法对增加复杂性和长度的证明的有效性。我们的系统$\mathsf｛S4Eq｝$在由10000对等效程序组成的精心策划的数据集上获得了97%的证明成功。,机器学习，程序等价，自监督学习，符号推理,,,
94XJ2YPX,2023,https://doi.org/10.1109/TSE.2022.3208210,TSE 2023,Impact of Software Engineering Research in Practice: A Patent and Author Survey Analysis,"Existing work on the practical impact of software engineering (SE) research examines industrial relevance rather than adoption of study results, hence the question of how results have been practically applied remains open. To answer this and investigate the outcomes of impactful research, we performed a quantitative and qualitative analysis of 4 354 SE patents citing 1 690 SE papers published in four leading SE venues between 1975–2017. Moreover, we conducted a survey on 475 authors of 593 top-cited and awarded publications, achieving 26% response rate. Overall, researchers have equipped practitioners with various tools, processes, and methods, and improved many existing products. SE practice values knowledge-seeking research and is impacted by diverse cross-disciplinary SE areas. Practitioner-oriented publication venues appear more impactful than researcher-oriented ones, while industry-related tracks in conferences could enhance their impact. Some research works did not reach a wide footprint due to limited funding resources or unfavorable cost-benefit trade-off of the proposed solutions. The need for higher SE research funding could be corroborated through a dedicated empirical study. In general, the assessment of impact is subject to its definition. Therefore, academia and industry could jointly agree on a formal description to set a common ground for subsequent research on the topic.","Software engineering,practical impact,empirical study,survey,patent citations",软件工程研究在实践中的影响：专利和作者调查分析,关于软件工程（SE）研究的实际影响的现有工作考察了行业相关性，而不是研究结果的采用，因此，如何实际应用研究结果的问题仍然悬而未决。为了回答这个问题并调查有影响力的研究的结果，我们对4354项SE专利进行了定量和定性分析，引用了1975年至2017年间在四个主要SE场所发表的1690篇SE论文。此外，我们对593篇顶级引用和获奖出版物中的475位作者进行了调查，获得了26%的回复率。总体而言，研究人员为从业者配备了各种工具、流程和方法，并改进了许多现有产品。SE实践重视知识寻求研究，并受到不同跨学科SE领域的影响。以从业者为导向的出版场所似乎比以研究者为导向的场所更有影响力，而会议中与行业相关的轨道可以增强其影响力。由于资金资源有限或拟议解决方案的成本效益权衡不利，一些研究工作没有达到广泛的覆盖范围。可以通过专门的实证研究来证实对更高SE研究资金的需求。一般来说，影响的评估取决于其定义。因此，学术界和工业界可以共同商定一个正式的描述，为随后对该主题的研究奠定共同基础。,软件工程，实践影响，实证研究，调查，专利引用,,,
E2PCUIY9,2023,https://doi.org/10.1109/TSE.2022.3143766,TSE 2023,Spork: Structured Merge for Java With Formatting Preservation,"The highly parallel workflows of modern software development have made merging of source code a common activity for developers. The state of the practice is based on line-based merge, which is ubiquitously used with “git merge”. Line-based merge is however a generalized technique for any text that cannot leverage the structured nature of source code, making merge conflicts a common occurrence. As a remedy, research has proposed structured merge tools, which typically operate on abstract syntax trees instead of raw text. Structured merging greatly reduces the prevalence of merge conflicts but suffers from important limitations, the main ones being a tendency to alter the formatting of the merged code and being prone to excessive running times. In this paper, we present spork, a novel structured merge tool for java. spork is unique as it preserves formatting to a significantly greater degree than comparable state-of-the-art tools. spork is also overall faster than the state of the art, in particular significantly reducing worst-case running times in practice. We demonstrate these properties by replaying 1740 real-world file merges collected from 119 open-source projects, and further demonstrate several key differences between spork and the state of the art with in-depth case studies.","Version control,structured merge",Spork:保留格式的Java结构化合并,现代软件开发的高度并行的工作流程使源代码的合并成为开发人员的常见活动。实践的状态是基于行的合并，它与“gitmerge”一起广泛使用。然而，基于行的合并是一种通用技术，适用于任何无法利用源代码的结构化特性的文本，这使得合并冲突很常见。作为补救措施，研究人员提出了结构化合并工具，通常在抽象语法树上操作，而不是在原始文本上操作。结构化合并大大降低了合并冲突的普遍性，但也存在一些重要的局限性，主要的局限性是倾向于更改合并代码的格式，并且容易导致运行时间过长。在本文中，我们提出了一种新的用于java的结构化合并工具spork。spork是独一无二的，因为它在很大程度上保留了格式，而不是类似的最先进的工具。spork总体上也比现有技术更快，特别是在实践中显著减少了最坏情况下的运行时间。我们通过回放从119个开源项目中收集的1740个真实世界的文件合并来证明这些特性，并通过深入的案例研究进一步证明了spork与现有技术之间的几个关键差异。,版本控制，结构化合并,,,
U8KLN8ZI,2023,https://doi.org/10.1109/TSE.2023.3255177,TSE 2023,Invalidator: Automated Patch Correctness Assessment Via Semantic and Syntactic Reasoning,"Automated program repair (APR) faces the challenge of test overfitting, where generated patches pass validation tests but fail to generalize. Existing methods for patch assessment involve generating new tests or manual inspection, which can be time-consuming or biased. In this paper, we propose a novel technique, Invalidator, to automatically assess the correctness of APR-generated patches via semantic and syntactic reasoning. Invalidator leverages program invariants to reason about program semantics while also capturing program syntax through language semantics learned from a large code corpus using a pre-trained language model. Given a buggy program and the developer-patched program, Invalidator infers likely invariants on both programs. Then, Invalidator determines that an APR-generated patch overfits if: (1) it violates correct specifications or (2) maintains erroneous behaviors from the original buggy program. In case our approach fails to determine an overfitting patch based on invariants, Invalidator utilizes a trained model from labeled patches to assess patch correctness based on program syntax. The benefit of Invalidator is threefold. First, Invalidator leverages both semantic and syntactic reasoning to enhance its discriminative capability. Second, Invalidator does not require new test cases to be generated, but instead only relies on the current test suite and uses invariant inference to generalize program behaviors. Third, Invalidator is fully automated. Experimental results demonstrate that Invalidator outperforms existing methods in terms of Accuracy and F-measure, correctly identifying 79% of overfitting patches and detecting 23% more overfitting patches than the best baseline.","Automated patch correctness assessment,automated program repair,code representations,overfitting problem,program invariants",Invalidator：基于语义和句法推理的自动补丁正确性评估,自动程序修复（APR）面临着测试过拟合的挑战，生成的补丁通过了验证测试，但无法泛化。现有的补丁评估方法涉及生成新的测试或手动检查，这可能很耗时或有偏见。在本文中，我们提出了一种新的技术Invalidator，通过语义和句法推理自动评估APR生成的补丁的正确性。Invalidator利用程序不变量来推理程序语义，同时还通过使用预先训练的语言模型从大型代码语料库中学习的语言语义来捕获程序语法。给定一个有缺陷的程序和开发人员修补的程序，Invalidator推断出这两个程序上可能的不变量。然后，Invalidator确定APR生成的补丁过拟合，如果：（1）它违反了正确的规范，或者（2）保持了原始错误程序的错误行为。如果我们的方法无法基于不变量确定过拟合补丁，Invalidator会使用来自标记补丁的训练模型来基于程序语法评估补丁的正确性。无效宣告人的好处有三方面。首先，Invalidator利用语义和句法推理来增强其辨别能力。其次，Invalidator不需要生成新的测试用例，而是只依赖于当前的测试套件，并使用不变推理来概括程序行为。第三，无效宣告人是完全自动化的。实验结果表明，Invalidator在精度和F-测度方面优于现有方法，正确识别了79%的过拟合斑块，检测到的过拟合贴片比最佳基线多23%。,自动补丁正确性评估，自动程序修复，代码表示，过拟合问题，程序不变量,,,
WCU3INTR,2023,https://doi.org/10.1109/TSE.2022.3182663,TSE 2023,Bootstrapping Automated Testing for RESTful Web Services,"Modern RESTful services expose RESTful APIs to integrate with diversified applications. Most RESTful API parameters are weakly typed, which greatly increases the possible input value space. Weakly-typed parameters pose difficulties for automated testing tools to generate effective test cases to reveal web service defects related to parameter validation. We call this phenomenon the type collapse problem. To remedy this problem, we introduce FET (Format-encoded Type) techniques, including the FET, the FET lattice, and the FET inference to model fine-grained information for API parameters. Inferred FET can enhance parameter validation, such as generating a parameter validator for a certain RESTful server. Enhanced by FET techniques, automated testing tools can generate targeted test cases. We demonstrate Leif, a trace-driven fuzzing tool, as a proof-of-concept implementation of FET techniques. Experiment results on 27 commercial services show that FET inference precisely captures documented parameter definitions, which helps Leif discover 11 new bugs and reduce $72\% - 86\%$ fuzzing time compared to state-of-the-art fuzzers. Leveraged by the inter-parameter dependency inference, Leif saves $15\%$ fuzzing time.","Fuzz testing,RESTful web service,type inference",RESTful Web服务的自举自动测试,现代RESTful服务公开了RESTful API以与多样化的应用程序集成。大多数RESTful API参数都是弱类型的，这大大增加了可能的输入值空间。弱类型参数给自动化测试工具生成有效的测试用例以揭示与参数验证相关的web服务缺陷带来了困难。我们把这种现象称为类型崩溃问题。为了解决这个问题，我们引入了FET（格式编码类型）技术，包括FET、FET晶格和FET推理，以对API参数的细粒度信息进行建模。推断FET可以增强参数验证，例如为某个RESTful服务器生成参数验证器。通过FET技术的增强，自动化测试工具可以生成有针对性的测试用例。我们展示了Leif，一个跟踪驱动的模糊工具，作为FET技术的概念验证实现。在27个商业服务上的实验结果表明，FET推理准确地捕获了记录在案的参数定义，这有助于Leif发现11个新的错误，并与最先进的模糊器相比减少了$72\%-86\%$的模糊时间。通过参数间依赖推理，Leif节省了$15\%%$的模糊处理时间。,模糊测试，RESTful web服务，类型推理,,,
E3S87385,2023,https://doi.org/10.1109/TSE.2022.3199169,TSE 2023,A Theory of Organizational Structures for Development and Infrastructure Professionals,"DevOps and continuous delivery have impacted the organizational structures of development and infrastructure groups in software-producing organizations. Our research aims at revealing the different options adopted by the software industry to organize such groups, understanding why different organizations adopt distinct structures, and discovering how organizations handle the drawbacks of each structure. We interviewed 68 carefully-selected IT professionals, 45 working in Brazil, 10 in the USA, 8 in Europe, 1 in Canada, and 4 in globally distributed teams. By analyzing these conversations through a Grounded Theory process, we identified conditions, causes, reasons to avoid, consequences, and contingencies related to each discovered structure (segregated departments, collaborative departments, API-mediated departments, and single department). In this way, we offer a theory to explain organizational structures for development and infrastructure professionals. This theory can support practitioners and researchers in comprehending and discussing the DevOps phenomenon and its related issues, and also provides valuable input to practitioners’ decision-making.","DevOps,software teams,organizational structures,continuous delivery,software engineering,grounded theory",发展和基础设施专业人员的组织结构理论,DevOps和持续交付影响了软件生产组织中开发和基础设施小组的组织结构。我们的研究旨在揭示软件行业组织此类团体所采用的不同选择，了解不同组织为什么采用不同的结构，并发现组织如何处理每种结构的缺点。我们采访了68名精心挑选的IT专业人员，其中45人在巴西工作，10人在美国，8人在欧洲，1人在加拿大，4人在全球分布的团队中。通过基础理论过程分析这些对话，我们确定了与每个发现的结构（分离部门、协作部门、API中介部门和单个部门）相关的条件、原因、避免的原因、后果和意外事件。通过这种方式，我们为开发和基础设施专业人员提供了一个解释组织结构的理论。该理论可以支持从业者和研究人员理解和讨论DevOps现象及其相关问题，也为从业者的决策提供了宝贵的投入。,DevOps，软件团队，组织结构，持续交付，软件工程，基础理论,,,
HEJ43FFI,2023,https://doi.org/10.1109/TSE.2022.3152148,TSE 2023,An Empirical Study of Yanked Releases in the Rust Package Registry,"Cargo, the software packaging manager of Rust, provides a yank mechanism to support release-level deprecation, which can prevent packages from depending on yanked releases. Most prior studies focused on code-level (i.e., deprecated APIs) and package-level deprecation (i.e., deprecated packages). However, few studies have focused on release-level deprecation. In this study, we investigate how often and how the yank mechanism is used, the rationales behind its usage, and the adoption of yanked releases in the Cargo ecosystem. Our study shows that 9.6% of the packages in Cargo have at least one yanked release, and the proportion of yanked releases kept increasing from 2014 to 2020. Package owners yank releases for other reasons than withdrawing a defective release, such as fixing a release that does not follow semantic versioning or indicating a package is removed or replaced. In addition, we found that 46% of the packages directly adopted at least one yanked release and the yanked releases propagated through the dependency network, which leads to 1.4% of the releases in the ecosystem having unresolved dependencies.","Software ecosystems,release deprecation,yanking,rust,cargo",Rust Package注册表中Yanked释放的实证研究,Cargo是Rust的软件包管理器，它提供了一种yank机制来支持发布级别的弃用，这可以防止包依赖于yank发布。以前的大多数研究都集中在代码级别（即不推荐使用的API）和包级别的不推荐使用（即不建议使用的包）。然而，很少有研究关注发布级别的弃用。在这项研究中，我们调查了扬克机制的使用频率和方式，其使用背后的理由，以及在Cargo生态系统中采用扬克释放的情况。我们的研究表明，Cargo中9.6%的包裹至少有一次猛拉释放，从2014年到2020年，猛拉释放的比例不断增加。除了撤回有缺陷的发布之外，包所有者还会因为其他原因而取消发布，例如修复不遵循语义版本控制的发布，或者指示删除或替换包。此外，我们发现46%的软件包直接采用了至少一个yanked版本，并且yanked发布通过依赖关系网络传播，这导致生态系统中1.4%的发布具有未解决的依赖关系。,软件生态系统，发布降级，猛拉，生锈，货物,,,
K4LL6B4L,2023,https://doi.org/10.1109/TSE.2022.3153522,TSE 2023,Generating Concise Patches for Newly Released Programming Assignments,"In programming courses, providing students with concise and constructive feedback on faulty submissions (programs) is highly desirable. However, providing feedback manually is often time-consuming and tedious. To release tutors from the manual construction of concise feedback, researchers have proposed approaches such as CLARA and Refactory to construct feedback automatically. The key to such approaches is to fix a faulty program by making it equivalent to one of its correct reference programs whose overall structure is identical to that of the faulty submission. However, for a newly released assignment, it is likely that there are no correct reference programs at all, let alone correct reference programs sharing identical structure with the faulty submission. Therefore, in this paper, we propose AssignmentMender generating concise patches for newly released assignments. The key insight of AssignmentMender is that a faulty submission can be repaired by reusing fine-grained code snippets from submissions (even when they are faulty) for the same assignment. It automatically locates suspicious code in the faulty program and leverages static analysis to retrieve reference code from existing submissions with a graph-based matching algorithm. Finally, it generates candidate patches by modifying the suspicious code based on the reference code. Different from existing approaches, AssignmentMender exploits faulty submissions in addition to bug-free submissions to generate patches. Another advantage of AssignmentMender is that it can leverage submissions whose overall structures are different from those of the to-be-fixed submission. Evaluation results on 128 faulty submissions from 10 assignments show that AssignmentMender improves the state-of-the-art in feedback generation for newly released assignments. A case study involving 40 students and 80 submissions further provides initial evidence showing that the proposed approach is useful in practice.","Feedback generation,program repair,programming assignments",为新发布的编程分配生成简明补丁,在编程课程中，为学生提供关于错误提交（程序）的简洁和建设性的反馈是非常可取的。然而，手动提供反馈通常耗时且乏味。为了让导师摆脱简洁反馈的手动构建，研究人员提出了CLARA和Refactory等方法来自动构建反馈。这种方法的关键是修复一个错误的程序，使其等效于一个正确的参考程序，该参考程序的总体结构与错误提交的程序相同。然而，对于一个新发布的作业，很可能根本没有正确的参考程序，更不用说与错误提交共享相同结构的正确参考程序了。因此，在本文中，我们建议AssignmentMender为新发布的作业生成简洁的补丁。AssignmentMender的关键见解是，可以通过对同一分配重用提交中的细粒度代码片段（即使它们有错误）来修复有错误的提交。它自动定位故障程序中的可疑代码，并利用静态分析通过基于图的匹配算法从现有提交中检索参考代码。最后，它通过根据参考代码修改可疑代码来生成候选补丁。与现有方法不同，AssignmentMender除了利用无错误提交来生成补丁外，还利用错误提交来产生补丁。AssignmentMender的另一个优点是，它可以利用整体结构与待固定提交不同的提交。对10份作业中128份错误提交的评估结果显示，AssignmentMender改进了最新发布作业的反馈生成技术。一项涉及40名学生和80份提交材料的案例研究进一步提供了初步证据，表明拟议的方法在实践中是有用的。,反馈生成，程序修复，编程分配,,,
MBMYLC77,2023,https://doi.org/10.1109/TSE.2023.3266157,TSE 2023,A Zone-Based Model for Analysis of Dependent Failures in Requirements Inspection,"In the software development life cycle, the quality of the requirements specification affects the overall quality of the subsequent phases and hence, the software product. The requirements specification is usually inspected by an inspection team to detect defects. To enhance the quality of the requirements specification, one conventional strategy usually used is adding redundancies to the inspection team. However, this strategy suffers from the problem of dependent failures of the redundant inspectors which was not studied systematically in previous research. To analyze the dependent failures and independent failures in an inspection team, this paper first defines the independent failures and dependent failures in an inspection team from the perspective of human errors. Then a quantification model, i.e., the Zone-based Model, is proposed to analyze the dependent and independent failures. The Zone-based Model considers the following situations: 1) the probability of failures of an inspector may be high; 2) the probability of failures of the inspectors may be different; 3) the failures in an inspection team can be a combination of dependent failures and independent failures. By considering all those situations, the Z model has a meaningful interpretation and a convincing assessment of the failures of an inspection team. To verify the effectiveness of the new model, the Zone-based model is compared to conventional models using simulation data. The results show that the Zone-based model is significantly better than the traditional models in analyzing the independent and dependent failures.","Common cause failures,dependent failures,requirements inspection,zone-based model",基于区域的需求检验相关故障分析模型,在软件开发生命周期中，需求规范的质量影响后续阶段的整体质量，从而影响软件产品。需求规范通常由检查小组进行检查，以检测缺陷。为了提高需求规范的质量，通常使用的一种传统策略是向检查团队添加冗余。然而，这种策略存在冗余检查员的依赖性故障问题，这在以前的研究中没有得到系统的研究。为了分析检查组中的相关故障和独立故障，本文首先从人为失误的角度定义了检查组中独立故障和相关故障。然后，提出了一个量化模型，即基于区域的模型，来分析依赖和独立故障。基于区域的模型考虑以下情况：1）检查员失败的概率可能很高；2） 检查员失败的可能性可能不同；3） 检查组中的故障可以是相关故障和独立故障的组合。通过考虑所有这些情况，Z模型对检查组的失败做出了有意义的解释和令人信服的评估。为了验证新模型的有效性，使用仿真数据将基于区域的模型与传统模型进行了比较。结果表明，基于区域的模型在分析独立和相关故障方面明显优于传统模型。,常见原因故障，相关故障，需求检查，基于区域的模型,,,
K7DTJWNP,2023,https://doi.org/10.1109/TSE.2022.3224378,TSE 2023,Self-Admitted Technical Debt in the Embedded Systems Industry: An Exploratory Case Study,"Technical debt denotes shortcuts taken during software development, mostly for the sake of expedience. When such shortcuts are admitted explicitly by developers (e.g., writing a TODO/Fixme comment), they are termed as Self-Admitted Technical Debt or SATD. There has been a fair amount of work studying SATD management in Open Source projects, but SATD in industry is relatively unexplored. At the same time, there is no work focusing on developers’ perspectives towards SATD and its management. To address this, we conducted an exploratory case study in cooperation with an industrial partner to study how they think of SATD and how they manage it. Specifically, we collected data by identifying and characterizing SATD in different sources (issues, source code comments, and commits) and carried out a series of interviews with 12 software practitioners. The results show: 1) the core characteristics of SATD in industrial projects; 2) developers’ attitudes towards identified SATD and statistics; 3) triggers for practitioners to introduce and repay SATD; 4) relations between SATD in different sources; 5) practices used to manage SATD; 6) challenges and tooling ideas for SATD management.","Technical debt,self-admitted technical debt,mining software repositories,source code comment,issue tracking system,commit,empirical study",嵌入式系统行业的技术债务——一个探索性案例研究,技术债务指的是软件开发过程中走的捷径，主要是为了权宜之计。当开发人员明确承认这种快捷方式时（例如，编写TODO/Fixme注释），它们被称为自行承认的技术债务或SATD。在开源项目中研究SATD管理已经有相当多的工作，但行业中的SATD相对未被探索。同时，没有任何工作关注开发人员对SATD及其管理的看法。为了解决这一问题，我们与行业合作伙伴合作进行了一项探索性案例研究，以研究他们如何看待SATD以及如何管理它。具体而言，我们通过识别和表征不同来源（问题、源代码注释和提交）的SATD来收集数据，并对12名软件从业者进行了一系列采访。研究结果表明：1）SATD在产业项目中的核心特征；2） 开发人员对已确定的SATD和统计数据的态度；3） 从业者引入和偿还SATD的触发因素；4） 不同来源SATD之间的关系；5） 用于管理SATD的做法；6） SATD管理的挑战和工具理念。,技术债务，自我承认的技术债务，挖掘软件库，源代码评论，问题跟踪系统，承诺，实证研究,,,
NL2MUMZ2,2023,https://doi.org/10.1109/TSE.2022.3224053,TSE 2023,To Follow or Not to Follow: Understanding Issue/Pull-Request Templates on GitHub,"For most Open Source Software (OSS) projects, issues and Pull-requests (PR) are the primary means by which stakeholders of a project report and discuss software problems and code changes, and their descriptions are important for people to understand them. To help ensure the informational quality of issue/PR descriptions, GitHub introduced the issue/PR template feature, which pre-populates the description for anyone trying to open a new issue/PR. To better understand this feature, we report on a large-scale, mixed-methods empirical study of templates that explores contents, impacts, and perceptions. Our results show that templates typically contain elements to greet contributors, explain project guidelines, and collect relevant information. After template adoption, the monthly volume of incoming issues and PRs decreases, and issues have fewer monthly discussion comments and longer resolution duration. Although both contributors and maintainers positively rated the usefulness of templates from various aspects, they also reported challenges in using templates (e.g., excessive and irrelevant information request) and suggested potential improvements of the template feature (e.g., better user interaction and advanced automation). This work contributes to the informed use and targeted improvement of templates to enhance OSS practitioners’ collaboration and interaction.","GitHub,issue template,open source software,pull-request template",遵循或不遵循：理解GitHub上的Issue/Pull请求模板,对于大多数开源软件（OSS）项目来说，问题和拉取请求（PR）是项目利益相关者报告和讨论软件问题和代码更改的主要手段，它们的描述对人们理解它们很重要。为了帮助确保问题/PR描述的信息质量，GitHub引入了问题/PR模板功能，该功能为任何试图打开新问题/PR的人预先填充描述。为了更好地理解这一特征，我们报告了一项大规模的、混合方法的模板实证研究，该研究探索了内容、影响和感知。我们的结果表明，模板通常包含一些元素，用于问候贡献者、解释项目指导方针和收集相关信息。采用模板后，每月收到的问题和PR数量减少，问题的每月讨论评论更少，解决时间更长。尽管贡献者和维护者都从各个方面积极评价了模板的有用性，但他们也报告了使用模板的挑战（例如，过多和不相关的信息请求），并建议对模板功能进行潜在的改进（例如，更好的用户交互和高级自动化）。这项工作有助于在知情的情况下使用和有针对性地改进模板，以加强开放源码软件从业者的协作和互动。,GitHub，问题模板，开源软件，拉取请求模板,,,
FGN6R7VJ,2023,https://doi.org/10.1109/TSE.2022.3204589,TSE 2023,DSSDPP: Data Selection and Sampling Based Domain Programming Predictor for Cross-Project Defect Prediction,"Cross-project defect prediction (CPDP) refers to recognizing defective software modules in one project (i.e., target) using historical data collected from other projects (i.e., source), which can help developers find defects and prioritize their testing efforts. Unfortunately, there often exists large distribution difference between the source and target data. Most CPDP methods neglect to select the appropriate source data for a given target at the project level. More importantly, existing CPDP models are parametric methods, which usually require intensive parameter selection and tuning to achieve better prediction performance. This would hinder wide applicability of CPDP in practice. Moreover, most CPDP methods do not address the cross-project class imbalance problem. These limitations lead to suboptimal CPDP results. In this paper, we propose a novel data selection and sampling based domain programming predictor (DSSDPP) for CPDP, which addresses the above limitations. DSSDPP is a non-parametric CPDP method, which can perform knowledge transfer across projects without the need for parameter selection and tuning. By exploiting the structures of source and target data, DSSDPP can learn a discriminative transfer classifier for identifying defects of the target project. Extensive experiments on 22 projects from four datasets indicate that DSSDPP achieves better MCC and AUC results against a range of competing methods both in the single-source and multi-source scenarios. Since DSSDPP is easy, effective, extensible, and efficient, we suggest that future work can use it with the well-chosen source data to conduct CPDP especially for the projects with limited computational budget.","Cross-project defect prediction,domain programming predictor,data selection,data sampling,transfer learning,software quality assurance",DSSDPP:用于跨项目缺陷预测的基于数据选择和采样的领域规划预测器,跨项目缺陷预测（CPDP）是指使用从其他项目（即源）收集的历史数据来识别一个项目（即目标）中的缺陷软件模块，这可以帮助开发人员发现缺陷并优先考虑他们的测试工作。不幸的是，源数据和目标数据之间往往存在很大的分布差异。大多数CPDP方法忽略了在项目级别为给定目标选择适当的源数据。更重要的是，现有的CPDP模型是参数方法，通常需要密集的参数选择和调整才能获得更好的预测性能。这将阻碍国家方案文件在实践中的广泛适用。此外，大多数CPDP方法并没有解决跨项目类的不平衡问题。这些限制导致CPDP结果不理想。在本文中，我们提出了一种新的基于数据选择和采样的CPDP领域规划预测器（DSSDPP），它解决了上述限制。DSSDPP是一种非参数CPDP方法，它可以在不需要参数选择和调整的情况下跨项目执行知识转移。通过利用源数据和目标数据的结构，DSSDPP可以学习用于识别目标项目缺陷的判别转移分类器。在来自四个数据集的22个项目上进行的大量实验表明，DSSDPP在单源和多源场景中与一系列竞争方法相比，获得了更好的MCC和AUC结果。由于DSSDPP简单、有效、可扩展且高效，我们建议未来的工作可以将其与精心选择的源数据一起使用，以进行CPDP，特别是对于计算预算有限的项目。,跨项目缺陷预测，领域编程预测器，数据选择，数据采样，迁移学习，软件质量保证,,,
5PTBZPCF,2023,https://doi.org/10.1109/TSE.2023.3308952,TSE 2023,Do Pretrained Language Models Indeed Understand Software Engineering Tasks?,"Artificial intelligence (AI) for software engineering (SE) tasks has recently achieved promising performance. In this article, we investigate to what extent the pre-trained language model truly understands those SE tasks such as code search, code summarization, etc. We conduct a comprehensive empirical study on a board set of AI for SE (AI4SE) tasks by feeding them with variant inputs: 1) with various masking rates and 2) with sufficient input subset method. Then, the trained models are evaluated on different SE tasks, including code search, code summarization, and duplicate bug report detection. Our experimental results show that pre-trained language models are insensitive to the given input, thus they achieve similar performance in these three SE tasks. We refer to this phenomenon as overinterpretation, where a model confidently makes a decision without salient features, or where a model finds some irrelevant relationships between the final decision and the dataset. Our study investigates two approaches to mitigate the overinterpretation phenomenon: whole word mask strategy and ensembling. To the best of our knowledge, we are the first to reveal this overinterpretation phenomenon to the AI4SE community, which is an important reminder for researchers to design the input for the models and calls for necessary future work in understanding and implementing AI4SE tasks.","Overinterpretation,deep learning,pre-trained language model,software engineering",预训练语言模型确实理解软件工程任务吗？,用于软件工程（SE）任务的人工智能（AI）最近取得了有希望的性能。在这篇文章中，我们研究了预先训练的语言模型在多大程度上真正理解SE任务，如代码搜索、代码摘要等。我们对一组用于SE的AI（AI4SE）任务进行了全面的实证研究，通过向它们提供变量输入：1）具有不同的掩蔽率，2）具有足够的输入子集方法。然后，在不同的SE任务上评估训练的模型，包括代码搜索、代码摘要和重复错误报告检测。我们的实验结果表明，预先训练的语言模型对给定的输入不敏感，因此它们在这三个SE任务中获得了相似的性能。我们将这种现象称为过度解释，即模型自信地做出没有显著特征的决策，或者模型发现最终决策与数据集之间存在一些不相关的关系。我们的研究调查了两种缓解过度解读现象的方法：全词屏蔽策略和组合。据我们所知，我们是第一个向AI4SE社区揭示这种过度解释现象的人，这是对研究人员设计模型输入的重要提醒，并呼吁未来在理解和实施AI4SE任务方面进行必要的工作。,过度解读，深度学习，预先训练的语言模型，软件工程,,,
CHUDIFWM,2023,https://doi.org/10.1109/TSE.2022.3163614,TSE 2023,Large-Scale Empirical Study of Inline Assembly on 7.6 Million Ethereum Smart Contracts,"Being the most popular programming language for developing Ethereum smart contracts, Solidity allows using inline assembly to gain fine-grained control. Although many empirical studies on smart contracts have been conducted, to the best of our knowledge, none has examined inline assembly in smart contracts. To fill the gap, in this paper, we conduct the first large-scale empirical study of inline assembly on more than 7.6 million open-source Ethereum smart contracts from three aspects, namely, source code, bytecode, and transactions after designing new approaches to tackle several technical challenges. Through a thorough quantitative and qualitative analysis of the collected data, we obtain many new observations and insights. Moreover, by conducting a questionnaire survey on using inline assembly in smart contracts, we draw new insights from the valuable feedback. This work sheds light on the development of smart contracts as well as the evolution of Solidity and its compilers.","Ethereum,smart contract,Solidity,inline assembly,Yul",760万以太坊智能合约内联组装的大规模实证研究,作为开发以太坊智能合约最流行的编程语言，Solidity允许使用内联汇编来获得细粒度的控制。尽管已经对智能合约进行了许多实证研究，但据我们所知，没有一项研究智能合约中的内联组装。为了填补这一空白，在设计了应对多项技术挑战的新方法后，本文首次从源代码、字节码和交易三个方面对760多万个开源以太坊智能合约进行了大规模的内联组装实证研究。通过对收集到的数据进行全面的定量和定性分析，我们获得了许多新的观察和见解。此外，通过对智能合约中使用内联组装进行问卷调查，我们从有价值的反馈中获得了新的见解。这项工作揭示了智能合约的发展以及Solidity及其编译器的演变。,以太坊，智能合约，Solidity，内联组装，Yul,,,
IU7T38XC,2023,https://doi.org/10.1109/TSE.2022.3185458,TSE 2023,Predictive Comment Updating With Heuristics and AST-Path-Based Neural Learning: A Two-Phase Approach,"Just-in-time comment update is a promising way to reduce the burden of developers during software maintenance and evolution. Existing approaches can be divided into two categories: the heuristic-based approach and the deep-learning-based approach. The heuristic-based approach is restricted to a specific type of comment updates (i.e., code-indicative updates), but performs well on such type. The effectiveness of deep-learning-based approach is limited but it can handle diverse comment updates. Considering the complementary advantages of existing approaches, an intuitive idea is to combine them for better performance. To investigate this idea, we first conduct a pre-study experiment which shows that to construct an effective comment updater by combining heuristic-based and deep-learning-based approaches, we need to tackle two main challenges: 1) the heuristic-based approach may bring side effects to cases which cannot be updated by it; and 2) the current deep-learning-based approach is with limited effectiveness. Then, we propose a novel two-phase approach named S_Coachto cope with these two challenges and effectively perform comment updates. In the first phase, S_Coachintegrates nine distinctive features identified through our large-scale empirical analysis into a predictive model, which can predict whether the contents of the comment updates can be found in the corresponding code changes, namely, the comment updates are code-indicative updates. If so, the updates are then generated by an off-the-shelf heuristic-based approach; otherwise, S_Coachleverages a deep learning model, which we specially designed for non-code-indicative updates, to infer the new comment based on the old comment and code change. Motivated by our manual observation on the limitation of existing approaches on non-code-indicative updates, our deep learning model adopts the Abstract Syntax Tree path technique, which can capture the program structure information for effectively embedding code changes. Our evaluation shows that our approach outperforms the state-of-the-art by around 20% with respect to the number of correct comments it generates. Via in-depth analysis, we illustrate the rationale of each design decision as well as point out potential directions.","Comment update,deep learning,code embedding",基于启发式和AST路径的神经学习的预测注释更新：一种两阶段方法,即时评论更新是一种很有前途的方法，可以减轻开发人员在软件维护和开发过程中的负担。现有的方法可以分为两类：基于启发式的方法和基于深度学习的方法。基于启发式的方法仅限于特定类型的注释更新（即，代码指示更新），但在这种类型上表现良好。基于深度学习的方法的有效性是有限的，但它可以处理不同的评论更新。考虑到现有方法的互补优势，一个直观的想法是将它们结合起来以获得更好的性能。为了研究这一想法，我们首先进行了一个预研究实验，该实验表明，要通过结合基于启发式和基于深度学习的方法来构建一个有效的评论更新器，我们需要解决两个主要挑战：1）基于启发式的方法可能会给无法更新的情况带来副作用；以及2）当前基于深度学习的方法的有效性有限。然后，我们提出了一种新的两阶段方法S_Coacht，以应对这两个挑战并有效地执行评论更新。在第一阶段，S_Coachin将我们通过大规模实证分析确定的九个显著特征整合到一个预测模型中，该模型可以预测评论更新的内容是否可以在相应的代码变化中找到，即评论更新是代码指示更新。如果是，则通过现成的基于启发式的方法生成更新；否则，S_Coachleverages使用了一个深度学习模型，我们专门为非代码指示更新设计了该模型，以基于旧注释和代码更改来推断新注释。受我们对现有方法在非代码指示更新方面的局限性的手动观察的启发，我们的深度学习模型采用了抽象语法树路径技术，该技术可以捕获程序结构信息，以有效嵌入代码更改。我们的评估表明，就生成的正确评论数量而言，我们的方法比最先进的方法高出约20%。通过深入分析，我们阐述了每一个设计决策的基本原理，并指出了潜在的方向。,评论更新，深度学习，代码嵌入,,,
F6T96IHE,2023,https://doi.org/10.1109/TSE.2023.3272309,TSE 2023,What Not to Test (For Cyber-Physical Systems),"For simulation-based systems, finding a set of test cases with the least cost by exploring multiple goals is a complex task. Domain-specific optimization goals (e.g., maximize output variance) are useful for guiding the rapid selection of test cases via mutation. But evaluating the selected test cases via mutation (that can distinguish the current program from) is a different goal to domain-specific optimizations. While the optimization goals can be used to guide the mutation analysis, that guidance should be viewed as a weak indicator since it can hurt the mutation effectiveness goals by focusing too much on the optimization goals. Based on the above, this paper proposes DoLesS (Domination with Least Squares Approximation) that selects the minimal and effective test cases by averaging over a coarse-grained grid of the information gained from multiple optimizations goals. DoLesS applies an inverted least squares approximation approach to find a minimal set of tests that can distinguish better from worse parts of the optimization goals. When tested on multiple simulation-based systems, DoLesS performs as well or even better as the prior state-of-the-art, while running 80-360 times faster on average (seconds instead of hours).","Search-based software engineering,modeling and model-driven engineering,validation and verification,software testing,simulation-based testing,multi-goal optimization",不测试的内容（针对网络物理系统）,对于基于仿真的系统来说，通过探索多个目标来找到一组成本最低的测试用例是一项复杂的任务。特定领域的优化目标（例如，最大化输出方差）有助于指导通过突变快速选择测试用例。但是，通过突变（可以区分当前程序）评估选定的测试用例与特定领域的优化是不同的目标。虽然优化目标可用于指导突变分析，但该指导应被视为一个薄弱指标，因为它可能会因过于关注优化目标而损害突变有效性目标。基于以上，本文提出了DoLesS（最小二乘逼近支配），通过对从多个优化目标中获得的信息进行粗粒度网格平均，来选择最小和有效的测试用例。DoLesS应用反向最小二乘近似方法来找到一组最小的测试，这些测试可以区分优化目标的优缺点。当在多个基于模拟的系统上进行测试时，DoLesS的性能与之前的最先进系统一样好，甚至更好，同时平均运行速度快80-360倍（秒而不是小时）。,基于搜索的软件工程，建模和模型驱动的工程，验证和验证，软件测试，基于模拟的测试，多目标优化,,,
Z9NJT7ZI,2023,https://doi.org/10.1109/TSE.2023.3253700,TSE 2023,SLocator: Localizing the Origin of SQL Queries in Database-Backed Web Applications,"In database-backed web applications, developers often leverage Object-Relational Mapping (ORM) frameworks for database accesses. ORM frameworks provide an abstraction of the underlying database access details so that developers can focus on implementing the business logic of the application. However, due to the abstraction, developers may not know where and how a problematic SQL query is generated in the application code, causing challenges in debugging database access problems. In this paper, we propose an approach, called SLocator, which locates where a SQL query is generated in the application code. SLocator is a hybrid approach that leverages both static analysis and information retrieval (IR) techniques. SLocator uses static analysis to infer the database access for every possible path in the control flow graph. Then, given a SQL query, SLocator applies IR techniques to find the control flow path (i.e., a sequence of methods called in an interprocedural control flow graph) whose inferred database access has the highest similarity ranking. We implement SLocator for Java’s official ORM API specification (JPA) and evaluate SLocator on seven open source Java applications. We find that SLocator is able to locate the control flow path that generates a SQL query with a Top@1 accuracy ranging from 37.4% to 70% for SQL queries in sessions, and 30.7% to 69.2% for individual SQL queries; and Top@5 ranging from 78.3% to 95.5% for SQL queries in sessions, and 59.1% to 100% for individual SQL queries. We also conduct a study to illustrate how SLocator may be used for locating issues in the database access code.","Information retrieval,localization,object-relational mapping,static analysis",SLocator：在数据库支持的Web应用程序中定位SQL查询的来源,在数据库支持的web应用程序中，开发人员经常利用对象关系映射（ORM）框架进行数据库访问。ORM框架提供了底层数据库访问细节的抽象，以便开发人员能够专注于实现应用程序的业务逻辑。然而，由于抽象，开发人员可能不知道在应用程序代码中在哪里以及如何生成有问题的SQL查询，这会给调试数据库访问问题带来挑战。在本文中，我们提出了一种称为SLocator的方法，它可以定位在应用程序代码中生成SQL查询的位置。SLocator是一种利用静态分析和信息检索（IR）技术的混合方法。SLocator使用静态分析来推断控制流图中每个可能路径的数据库访问。然后，给定SQL查询，SLocator应用IR技术来找到推断的数据库访问具有最高相似性排名的控制流路径（即过程间控制流图中调用的一系列方法）。我们为Java的官方ORM API规范（JPA）实现了SLocator，并在七个开源Java应用程序上评估了SLocater。我们发现SLocator能够定位生成SQL查询的控制流路径Top@1会话中SQL查询的准确率为37.4%至70%，单个SQL查询的正确率为30.7%至69.2%；和Top@5会话中的SQL查询为78.3%至95.5%，单个SQL查询为59.1%至100%。我们还进行了一项研究，以说明SLocator如何用于查找数据库访问代码中的问题。,信息检索，本地化，对象关系映射，静态分析,,,
H8B3AMP2,2023,https://doi.org/10.1109/TSE.2023.3292399,TSE 2023,Automated Question Title Reformulation by Mining Modification Logs From Stack Overflow,"In Stack Overflow, developers may not clarify and summarize the critical problems in the question titles due to a lack of domain knowledge or poor writing skills. Previous studies mainly focused on automatically generating the question titles by analyzing the posts’ problem descriptions and code snippets. In this study, we aim to improve title quality from the perspective of question title reformulation and propose a novel approach QETRA motivated by the findings of our formative study. Specifically, by mining modification logs from Stack Overflow, we first extract title reformulation pairs containing the original title and the reformulated title. Then we resort to multi-task learning by formalizing title reformulation for each programming language as separate but related tasks. Later we adopt a pre-trained model T5 to automatically learn the title reformulation patterns. Automated evaluation and human study both show the competitiveness of QETRA after compared with six state-of-the-art baselines. Moreover, our ablation study results also confirm that our studied question title reformulation task is more practical than the direct question title generation task for generating high-quality titles. Finally, we develop a browser plugin based on QETRA to facilitate the developers to perform title reformulation. Our study provides a new perspective for studying the quality of post titles and can further generate high-quality titles.","Stack Overflow mining,question post quality assurance,question title reformulation,modification logs,deep learning",从堆栈溢出中挖掘修改日志实现问题标题的自动编排,在Stack Overflow中，由于缺乏领域知识或写作技能差，开发人员可能无法澄清和总结问题标题中的关键问题。以往的研究主要集中在通过分析帖子的问题描述和代码片段来自动生成问题标题。在本研究中，我们旨在从问题标题重新表述的角度提高标题质量，并在形成性研究的基础上提出了一种新的方法QETRA。具体来说，通过从Stack Overflow中挖掘修改日志，我们首先提取包含原始标题和重新表述标题的标题重新表述对。然后，我们通过将每种编程语言的标题重新表述形式化为单独但相关的任务来进行多任务学习。后来，我们采用了一个预先训练的模型T5来自动学习标题重新表述模式。在与六个最先进的基线进行比较后，自动评估和人类研究都显示了QETRA的竞争力。此外，我们的消融研究结果也证实，在生成高质量标题方面，我们研究的问题标题改写任务比直接问题标题生成任务更实用。最后，我们开发了一个基于QETRA的浏览器插件，以方便开发人员进行标题重新表述。我们的研究为研究职称质量提供了一个新的视角，可以进一步生成高质量的职称。,Stack Overflow挖掘，题后质量保证，题名重制，修改日志，深度学习,,,
7GNY2LDW,2023,https://doi.org/10.1109/TSE.2022.3150876,TSE 2023,Nighthawk: Fully Automated Localizing UI Display Issues via Visual Understanding,"Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the upgrading of mobile devices and the development of aesthetics, the visual effects of the GUI are more and more attracting, and users pay more attention to the accessibility and usability of applications. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, component occlusion, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a fully automated approach, Nighthawk, based on deep learning for modelling visual information of the GUI screenshot. Nighthawk can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. At the same time, training the model needs a large amount of labeled buggy screenshots, which requires considerable manual effort to prepare them. We therefore propose a heuristic-based training data auto-generation method to automatically generate the labeled training data. The evaluation demonstrates that our Nighthawk can achieve average 0.84 precision and 0.84 recall in detecting UI display issues, average 0.59 AP and 0.60 AR in localizing these issues. We also evaluate Nighthawk with popular Android apps on Google Play and F-Droid, and successfully uncover 151 previously-undetected UI display issues with 75 of them being confirmed or fixed so far.","UI display,mobile app,UI testing,deep learning,object detection",夜鹰：通过视觉理解实现UI显示问题的全自动本地化,图形用户界面（GUI）在软件应用程序和最终用户之间提供了一个视觉桥梁，通过它他们可以相互交互。随着移动设备的升级和美学的发展，GUI的视觉效果越来越吸引人，用户也越来越关注应用程序的可访问性和可用性。然而，这种GUI复杂性给GUI实现带来了巨大挑战。根据我们对众包测试错误报告的初步研究，由于软件或硬件兼容性，在不同设备上的GUI渲染过程中，总是会出现文本重叠、组件遮挡、图像丢失等显示问题。它们会对应用程序的可用性产生负面影响，导致用户体验不佳。为了检测这些问题，我们提出了一种基于深度学习的全自动方法Nighthawk，用于对GUI屏幕截图的视觉信息进行建模。夜鹰可以检测到有显示问题的GUI，也可以在给定的GUI中定位问题的详细区域，以指导开发人员修复错误。同时，训练模型需要大量标记的bug屏幕截图，这需要大量的手动准备。因此，我们提出了一种基于启发式的训练数据自动生成方法来自动生成标记的训练数据。评估表明，我们的夜鹰在检测UI显示问题方面可以达到平均0.84的精度和0.84的召回率，在定位这些问题方面可以实现平均0.59的AP和0.60的AR。我们还使用Google Play和F-Droid上流行的Android应用程序评估了夜鹰，并成功发现了151个以前未被发现的UI显示问题，其中75个问题已被确认或修复。,UI显示，移动应用程序，UI测试，深度学习，对象检测,,,
CILQKYEC,2023,https://doi.org/10.1109/TSE.2022.3156787,TSE 2023,"Inconsistent Defect Labels: Essence, Causes, and Influence","The label quality of defect data sets has a direct influence on the reliability of defect prediction models. In this paper, we conduct a systematic study of inconsistent defect labels in multi-version-project defect data sets, i.e., many instances having the same source code but different labels over multiple versions of a software project. First, we report the phenomena of inconsistent labels by real examples and analyze their essence in the context of defect prediction. Then, we uncover the causes that lead to the occurrence of inconsistent labels for the representative label collection approaches. Finally, we investigate the actual influence of inconsistent labels on defect prediction models. We find that inconsistent labels in general exist in six multi-version-project defect data sets (either widely used or the most up-to-date in the literature) collected by diverse label collection approaches. In particular, inconsistent labels in a training data set significantly reduce the prediction performance of a model, while inconsistent labels in a test data set can lead to a considerable evaluation bias on the real performance. Therefore, we recommend that: on the one hand, researchers leverage our findings to make targeted methodological improvements on existing defect label collection approaches to reduce the generation of inconsistent labels; on the other hand, practitioners detect and exclude inconsistent labels in defect data sets to avoid their potential negative influence on defect prediction.","Inconsistent label,multi-version,cross-version,defect prediction,label quality",不一致的缺陷标签：本质、原因和影响,缺陷数据集的标签质量直接影响缺陷预测模型的可靠性。在本文中，我们对多版本项目缺陷数据集中不一致的缺陷标签进行了系统的研究，即在软件项目的多个版本上，许多实例具有相同的源代码但具有不同的标签。首先，我们通过实例报道了标签不一致的现象，并在缺陷预测的背景下分析了其本质。然后，我们为具有代表性的标签收集方法揭示了导致标签不一致的原因。最后，我们研究了不一致标签对缺陷预测模型的实际影响。我们发现，通过不同的标签收集方法收集的六个多版本项目缺陷数据集（广泛使用或文献中最新的）中通常存在不一致的标签。特别是，训练数据集中不一致的标签会显著降低模型的预测性能，而测试数据集中不统一的标签会导致对真实性能的相当大的评估偏差。因此，我们建议：一方面，研究人员利用我们的发现，对现有的缺陷标签收集方法进行有针对性的方法改进，以减少不一致标签的产生；另一方面，从业者检测并排除缺陷数据集中的不一致标签，以避免它们对缺陷预测的潜在负面影响。,标签不一致，多版本，跨版本，缺陷预测，标签质量,,,
YWHYYBHM,2023,https://doi.org/10.1109/TSE.2022.3150333,TSE 2023,Enhancing the Capability of Testing-Based Formal Verification by Handling Operations in Software Packages,"Testing a program based on its specification is necessary to ensure that the program meets its desired functionality. Formal methods, based on some mathematical theories, are often used to enhance the quality of systems but suffer from difficulties in application. The Testing-Based Formal Verification (TBFV) is proposed as an alternative to ensure the correctness of all traversed program paths, but is limited and impractical due to the lack of the capability of dealing with operations (e.g., methods defined in classes) provided in software packages. In this paper, we provide an axiomatic approach to dealing with this problem so as to enhance the capability of the TBFV. In particular, we focus on the Vector, ArrayList, and LinkedList classes in Java. We present both an example to demonstrate how our approach works properly and two small experiments conducted to evaluate the performance of our approach by comparing it with the specification-based testing (SBT). The result shows that our approach is more than 30% superior to the SBT in bug detection.","Specification,testing,formal verification,Hoare logic,method invocation",通过处理软件包中的操作增强基于测试的形式化验证能力,根据程序规范测试程序是必要的，以确保程序满足其所需的功能。基于一些数学理论的形式化方法通常用于提高系统的质量，但在应用中存在困难。基于测试的形式验证（TBFV）被提议作为一种替代方案，以确保所有遍历的程序路径的正确性，但由于缺乏处理软件包中提供的操作（例如，类中定义的方法）的能力，它是有限的和不切实际的。在本文中，我们提供了一种公理化的方法来处理这个问题，以提高TBFV的能力。特别是，我们关注Java中的Vector、ArrayList和LinkedList类。我们给出了一个例子来证明我们的方法是如何正确工作的，并进行了两个小实验，通过将其与基于规范的测试（SBT）进行比较来评估我们的方法的性能。结果表明，我们的方法在错误检测方面比SBT优越30%以上。,规范，测试，形式验证，霍尔逻辑，方法调用,,,
6CK4TFRQ,2023,https://doi.org/10.1109/TSE.2021.3138909,TSE 2023,Just-In-Time Obsolete Comment Detection and Update,"Comments are valuable resources for the development, comprehension and maintenance of software. However, while changing code, developers sometimes neglect the evolution of the corresponding comments, resulting in obsolete comments. Such obsolete comments can mislead developers and introduce bugs in the future, and are therefore detrimental. We notice that by detecting and updating obsolete comments in time with code changes, obsolete comments can be effectively reduced and even avoided. We refer to this task as Just-In-Time (JIT) Obsolete Comment Detection and Update. In this work, we propose a two-stage framework named CUP$^\mathrm{2}$2 (Two-stage Comment UPdater) to automate this task. CUP$^\mathrm{2}$ consists two components, i.e., an Obsolete Comment Detector named OCD and a Comment UPdater named CUP, each of which relies on a distinct neural network model to perform detection (updates). Specifically, given a code change and a corresponding comment, CUP$^\mathrm{2}$ first leverages OCD to predict whether this comment should be updated. If the answer is yes, CUP will be used to generate the new version of the comment automatically. To evaluate CUP$^\mathrm{2}$, we build a large-scale dataset with over 4 million code-comment change samples. Our dataset focuses on method-level code changes and updates on method header comments considering the importance and widespread use of such comments. Evaluation results show that 1) both OCD and CUP outperform their baselines by significant margins, and 2) CUP$^\mathrm{2}$ performs better than a rule-based baseline. Specifically, the comments generated by CUP$^\mathrm{2}$ are identical to the ground truth for 41.8% of the samples that are predicted to be positive by OCD. We believe CUP$^\mathrm{2}$ can help developers detect obsolete comments, better understand where and how to update obsolete comments and reduce their edits on obsolete comment updates.","Code-comment co-evolution,obsolete comment detection,comment update",即时过时评论检测和更新,注释是开发、理解和维护软件的宝贵资源。然而，在更改代码时，开发人员有时会忽略相应注释的演变，导致注释过时。这种过时的评论可能会误导开发人员，并在未来引入错误，因此是有害的。我们注意到，通过随着代码更改及时检测和更新过时的注释，可以有效地减少甚至避免过时的注释。我们将此任务称为实时（JIT）过时注释检测和更新。在这项工作中，我们提出了一个名为CUP$^\mathrm｛2｝$2（两阶段注释更新器）的两阶段框架来自动化这项任务。CUP$^\mathrm｛2｝$由两个组件组成，即名为OCD的过时注释检测器和名为CUP的注释更新器，每个组件都依赖于不同的神经网络模型来执行检测（更新）。具体来说，给定代码更改和相应的注释，CUP$^\mathrm{2}$首先利用OCD来预测是否应该更新此注释。如果答案是肯定的，CUP将用于自动生成新版本的注释。为了评估CUP$^\mathrm{2}$，我们构建了一个包含超过400万个代码注释更改样本的大规模数据集。考虑到方法头注释的重要性和广泛使用，我们的数据集侧重于方法级代码更改和方法头注释更新。评估结果显示，1）OCD和CUP都显著优于其基线，2）CUP$^\mathrm{2}$的性能优于基于规则的基线。具体而言，CUP$^\mathrm{2}$生成的评论与41.8%的OCD预测为阳性的样本的基本事实相同。我们相信CUP$^\mathrm｛2｝$可以帮助开发人员检测过时的评论，更好地了解在哪里以及如何更新过时评论，并减少他们对过时评论更新的编辑。,代码注释协同进化，过时注释检测，注释更新,,,
N7NWSJ9I,2023,https://doi.org/10.1109/TSE.2022.3233901,TSE 2023,GraphSearchNet: Enhancing GNNs via Capturing Global Dependencies for Semantic Code Search,"Code search aims to retrieve accurate code snippets based on a natural language query to improve software productivity and quality. With the massive amount of available programs such as (on GitHub or Stack Overflow), identifying and localizing the precise code is critical for the software developers. In addition, Deep learning has recently been widely applied to different code-related scenarios, e.g., vulnerability detection, source code summarization. However, automated deep code search is still challenging since it requires a high-level semantic mapping between code and natural language queries. Most existing deep learning-based approaches for code search rely on the sequential text i.e., feeding the program and the query as a flat sequence of tokens to learn the program semantics while the structural information is not fully considered. Furthermore, the widely adopted Graph Neural Networks (GNNs) have proved their effectiveness in learning program semantics, however, they also suffer the problem of capturing the global dependencies in the constructed graph, which limits the model learning capacity. To address these challenges, in this paper, we design a novel neural network framework, named GraphSearchNet, to enable an effective and accurate source code search by jointly learning the rich semantics of both source code and natural language queries. Specifically, we propose to construct graphs for the source code and queries with bidirectional GGNN (BiGGNN) to capture the local structural information of the source code and queries. Furthermore, we enhance BiGGNN by utilizing the multi-head attention module to supplement the global dependencies that BiGGNN missed to improve the model learning capacity. The extensive experiments on Java and Python programming language from the public benchmark CodeSearchNet confirm that GraphSearchNet outperforms current state-of-the-art works by a significant margin.","Code search,graph neural networks,multi-head attention",GraphSearchNet：通过捕获语义代码搜索的全局依赖性来增强GNN,代码搜索旨在基于自然语言查询检索准确的代码片段，以提高软件生产力和质量。随着大量可用程序（如GitHub或Stack Overflow）的出现，识别和本地化精确的代码对软件开发人员来说至关重要。此外，深度学习最近被广泛应用于不同的代码相关场景，例如漏洞检测、源代码摘要。然而，自动深度代码搜索仍然具有挑战性，因为它需要代码和自然语言查询之间的高级语义映射。大多数现有的基于深度学习的代码搜索方法都依赖于顺序文本，即，将程序和查询作为一个扁平的令牌序列来学习程序语义，而没有充分考虑结构信息。此外，被广泛采用的图神经网络（GNN）已经证明了其在学习程序语义方面的有效性，然而，它们也面临着在构建的图中捕获全局依赖关系的问题，这限制了模型的学习能力。为了应对这些挑战，在本文中，我们设计了一个新的神经网络框架，名为GraphSearchNet，通过联合学习源代码和自然语言查询的丰富语义，实现有效和准确的源代码搜索。具体而言，我们建议使用双向GGNN（BiGGNN）为源代码和查询构建图，以捕获源代码和询问的局部结构信息。此外，我们通过利用多头注意力模块来补充BiGGNN遗漏的全局依赖性，以提高模型学习能力，从而增强了BiGGNN。公共基准CodeSearchNet对Java和Python编程语言进行的大量实验证实，GraphSearchNet显著优于当前最先进的作品。,代码搜索，图形神经网络，多头注意力,,,
GEPAGSZR,2023,https://doi.org/10.1109/TSE.2023.3285280,TSE 2023,Task-Oriented ML/DL Library Recommendation Based on a Knowledge Graph,"AI applications often use ML/DL (Machine Learning/Deep Learning) models to implement specific AI tasks. As application developers usually are not AI experts, they often choose to integrate existing implementations of ML/DL models as libraries for their AI tasks. As an active research area, AI attracts many researchers and produces a lot of papers every year. Many of the papers propose ML/DL models for specific tasks and provide their implementations. However, it is not easy for developers to find ML/DL libraries that are suitable for their tasks. The challenges lie in not only the fast development of AI application domains and techniques, but also the lack of detailed information of the libraries such as environmental dependencies and supporting resources. In this paper, we conduct an empirical study on ML/DL library seeking questions on Stack Overflow to understand the developers’ requirements for ML/DL libraries. Based on the findings of the study, we propose a task-oriented ML/DL library recommendation approach, called MLTaskKG. It constructs a knowledge graph that captures AI tasks, ML/DL models, model implementations, repositories, and their relationships by extracting knowledge from different sources such as ML/DL resource websites, papers, ML/DL frameworks, and repositories. Based on the knowledge graph, MLTaskKG recommends ML/DL libraries for developers by matching their requirements on tasks, model characteristics, and implementation information. Our evaluation shows that 92.8% of the tuples sampled from the resulting knowledge graph are correct, demonstrating the high quality of the knowledge graph. A further experiment shows that MLTaskKG can help developers find suitable ML/DL libraries using 47.6% shorter time and with 68.4% higher satisfaction.","Deep learning,knowledge graph,library recommendation,machine learning",基于知识图的面向任务的ML/DL库推荐,人工智能应用程序通常使用ML/DL（机器学习/深度学习）模型来实现特定的人工智能任务。由于应用程序开发人员通常不是人工智能专家，他们通常选择将ML/DL模型的现有实现集成为其人工智能任务的库。人工智能作为一个活跃的研究领域，吸引了许多研究人员，每年都会发表大量论文。许多论文提出了用于特定任务的ML/DL模型，并提供了它们的实现。然而，对于开发人员来说，要找到适合他们任务的ML/DL库并不容易。挑战不仅在于人工智能应用领域和技术的快速发展，还在于缺乏图书馆的详细信息，如环境依赖性和支持资源。在本文中，我们对ML/DL库进行了实证研究，在Stack Overflow上寻求问题，以了解开发人员对ML/DL库的需求。基于研究结果，我们提出了一种面向任务的ML/DL库推荐方法，称为MLTaskKG。它构建了一个知识图，通过从不同来源（如ML/DL资源网站、论文、ML/DL框架和存储库）提取知识，捕捉人工智能任务、ML/DL模型、模型实现、存储库及其关系。基于知识图，MLTaskKG通过匹配开发人员对任务、模型特征和实现信息的需求，为开发人员推荐ML/DL库。我们的评估表明，从生成的知识图中采样的元组中，92.8%是正确的，这证明了知识图的高质量。进一步的实验表明，MLTaskKG可以帮助开发人员找到合适的ML/DL库，使用时间缩短47.6%，满意度提高68.4%。,深度学习，知识图谱，图书馆推荐，机器学习,,,
372WRGMY,2023,https://doi.org/10.1109/TSE.2022.3228630,TSE 2023,Generating Structurally Realistic Models With Deep Autoregressive Networks,"Model generators are important tools in model-based systems engineering to automate the creation of software models for tasks like testing and benchmarking. Previous works have established four properties that a generator should satisfy: consistency, diversity, scalability, and structural realism. Although several generators have been proposed, none of them is focused on realism. As a result, automatically generated models are typically simple and appear synthetic. This work proposes a new architecture for model generators which is specifically designed to be structurally realistic. Given a dataset consisting of several models deemed as real models, this type of generators is able to produce new models which are structurally similar to the models in the dataset, but are fundamentally novel models. Our implementation, named ModelMime (M2), is based on a deep autoregressive model which combines a Graph Neural Network with a Recurrent Neural Network. We decompose each model into a sequence of edit operations, and the neural network is trained in the task of predicting the next edit operation given a partial model. At inference time, the system produces new models by sampling edit operations and iteratively completing the model. We have evaluated M2 with respect to three state-of-the-art generators, showing that 1) our generator outperforms the others in terms of the structurally realistic property 2) the models generated by M2 are most of the time consistent, 3) the diversity of the generated models is at least the same as the real ones and, 4) the generation process is scalable once the generator is trained.","Model generators,realistic models,graph neural networks,recurrent neural networks,generative models",用深度自回归网络生成结构真实模型,模型生成器是基于模型的系统工程中的重要工具，用于自动创建用于测试和基准测试等任务的软件模型。先前的工作已经建立了生成器应该满足的四个特性：一致性、多样性、可扩展性和结构真实性。尽管已经提出了几个生成器，但没有一个侧重于现实主义。因此，自动生成的模型通常很简单，而且看起来是合成的。这项工作为模型生成器提出了一种新的架构，该架构专门设计为结构逼真。给定一个由几个被视为真实模型的模型组成的数据集，这种类型的生成器能够生成新的模型，这些模型在结构上与数据集中的模型相似，但从根本上来说是新颖的模型。我们的实现名为ModelMime（M2），基于深度自回归模型，该模型结合了图神经网络和递归神经网络。我们将每个模型分解为一系列编辑操作，并在给定部分模型的情况下训练神经网络来预测下一次编辑操作。在推理时，系统通过采样编辑操作并迭代完成模型来生成新的模型。我们已经就三个最先进的生成器对M2进行了评估，结果表明：1）我们的生成器在结构真实性方面优于其他生成器；2）M2生成的模型在大多数时间上是一致的；3）生成的模型的多样性至少与真实模型相同；4）一旦生成器经过训练，生成过程就可以扩展。,模型生成器，现实模型，图神经网络，递归神经网络，生成模型,,,
YMH5QKVJ,2023,https://doi.org/10.1109/TSE.2022.3150788,TSE 2023,Learning Configurations of Operating Environment of Autonomous Vehicles to Maximize their Collisions,"Autonomous vehicles must operate safely in their dynamic and continuously-changing environment. However, the operating environment of an autonomous vehicle is complicated and full of various types of uncertainties. Additionally, the operating environment has many configurations, including static and dynamic obstacles with which an autonomous vehicle must avoid collisions. Though various approaches targeting environment configuration for autonomous vehicles have shown promising results, their effectiveness in dealing with a continuous-changing environment is limited. Thus, it is essential to learn realistic environment configurations of continuously-changing environment, under which an autonomous vehicle should be tested regarding its ability to avoid collisions. Featured with agents dynamically interacting with the environment, Reinforcement Learning (RL) has shown great potential in dealing with complicated problems requiring adapting to the environment. To this end, we present an RL-based environment configuration learning approach, i.e., DeepCollision, which intelligently learns environment configurations that lead an autonomous vehicle to crash. DeepCollision employs Deep Q-Learning as the RL solution, and selects collision probability as the safety measure, to construct the reward function. We trained four DeepCollision models and conducted an experiment to compare them with two baselines, i.e., random and greedy. Results show that DeepCollision demonstrated significantly better effectiveness in generating collisions compared with the baselines. We also provide recommendations on configuring DeepCollision with the most suitable time interval based on different road structures.","Autonomous vehicle testing,reinforcement learning,environment configuration",学习自动驾驶汽车运行环境的配置以最大限度地提高碰撞,自动驾驶汽车必须在其动态和不断变化的环境中安全运行。然而，自动驾驶汽车的运行环境是复杂的，充满了各种类型的不确定性。此外，操作环境有许多配置，包括自动驾驶汽车必须避免碰撞的静态和动态障碍物。尽管针对自动驾驶汽车环境配置的各种方法已经显示出有希望的结果，但它们在应对持续变化的环境方面的有效性是有限的。因此，学习持续变化的环境的现实环境配置是至关重要的，在这种环境下，自动驾驶汽车应该测试其避免碰撞的能力。强化学习具有主体与环境动态交互的特点，在处理需要适应环境的复杂问题方面显示出巨大的潜力。为此，我们提出了一种基于RL的环境配置学习方法，即深度碰撞，它可以智能地学习导致自动驾驶汽车碰撞的环境配置。DeepCollision采用深度Q学习作为RL解决方案，并选择碰撞概率作为安全度量，构建奖励函数。我们训练了四个深度碰撞模型，并进行了一项实验，将它们与随机和贪婪两种基线进行比较。结果表明，与基线相比，深度碰撞在生成碰撞方面表现出明显更好的有效性。我们还根据不同的道路结构，提供关于使用最合适的时间间隔配置DeepCollision的建议。,自动驾驶汽车测试，强化学习，环境配置,,,
FSFFSW9E,2023,https://doi.org/10.1109/TSE.2023.3270740,TSE 2023,A Taxonomy of Testable HTML5 Canvas Issues,"The HTML5 &lt;canvas&gt; is widely used to display high quality graphics in web applications. However, the combination of web, GUI, and visual techniques that are required to build &lt;canvas&gt; applications, together with the lack of testing and debugging tools, makes developing such applications very challenging. To help direct future research on testing &lt;canvas&gt; applications, in this paper we present a taxonomy of testable &lt;canvas&gt; issues. First, we extracted 2,403 &lt;canvas&gt;-related issue reports from 123 open source GitHub projects that use the HTML5 &lt;canvas&gt;. Second, we constructed our taxonomy by manually classifying a random sample of 332 issue reports. Our manual classification identified five broad categories of testable &lt;canvas&gt; issues, such as Visual and Performance issues. We found that Visual issues are the most frequent (35%), while Performance issues are relatively infrequent (5%). We also found that many testable &lt;canvas&gt; issues that present themselves visually on the &lt;canvas&gt; are actually caused by other components of the web application. Our taxonomy of testable &lt;canvas&gt; issues can be used to steer future research into &lt;canvas&gt; issues and testing.","Issue reports,issue taxonomy,web applications,html5 canvas",可测试HTML5 Canvas问题的分类,HTML5&lt；画布&gt；被广泛用于在web应用程序中显示高质量的图形。然而，构建&lt；画布&gt；应用程序，再加上缺乏测试和调试工具，使得开发此类应用程序非常具有挑战性。为了帮助指导未来对测试&lt；画布&gt；在本文中，我们提出了可测试&lt；画布&gt；问题。首先，我们提取2403&lt；画布&gt-来自123个使用HTML5&lt；画布&gt；。其次，我们通过手动对332份问题报告的随机样本进行分类来构建我们的分类法。我们的手动分类确定了五大类可测试&lt；画布&gt；问题，例如视觉和性能问题。我们发现，视觉问题是最常见的（35%），而性能问题相对较少（5%）。我们还发现，许多可测试的&lt；画布&gt；在&lt；画布&gt；实际上是由web应用程序的其他组件引起的。我们的可测试&lt；画布&gt；问题可以用于引导未来的研究进入&lt；画布&gt；问题和测试。,问题报告，问题分类法，web应用程序，html5画布,,,
ZYT4VT5G,2023,https://doi.org/10.1109/TSE.2023.3253145,TSE 2023,A Framework for Emotion-Oriented Requirements Change Handling in Agile Software Engineering,"Background: Requirements Changes (RCs) – the additions/modifications/deletions of functional/non-functional requirements in software products – are challenging for software practitioners to handle. Handling some changes may significantly impact the emotions of the practitioners. Objective: We wanted to know the key challenges that make RC handling difficult, how these impact the emotions of software practitioners, what influences their RC handling, and how RC handling can be made less emotionally challenging. Method: We followed a mixed-methods approach. We conducted two survey studies, with 40 participants and 201 participants respectively. The presentation of key quantitative data was followed by descriptive statistical analysis, and the qualitative data was analysed using Strauss–Corbinian Grounded Theory, and Socio–Technical Grounded Theory analysis techniques. Findings: We found (1) several key factors that make RC handling an emotional challenge, (2) varying emotions that practitioners feel when it is challenging to handle RCs, (3) how stakeholders, including practitioners themselves, peers, managers and customers, influence the RC handling and how practitioners feel due to the stakeholder influence, and (4) practices that can be used to better handle RCs. Conclusion: Some challenges are technical and some are social which also belong to aspects of agile practice, emotional intelligence, and likely belong to cognitive intelligence. Therefore, to better handle RCs with positive emotions in socio–technical environments, agility, emotional intelligence, and cognitive intelligence need to work in synergy with each other.","Affects,agile,changes,emotions,emotional intelligence,human factors,job-related affective well-being scale,mixed-methods,requirements,software engineering,socio-technical grounded theory,software teams,well-being,workplace awareness",敏捷软件工程中面向情感的需求变更处理框架,背景：需求变更（RC）——软件产品中功能/非功能需求的添加/修改/删除——对软件从业者来说是一项挑战。处理一些变化可能会对从业者的情绪产生重大影响。目标：我们想知道使RC处理困难的关键挑战，这些挑战如何影响软件从业者的情绪，是什么影响他们的RC处理，以及如何降低RC处理在情绪上的挑战。方法：我们采用混合方法。我们进行了两项调查研究，分别有40名参与者和201名参与者。关键定量数据的呈现之后是描述性统计分析，定性数据使用Strauss–Corbinian基础理论和社会技术基础理论分析技术进行分析。研究结果：我们发现（1）使RC处理情绪挑战的几个关键因素，（2）从业者在处理RC具有挑战性时感受到的不同情绪，（3）利益相关者，包括从业者自己、同行、管理者和客户，如何影响RC处理，以及从业者如何因利益相关者的影响而感受到，以及（4）可用于更好地处理RC的实践。结论：有些挑战是技术性的，有些是社会性的，也属于敏捷实践、情商方面，可能属于认知智力。因此，为了在社会技术环境中更好地处理具有积极情绪的RC，敏捷性、情商和认知智能需要相互协同工作。,影响，敏捷，变化，情绪，情商，人为因素，与工作相关的情感幸福感量表，混合方法，需求，软件工程，社会技术基础理论，软件团队，幸福感，工作场所意识,,,
ZUW68S8U,2023,https://doi.org/10.1109/TSE.2022.3172925,TSE 2023,The Emotional Roller Coaster of Responding to Requirements Changes in Software Engineering,"Background: A preliminary study we conducted showed that software practitioners respond to requirements changes (RCs) with different emotions, and that their emotions vary at stages of the RC handling life cycle, such as receiving, developing, and delivering RCs. Furthermore, such developer emotions have direct linkages to cognition, productivity, and decision making. Therefore, it is important to gain a comprehensive understanding the role of emotions in a critical scenarios like handling RCs. Objective: We wanted to study how practitioners emotionally respond to RCs. Method: We conducted a world-wide survey with the participation of 201 software practitioners. In our survey, we used the Job-related Affective Well-being Scale (JAWS) and open-ended questions to capture participants’ emotions when handling RCs in their work and query about the different circumstances when they feel these emotions. We used a combined approach of statistical analysis, JAWS, and Socio-Technical Grounded Theory (STGT) for Data Analysis to analyse our survey data. Findings: We identified (1) emotional responses to RCs, i.e., the most common emotions felt by practitioners when handling RCs; (2) different stimuli – such as the RC, the practitioner, team, manager, customer – that trigger these emotions through their own different characteristics; (3) emotion dynamics, i.e., the changes in emotions during the RC handling life cycle; (4) RC stages where particular emotions are triggered; and (5) time related aspects that regulate the emotion dynamics. Conclusion: Practitioners are not pleased with receiving RCs all the time. Last minute RCs introduced closer to a deadline especially violate emotional well-being of practitioners. We present some practical recommendations for practitioners to follow, including a dual-purpose emotion-centric decision guide to help decide when to introduce or accept an RC, and some future key research directions.","Affects,changes,emotions,human aspects,job–related affective well–being scale,mixed-methods,requirements,socio-technical grounded theory,software engineering,software teams,well–being,workplace awareness",软件工程中应对需求变化的情绪过山车,背景：我们进行的一项初步研究表明，软件从业者对需求变化（RC）的反应具有不同的情绪，并且他们的情绪在RC处理生命周期的各个阶段有所不同，例如接收、开发和交付RC。此外，这种开发者情绪与认知、生产力和决策有着直接的联系。因此，全面了解情绪在处理RC等关键场景中的作用很重要。目的：我们想研究从业者对RC的情绪反应。方法：我们进行了一项全球范围的调查，共有201名软件从业者参与。在我们的调查中，我们使用了与工作相关的情感幸福感量表（JAWS）和开放式问题来捕捉参与者在工作中处理RC时的情绪，并询问他们感受到这些情绪时的不同情况。我们使用了统计分析、JAWS和数据分析的社会技术基础理论（STGT）的组合方法来分析我们的调查数据。研究结果：我们确定了（1）对RC的情绪反应，即从业者在处理RC时感受到的最常见的情绪；（2） 不同的刺激——比如RC、从业者、团队、经理、客户——通过他们自己不同的特征触发这些情绪；（3） 情绪动力学，即RC处理生命周期中的情绪变化；（4） 触发特定情绪的RC阶段；以及（5）调节情绪动力学的时间相关方面。结论：从业者对一直接受随机对照治疗并不满意。在接近最后期限的最后一刻引入的RC尤其侵犯了从业者的情绪健康。我们提出了一些实用的建议供从业者遵循，包括一份以情感为中心的两用决策指南，以帮助决定何时引入或接受RC，以及一些未来的关键研究方向。,影响，变化，情绪，人的方面，与工作相关的情感幸福感量表，混合方法，要求，社会技术基础理论，软件工程，软件团队，幸福感，工作场所意识,,,
9LTY8BRZ,2023,https://doi.org/10.1109/TSE.2023.3274153,TSE 2023,Detecting Android API Compatibility Issues With API Differences,"Android application programming interface (API) enables app developers to harness the functionalities of Android devices by interfacing with services and hardware using a Software Development Kit (SDK). However, API frequently evolves together with its associated SDK, and compatibility issues may arise when the API level supported by the underlying device differs from the API level targeted by app developers. These issues can lead to unexpected behaviors, resulting in a bad user experience. This article presents ACID, a novel approach to detecting Android API compatibility issues induced by API evolution. It detects both API invocation compatibility issues and API callback compatibility issues using API differences and static analysis of the app code. Experiments with 20 benchmark apps show that ACID is more accurate and faster than the state-of-the-art techniques in detecting API compatibility issues. The application of ACID on 2965 real-world apps further demonstrates its practical applicability. To eliminate the false positives reported by ACID, this article also presents a simple yet effective method to quickly verify the compatibility issues by selecting and executing the relevant tests from app's test suite, and experimental results demonstrate the verification method can eliminate most false positives when app's test suite has good coverage of the API usages.","API differences,API evolution,android,compatibility issues,program analysis,test selection",使用API差异检测Android API兼容性问题,Android应用程序编程接口（API）使应用程序开发人员能够通过使用软件开发工具包（SDK）与服务和硬件接口，利用Android设备的功能。然而，API经常与其相关的SDK一起发展，当底层设备支持的API级别与应用程序开发人员针对的API级别不同时，可能会出现兼容性问题。这些问题可能会导致意外行为，从而导致糟糕的用户体验。本文介绍了ACID，这是一种检测API进化引起的Android API兼容性问题的新方法。它使用API差异和应用程序代码的静态分析来检测API调用兼容性问题和API回调兼容性问题。对20个基准应用程序的实验表明，ACID在检测API兼容性问题方面比最先进的技术更准确、更快。ACID在2965个真实世界应用程序上的应用进一步证明了它的实用性。为了消除ACID报告的误报，本文还提出了一种简单而有效的方法，通过从应用程序的测试套件中选择并执行相关测试来快速验证兼容性问题，实验结果表明，当应用程序的检测套件对API使用有很好的覆盖时，该验证方法可以消除大多数误报。,API差异，API演进，安卓，兼容性问题，程序分析，测试选择,,,
J69Q6TTG,2023,https://doi.org/10.1109/TSE.2023.3267848,TSE 2023,"JavaScript Dead Code Identification, Elimination, and Empirical Assessment","Web apps are built by using a combination of HTML, CSS, and JavaScript. While building modern web apps, it is common practice to make use of third-party libraries and frameworks, as to improve developers’ productivity and code quality. Alongside these benefits, the adoption of such libraries results in the introduction of JavaScript dead code, i.e., code implementing unused functionalities. The costs for downloading and parsing dead code can negatively contribute to the loading time and resource usage of web apps. The goal of our study is two-fold. First, we present Lacuna, an approach for automatically detecting and eliminating JavaScript dead code from web apps. The proposed approach supports both static and dynamic analyses, it is extensible and can be applied to any JavaScript code base, without imposing constraints on the coding style or on the use of specific JavaScript constructs. Second, by leveraging Lacuna we conduct an experiment to empirically evaluate the run-time overhead of JavaScript dead code in terms of energy consumption, performance, network usage, and resource usage in the context of mobile web apps. We applied Lacuna four times on 30 mobile web apps independently developed by third-party developers, each time eliminating dead code according to a different optimization level provided by Lacuna. Afterward, each different version of the web app is executed on an Android device, while collecting measures to assess the potential run-time overhead caused by dead code. Experimental results, among others, highlight that the removal of JavaScript dead code has a positive impact on the loading time of mobile web apps, while significantly reducing the number of bytes transferred over the network.","Dead code,JavaScript",JavaScript死代码识别、消除和经验评估,Web应用程序是通过使用HTML、CSS和JavaScript的组合构建的。在构建现代网络应用程序时，通常使用第三方库和框架，以提高开发人员的生产力和代码质量。除了这些好处之外，采用这样的库还引入了JavaScript死代码，即实现未使用功能的代码。下载和解析死代码的成本可能会对web应用程序的加载时间和资源使用产生负面影响。我们研究的目的有两个。首先，我们介绍了Lacuna，一种自动检测和消除web应用程序中JavaScript死代码的方法。所提出的方法支持静态和动态分析，它是可扩展的，可以应用于任何JavaScript代码库，而不会对编码风格或特定JavaScript结构的使用施加限制。其次，通过利用Lacuna，我们进行了一项实验，从移动网络应用程序的能耗、性能、网络使用和资源使用等方面实证评估JavaScript死代码的运行时开销。我们在第三方开发者独立开发的30款移动网络应用程序上应用了四次Lacuna，每次都根据Lacuna提供的不同优化级别消除了死代码。之后，每个不同版本的网络应用程序都在安卓设备上执行，同时收集措施来评估死代码造成的潜在运行时开销。实验结果等突出表明，去除JavaScript死代码对移动网络应用程序的加载时间有积极影响，同时显著减少了通过网络传输的字节数。,死代码，JavaScript,,,
F7YL22FG,2023,https://doi.org/10.1109/TSE.2023.3298432,TSE 2023,Optimizing Highly-Parallel Simulation-Based Verification of Cyber-Physical Systems,"Cyber-Physical Systems (CPSs), comprising both software and physical components, arise in many industry-relevant domains and are often mission- or safety-critical. System-Level Verification (SLV) of CPSs aims at certifying that given (e.g., safety or liveness) specifications are met, or at estimating the value of some Key Performance Indicators, when the system runs in its operational environment, that is in presence of inputs and/or of additional, uncontrolled disturbances. To enable SLV of complex systems from the early design phases, the currently most adopted approach envisions the simulation of a system model under the (time bounded) operational scenarios deemed of interest. Unfortunately, simulation-based SLV can be computationally prohibitive (years of sequential simulation), since system model simulation is computationally intensive and the set of scenarios of interest can be extremely large. In this article, we present a technique that, given a collection of scenarios of interest (extracted from databases or from symbolic structures), computes parallel shortest simulation campaigns, which drive a possibly large number of system model simulators running in parallel in a HPC infrastructure through all (and only) those scenarios in the user-defined (possibly random) order, by wisely avoiding multiple simulations of repeated trajectories, thus minimising completion time. Our experiments on SLV of Modelica/FMU and Simulink models with up to almost 200 million scenarios show that our optimisation yields speedups as high as 8$\boldsymbol{\times}$. This, together with the enabled massive parallelisation, makes practically viable (a few weeks in a HPC infrastructure) verification tasks (both statistical and exhaustive) which would otherwise take inconceivably long time.","System-level verification,simulation,cyber-physical systems,systems engineering",优化基于高度并行仿真的网络物理系统验证,网络物理系统（CPSs）由软件和物理组件组成，出现在许多行业相关领域，通常对任务或安全至关重要。CPSs的系统级验证（SLV）旨在证明满足给定的（例如，安全性或有效性）规范，或者当系统在其运行环境中运行时，即存在输入和/或额外的、不受控制的干扰时，估计一些关键性能指标的值。为了从早期设计阶段实现复杂系统的SLV，目前最常用的方法设想在感兴趣的（有时限的）操作场景下模拟系统模型。不幸的是，基于仿真的SLV可能在计算上令人望而却步（多年的顺序仿真），因为系统模型仿真在计算上是密集的，并且感兴趣的场景集可能非常大。在本文中，我们提出了一种技术，在给定一组感兴趣的场景（从数据库或符号结构中提取）的情况下，计算并行的最短模拟活动，这将驱动可能大量的系统模型模拟器在HPC基础设施中并行运行，以用户定义的（可能是随机的）顺序通过所有（并且仅）这些场景，通过明智地避免重复轨迹的多次模拟，从而最大限度地缩短完成时间。我们在Modelica/FMU和Simulink模型的SLV上进行的实验表明，我们的优化产生了高达8$\boldsymbol｛\times｝$的加速。这一点，再加上启用的大规模并行化，使得实际可行（在HPC基础设施中需要几周时间）的验证任务（包括统计和详尽的），否则将花费难以置信的长时间。,系统级验证，仿真，网络物理系统，系统工程,,,
PJIL5QQ6,2023,https://doi.org/10.1109/TSE.2023.3315935,TSE 2023,Hyperparameter Optimization for AST Differencing,"Computing the differences between two versions of the same program is an essential task for software development and software evolution research. AST differencing is the most advanced way of doing so, and an active research area. Yet, AST differencing algorithms rely on configuration parameters that may have a strong impact on their effectiveness. In this paper, we present a novel approach named DAT (D iff Auto Tuning) for hyperparameter optimization of AST differencing. We thoroughly state the problem of hyper-configuration for AST differencing. We evaluate our data-driven approach DAT to optimize the edit-scripts generated by the state-of-the-art AST differencing algorithm named GumTree in different scenarios. DAT is able to find a new configuration for GumTree that improves the edit-scripts in 21.8% of the evaluated cases.","Software evolution,Tree differencing,Abstract Syntax Trees (AST),hyperparameter optimization, edit-script",AST差分的超参数优化,计算同一程序的两个版本之间的差异是软件开发和软件进化研究的一项重要任务。AST差异是最先进的方法，也是一个活跃的研究领域。然而，AST差分算法依赖于可能对其有效性产生强烈影响的配置参数。在本文中，我们提出了一种新的方法DAT（D iff Auto Tuning），用于AST差分的超参数优化。我们充分阐述了AST差异的超配置问题。我们评估了我们的数据驱动方法DAT，以优化由最先进的名为GumTree的AST差分算法在不同场景中生成的编辑脚本。DAT能够为GumTree找到一种新的配置，在21.8%的评估案例中改进了编辑脚本。,软件进化，树差异，抽象语法树（AST），超参数优化，编辑脚本,,,
6BN9D9FA,2023,https://doi.org/10.1109/TSE.2022.3183297,TSE 2023,Using Transfer Learning for Code-Related Tasks,"Deep learning (DL) techniques have been used to support several code-related tasks such as code summarization and bug-fixing. In particular, pre-trained transformer models are on the rise, also thanks to the excellent results they achieved in Natural Language Processing (NLP) tasks. The basic idea behind these models is to first pre-train them on a generic dataset using a self-supervised task (e.g., filling masked words in sentences). Then, these models are fine-tuned to support specific tasks of interest (e.g., language translation). A single model can be fine-tuned to support multiple tasks, possibly exploiting the benefits of transfer learning. This means that knowledge acquired to solve a specific task (e.g., language translation) can be useful to boost performance on another task (e.g., sentiment classification). While the benefits of transfer learning have been widely studied in NLP, limited empirical evidence is available when it comes to code-related tasks. In this paper, we assess the performance of the Text-To-Text Transfer Transformer (T5) model in supporting four different code-related tasks: (i) automatic bug-fixing, (ii) injection of code mutants, (iii) generation of assert statements, and (iv) code summarization. We pay particular attention in studying the role played by pre-training and multi-task fine-tuning on the model's performance. We show that (i) the T5 can achieve better performance as compared to state-of-the-art baselines; and (ii) while pre-training helps the model, not all tasks benefit from a multi-task fine-tuning.","Deep learning,empirical software engineering",将迁移学习用于代码相关任务,深度学习（DL）技术已被用于支持一些与代码相关的任务，如代码摘要和错误修复。特别是，预训练的transformer模型正在兴起，这也要归功于它们在自然语言处理（NLP）任务中取得的出色结果。这些模型背后的基本思想是首先使用自我监督任务（例如，在句子中填充屏蔽词）在通用数据集上对它们进行预训练。然后，对这些模型进行微调，以支持感兴趣的特定任务（例如，语言翻译）。单个模型可以进行微调以支持多个任务，可能会利用迁移学习的好处。这意味着，为解决特定任务（例如，语言翻译）而获得的知识可能有助于提高另一项任务（例如情绪分类）的性能。虽然迁移学习的好处在NLP中得到了广泛的研究，但在涉及代码相关任务时，可用的经验证据有限。在本文中，我们评估了文本到文本传输转换器（T5）模型在支持四种不同的代码相关任务方面的性能：（i）自动错误修复，（ii）代码突变体的注入，（iii）断言语句的生成，以及（iv）代码摘要。我们特别关注预训练和多任务微调对模型性能的影响。我们表明：（i）与最先进的基线相比，T5可以实现更好的性能；以及（ii）虽然预训练有助于模型，但并非所有任务都受益于多任务微调。,深度学习，经验软件工程,,,
IGVSF37A,2023,https://doi.org/10.1109/TSE.2022.3192249,TSE 2023,The Duality in Computing SSA Programs and Control Dependency,"Control dependency (CD) and Static Single Assignment (SSA) form are the basis of many program analyses, transformation, and optimization techniques, and these are implemented and used by modern compilers such as GCC and LLVM. Most state-of-the-art algorithms approximate these computations by using postdominator relations and dominance frontiers (DF) respectively for efficiency reasons which have been used for over three decades. Dominator-based SSA transformation and control dependencies exhibit a non-dual relationship. Recently, it has been shown that DF-based SSA computation is grossly imprecise, and Weak and Strong Control Closure (WCC and SCC) have wider applicability in capturing control dependencies than postdominator-based CD computation. Our main contribution in this article is the proof of duality between the generation of $\phi$ functions and the computation of weakly deciding (WD) vertices which are the most computationally expensive part of SSA program construction and WCC/SCC computation respectively. We have provided a duality theorem and its constructive proof by means of an algorithm that can compute both the $\phi$ functions and the WD vertices seamlessly. We have used this algorithm to compute SSA programs and WCC, and performed experiments on real-world industrial benchmarks. The practical efficiency of our algorithm is (i) almost equal to the best state-of-the-art algorithm in computing WCC, and (ii) closer to (but not as efficient as) the DF-based algorithms in computing SSA programs. Moreover, our algorithm achieves the ultimate precision in computing WCC and SSA programs with respect to the inputs of these algorithms and obtains wider applicability in the WCC computation (handling nonterminating programs).","SSA program,control dependency,weak control closure,strong control closure,nontermination,duality,   $\phi$     ϕ     function",SSA程序计算中的对偶性与控制依赖性,控制依赖（CD）和静态单分配（SSA）形式是许多程序分析、转换和优化技术的基础，这些技术被GCC和LLVM等现代编译器实现和使用。出于效率原因，大多数最先进的算法通过分别使用支配后关系和支配边界（DF）来近似这些计算，这些算法已经使用了三十多年。基于支配者的SSA变换和控制依赖关系表现出非对偶关系。最近，研究表明，基于DF的SSA计算是非常不精确的，弱和强控制闭包（WCC和SCC）在捕获控制依赖性方面比基于后支配者的CD计算具有更广泛的适用性。我们在本文中的主要贡献是证明$\phi$函数的生成和弱判定（WD）顶点的计算之间的对偶性，这分别是SSA程序构建和WCC/SCC计算中计算成本最高的部分。我们通过一种可以无缝计算$\phi$函数和WD顶点的算法，提供了对偶定理及其构造性证明。我们已经使用该算法来计算SSA程序和WCC，并在真实世界的工业基准上进行了实验。我们的算法的实际效率（i）几乎等于计算WCC中最先进的算法，以及（ii）在计算SSA程序中更接近（但不如）基于DF的算法。此外，我们的算法在计算WCC和SSA程序时相对于这些算法的输入达到了最高精度，并在WCC计算（处理非终止程序）中获得了更广泛的适用性。,SSA程序，控制依赖，弱控制闭包，强控制闭包，非终止，对偶，$\phi$,,,
MJ5I3JZV,2023,https://doi.org/10.1109/TSE.2018.2870388,TSE 2023,Finding Trends in Software Research,"Text mining methods can find large scale trends within research communities. For example, using stable Latent Dirichlet Allocation (a topic modeling algorithm) this study found 10 major topics in 35,391 SE research papers from 34 leading SE venues over the last 25 years (divided, evenly, between conferences and journals). Out study also shows how those topics have changed over recent years. Also, we note that (in the historical record) mono-focusing on a single topic can lead to fewer citations than otherwise. Further, while we find no overall gender bias in SE authorship, we note that women are under-represented in the top-most cited papers in our field. Lastly, we show a previously unreported dichotomy between software conferences and journals (so research topics that succeed at conferences might not succeed at journals, and vice versa). An important aspect of this work is that it is automatic and quickly repeatable (unlike prior SE bibliometric studies that used tediously slow and labor intensive methods). Automation is important since, like any data mining study, its conclusions are skewed by the data used in the analysis. The automatic methods of this paper make it far easier for other researchers to re-apply the analysis to new data, or if they want to use different modeling assumptions.","Software engineering,bibliometrics,topic modeling,text mining",寻找软件研究的趋势,文本挖掘方法可以在研究社区中找到大规模的趋势。例如，使用稳定的潜在狄利克雷分配（一种主题建模算法），这项研究在过去25年中，在34个领先的SE场所的35391篇SE研究论文中发现了10个主要主题（在会议和期刊之间平均分配）。这项研究还显示了这些话题近年来的变化。此外，我们注意到（在历史记录中）单声道关注单个主题会导致比其他情况下更少的引用。此外，虽然我们在SE作者中没有发现总体的性别偏见，但我们注意到，在我们领域被引用最多的论文中，女性的代表性不足。最后，我们展示了以前未报道的软件会议和期刊之间的二分法（因此，在会议上成功的研究主题可能不会在期刊上成功，反之亦然）。这项工作的一个重要方面是它是自动的和快速可重复的（不像以前的SE文献计量学研究那样使用冗长缓慢和劳动密集的方法）。自动化很重要，因为与任何数据挖掘研究一样，分析中使用的数据会扭曲其结论。这篇论文的自动方法使其他研究人员更容易将分析重新应用于新数据，或者如果他们想使用不同的建模假设。,软件工程，文献计量学，主题建模，文本挖掘,,,
PWFIJLE3,2023,https://doi.org/10.1109/TSE.2022.3230059,TSE 2023,Mission Specification Patterns for Mobile Robots: Providing Support for Quantitative Properties,"With many applications across domains as diverse as logistics, healthcare, and agriculture, service robots are in increasingly high demand. Nevertheless, the designers of these robots often struggle with specifying their tasks in a way that is both human-understandable and sufficiently precise to enable automated verification and planning of robotic missions. Recent research has addressed this problem for the functional aspects of robotic missions through the use of mission specification patterns. These patterns support the definition of robotic missions involving, for instance, the patrolling of a perimeter, the avoidance of unsafe locations within an area, or reacting to specific events. Our article introduces a catalog of QUantitAtive RoboTic mission spEcificaTion patterns (QUARTET) that tackles the complementary and equally important challenge of specifying the reliability, performance, resource usage, and other key quantitative properties of robotic missions. Identified using a methodology that included the analysis of 73 research papers published in 17 leading software engineering and robotics venues between 2014–2021, our 22 QUARTET patterns are defined in a tool-supported domain-specific language. As such, QUARTET enables: (i) the precise definition of quantitative robotic-mission requirements and (ii) the translation of these requirements into probabilistic reward computation tree logic (PRCTL), supporting their formal verification and automated planning of robotic missions. We demonstrate the applicability of QUARTET by showing that it supports the specification of over 95% of the quantitative robotic mission requirements from a systematically selected set of recent research papers, of which 75% can be automatically translated into PRCTL for the purposes of verification through model checking and mission planning.","Robotics software engineering,robotic missions specification,quantitative properties,domain-specific languages,probabilistic reward computation tree logic",移动机器人任务规范模式：为定量特性提供支持,随着物流、医疗保健和农业等领域的许多应用，服务机器人的需求越来越高。尽管如此，这些机器人的设计者经常难以以一种既能让人理解又足够精确的方式来指定他们的任务，从而实现机器人任务的自动化验证和规划。最近的研究已经通过使用任务规范模式来解决机器人任务的功能方面的这个问题。这些模式支持机器人任务的定义，例如，涉及周边巡逻、避开区域内不安全的位置或对特定事件做出反应。我们的文章介绍了QUantitive RoboTic任务spEcificaTion模式（QUARTET）的目录，该目录解决了指定机器人任务的可靠性、性能、资源使用和其他关键定量特性这一互补且同样重要的挑战。我们使用的方法包括对2014-20121年间在17个领先的软件工程和机器人领域发表的73篇研究论文的分析，我们的22个QUARTET模式是用一种工具支持的特定领域语言定义的。因此，QUARTET能够：（i）精确定义定量机器人任务需求，以及（ii）将这些需求转化为概率奖励计算树逻辑（PRCTL），支持其正式验证和机器人任务的自动化规划。我们证明了QUARTET的适用性，表明它支持系统选择的一组最新研究论文中超过95%的定量机器人任务需求的规范，其中75%可以自动转换为PRCTL，用于通过模型检查和任务规划进行验证。,机器人软件工程，机器人任务规范，定量特性，特定领域语言，概率奖励计算树逻辑,,,
V2RYTDEL,2023,https://doi.org/10.1109/TSE.2023.3252671,TSE 2023,LeakageVerif: Efficient and Scalable Formal Verification of Leakage in Symbolic Expressions,"Side-channel attacks are a powerful class of attacks targeting cryptographic devices. Masking is a popular protection technique to thwart such attacks as it can be theoretically proven secure. However, correctly implementing masking schemes is a non-trivial task and error-prone. If several techniques have been proposed to formally verify masked implementations, they all come with limitations regarding expressiveness, scalability or accuracy. In this work, we propose a symbolic approach, based on a variant of the classical substitution method, for formally verifying arithmetic and boolean masked programs. This approach is more accurate and scalable than existing approaches thanks to a careful design and implementation of key heuristics, algorithms and data structures involved in the verification process. We present all the details of this approach and the open-source tool called LeakageVerif which implements it as a python library, and which offers constructions for symbolic expressions and functions for their verification. We compare LeakageVerif to three existing state-of-the-art tools on a set of 46 masked programs, and we show that it has very good scalability and accuracy results while providing all the necessary constructs for describing algorithmic to assembly masking schemes. Finally, we also provide the set of 46 benchmarks, named MaskedVerifBenchs and written for comparing the different verification tools, in the hope that they will be useful to the community for future comparisons.","Application security,computer security,crypt ography,industry applications,security,side-channel attacks",LeakageVerif：符号表达式中泄漏的有效且可扩展的形式化验证,侧信道攻击是一类针对加密设备的强大攻击。屏蔽是一种流行的保护技术，可以阻止此类攻击，因为理论上可以证明它是安全的。然而，正确地实现掩蔽方案是一项不平凡的任务，并且容易出错。如果已经提出了几种技术来正式验证屏蔽实现，那么它们在表达性、可伸缩性或准确性方面都有局限性。在这项工作中，我们提出了一种基于经典替换方法变体的符号方法，用于形式化验证算术和布尔掩码程序。由于仔细设计和实现了验证过程中涉及的关键启发式、算法和数据结构，这种方法比现有方法更准确、更可扩展。我们介绍了这种方法的所有细节，以及名为LeakageVerif的开源工具，该工具将其实现为python库，并为符号表达式和函数的验证提供构造。我们将LeakageVerif与三个现有的最先进的工具在一组46个屏蔽程序上进行了比较，结果表明它具有非常好的可扩展性和准确性，同时提供了描述算法到组装屏蔽方案的所有必要结构。最后，我们还提供了一组46个基准测试，名为MaskedVerifBench，用于比较不同的验证工具，希望它们对社区未来的比较有用。,应用程序安全，计算机安全，密码术，行业应用程序，安全，侧通道攻击,,,
XBPV736I,2023,https://doi.org/10.1109/TSE.2023.3272631,TSE 2023,Discovering Reusable Functional Features in Legacy Object-Oriented Systems,"Typical object-oriented (OO) systems implement several functional features that are interwoven into class hierarchies. In the absence of aspect-oriented techniques to develop and compose these features, developers resort to object-oriented design and programming idioms to separate features as well as possible. Given a legacy OO system, discovering existing functional features helps understand the design of the system and extract these features to ease their maintenance and reuse. We want to discover candidate functional features in OO systems. We first define functional features and then discuss the footprints that such features are likely to leave in an OO system. We identify three such footprints: (1) multiple inheritance, (2) delegation, and (3) ad-hoc. We develop a set of algorithms for identifying such footprints in OO code and implemented them for the Java language using Eclipse JDT. In this article, we present the algorithms, and the results of applying the corresponding tools on five open-source systems: FreeMind, JavaWebMail, JHotDraw, JReversePro, and Lucene. Our experimental results show that: (1) the different algorithms can identify interesting and useful candidate functional features in OO systems, (2) they can identify opportunities for refactoring, and (3) they are complementary and could help developers.","Delegation,feature discovery,formal concept analysis,functional feature,multiple inheritance",发现遗留面向对象系统中的可重用功能特征,典型的面向对象（OO）系统实现了交织在类层次结构中的几个功能特性。在缺乏面向方面的技术来开发和组合这些特性的情况下，开发人员会采用面向对象的设计和编程习惯来尽可能地分离特性。给定一个遗留的OO系统，发现现有的功能特性有助于理解系统的设计，并提取这些特性以便于维护和重用。我们希望发现OO系统中的候选功能特性。我们首先定义功能特性，然后讨论这些特性可能在OO系统中留下的足迹。我们确定了三个这样的足迹：（1）多重继承，（2）委派和（3）自组织。我们开发了一组用于在OO代码中识别此类足迹的算法，并使用EclipseJDT为Java语言实现了这些算法。在本文中，我们介绍了算法，以及在五个开源系统上应用相应工具的结果：FreeMind、JavaWebMail、JHotDraw、JReversePro和Lucene。我们的实验结果表明：（1）不同的算法可以识别OO系统中有趣和有用的候选功能特征，（2）它们可以识别重构的机会，（3）它们是互补的，可以帮助开发人员。,委派，特征发现，形式化概念分析，功能特征，多重继承,,,
HLDCANZZ,2023,https://doi.org/10.1109/TSE.2022.3178469,TSE 2023,On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain,"Transformers are the current state-of-the-art of natural language processing in many domains and are using traction within software engineering research as well. Such models are pre-trained on large amounts of data, usually from the general domain. However, we only have a limited understanding regarding the validity of transformers within the software engineering domain, i.e., how good such models are at understanding words and sentences within a software engineering context and how this improves the state-of-the-art. Within this article, we shed light on this complex, but crucial issue. We compare BERT transformer models trained with software engineering data with transformers based on general domain data in multiple dimensions: their vocabulary, their ability to understand which words are missing, and their performance in classification tasks. Our results show that for tasks that require understanding of the software engineering context, pre-training with software engineering data is valuable, while general domain models are sufficient for general language understanding, also within the software engineering domain.","Natural language processing,software engineering,transformers",软件工程领域自然语言处理的预训练转换器的有效性,Transformers是目前许多领域中最先进的自然语言处理技术，也在软件工程研究中发挥着重要作用。这种模型是在大量数据上预先训练的，这些数据通常来自一般领域。然而，我们对软件工程领域中转换器的有效性只有有限的了解，即这种模型在理解软件工程上下文中的单词和句子方面有多好，以及这如何提高最先进的技术。在本文中，我们将阐明这个复杂但至关重要的问题。我们将使用软件工程数据训练的BERT转换器模型与基于通用领域数据的转换器在多个维度上进行了比较：它们的词汇表、理解缺失单词的能力以及它们在分类任务中的表现。我们的结果表明，对于需要理解软件工程上下文的任务，使用软件工程数据进行预训练是有价值的，而通用领域模型对于通用语言理解是足够的，同样在软件工程领域内也是如此。,自然语言处理，软件工程，transformers,,,
GFL7T4K2,2023,https://doi.org/10.1109/TSE.2023.3265855,TSE 2023,Identifying Concepts in Software Projects,"When working on a project, software developers must be familiar with computing concepts, standards, and technologies related to the project. We present a novel approach, called Scode, to automatically identify those concepts using the project's documentation. Scode combines entity linking and network analysis techniques specialized for the software development domain. In addition to concepts explicitly mentioned in the documentation, Scode can retrieve implicit concepts related to the project's domain. Concepts identified by Scode have a recognized meaning that is consistent across projects. We compared Scode to different baselines and found that it is more effective at mapping projects to a consistent concept space.","Concepts identification,semantic analysis,software documentation,Wikipedia mining",识别软件项目中的概念,在进行项目时，软件开发人员必须熟悉与项目相关的计算概念、标准和技术。我们提出了一种新的方法，称为Scode，使用项目文档自动识别这些概念。Scode结合了专门用于软件开发领域的实体链接和网络分析技术。除了文档中明确提到的概念外，Scode还可以检索与项目领域相关的隐含概念。Scode确定的概念具有公认的意义，在各个项目中都是一致的。我们将Scode与不同的基线进行了比较，发现它在将项目映射到一致的概念空间方面更有效。,概念识别，语义分析，软件文档，维基百科挖掘,,,
Z6M8M2DZ,2023,https://doi.org/10.1109/TSE.2022.3220236,TSE 2023,Perceptions on the Utility of Community Question and Answer Websites Like Stack Overflow to Software Developers,"Software developers make use of on crowdsourcing during development. Beyond learning from others, developers use online portals such as Stack Overflow as a vehicle for collaboration. However, little is known about developers’ experiences on such platforms, particularly around problems that are encountered online. Such insights could benefit software developers in terms of recommendations for pitfalls to avoid, ways to exploit crowdsourced knowledge, and the provision of insights to improve online code sharing communities. We interviewed 50 practitioners to fill this gap, where outcomes show that software developers’ use of online portals is targeted, and such portals are a lifeline to modern software development. Practitioners are facilitated with code solutions and debugging, often in a very timely fashion. While these experiences are largely positive, practitioners also encounter negative experiences online, some of which could be significantly deleterious to the community. We discuss the implications of these findings, such as creating awareness of the quality and reliability of code snippets, improving code searches, code validation and outdated code detection and attribution of code snippets.","Community question answering portals,crowdsourced knowledge,stack overflow",软件开发人员对Stack Overflow等社区问答网站效用的认识,软件开发人员在开发过程中利用众包。除了向他人学习，开发人员还使用Stack Overflow等在线门户网站作为协作工具。然而，人们对开发人员在此类平台上的体验知之甚少，尤其是在网上遇到的问题上。这样的见解可以让软件开发人员从避免陷阱的建议、利用众包知识的方法以及提供见解以改善在线代码共享社区等方面受益。我们采访了50名从业者来填补这一空白，结果表明，软件开发人员对在线门户网站的使用是有针对性的，而这些门户网站是现代软件开发的生命线。代码解决方案和调试通常以非常及时的方式为从业者提供便利。虽然这些经历在很大程度上是积极的，但从业者在网上也会遇到消极的经历，其中一些可能对社区有害。我们讨论了这些发现的含义，例如提高对代码片段质量和可靠性的认识，改进代码搜索、代码验证以及过时代码检测和代码片段的归属。,社区答疑门户网站，众包知识，堆栈溢出,,,
5CRAWCQX,2023,https://doi.org/10.1109/TSE.2022.3228334,TSE 2023,Instance Space Analysis of Search-Based Software Testing,"Search-based software testing (SBST) is now a mature area, with numerous techniques developed to tackle the challenging task of software testing. SBST techniques have shown promising results and have been successfully applied in the industry to automatically generate test cases for large and complex software systems. Their effectiveness, however, has been shown to be problem dependent. In this paper, we revisit the problem of objective performance evaluation of SBST techniques in light of recent methodological advances – in the form of Instance Space Analysis (ISA) – enabling the strengths and weaknesses of SBST techniques to be visualised and assessed across the broadest possible space of problem instances (software classes) from common benchmark datasets. We identify features of SBST problems that explain why a particular instance is hard for an SBST technique, reveal areas of hard and easy problems in the instance space of existing benchmark datasets, and identify the strengths and weaknesses of state-of-the-art SBST techniques. In addition, we examine the diversity and quality of common benchmark datasets used in experimental evaluations.","Automated software testing,algorithm selection,instance space analysis",基于搜索的软件测试实例空间分析,基于搜索的软件测试（SBST）现在是一个成熟的领域，开发了许多技术来解决软件测试的挑战性任务。SBST技术已显示出有希望的结果，并已成功应用于行业中，为大型复杂软件系统自动生成测试用例。然而，它们的有效性已被证明取决于问题。在本文中，我们根据最近的方法学进展，以实例空间分析（ISA）的形式，重新审视了SBST技术的客观性能评估问题，使SBST技术在通用基准数据集的问题实例（软件类）的尽可能广泛的空间中进行可视化和评估。我们确定了SBST问题的特征，这些特征解释了为什么特定实例对于SBST技术来说是困难的，揭示了现有基准数据集实例空间中的难题和难题领域，并确定了最先进的SBST技术的优势和劣势。此外，我们还检查了实验评估中使用的通用基准数据集的多样性和质量。,自动化软件测试，算法选择，实例空间分析,,,
JD9K7HES,2023,https://doi.org/10.1109/TSE.2023.3281275,TSE 2023,Multi-Granularity Detector for Vulnerability Fixes,"With the increasing reliance on Open Source Software, users are exposed to third-party library vulnerabilities. Software Composition Analysis (SCA) tools have been created to alert users of such vulnerabilities. SCA requires the identification of vulnerability-fixing commits. Prior works have proposed methods that can automatically identify such vulnerability-fixing commits. However, identifying such commits is highly challenging, as only a very small minority of commits are vulnerability fixing. Moreover, code changes can be noisy and difficult to analyze. We observe that noise can occur at different levels of detail, making it challenging to detect vulnerability fixes accurately. To address these challenges and boost the effectiveness of prior works, we propose MiDas (Multi-Granularity Detector for Vulnerability Fixes). Unique from prior works, MiDas constructs different neural networks for each level of code change granularity, corresponding to commit-level, file-level, hunk-level, and line-level, following their natural organization and then use an ensemble model combining all base models to output the final prediction. This design allows MiDas to better cope with the noisy and highly-imbalanced nature of vulnerability-fixing commit data. In addition, to reduce the human effort required to inspect code changes, we have designed an effort-aware adjustment for MiDas's outputs based on commit length. The evaluation result demonstrates that MiDas outperforms the current state-of-the-art baseline on both Java and Python-based datasets in terms of AUC by 4.9% and 13.7%, respectively. Furthermore, in terms of two effort-aware metrics, i.e., EffortCost@L and Popt@L, MiDas also performs better than the state-of-the-art baseline up to 28.2% and 15.9% on Java, 60% and 51.4% on Python, respectively.","Vulnerability-fixing commit classification,machine learning,deep learning,software security",漏洞修复的多粒度检测器,随着对开源软件的日益依赖，用户暴露在第三方库漏洞中。已经创建了软件组成分析（SCA）工具来提醒用户此类漏洞。SCA要求识别漏洞修复提交。先前的工作已经提出了可以自动识别这种漏洞修复提交的方法。然而，识别此类提交非常具有挑战性，因为只有极少数提交是漏洞修复。此外，代码更改可能是有噪声的，并且很难进行分析。我们观察到，噪声可能发生在不同的细节级别，因此准确检测漏洞修复具有挑战性。为了解决这些挑战并提高先前工作的有效性，我们提出了MiDas（漏洞修复的多粒度检测器）。与之前的工作不同，MiDas为每个级别的代码更改粒度构建了不同的神经网络，对应于提交级别、文件级别、大块级别和行级别，遵循它们的自然组织，然后使用组合所有基本模型的集成模型来输出最终预测。这种设计使MiDas能够更好地应对漏洞修复提交数据的嘈杂和高度不平衡性质。此外，为了减少检查代码更改所需的人工工作量，我们根据提交长度为MiDas的输出设计了一个工作量感知调整。评估结果表明，MiDas在基于Java和Python的数据集上的AUC分别比当前最先进的基线高4.9%和13.7%。此外就两个努力感知度量而言。，EffortCost@L和Popt@L，MiDas在Java上的性能也优于最先进的基线，分别高达28.2%和15.9%，在Python上分别高达60%和51.4%。,漏洞修复提交分类，机器学习，深度学习，软件安全,,,
3JW66I84,2023,https://doi.org/10.1109/TSE.2023.3237460,TSE 2023,Assessing TD Macro-Management: A Nested Modeling Statistical Approach,"Quality improvement can be performed at the: (a) micro-management level: interventions applied at a fine-grained level (e.g., at a class or method level, by applying a refactoring); or (b) macro-management level: interventions applied at a large-scale (e.g., at project level, by using a new framework or imposing a quality gate). By considering that the outcome of any activity can be characterized as the product of impact and scale, in this paper we aim at exploring the impact of Technical Debt (TD) Macro-Management, whose scale is by definition larger than TD Micro-Management. By considering that TD artifacts reside at the micro-level, the problem calls for a nested model solution; i.e., modeling the structure of the problem: artifacts have some inherent characteristics (e.g., size and complexity), but obey the same project management rules (e.g., quality gates, CI/CD features, etc.). In this paper, we use the Under-Bagging based Generalized Linear Mixed Models approach, to unveil project management activities that are associated with the existence of HIGH_TD artifacts, through an empirical study on 100 open-source projects. The results of the study confirm that micro-management parameters are associated with the probability of a class to be classified as HIGH_TD, but the results can be further improved by controlling some project-level parameters. Based on the findings of our nested analysis, we can advise practitioners on macro-technical debt management approaches (such as “control the number of commits per day”, “adopt quality control practices”, and “separate testing and development teams”) that can significantly reduce the probability of all software artifacts to concentrate HIGH_TD. Although some of these findings are intuitive, this is the first work that delivers empirical quantitative evidence on the relation between TD values and project- or process-level metrics.","Technical debt,metrics,measurement,quality analysis and evaluation,software maintenance",TD宏观管理评估：一种嵌套建模统计方法,质量改进可以在以下方面进行：（a）微观管理层面：在细粒度层面应用的干预措施（例如，在类或方法层面，通过应用重构）；或（b）宏观管理层面：大规模实施的干预措施（例如，在项目层面，通过使用新框架或实施质量门）。考虑到任何活动的结果都可以被描述为影响和规模的产物，本文旨在探讨技术债务宏观管理的影响，其规模从定义上来说大于技术债务微观管理。考虑到TD工件存在于微观层面，该问题需要嵌套模型解决方案；即，对问题的结构进行建模：工件具有一些固有特征（如大小和复杂性），但遵循相同的项目管理规则（如质量门、CI/CD特征等）。在本文中，我们使用基于Under-Bagging的广义线性混合模型方法，来揭示与HIGH_TD工件的存在相关的项目管理活动，通过对100个开源项目的实证研究。研究结果证实，微观管理参数与类别被归类为HIGH_TD的概率有关，但可以通过控制一些项目级参数来进一步改进结果。基于我们嵌套分析的结果，我们可以就宏观技术债务管理方法（如“控制每天的提交数量”、“采用质量控制实践”和“单独的测试和开发团队”）向从业者提供建议，这些方法可以显著降低所有软件工件集中HIGH_TD的概率。尽管其中一些发现是直观的，但这是第一项提供TD值与项目或流程级别指标之间关系的实证定量证据的工作。,技术债务，度量，测量，质量分析和评估，软件维护,,,
7Z6BIDQK,2023,https://doi.org/10.1109/TSE.2022.3207149,TSE 2023,Open Science in Software Engineering: A Study on Deep Learning-Based Vulnerability Detection,"Open science is a practice that makes scientific research publicly accessible to anyone, hence is highly beneficial. Given the benefits, the software engineering (SE) community has been diligently advocating open science policies during peer reviews and publication processes. However, to this date, there has been few studies that look into the status and issues of open science in SE from a systematic perspective. In this paper, we set out to start filling this gap. Given the great breadth of SE in general, we constrained our scope to a particular topic area in SE as an example case. Recently, an increasing number of deep learning (DL) approaches have been explored in SE, including DL-based software vulnerability detection, a popular, fast-growing topic that addresses an important problem in software security. We exhaustively searched the literature in this area and identified 55 relevant works that propose a DL-based vulnerability detection approach. This was then followed by comprehensively investigating the four integral aspects of open science: availability, executability, reproducibility, and replicability. Among other findings, our study revealed that only a small percentage (25.5%) of the studied approaches provided publicly available tools. Some of these available tools did not provide sufficient documentation and complete implementation, making them not executable or not reproducible. The uses of balanced or artificially generated datasets caused significantly overrated performance of the respective techniques, making most of them not replicable. Based on our empirical results, we made actionable suggestions on improving the state of open science in each of the four aspects. We note that our results and recommendations on most of these aspects (availability, executability, reproducibility) are not tied to the nature of the chosen topic (DL-based vulnerability detection) hence are likely applicable to other SE topic areas. We also believe our results and recommendations on replicability to be applicable to other DL-based topics in SE as they are not tied to (the particular application of DL in) detecting software vulnerabilities.","Open science,availability,executability,reproducibility,replicability,case study,vulnerability detection,deep learning",软件工程中的开放科学：基于深度学习的漏洞检测研究,开放科学是一种让任何人都能公开进行科学研究的实践，因此非常有益。考虑到这些好处，软件工程（SE）社区在同行评审和出版过程中一直在努力倡导开放科学政策。然而，到目前为止，很少有研究从系统的角度来研究SE中开放科学的现状和问题。在这篇论文中，我们着手填补这一空白。考虑到SE的广泛性，作为一个示例，我们将我们的范围限制在SE中的特定主题领域。最近，在SE中探索了越来越多的深度学习（DL）方法，包括基于DL的软件漏洞检测，这是一个流行的、快速发展的主题，解决了软件安全中的一个重要问题。我们详尽地搜索了该领域的文献，并确定了55篇提出基于DL的漏洞检测方法的相关工作。随后，全面研究了开放科学的四个组成部分：可用性、可执行性、可再现性和可复制性。在其他发现中，我们的研究表明，只有一小部分（25.5%）的研究方法提供了公开可用的工具。其中一些可用的工具没有提供足够的文档和完整的实现，使得它们不可执行或不可复制。使用平衡的或人工生成的数据集导致相应技术的性能被严重高估，使大多数技术无法复制。基于我们的实证结果，我们从四个方面提出了改善开放科学状况的可行建议。我们注意到，我们在大多数方面（可用性、可执行性、再现性）的结果和建议与所选主题的性质（基于DL的漏洞检测）无关，因此可能适用于其他SE主题领域。我们还认为，我们关于可复制性的结果和建议适用于SE中其他基于DL的主题，因为它们与检测软件漏洞（DL在中的特定应用）无关。,开放科学，可用性，可执行性，再现性，可复制性，案例研究，漏洞检测，深度学习,,,
IJJHB7T2,2023,https://doi.org/10.1109/TSE.2023.3304851,TSE 2023,Runtime Evolution of Bitcoin's Consensus Rules,"The runtime evolution of a system concerns the ability to make changes during runtime without disrupting the service. Blockchain systems need to provide continuous service and integrity. Similar challenges have been observed in centrally controlled distributed systems or mobile applications that handle runtime evolution, mainly by supporting compatible changes or running different versions concurrently. However, these solutions are not applicable in the case of blockchains, and thus, new solutions are required. This study investigates Bitcoin consensus evolution by analysing over a decade of data from Bitcoin's development channels using Strauss’ grounded theory approach and root cause analysis. The results show nine deployment features which form nine deployment techniques and ten lessons learned. Our results illustrate how different deployment techniques fit different contexts and pose different levels of consensus failure risks. Furthermore, we provide guidelines for risk minimisation during consensus rule deployment for blockchain in general and Bitcoin in particular.","Bitcoin,blockchain,consensus,grounded theory,root cause analysis,runtime evolution",比特币共识规则的运行时演化,系统的运行时演化涉及在运行时进行更改而不中断服务的能力。区块链系统需要提供持续的服务和完整性。在集中控制的分布式系统或处理运行时演变的移动应用程序中也观察到了类似的挑战，主要是通过支持兼容的更改或同时运行不同的版本。然而，这些解决方案不适用于区块链，因此需要新的解决方案。本研究使用Strauss的扎根理论方法和根本原因分析，通过分析比特币发展渠道十多年的数据，调查了比特币共识的演变。结果显示了九个部署特征，形成了九种部署技术和十个经验教训。我们的研究结果说明了不同的部署技术如何适应不同的环境，并带来不同程度的共识失败风险。此外，我们为区块链，特别是比特币的共识规则部署过程中的风险最小化提供了指导方针。,比特币，区块链，共识，扎根理论，根本原因分析，运行时进化,,,
ERZIK3PV,2023,https://doi.org/10.1109/TSE.2023.3277564,TSE 2023,Syntactic Versus Semantic Similarity of Artificial and Real Faults in Mutation Testing Studies,"Fault seeding is typically used in empirical studies to evaluate and compare test techniques. Central to these techniques lies the hypothesis that artificially seeded faults involve some form of realistic properties and thus provide realistic experimental results. In an attempt to strengthen realism, a recent line of research uses machine learning techniques, such as deep learning and Natural Language Processing, to seed faults that look like (syntactically) real ones, implying that fault realism is related to syntactic similarity. This raises the question of whether seeding syntactically similar faults indeed results in semantically similar faults and, more generally whether syntactically dissimilar faults are far away (semantically) from the real ones. We answer this question by employing 4 state-of-the-art fault-seeding techniques (PiTest - a popular mutation testing tool, IBIR - a tool with manually crafted fault patterns, DeepMutation - a learning-based fault seeded framework and $\mu$BERT - a mutation testing tool based on the pre-trained language model CodeBERT) that operate in a fundamentally different way, and demonstrate that syntactic similarity does not reflect semantic similarity. We also show that 65.11%, 76.44%, 61.39% and 9.76% of the real faults of Defects4J V2 are semantically resembled by PiTest, IBIR, $\mu$BERT and DeepMutation faults, respectively.","Fault injection,fault seeding,machine learning,mutation testing,semantic model,syntactic distance",突变测试研究中人工失误与真实失误的句法相似性与语义相似性,故障种子通常用于实证研究，以评估和比较测试技术。这些技术的核心是假设人工播种的断层涉及某种形式的真实特性，从而提供真实的实验结果。为了加强真实性，最近的一项研究使用机器学习技术，如深度学习和自然语言处理，来播种（在句法上）看起来像真实的错误，这意味着错误真实性与句法相似性有关。这就提出了一个问题，即播种语法相似的错误是否确实会导致语义相似的错误，更普遍地说，语法不相似的错误与真实的错误是否相距遥远（语义上）。我们通过使用4种最先进的故障种子技术来回答这个问题（PiTest-一种流行的突变测试工具，IBIR-一种具有手动制作的故障模式的工具，DeepMutuation-一种基于学习的故障种子框架，$\mu$BERT-一种以预训练的语言模型CodeBERT为基础的突变检测工具），它们以根本不同的方式操作，并证明句法相似性并不反映语义相似性。我们还发现，Defects4J V2的真实错误中，分别有65.11%、76.44%、61.39%和9.76%在语义上与PiTest、IBIR、$\mu$BERT和DeepMutuation错误相似。,故障注入，故障播种，机器学习，变异测试，语义模型，句法距离,,,
CVS7BGTD,2023,https://doi.org/10.1109/TSE.2023.3250479,TSE 2023,Empirical Validation of Automated Vulnerability Curation and Characterization,"Prior research has shown that public vulnerability systems such as US National Vulnerability Database (NVD) rely on a manual, time-consuming, and error-prone process which has led to inconsistencies and delays in releasing final vulnerability results. This work provides an approach to curate vulnerability reports in real-time and map textual vulnerability reports to machine readable structured vulnerability attribute data. Designed to support the time consuming human analysis done by vulnerability databases, the system leverages the Common Vulnerabilities and Exposures (CVE) list of vulnerabilities and the vulnerability attributes described by the National Institute of Standards and Technology (NIST) Vulnerability Description Ontology (VDO) framework. Our work uses Natural Language Processing (NLP), Machine Learning (ML) and novel Information Theoretical (IT) methods to provide automated techniques for near real-time publishing, and characterization of vulnerabilities using 28 attributes in 5 domains. Experiment results indicate that vulnerabilities can be evaluated up to 95 hours earlier than using manual methods, they can be characterized with F-Measure values over 0.9, and the proposed automated approach could save up to 47% of the time spent for CVE characterization.","CVE,NIST vulnerability description ontology,software vulnerability,vulnerability characterization",漏洞自动控制与表征的实证验证,先前的研究表明，美国国家漏洞数据库（NVD）等公共漏洞系统依赖于手动、耗时且容易出错的过程，这导致了最终漏洞结果的发布不一致和延迟。这项工作提供了一种实时策划漏洞报告并将文本漏洞报告映射到机器可读的结构化漏洞属性数据的方法。该系统旨在支持漏洞数据库进行耗时的人工分析，利用了美国国家标准与技术研究所（NIST）漏洞描述本体论（VDO）框架描述的常见漏洞和暴露（CVE）漏洞列表和漏洞属性。我们的工作使用自然语言处理（NLP）、机器学习（ML）和新的信息理论（IT）方法，为近实时发布提供自动化技术，并使用5个领域的28个属性对漏洞进行表征。实验结果表明，与使用手动方法相比，可以提前95小时对漏洞进行评估，可以用超过0.9的F-Measure值对其进行表征，并且所提出的自动化方法可以节省高达47%的CVE表征时间。,CVE，NIST漏洞描述本体，软件漏洞，漏洞表征,,,
UGRQTIF3,2023,https://doi.org/10.1109/TSE.2022.3177228,TSE 2023,SCS-Gan: Learning Functionality-Agnostic Stylometric Representations for Source Code Authorship Verification,"In recent years, the number of anonymous script-based fileless malware attacks and software copyright disputes has increased rapidly. In the literature, automated Code Authorship Analysis (CAA) techniques have been proposed to reduce the manual effort in identifying those attacks and issues. Most CAA techniques aim to solve the task of Authorship Attribution (AA), i.e., identifying the actual author of a source code fragment from a given set of candidate authors. However, in many real-world scenarios, investigators do not have a predefined set of authors containing the actual author at the time of investigation, i.e., contradicting AA's assumption. Additionally, existing AA techniques ignore the influence of code functionality when identifying the authorship, which leads to biased matching simply based on code functionality. Different from AA, the task of (extreme) Authorship Verification (AV) is to decide if two texts were written by the same person or not. AV techniques do not need a predefined author set and thus could be applied in more code authorship-related applications than AA. To our knowledge, there is no previous work attempting to solve the AV problem for the source code. To fill the gap, we propose a novel adversarial neural network, namely SCS-Gan, that can learn a stylometric representation of code for automated AV. With the multi-head attention mechanism, SCS-Gan focuses on the code parts that are most informative regarding personal styles and generates functionality-agnostic stylometric representations through adversarial training. We benchmark SCS-Gan and two state-of-the-art code representation models on four out-of-sample datasets collected from a real-world programming competition. Our experiment results show that SCS-Gan outperforms the baselines on all four out-of-sample datasets.","Cyber threat intelligence,representation learning,adversarial learning,authorship analysis,code authorship verification",SCS Gan：用于源代码作者身份验证的学习功能不可知风格表示,近年来，基于匿名脚本的无文件恶意软件攻击和软件版权纠纷的数量迅速增加。在文献中，已经提出了自动代码作者分析（CAA）技术，以减少识别这些攻击和问题的手动工作量。大多数CAA技术旨在解决作者归因（AA）任务，即从给定的候选作者集中识别源代码片段的实际作者。然而，在许多现实世界的场景中，调查人员在调查时没有包含实际作者的预定义作者集，即与AA的假设相矛盾。此外，现有的AA技术在识别作者时忽略了代码功能的影响，这导致了仅仅基于代码功能的有偏见的匹配。与AA不同，（极端）作者身份验证（AV）的任务是决定两篇文章是否由同一个人撰写。AV技术不需要预先定义的作者集，因此可以应用于比AA更多的与代码作者相关的应用程序。据我们所知，以前没有任何工作试图解决源代码的AV问题。为了填补这一空白，我们提出了一种新的对抗性神经网络，即SCS Gan，它可以学习用于自动AV的代码的风格表示。利用多头注意力机制，SCS Gan专注于关于个人风格信息最多的代码部分，并通过对抗性训练生成功能不可知的风格表示。我们在从真实世界的编程竞赛中收集的四个样本数据集上对SCS Gan和两个最先进的代码表示模型进行了基准测试。我们的实验结果表明，SCS Gan在所有四个样本外数据集上都优于基线。,网络威胁情报，表征学习，对抗性学习，作者分析，代码作者验证,,,
WJCP3TY2,2023,https://doi.org/10.1109/TSE.2022.3154672,TSE 2023,An Empirical Study on Log Level Prediction for Multi-Component Systems,"Logging statements are used to trace the execution of a software system. Practitioners leverage different logging information (e.g., the content of a log message) to decide for each logging statement an appropriate log level, which is leveraged to adjust the verbosity of logs so that only important log messages are traced. Deciding for the log level can be done differently from one to another component of a multi-component system, such as OpenStack and its 28 components. For example, a component might aim for increasing the verbosity of its log messages, while another component for the same multi-component system might aim at decreasing such a verbosity. Such different logging strategies can exist since each component can be developed and maintained by a different team. While a prior work leveraged an ordinal regression model to recommend the appropriate log level for a new logging statement, their evaluation did not consider the particularities that each component can have within a multi-component system. For instance, their model might not perform well at each component level of a multi-component system. The same model’s interpretability can mislead the developers of each component that has its unique logging strategy. In this paper, we quantify the impact of the particularities of each component of a multi-component system on the performance and interpretability of the log level prediction model of prior work. We observe that the performance of the log level prediction models that are trained at the whole project level (aka., global models) have lower performances (AUC) on 72% to 100% of the components of our five evaluated multi-component systems, compared to the same models when evaluated on the whole multi-component system. We observe that the models that are trained at the component level (aka., local models) statistically outperform the global model on 33% to 77% of the components of our evaluated multi-component systems. Furthermore, we observe that the rankings of the most important features that are obtained from the global models are statistically different from the feature importance rankings of 50% to 87% of the local models of our evaluated multi-component systems. Finally, we observe that 60% and 35% of the Spring and OpenStack components do not have enough data points to train their own local models (aka., data lacking components). Leveraging a peer-local model for such type of components is more promising than using the global model.","Log level prediction,machine learning,multi-component systems,software logging",多分量系统对数水平预测的实证研究,日志记录语句用于跟踪软件系统的执行情况。从业者利用不同的日志记录信息（例如，日志消息的内容）为每个日志记录语句决定适当的日志级别，该级别用于调整日志的详细程度，以便只跟踪重要的日志消息。多组件系统的不同组件（如OpenStack及其28个组件）可以以不同的方式决定日志级别。例如，一个组件可能旨在增加其日志消息的详细程度，而同一个多组件系统的另一个组件则可能旨在减少这种详细程度。由于每个组件都可以由不同的团队开发和维护，因此可以存在这种不同的日志记录策略。虽然先前的工作利用有序回归模型为新的日志记录语句推荐适当的日志级别，但他们的评估没有考虑多组件系统中每个组件可能具有的特殊性。例如，他们的模型可能在多组件系统的每个组件级别上都表现不佳。同一个模型的可解释性可能会误导每个具有独特日志策略的组件的开发人员。在本文中，我们量化了多组件系统中每个组件的特殊性对先前工作的对数级预测模型的性能和可解释性的影响。我们观察到，在整个项目级别上训练的对数级预测模型（即全局模型）的性能在我们五个评估的多组分系统的72%至100%的组件上的性能（AUC）低于在整个多组分体系上评估的相同模型。我们观察到，在组件级别训练的模型（也称为局部模型）在我们评估的多组件系统的33%至77%的组件上统计上优于全局模型。此外，我们观察到，从全局模型中获得的最重要特征的排名在统计上与我们评估的多分量系统的局部模型的50%至87%的特征重要性排名不同。最后，我们观察到60%和35%的Spring和OpenStack组件没有足够的数据点来训练他们自己的本地模型（也就是缺乏数据的组件）。对于此类组件，利用对等本地模型比使用全局模型更有前景。,日志级别预测，机器学习，多组件系统，软件日志记录,,,
WK52RWSG,2023,https://doi.org/10.1109/TSE.2022.3171469,TSE 2023,Pride: Prioritizing Documentation Effort Based on a PageRank-Like Algorithm and Simple Filtering Rules,"Code documentation can be helpful in many software quality assurance tasks. However, due to resource constraints (e.g., time, human resources, and budget), programmers often cannot document their work completely and timely. In the literature, two approaches (one is supervised and the other is unsupervised) have been proposed to prioritize documentation effort to ensure the most important classes to be documented first. However, both of them contain several limitations. The supervised approach overly relies on a difficult-to-obtain labeled data set and has high computation cost. The unsupervised one depends on a graph representation of the software structure, which is inaccurate since it neglects many important couplings between classes. In this paper, we propose an improved approach, named Pride, to prioritize documentation effort. First, Pride uses a weighted directed class coupling network to precisely describe classes and their couplings. Second, we propose a PageRank-like algorithm to quantify the importance of classes in the whole class coupling network. Third, we use a set of software metrics to quantify source code complexity and further propose a simple but easy-to-operate filtering rule. Fourth, we sort all the classes according to their importance in descending order and use the filtering rule to filter out unimportant classes. Finally, a threshold $k$ is utilized, and the top-$k$% ranked classes are the identified important classes to be documented first. Empirical results on a set of nine software systems show that, according to the average ranking of the Friedman test, Pride is superior to the existing approaches in the whole data set.","Code documentation,program comprehension,software maintenance,PageRank,software metrics",Pride：基于类似PageRank算法和简单过滤规则的文档工作优先级,代码文档在许多软件质量保证任务中都很有用。然而，由于资源限制（例如，时间、人力资源和预算），程序员通常无法完整、及时地记录他们的工作。在文献中，提出了两种方法（一种是有监督的，另一种是无监督的）来优先考虑文档工作，以确保首先记录最重要的类。然而，两者都有一些局限性。监督方法过度依赖于难以获得的标记数据集，并且具有高计算成本。无监督的依赖于软件结构的图表示，这是不准确的，因为它忽略了类之间的许多重要耦合。在本文中，我们提出了一种改进的方法，称为Pride，以优先考虑文档工作。首先，Pride使用加权有向类耦合网络来精确描述类及其耦合。其次，我们提出了一种类似PageRank的算法来量化类在整个类耦合网络中的重要性。第三，我们使用一组软件度量来量化源代码的复杂性，并进一步提出了一个简单但易于操作的过滤规则。第四，我们按照重要性降序对所有类进行排序，并使用过滤规则过滤掉不重要的类。最后，使用阈值$k$，排名前-$k$%的类是要首先记录的已确定的重要类。对一组九个软件系统的实证结果表明，根据弗里德曼测试的平均排名，Pride在整个数据集中优于现有方法。,代码文档，程序理解，软件维护，PageRank，软件度量,,,
AHQ8BNIG,2023,https://doi.org/10.1109/TSE.2022.3214796,TSE 2023,PopArt: Ranked Testing Efficiency,"Too often, programmers are under pressure to maximize their confidence in the correctness of their code with a tight testing budget. Should they spend some of that budget on finding “interesting” inputs or spend their entire testing budget on test executions? Work on testing efficiency has explored two competing approaches to answer this question: systematic partition testing (ST), which defines a testing partition and tests its parts, and random testing (RT), which directly samples inputs with replacement. A consensus as to which is better when has yet to emerge. We present Probability Ordered Partition Testing (PopArt), a new systematic partition-based testing strategy that visits the parts of a testing partition in decreasing probability order and in doing so leverages any non-uniformity over that partition. We show how to construct a homogeneous testing partition, a requirement for systematic testing, by using an executable oracle and the path partition. A program's path partition is a naturally occurring testing partition that is usually skewed for the simple reason that some paths execute more frequently than others. To confirm this conventional wisdom, we instrument programs from the Codeflaws repository and find that 80% of them have a skewed path probability distribution. PopArt visits the parts of a testing partition in decreasing probability order. We then compare PopArt with RT to characterise the configuration space in which each is more efficient. We show that, when simulating Codeflaws, PopArt outperforms RT after $100{,}000$ executions. Our results reaffirm RT's power for very small testing budgets but also show that for any application requiring high (above 90%) probability-weighted coverage PopArt should be preferred. In such cases, despite paying more for each test execution, we prove that PopArt outperforms RT: it traverses parts whose cumulative probability bounds that of random testing, showing that sampling without replacement pays for itself, given a nonuniform probability over a testing partition.","Software testing,randomness,systematic,efficiency,probability distribution",PopArt：测试效率排名,程序员经常面临压力，在测试预算紧张的情况下，要最大限度地提高他们对代码正确性的信心。他们应该把部分预算花在寻找“有趣”的输入上，还是把全部测试预算花在测试执行上？关于测试效率的工作探索了两种相互竞争的方法来回答这个问题：系统分区测试（ST）和随机测试（RT），前者定义了测试分区并测试其部分，后者通过替换直接对输入进行采样。对于哪一个更好，还没有达成共识。我们提出了概率有序分区测试（PopArt），这是一种新的基于分区的系统测试策略，它以递减的概率顺序访问测试分区的各个部分，并在这样做的过程中利用该分区上的任何不均匀性。我们展示了如何通过使用可执行预言机和路径分区来构建同构测试分区，这是系统测试的要求。程序的路径分区是一个自然出现的测试分区，它通常是倾斜的，原因很简单，即某些路径的执行频率高于其他路径。为了证实这一传统观点，我们对Codedefects存储库中的程序进行了测试，发现其中80%的程序具有偏斜的路径概率分布。PopArt按递减概率顺序访问测试分区的各个部分。然后，我们将PopArt与RT进行比较，以表征配置空间，其中每种配置空间都更有效。我们表明，在模拟Codedefect时，PopArt在执行$100{，}000$后的性能优于RT。我们的结果重申了RT在非常小的测试预算方面的能力，但也表明，对于任何需要高（超过90%）概率加权覆盖率的应用程序，PopArt应该是首选。在这种情况下，尽管每次测试执行都要付出更多的代价，但我们证明了PopArt的性能优于RT：它遍历了累积概率与随机测试的累积概率相界的部分，表明在给定测试分区上的不均匀概率的情况下，不进行替换的采样是有代价的。,软件测试，随机性，系统性，效率，概率分布,,,
Z4IKSU3A,2023,https://doi.org/10.1109/TSE.2023.3241639,TSE 2023,Static Analysis of JNI Programs via Binary Decompilation,"JNI programs are widely used thanks to the combined benefits of C and Java programs. However, because understanding the interaction behaviors between two different programming languages is challenging, JNI program development is difficult to get right and vulnerable to security attacks. Thus, researchers have proposed static analysis of JNI program source code to detect bugs and security vulnerabilities in JNI programs. Unfortunately, such source code analysis is not applicable to compiled JNI programs that are not open-sourced or open-source JNI programs containing third-party binary libraries. While JN-SAF, the state-of-the-art analyzer for compiled JNI programs, can analyze binary code, it has several limitations due to its symbolic execution and summary-based bottom-up analysis. In this paper, we propose a novel approach to statically analyze compiled JNI programs without their source code using binary decompilation. Unlike JN-SAF that analyzes binaries directly, our approach decompiles binaries and analyzes JNI programs with the decompiled binaries using an existing JNI program analyzer for source code. To decompile binaries to compilable C source code with precise JNI-interoperation-related types, we improve an existing decompilation tool by leveraging the characteristics of JNI programs. Our evaluation shows that the approach is precise as almost the same as the state-of-the-art JNI program analyzer for source code, and more precise than JN-SAF.","Java native interface,binary decompilation,static analysis",JNI程序的二进制反编译静态分析,由于C和Java程序的综合优势，JNI程序被广泛使用。然而，由于理解两种不同编程语言之间的交互行为具有挑战性，JNI程序开发很难正确进行，并且容易受到安全攻击。因此，研究人员提出了对JNI程序源代码进行静态分析，以检测JNI程序中的漏洞和安全漏洞。不幸的是，这种源代码分析不适用于非开源的已编译JNI程序或包含第三方二进制库的开源JNI程序。JN-SAF是用于编译JNI程序的最先进的分析器，它可以分析二进制代码，但由于其符号执行和基于摘要的自下而上的分析，它有一些局限性。在本文中，我们提出了一种新的方法来静态分析已编译的JNI程序，而不使用二进制反编译的源代码。与直接分析二进制文件的JN-SAF不同，我们的方法对二进制文件进行反编译，并使用现有的源代码JNI程序分析器用反编译的二进制文件分析JNI程序。为了将二进制文件反编译为具有精确的JNI互操作相关类型的可编译C源代码，我们利用JNI程序的特性改进了现有的反编译工具。我们的评估表明，该方法与最先进的源代码JNI程序分析器几乎相同，并且比JN-SAF更精确。,Java原生接口，二进制反编译，静态分析,,,
SM5B52TL,2023,https://doi.org/10.1109/TSE.2022.3231621,TSE 2023,Learning Approximate Execution Semantics From Traces for Binary Function Similarity,"Detecting semantically similar binary functions – a crucial capability with broad security usages including vulnerability detection, malware analysis, and forensics – requires understanding function behaviors and intentions. This task is challenging as semantically similar functions can be compiled to run on different architectures and with diverse compiler optimizations or obfuscations. Most existing approaches match functions based on syntactic features without understanding the functions’ execution semantics. We present Trex, a transfer-learning-based framework, to automate learning approximate execution semantics explicitly from functions’ traces collected via forced-execution (i.e., by violating the control flow semantics) and transfer the learned knowledge to match semantically similar functions. While it is known that forced-execution traces are too imprecise to be directly used to detect semantic similarity, our key insight is that these traces can instead be used to teach an ML model approximate execution semantics of diverse instructions and their compositions. We thus design a pretraining task, which trains the model to learn approximate execution semantics from the two modalities (i.e., forced-executed code and traces) of the function. We then finetune the pretrained model to match semantically similar functions. We evaluate Trex on 1,472,066 functions from 13 popular software projects, compiled to run on 4 architectures (x86, x64, ARM, and MIPS), and with 4 optimizations (O0-O3) and 5 obfuscations. Trex outperforms the state-of-the-art solutions by 7.8%, 7.2%, and 14.3% in cross-architecture, optimization, and obfuscation function matching, respectively, while running 8× faster. Ablation studies suggest that the pretraining significantly boosts the function matching performance, underscoring the importance of learning execution semantics. Our case studies demonstrate the practical use-cases of Trex – on 180 real-world firmware images, Trex uncovers 14 vulnerabilities not disclosed by previous studies. We release the code and dataset of Trex at https://github.com/CUMLSec/trex.","Binary analysis,large language models,software security",从二元函数相似性的迹学习近似执行语义,检测语义相似的二进制函数——这是一项具有广泛安全用途的关键功能，包括漏洞检测、恶意软件分析和取证——需要了解函数的行为和意图。这项任务具有挑战性，因为语义相似的函数可以编译为在不同的体系结构上运行，并具有不同的编译器优化或模糊处理。大多数现有的方法基于语法特征匹配函数，而不了解函数的执行语义。我们提出了Trex，一个基于迁移学习的框架，用于从通过强制执行（即，通过违反控制流语义）收集的函数跟踪中显式地自动学习近似执行语义，并将学习到的知识转移到语义相似的函数中。虽然众所周知，强制执行跟踪过于不精确，无法直接用于检测语义相似性，但我们的关键见解是，这些跟踪可以用于教授ML模型不同指令及其组成的近似执行语义。因此，我们设计了一个预训练任务，该任务训练模型从函数的两种模式（即强制执行的代码和跟踪）中学习近似的执行语义。然后，我们对预训练的模型进行微调，以匹配语义相似的函数。我们在13个流行软件项目的1472066个函数上评估了Trex，这些函数被编译为在4种架构（x86、x64、ARM和MIPS）上运行，并进行了4次优化（O0-O3）和5次模糊处理。Trex在跨架构、优化和模糊功能匹配方面分别比最先进的解决方案高7.8%、7.2%和14.3%，同时运行速度快8倍。消融研究表明，预训练显著提高了函数匹配性能，强调了学习执行语义的重要性。我们的案例研究展示了Trex的实际使用案例——在180个真实世界的固件映像上，Trex发现了14个先前研究未披露的漏洞。我们在发布Trex的代码和数据集https://github.com/CUMLSec/trex.,二进制分析，大型语言模型，软件安全,,,
VQTV3XBN,2023,https://doi.org/10.1109/TSE.2022.3220713,TSE 2023,FairMask: Better Fairness via Model-Based Rebalancing of Protected Attributes,"Context: Machine learning software can generate models that inappropriately discriminate against specific protected social groups (e.g., groups based on gender, ethnicity, etc.). Motivated by those results, software engineering researchers have proposed many methods for mitigating those discriminatory effects. While those methods are effective in mitigating bias, few of them can provide explanations on what is the root cause of bias. Objective: We aim to better detect and mitigate algorithmic discrimination in machine learning software problems. Method: Here we propose ${{\sf FairMask}}$, a model-based extrapolation method that is capable of both mitigating bias and explaining the cause. In our ${{\sf FairMask}}$ approach, protected attributes are represented by models learned from the other independent variables (and these models offer extrapolations over the space between existing examples). We then use the extrapolation models to relabel protected attributes later seen in testing data or deployment time. Our approach aims to offset the biased predictions of the classification model by rebalancing the distribution of protected attributes. Results: The experiments of this paper show that, without compromising (original) model performance, ${{\sf FairMask}}$ can achieve significantly better group and individual fairness (as measured in different metrics) than benchmark methods. Moreover, compared to another instance-based rebalancing method, our model-based approach shows faster runtime and thus better scalability. Conclusion: Algorithmic decision bias can be removed via extrapolation that corrects the misleading latent correlation between the protected attributes and other non-protected ones. As evidence for this, our proposed ${{\sf FairMask}}$ is not only performance-wise better (measured by fairness and performance metrics) than two state-of-the-art fairness algorithms. Reproduction Package: In order to better support open science, all scripts and data used in this study are available online at https://github.com/anonymous12138/biasmitigation.","Software fairness,explanation,bias mitigation",FairMask：通过基于模型的受保护属性重新平衡获得更好的公平性,背景：机器学习软件可以生成不适当地歧视特定受保护社会群体（例如，基于性别、种族等的群体）的模型。受这些结果的启发，软件工程研究人员提出了许多减轻这些歧视影响的方法。虽然这些方法在减轻偏见方面是有效的，但很少有方法能解释偏见的根本原因。目的：我们旨在更好地检测和减轻机器学习软件问题中的算法歧视。方法：在这里，我们提出了$｛\sf FairMask｝｝$，这是一种基于模型的外推方法，能够减轻偏差并解释原因。在我们的$｛\sf FairMask｝｝$方法中，受保护的属性由从其他自变量学习的模型表示（这些模型在现有示例之间的空间上提供外推）。然后，我们使用外推模型重新标记稍后在测试数据或部署时间中看到的受保护属性。我们的方法旨在通过重新平衡受保护属性的分布来抵消分类模型的偏差预测。结果：本文的实验表明，在不影响（原始）模型性能的情况下，$｛\sf FairMask｝｝$可以实现比基准方法更好的群体和个人公平性（以不同的度量衡量）。此外，与另一种基于实例的再平衡方法相比，我们基于模型的方法显示出更快的运行时间和更好的可扩展性。结论：算法决策偏差可以通过外推法消除，该外推法纠正了受保护属性和其他非受保护属性之间误导性的潜在相关性。作为这一点的证据，我们提出的$｛\sf FairMask｝｝$不仅在性能方面（通过公平性和性能指标衡量）比两种最先进的公平算法更好。复制包：为了更好地支持开放科学，本研究中使用的所有脚本和数据均可在线访问https://github.com/anonymous12138/biasmitigation.,软件公平，解释，偏差缓解,,,
4PIEJZC6,2023,https://doi.org/10.1109/TSE.2022.3197063,TSE 2023,"Revisiting, Benchmarking and Exploring API Recommendation: How Far Are We?","Application Programming Interfaces (APIs), which encapsulate the implementation of specific functions as interfaces, greatly improve the efficiency of modern software development. As the number of APIs grows up fast nowadays, developers can hardly be familiar with all the APIs and usually need to search for appropriate APIs for usage. So lots of efforts have been devoted to improving the API recommendation task. However, it has been increasingly difficult to gauge the performance of new models due to the lack of a uniform definition of the task and a standardized benchmark. For example, some studies regard the task as a code completion problem, while others recommend relative APIs given natural language queries. To reduce the challenges and better facilitate future research, in this paper, we revisit the API recommendation task and aim at benchmarking the approaches. Specifically, the paper groups the approaches into two categories according to the task definition, i.e., query-based API recommendation and code-based API recommendation. We study 11 recently-proposed approaches along with 4 widely-used IDEs. One benchmark named APIBench is then built for the two respective categories of approaches. Based on APIBench, we distill some actionable insights and challenges for API recommendation. We also achieve some implications and directions for improving the performance of recommending APIs, including appropriate query reformulation, data source selection, low resource setting, user-defined APIs, and query-based API recommendation with usage patterns.","API recommendation,benchmark,empirical study",修订、基准和探索API建议：我们还有多远？,应用程序编程接口（API）将特定功能的实现封装为接口，极大地提高了现代软件开发的效率。随着API数量的快速增长，开发人员很难熟悉所有的API，通常需要搜索合适的API来使用。为了改进API的推荐任务，我们付出了大量的努力。然而，由于缺乏对任务的统一定义和标准化基准，衡量新模型的性能变得越来越困难。例如，一些研究将任务视为代码完成问题，而另一些研究则建议在给定自然语言查询的情况下使用相关API。为了减少挑战并更好地促进未来的研究，在本文中，我们重新审视了API的建议任务，并旨在对这些方法进行基准测试。具体来说，本文根据任务定义将方法分为两类，即基于查询的API推荐和基于代码的API推荐。我们研究了最近提出的11种方法以及4种广泛使用的IDE。然后为这两类方法构建一个名为APIBench的基准。基于APIBench，我们为API推荐提取了一些可操作的见解和挑战。我们还实现了提高推荐API性能的一些启示和方向，包括适当的查询重新制定、数据源选择、低资源设置、用户定义的API以及基于查询的API推荐和使用模式。,API推荐，基准，实证研究,,,
FJLHWQKC,2023,https://doi.org/10.1109/TSE.2022.3147008,TSE 2023,An Experimental Assessment of Using Theoretical Defect Predictors to Guide Search-Based Software Testing,"Automated test generators, such as search-based software testing (SBST) techniques are primarily guided by coverage information. As a result, they are very effective at achieving high code coverage. However, is high code coverage alone sufficient to detect bugs effectively? In this paper, we propose a new SBST technique, predictive many objective sorting algorithm (PreMOSA), which augments coverage information with defect prediction information to decide where to increase the test coverage in the class under test (CUT). Through an experimental evaluation using 420 labelled bugs on the Defects4J benchmark and using theoretical defect predictors, we demonstrate the improved effectiveness and efficiency of PreMOSA in detecting bugs when using any acceptable defect predictor, i.e., a defect predictor with recall and precision $\geq$ 75%, compared to the state-of-the-art dynamic many objective sorting algorithm (DynaMOSA). PreMOSA detects up to 8.3% more labelled bugs on average than DynaMOSA when given a time budget of 2 minutes for test generation per CUT.","Search-Based Software Testing,Automated Test Generation,Defect Prediction",使用理论缺陷预测器指导基于搜索的软件测试的实验评估,自动化测试生成器，如基于搜索的软件测试（SBST）技术，主要由覆盖率信息指导。因此，它们在实现高代码覆盖率方面非常有效。然而，仅仅高代码覆盖率就足以有效地检测错误吗？在本文中，我们提出了一种新的SBST技术，即预测多目标排序算法（PreMOSA），该算法利用缺陷预测信息来增加覆盖率信息，以决定在测试类别（CUT）中在哪里增加测试覆盖率。通过在Defects4J基准上使用420个标记的缺陷进行实验评估，并使用理论缺陷预测因子，我们证明了当使用任何可接受的缺陷预测因子时，PreMOSA在检测缺陷方面的有效性和效率有所提高，即召回率和精度为$\geq$75%的缺陷预测函数，与最先进的动态多目标排序算法（DynaMOSA）相比。当每个CUT的测试生成时间预算为2分钟时，PreMOSA检测到的标记错误平均比DynaMOSA多8.3%。,基于搜索的软件测试，自动测试生成，缺陷预测,,,
DEQLPXXN,2023,https://doi.org/10.1109/TSE.2023.3256939,TSE 2023,New Techniques for Static Symmetry Breaking in Many-Sorted Finite Model Finding,"Symmetry in finite model finding problems of many-sorted first-order logic (MSFOL) can be exploited to reduce the number of interpretations considered during search, thereby improving solver performance for tools such as the Alloy Analyzer. We present a framework to soundly compose static symmetry breaking schemes for many-sorted finite model finding. Then, we introduce and prove the correctness of three static symmetry breaking schemes for MSFOL: 1) one for functions with distinct sorts in the domain and range; 2) one for functions where the range sort appears in the domain; and 3) one for predicates. We provide a novel presentation of sort inference in the context of symmetry breaking that yields a new mathematical link between sorts and symmetries. We empirically investigate how our symmetry breaking approaches affect solving performance.","Finite model finding,symmetry breaking",多排序有限模型求解中静态对称性破缺的新技术,可以利用多排序一阶逻辑（MSFOL）的有限模型查找问题中的对称性来减少搜索过程中考虑的解释数量，从而提高合金分析器等工具的求解器性能。我们提出了一个框架，以健全地组成静态对称打破方案，为许多排序的有限模型寻找。然后，我们引入并证明了MSFOL的三种静态对称破缺方案的正确性：1）一种方案适用于域和范围内具有不同排序的函数；2） 一个用于范围排序出现在域中的函数；和3）一个用于谓词。我们在对称性破缺的背景下提供了一种新颖的排序推理，它在排序和对称性之间产生了一种新的数学联系。我们实证研究了对称性破坏方法如何影响求解性能。,有限模型发现，对称破缺,,,
9483PCSE,2023,https://doi.org/10.1109/TSE.2022.3144348,TSE 2023,DeepLineDP: Towards a Deep Learning Approach for Line-Level Defect Prediction,"Defect prediction is proposed to assist practitioners effectively prioritize limited Software Quality Assurance (SQA) resources on the most risky files that are likely to have post-release software defects. However, there exist two main limitations in prior studies: (1) the granularity levels of defect predictions are still coarse-grained and (2) the surrounding tokens and surrounding lines have not yet been fully utilized. In this paper, we perform a survey study to better understand how practitioners perform code inspection in modern code review process, and their perception on a line-level defect prediction. According to the responses from 36 practitioners, we found that 50% of them spent at least 10 minutes to more than one hour to review a single file, while 64% of them still perceived that code inspection activity is challenging to extremely challenging. In addition, 64% of the respondents perceived that a line-level defect prediction tool would potentially be helpful in identifying defective lines. Motivated by the practitioners’ perspective, we present DeepLineDP, a deep learning approach to automatically learn the semantic properties of the surrounding tokens and lines in order to identify defective files and defective lines. Through a case study of 32 releases of 9 software projects, we find that the risk score of code tokens varies greatly depending on their location. Our DeepLineDP is 17%-37% more accurate than other file-level defect prediction approaches; is 47%-250% more cost-effective than other line-level defect prediction approaches; and achieves a reasonable performance when transferred to other software projects. These findings confirm that the surrounding tokens and surrounding lines should be considered to identify the fine-grained locations of defective files (i.e., defective lines).","Software quality assurance,line-level defect prediction,deep learning,explainable AI",DeepLineDP：一种用于线级缺陷预测的深度学习方法,缺陷预测旨在帮助从业者有效地将有限的软件质量保证（SQA）资源优先放在可能存在发布后软件缺陷的风险最大的文件上。然而，在先前的研究中存在两个主要的局限性：（1）缺陷预测的粒度级别仍然是粗粒度的；（2）周围的标记和周围的线尚未得到充分利用。在本文中，我们进行了一项调查研究，以更好地了解从业者如何在现代代码审查过程中执行代码检查，以及他们对行级缺陷预测的看法。根据36名从业人员的回复，我们发现50%的从业人员花了至少10分钟到1个多小时来审查一个文件，而64%的从业人员仍然认为代码检查活动具有挑战性，甚至极具挑战性。此外，64%的受访者认为线路级缺陷预测工具可能有助于识别缺陷线路。受从业者观点的启发，我们提出了DeepLineDP，这是一种深度学习方法，可以自动学习周围标记和行的语义属性，以识别有缺陷的文件和有缺陷的行。通过对9个软件项目的32个版本的案例研究，我们发现代码代币的风险评分因其位置的不同而有很大差异。我们的DeepLineDP比其他文件级缺陷预测方法准确17%-37%；与其他线路级缺陷预测方法相比，成本效益高47%至250%；并且在转移到其他软件项目时实现了合理的性能。这些发现证实，应该考虑周围的标记和周围的线来识别缺陷文件（即缺陷线）的细粒度位置。,软件质量保证，行级缺陷预测，深度学习，可解释的人工智能,,,
WPT4E4XT,2023,https://doi.org/10.1109/TSE.2023.3271417,TSE 2023,"Demystifying Random Number in Ethereum Smart Contract: Taxonomy, Vulnerability Identification, and Attack Detection","Recent years have witnessed explosive growth in blockchain smart contract applications. As smart contracts become increasingly popular and carry trillion dollars worth of digital assets, they become more of an appealing target for attackers, who have exploited vulnerabilities in smart contracts to cause catastrophic economic losses. Notwithstanding a proliferation of work that has been developed to detect an impressive list of vulnerabilities, the bad randomness vulnerability is overlooked by many existing tools. In this article, we make the first attempt to provide a systematic analysis of random numbers in Ethereum smart contracts, by investigating the principles behind pseudo-random number generation and organizing them into a taxonomy. We also lucubrate various attacks against bad random numbers and group them into four categories. Furthermore, we present RNVulDet – a tool that incorporates taint analysis techniques to automatically identify bad randomness vulnerabilities and detect corresponding attack transactions. To extensively verify the effectiveness of RNVulDet, we construct three new datasets: i) 34 well-known contracts that are reported to possess bad randomness vulnerabilities, ii) 214 popular contracts that have been rigorously audited before launch and are regarded as free of bad randomness vulnerabilities, and iii) a dataset consisting of 47,668 smart contracts and 49,951 suspicious transactions. We compare RNVulDet with three state-of-the-art smart contract vulnerability detectors, and our tool significantly outperforms them. Meanwhile, RNVulDet spends 2.98 s per contract on average, in most cases orders-of-magnitude faster than other tools. RNVulDet successfully reveals 44,264 attack transactions. Our implementation and datasets are released, hoping to inspire others.","Ethereum,smart contract,random number,vulnerability identification,attack detection,taint analysis",以太坊智能合约中随机数的解密：分类、漏洞识别和攻击检测,近年来，区块链智能合约应用呈爆炸式增长。随着智能合约越来越受欢迎，并携带价值万亿美元的数字资产，它们成为攻击者更具吸引力的目标，攻击者利用智能合约中的漏洞造成灾难性的经济损失。尽管已经开发了大量的工作来检测一系列令人印象深刻的漏洞，但许多现有工具忽视了糟糕的随机性漏洞。在本文中，我们首次尝试对以太坊智能合约中的随机数进行系统分析，方法是研究伪随机数生成背后的原理，并将其组织成一个分类法。我们还深入研究了针对坏随机数的各种攻击，并将其分为四类。此外，我们还介绍了RNVulDet——一种结合污点分析技术的工具，用于自动识别不良随机性漏洞并检测相应的攻击事务。为了广泛验证RNVulDet的有效性，我们构建了三个新的数据集：i）34个众所周知的合同被报告具有不良随机性漏洞，ii）214个流行的合同在发布前经过严格审计，被认为没有不良随机性弱点，以及iii）由47668个智能合约和49951个可疑交易组成的数据集。我们将RNVulDet与三个最先进的智能合约漏洞检测器进行了比较，我们的工具明显优于它们。同时，RNVulDet平均每份合同花费2.98秒，在大多数情况下比其他工具快几个数量级。RNVulDet成功揭示了44264个攻击事务。我们的实现和数据集发布了，希望能激励其他人。,以太，智能合约，随机数，漏洞识别，攻击检测，污点分析,,,
86DEKAZB,2023,https://doi.org/10.1109/TSE.2022.3212166,TSE 2023,Improved Management of Issue Dependencies in Issue Trackers of Large Collaborative Projects,"Issue trackers, such as Jira, have become the prevalent collaborative tools in software engineering for managing issues, such as requirements, development tasks, and software bugs. However, issue trackers inherently focus on the lifecycle of single issues, although issues have and express dependencies on other issues that constitute issue dependency networks in large complex collaborative projects. The objective of this study is to develop supportive solutions for the improved management of dependent issues in an issue tracker. This study follows the Design Science methodology, consisting of eliciting drawbacks and constructing and evaluating a solution and system. The study was carried out in the context of The Qt Company's Jira, which exemplifies an actively used, almost two-decade-old issue tracker with over 100,000 issues. The drawbacks capture how users operate with issue trackers to handle issue information in large, collaborative, and long-lived projects. The basis of the solution is to keep issues and dependencies as separate objects and automatically construct an issue graph. Dependency detections complement the issue graph by proposing missing dependencies, while consistency checks and diagnoses identify conflicting issue priorities and release assignments. Jira's plugin and service-based system architecture realize the functional and quality concerns of the system implementation. We show how to adopt the intelligent supporting techniques of an issue tracker in a complex use context and a large data-set. The solution considers an integrated and holistic system view, practical applicability and utility, and the practical characteristics of issue data, such as inherent incompleteness.","Bug,dependency,design science,issue,issue management,issue tracker,jira,release,requirement",改进大型协作项目问题跟踪器中问题依赖性的管理,问题跟踪器，如Jira，已成为软件工程中用于管理问题（如需求、开发任务和软件错误）的流行协作工具。然而，问题跟踪器本质上关注单个问题的生命周期，尽管问题具有并表达了对其他问题的依赖，这些问题构成了大型复杂协作项目中的问题依赖网络。本研究的目的是开发支持性解决方案，以改进问题跟踪器中相关问题的管理。本研究遵循设计科学的方法论，包括找出缺点，构建和评估解决方案和系统。这项研究是在The Qt Company的Jira的背景下进行的，该公司的Jira是一个使用了近20年、发行量超过100000期的问题跟踪器的例子。缺点是用户如何使用问题跟踪器来处理大型、协作和长期项目中的问题信息。解决方案的基础是将问题和依赖关系作为单独的对象，并自动构建问题图。相关性检测通过提出缺失的相关性来补充问题图，而一致性检查和诊断则确定了冲突的问题优先级和发布分配。Jira基于插件和服务的系统架构实现了系统实现的功能和质量问题。我们展示了如何在复杂的使用环境和大型数据集中采用问题跟踪器的智能支持技术。该解决方案考虑了集成和整体的系统视图、实际适用性和实用性，以及问题数据的实际特征，如固有的不完整性。,错误，依赖性，设计科学，问题，问题管理，问题跟踪器，JIRA，发布，要求,,,
LUNM2SNK,2023,https://doi.org/10.1109/TSE.2023.3265962,TSE 2023,Detecting and Characterizing Propagation of Security Weaknesses in Puppet-Based Infrastructure Management,"Despite being beneficial for managing computing infrastructure automatically, Puppet manifests are susceptible to security weaknesses, e.g., hard-coded secrets and use of weak cryptography algorithms. Adequate mitigation of security weaknesses in Puppet manifests is thus necessary to secure computing infrastructure that are managed with Puppet manifests. A characterization of how security weaknesses propagate and affect Puppet-based infrastructure management, can inform practitioners on the relevance of the detected security weaknesses, as well as help them take necessary actions for mitigation. We conduct an empirical study with 17,629 Puppet manifests with Taint Tracker for Puppet Manifests (TaintPup). We observe 2.4 times more precision, and 1.8 times more F-measure for TaintPup, compared to that of a state-of-the-art security static analysis tool. From our empirical study, we observe security weaknesses to propagate into 4,457 resources, i.e, Puppet-specific code elements used to manage infrastructure. A single instance of a security weakness can propagate into as many as 35 distinct resources. We observe security weaknesses to propagate into 7 categories of resources, which include resources used to manage continuous integration servers and network controllers. According to our survey with 24 practitioners, propagation of security weaknesses into data storage-related resources is rated to have the most severe impact for Puppet-based infrastructure management.","Configuration as code,devops,devsecops,empirical study,infrastructure as code,puppet,static analysis",在基于Puppet的基础设施管理中检测和表征安全弱点的传播,尽管Puppet清单有利于自动管理计算基础设施，但它容易受到安全弱点的影响，例如硬编码机密和使用弱密码算法。因此，充分缓解Puppet清单中的安全弱点对于保护使用Puppet列表管理的计算基础设施是必要的。描述安全弱点如何传播和影响基于Puppet的基础设施管理，可以告知从业者检测到的安全弱点的相关性，并帮助他们采取必要的缓解措施。我们使用“小狗清单的污点跟踪器”（TaintPup）对17629份小狗清单进行了实证研究。与最先进的安全静态分析工具相比，我们观察到TaintUp的精度提高了2.4倍，F-measure提高了1.8倍。从我们的实证研究中，我们观察到安全弱点会传播到4457个资源中，即用于管理基础设施的Puppet特定代码元素。安全弱点的单个实例可以传播到多达35个不同的资源中。我们观察到安全弱点传播到7类资源中，其中包括用于管理连续集成服务器和网络控制器的资源。根据我们对24名从业者的调查，安全弱点向数据存储相关资源的传播被认为对基于Puppet的基础设施管理产生了最严重的影响。,配置为代码，devop，devsecop，经验研究，基础架构为代码，木偶，静态分析,,,
JB3Z2N9M,2023,https://doi.org/10.1109/TSE.2023.3248113,TSE 2023,NCQ: Code Reuse Support for Node.js Developers,"Code reuse is an important part of software development. The adoption of code reuse practices is especially common among Node.js developers. The Node.js package manager, NPM, indexes over 1 Million packages and developers often seek out packages to solve programming tasks. Due to the vast number of packages, selecting the right package is difficult and time consuming. With the goal of improving productivity of developers that heavily reuse code through third-party packages, we present Node Code Query (NCQ), a Read-Eval-Print-Loop environment that allows developers to 1) search for NPM packages using natural language queries, 2) search for code snippets related to those packages, 3) automatically correct errors in these code snippets, 4) quickly setup new environments for testing those snippets, and 5) transition between search and editing modes. In two user studies with a total of 20 participants, we find that participants begin programming faster and conclude tasks faster with NCQ than with baseline approaches, and that they like, among other features, the search for code snippets and packages. Our results suggest that NCQ makes Node.js developers more efficient in reusing code.","Code reuse,code search,library selection",NCQ:Node.js开发人员的代码重用支持,代码重用是软件开发的重要组成部分。代码重用实践的采用在Node.js开发人员中尤其常见。Node.js包管理器NPM索引了100多万个包，开发人员经常寻找包来解决编程任务。由于包裹数量庞大，选择合适的包裹既困难又耗时。为了提高通过第三方包大量重用代码的开发人员的生产力，我们提出了节点代码查询（NCQ），这是一个Read-Eval-Print-Loop环境，允许开发人员1）使用自然语言查询搜索NPM包，2）搜索与这些包相关的代码片段，3）自动更正这些代码片段中的错误，4）快速设置用于测试这些片段的新环境，以及5）在搜索和编辑模式之间转换。在两项共有20名参与者的用户研究中，我们发现，与基线方法相比，参与者使用NCQ开始编程和完成任务的速度更快，并且他们喜欢搜索代码片段和包等功能。我们的结果表明，NCQ使Node.js开发人员在重用代码方面更有效率。,代码重用，代码搜索，库选择,,,
H25ACREZ,2023,https://doi.org/10.1109/TSE.2022.3150720,TSE 2023,Using the SOCIO Chatbot for UML Modelling: A Family of Experiments,"Context: Recent developments in natural language processing have facilitated the adoption of chatbots in typically collaborative software engineering tasks (such as diagram modelling). Families of experiments can assess the performance of tools and processes and, at the same time, alleviate some of the typical shortcomings of individual experiments (e.g., inaccurate and potentially biased results due to a small number of participants). Objective: Compare the usability of a chatbot for collaborative modelling (i.e., SOCIO) and an online web tool (i.e., Creately). Method: We conducted a family of three experiments to evaluate the usability of SOCIO against the Creately online collaborative tool in academic settings. Results: The student participants were faster at building class diagrams using the chatbot than with the online collaborative tool and more satisfied with SOCIO. Besides, the class diagrams built using the chatbot tended to be more concise —albeit slightly less complete. Conclusion: Chatbots appear to be helpful for building class diagrams. In fact, our study has helped us to shed light on the future direction for experimentation in this field and lays the groundwork for researching the applicability of chatbots in diagramming.","Chatbots,family of experiments,usability,modelling",使用SOCIO聊天机器人进行UML建模：一系列实验,背景：自然语言处理的最新发展促进了聊天机器人在典型的协作软件工程任务（如图表建模）中的应用。实验系列可以评估工具和过程的性能，同时缓解个别实验的一些典型缺点（例如，由于参与者人数较少，结果不准确且可能存在偏见）。目的：比较用于协作建模的聊天机器人（即SOCIO）和在线网络工具（即Creately）的可用性。方法：我们进行了一系列三项实验，以评估SOCIO在学术环境中与Creately在线协作工具的可用性。结果：与在线协作工具相比，学生参与者使用聊天机器人构建类图的速度更快，并且对SOCIO更满意。此外，使用聊天机器人构建的类图往往更简洁——尽管略不完整。结论：聊天机器人似乎有助于构建类图。事实上，我们的研究帮助我们阐明了该领域未来的实验方向，并为研究聊天机器人在绘图中的适用性奠定了基础。,聊天机器人，实验系列，可用性，建模,,,
VC98NX8C,2023,https://doi.org/10.1109/TSE.2022.3163682,TSE 2023,Increasing the Confidence of Deep Neural Networks by Coverage Analysis,"The great performance of machine learning algorithms and deep neural networks in several perception and control tasks is pushing the industry to adopt such technologies in safety-critical applications, as autonomous robots and self-driving vehicles. At present, however, several issues need to be solved to make deep learning methods more trustworthy, predictable, safe, and secure against adversarial attacks. Although several methods have been proposed to improve the trustworthiness of deep neural networks, most of them are tailored for specific classes of adversarial examples, hence failing to detect other corner cases or unsafe inputs that heavily deviate from the training samples. This paper presents a lightweight monitoring architecture based on coverage paradigms to enhance the model robustness against different unsafe inputs. In particular, four coverage analysis methods are proposed and tested in the architecture for evaluating multiple detection logic. Experimental results show that the proposed approach is effective in detecting both powerful adversarial examples and out-of-distribution inputs, introducing limited extra-execution time and memory requirements.","Neural networks coverage,DNNs robustness,adversarial examples detection",利用覆盖分析提高深度神经网络的置信度,机器学习算法和深度神经网络在几个感知和控制任务中的出色性能正在推动该行业在安全关键应用中采用此类技术，如自动驾驶机器人和自动驾驶汽车。然而，目前需要解决几个问题，以使深度学习方法更可信、更可预测、更安全、更安全地抵御对抗性攻击。尽管已经提出了几种方法来提高深度神经网络的可信度，但大多数方法都是针对特定类别的对抗性示例量身定制的，因此无法检测到严重偏离训练样本的其他角落情况或不安全输入。本文提出了一种基于覆盖范式的轻量级监控体系结构，以增强模型对不同不安全输入的鲁棒性。特别地，提出了四种覆盖分析方法，并在用于评估多检测逻辑的架构中进行了测试。实验结果表明，该方法在检测强大的对抗性示例和分布外输入方面都是有效的，引入了有限的额外执行时间和内存需求。,神经网络覆盖，DNNS健壮性，恶意示例检测,,,
6EGH2TEM,2023,https://doi.org/10.1109/TSE.2022.3233802,TSE 2023,The Unnecessity of Assuming Statistically Independent Tests in Bayesian Software Reliability Assessments,"When assessing a software-based system, the results of Bayesian statistical inference on operational testing data can provide strong support for software reliability claims. For inference, this data (i.e., software successes and failures) is often assumed to arise in an independent, identically distributed (i.i.d.) manner. In this paper we show how conservative Bayesian approaches make this assumption unnecessary, by incorporating one's doubts about the assumption into the assessment. We derive conservative confidence bounds on a system's probability of failure on demand (pfd), when operational testing reveals no failures. The generality and utility of the confidence bounds are illustrated in the assessment of a nuclear power-plant safety-protection system, under varying levels of skepticism about the i.i.d. assumption. The analysis suggests that the i.i.d. assumption can make Bayesian reliability assessments extremely optimistic – such assessments do not explicitly account for how software can be very likely to exhibit no failures during extensive operational testing despite the software's pfd being undesirably large.","Conservative Bayesian inference,CBI,dependability claims,independent software failures,operational testing,software reliability assessment,statistical testing",贝叶斯软件可靠性评估中不需要假设统计独立测试,在评估基于软件的系统时，对操作测试数据进行贝叶斯统计推断的结果可以为软件可靠性声明提供有力的支持。为了推断，通常假设这些数据（即软件成功和失败）以独立、相同分布（i.i.d.）的方式出现。在本文中，我们展示了保守的贝叶斯方法如何通过将人们对该假设的怀疑纳入评估中，使该假设变得不必要。当运行测试没有发现故障时，我们推导出系统按需故障概率（pfd）的保守置信界。置信界限的普遍性和实用性在对i.i.d.假设的不同怀疑程度下，在核电站安全保护系统的评估中得到了说明。分析表明，i.i.d.假设可以使贝叶斯可靠性评估变得非常乐观——尽管软件的pfd大得令人不快，但这种评估并没有明确说明软件在广泛的操作测试中很可能不会出现故障。,保守贝叶斯推理，CBI，可靠性声明，独立软件故障，操作测试，软件可靠性评估，统计测试,,,
9TQK526K,2023,https://doi.org/10.1109/TSE.2022.3192755,TSE 2023,On the Effectiveness of Transfer Learning for Code Search,"The Transformer architecture and transfer learning have marked a quantum leap in natural language processing, improving the state of the art across a range of text-based tasks. This paper examines how these advancements can be applied to and improve code search. To this end, we pre-train a BERT-based model on combinations of natural language and source code data and fine-tune it on pairs of StackOverflow question titles and code answers. Our results show that the pre-trained models consistently outperform the models that were not pre-trained. In cases where the model was pre-trained on natural language “and” source code data, it also outperforms an information retrieval baseline based on Lucene. Also, we demonstrated that the combined use of an information retrieval-based approach followed by a Transformer leads to the best results overall, especially when searching into a large search pool. Transfer learning is particularly effective when much pre-training data is available and fine-tuning data is limited. We demonstrate that natural language processing models based on the Transformer architecture can be directly applied to source code analysis tasks, such as code search. With the development of Transformer models designed more specifically for dealing with source code data, we believe the results of source code analysis tasks can be further improved.","Code search,transfer learning,source code modeling,multimodal embeddings,stackoverflow,deep learning",论迁移学习在代码搜索中的有效性,Transformer架构和迁移学习标志着自然语言处理的巨大飞跃，在一系列基于文本的任务中提高了技术水平。本文研究了如何将这些进步应用于代码搜索并改进代码搜索。为此，我们根据自然语言和源代码数据的组合预先训练了一个基于BERT的模型，并根据StackOverflow问题标题和代码答案对其进行了微调。我们的结果表明，预训练的模型始终优于未经预训练的模式。在模型基于自然语言“和”源代码数据进行预训练的情况下，它也优于基于Lucene的信息检索基线。此外，我们还演示了将基于信息检索的方法与Transformer相结合使用，可以获得最佳的总体结果，尤其是在搜索大型搜索池时。当有大量预训练数据可用且微调数据有限时，迁移学习尤其有效。我们证明了基于Transformer架构的自然语言处理模型可以直接应用于源代码分析任务，例如代码搜索。随着专门为处理源代码数据而设计的Transformer模型的开发，我们相信源代码分析任务的结果可以进一步改进。,代码搜索，迁移学习，源代码建模，多模式嵌入，堆栈溢出，深度学习,,,
UPTKN6QN,2023,https://doi.org/10.1109/TSE.2023.3269899,TSE 2023,CirFix: Automated Hardware Repair and its Real-World Applications,"This article presents CirFix, a framework for automatically repairing defects in hardware designs implemented in languages like Verilog. We propose a novel fault localization approach based on assignments to wires and registers, and a fitness function tailored to the hardware domain to bridge the gap between software-level automated program repair and hardware descriptions. We also present a benchmark suite of 32 defect scenarios corresponding to a variety of hardware projects. Overall, CirFix produces plausible repairs for 21/32 and correct repairs for 16/32 of the defect scenarios. Additionally, we evaluate CirFix's fault localization independently through a human study (n = 41), and find that the approach may be a beneficial debugging aid for complex multi-line hardware defects.","Circuit designs,automated repair,empirical study,user study",CirFix：自动化硬件修复及其在现实世界中的应用,本文介绍了CirFix，一个用于自动修复硬件设计中的缺陷的框架，该框架使用Verilog等语言实现。我们提出了一种新的故障定位方法，该方法基于对导线和寄存器的分配，以及针对硬件领域定制的适应度函数，以弥合软件级自动程序修复和硬件描述之间的差距。我们还提供了一套基准测试套件，其中包含32个缺陷场景，对应于各种硬件项目。总的来说，CirFix对21/32的缺陷场景进行了合理的修复，并对16/32的缺陷情况进行了正确的修复。此外，我们通过人体研究（n=41）独立评估了CirFix的故障定位，并发现该方法可能是复杂多线硬件缺陷的有益调试辅助工具。,电路设计，自动修复，经验研究，用户研究,,,
EYEX594H,2023,https://doi.org/10.1109/TSE.2023.3286179,TSE 2023,An Architectural Technical Debt Index Based on Machine Learning and Architectural Smells,"A key aspect of technical debt (TD) management is the ability to measure the amount of principal accumulated in a system. The current literature contains an array of approaches to estimate TD principal, however, only a few of them focus specifically on architectural TD, but none of them satisfies all three of the following criteria: being fully automated, freely available, and thoroughly validated. Moreover, a recent study has shown that many of the current approaches suffer from certain shortcomings, such as relying on hand-picked thresholds. In this article, we propose a novel approach to estimate architectural technical debt principal based on machine learning and architectural smells to address such shortcomings. Our approach can estimate the amount of technical debt principal generated by a single architectural smell instance. To do so, we adopt novel techniques from Information Retrieval to train a learning-to-rank machine learning model (more specifically, a gradient boosting machine) that estimates the severity of an architectural smell and ensure the transparency of the predictions. Then, for each instance, we statically analyse the source code to calculate the exact number of lines of code creating the smell. Finally, we combine these two values to calculate the technical debt principal. To validate the approach, we conducted a case study and interviewed 16 practitioners, from both open source and industry, and asked them about their opinions on the TD principal estimations for several smells detected in their projects. The results show that for 71% of instances, practitioners agreed that the estimations provided were representative of the effort necessary to refactor the smell.","Machine learning,technical debt,architectural smells,arcan,learning-to-rank,case study",基于机器学习和建筑气味的建筑技术债务指数,技术债务（TD）管理的一个关键方面是衡量系统中累积本金金额的能力。目前的文献包含了一系列评估TD原理的方法，然而，只有少数方法专门关注架构TD，但没有一种方法满足以下三个标准：完全自动化、免费可用和彻底验证。此外，最近的一项研究表明，目前的许多方法都存在某些缺点，例如依赖于手工选择的阈值。在本文中，我们提出了一种基于机器学习和架构气味的新方法来估计架构技术债务本金，以解决这些缺点。我们的方法可以估计单个架构气味实例产生的技术债务本金的数量。为此，我们采用了信息检索中的新技术来训练一个从学习到排序的机器学习模型（更具体地说，梯度提升机器），该模型可以估计建筑气味的严重程度，并确保预测的透明度。然后，对于每个实例，我们静态地分析源代码，以计算产生气味的确切代码行数。最后，我们将这两个值结合起来计算技术债务本金。为了验证该方法，我们进行了一项案例研究，采访了来自开源和行业的16名从业者，并询问他们对TD对其项目中检测到的几种气味的主要估计的意见。结果显示，对于71%的实例，从业者一致认为所提供的估计值代表了重构气味所需的努力。,机器学习，技术债务，建筑气味，阿尔坎，学习排名，案例研究,,,
PY72UBF9,2023,https://doi.org/10.1109/TSE.2022.3228308,TSE 2023,Sketch2Process: End-to-End BPMN Sketch Recognition Based on Neural Networks,"Process models play an important role in various software engineering contexts. Among others, they are used to capture business-related requirements and provide the basis for the development of process-oriented applications in low-code/no-code settings. To support modelers in creating, checking, and maintaining process models, dedicated tools are available. While these tools are generally considered as indispensable to capture process models for their later use, the initial version of a process model is often sketched on a whiteboard or a piece of paper. This has been found to have great advantages, especially with respect to communication and collaboration. It, however, also creates the need to subsequently transform the model sketch into a digital counterpart that can be further processed by modeling and analysis tools. Therefore, to automate this task, various so-called sketch recognition approaches have been defined in the past. Yet, these existing approaches are too limited for use in practice, since they, for instance, require sketches to be created on a digital device or do not address the recognition of edges or textual labels. Against this background, we use this paper to introduce Sketch2Process, the first end-to-end sketch recognition approach for process models captured using BPMN. Sketch2Process uses a neural network-based architecture to recognize the shapes, edges, and textual labels of highly expressive process models, covering 25 types of BPMN elements. To train and evaluate our approach, we created a dataset consisting of 704 hand-drawn and manually annotated BPMN models. Our experiments demonstrate that our approach is highly accurate and consistently outperforms the state of the art.","Requirements engineering,business process modeling,graphics recognition and interpretation",Sketch2Process:基于神经网络的端到端BPMN草图识别,过程模型在各种软件工程环境中发挥着重要作用。除其他外，它们用于捕获与业务相关的需求，并为在低代码/无代码设置中开发面向流程的应用程序提供基础。为了支持建模人员创建、检查和维护流程模型，提供了专用工具。虽然这些工具通常被认为是捕获流程模型以备日后使用所必需的，但流程模型的初始版本通常是在白板或一张纸上绘制的。已经发现这具有很大的优势，特别是在沟通和协作方面。然而，它也产生了随后将模型草图转换为数字对应物的需求，该数字对应物可以通过建模和分析工具进行进一步处理。因此，为了使这项任务自动化，过去已经定义了各种所谓的草图识别方法。然而，这些现有方法在实践中的使用过于有限，因为例如，它们需要在数字设备上创建草图，或者不涉及边缘或文本标签的识别。在此背景下，我们使用本文来介绍Sketch2Process，这是第一种用于使用BPMN捕获的流程模型的端到端草图识别方法。Sketch2Process使用基于神经网络的架构来识别高表达性流程模型的形状、边缘和文本标签，涵盖25种类型的BPMN元素。为了训练和评估我们的方法，我们创建了一个由704个手绘和手动注释的BPMN模型组成的数据集。我们的实验表明，我们的方法是高度准确的，并且始终优于现有技术。,需求工程，业务流程建模，图形识别和解释,,,
UTXTNCI5,2023,https://doi.org/10.1109/TSE.2022.3175752,TSE 2023,Annotative Software Product Line Analysis Using Variability-Aware Datalog,"Applying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersection of Product Line Engineering and software analysis. Different attempts have been made to “lift” particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (e.g., pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Soufflé Datalog engine. We evaluate our implementation on a set of Java and C-language benchmark annotative software product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually.","Software product lines,datalog,program analysis,pointer analysis,lifting,variability,doop,Soufflé",使用可变性感知数据日志的注释性软件产品线分析,将程序分析应用于软件产品线（SPLs）一直是产品线工程与软件分析交叉的基础研究问题。已经进行了不同的尝试来“提升”特定的产品级别分析，使其在整个产品线上运行。在本文中，我们处理了一类基于数据日志的分析（例如，指针和污点分析），研究了提升数据日志推理的理论方面，并在Soufflé数据日志引擎中实现了提升推理算法。我们评估了我们在一组Java和C语言基准注释性软件产品线上的实现。与单独对每种产品进行强力分析相比，我们在处理时间和事实数据库大小方面都有显著的节省（在其中一个基准测试上快了数十亿倍）。,软件产品线，数据库，程序分析，指针分析，提升，可变性，doop，Soufflé,,,
KHPSTPP7,2023,https://doi.org/10.1109/TSE.2023.3256362,TSE 2023,CoSS: Leveraging Statement Semantics for Code Summarization,"Automated code summarization tools allow generating descriptions for code snippets in natural language, which benefits software development and maintenance. Recent studies demonstrate that the quality of generated summaries can be improved by using additional code representations beyond token sequences. The majority of contemporary approaches mainly focus on extracting code syntactic and structural information from abstract syntax trees (ASTs). However, from the view of macro-structures, it is challenging to identify and capture semantically meaningful features due to fine-grained syntactic nodes involved in ASTs. To fill this gap, we investigate how to learn more code semantics and control flow features from the perspective of code statements. Accordingly, we propose a novel model entitled CoSS for code summarization. CoSS adopts a Transformer-based encoder and a graph attention network-based encoder to capture token-level and statement-level semantics from code token sequence and control flow graph, respectively. Then, after receiving two-level embeddings from encoders, a joint decoder with a multi-head attention mechanism predicts output sequences verbatim. Performance evaluations on Java, Python, and Solidity datasets validate that CoSS outperforms nine state-of-the-art (SOTA) neural code summarization models in effectiveness and is competitive in execution efficiency. Further, the ablation study reveals the contribution of each model component.","Attention mechanism,code summarization,control flow graph,graph neural network",CoSS：利用语句语义进行代码摘要,自动代码摘要工具允许用自然语言生成代码片段的描述，这有利于软件开发和维护。最近的研究表明，可以通过使用令牌序列之外的额外代码表示来提高生成摘要的质量。当代的大多数方法主要集中在从抽象语法树（AST）中提取代码的语法和结构信息。然而，从宏观结构的角度来看，由于AST中涉及细粒度的句法节点，识别和捕获语义上有意义的特征是具有挑战性的。为了填补这一空白，我们研究了如何从代码语句的角度学习更多的代码语义和控制流特性。因此，我们提出了一种新的代码摘要模型CoSS。CoSS采用基于Transformer的编码器和基于图注意力的编码器分别从代码令牌序列和控制流图中捕获令牌级和语句级语义。然后，在从编码器接收到两级嵌入之后，具有多头注意力机制的联合解码器逐字预测输出序列。对Java、Python和Solidity数据集的性能评估验证了CoSS在有效性方面优于九个最先进的（SOTA）神经代码摘要模型，并且在执行效率方面具有竞争力。此外，消融研究揭示了每个模型组件的贡献。,注意机制，代码摘要，控制流图，图神经网络,,,
Y3Y7QPZS,2023,https://doi.org/10.1109/TSE.2023.3274349,TSE 2023,CfgNet: A Framework for Tracking Equality-Based Configuration Dependencies Across a Software Project,"Modern software development incorporates various technologies, such as containerization, CI/CD pipelines, and build tools, which have to be jointly configured to enable building, testing, deployment, and execution of software systems. The vast configuration space spans several different configuration artifacts with their own syntax and semantics, encoding hundreds of configuration options and their values. The interplay of these technologies requires some level of coordination, which is realized by matching configurations. That is, configuration options and their according values may depend on other options and values from entirely different technologies and artifacts. This creates non-obvious configuration dependencies that are hard to track. The missing awareness and overview of such configuration dependencies across diverse configuration artifacts, tools, and frameworks can lead to dependency conflicts and severe configuration errors. We propose CfgNet, a framework that models the configuration landscape of a software project as a configuration network in an extensible and artifact-independent way. This way, we enable the early detection of possible dependency violations and proactively prevent misconfigurations during software development and maintenance. In a literature study, we found that the most common form of dependencies is the equality of values of different options. Based on this result, we developed an equality-based linker to determine dependent options across different artifacts. To demonstrate the extensibility of our framework, we also implemented nine plugins for popular technologies, such as Maven and Docker. To evaluate our approach, we injected and violated five real-world configuration dependencies extracted from Stack Overflow, which we support with our technology plugins, in five subject systems. CfgNet found all injected dependency violations and four additional ones already present in these systems. Moreover, we applied CfgNet to the commit history of 50 repositories selected from GitHub and found dependency conflicts in about two thirds of these repositories. We manually inspected 883 conflicts, with about 89 % true positives, demonstrating the need to reliably track cross-technology configuration dependencies and prevent their misconfiguration.","Configuration dependencies,configuration conflicts,services and components",CfgNet：一个在软件项目中跟踪基于平等的配置依赖关系的框架,现代软件开发包含了各种技术，如容器化、CI/CD管道和构建工具，这些技术必须进行联合配置，以实现软件系统的构建、测试、部署和执行。巨大的配置空间跨越了几个不同的配置工件，它们有自己的语法和语义，编码了数百个配置选项及其值。这些技术的相互作用需要一定程度的协调，这是通过匹配配置来实现的。也就是说，配置选项及其相应的值可能取决于来自完全不同的技术和工件的其他选项和值。这会产生难以跟踪的不明显的配置依赖关系。在不同的配置工件、工具和框架中，缺少对此类配置依赖关系的认识和概述可能会导致依赖关系冲突和严重的配置错误。我们提出了CfgNet，这是一个以可扩展和独立于工件的方式将软件项目的配置环境建模为配置网络的框架。通过这种方式，我们能够早期检测可能的依赖关系违规，并在软件开发和维护过程中积极防止错误配置。在一项文献研究中，我们发现最常见的依赖形式是不同选项的值相等。基于这一结果，我们开发了一个基于相等的链接器来确定不同工件之间的依赖选项。为了展示我们框架的可扩展性，我们还为流行技术实现了九个插件，如Maven和Docker。为了评估我们的方法，我们在五个主题系统中注入并违反了从Stack Overflow中提取的五个真实世界的配置依赖项，我们用技术插件支持它。CfgNet发现了所有注入的依赖性冲突以及这些系统中已经存在的四个附加冲突。此外，我们将CfgNet应用于从GitHub中选择的50个存储库的提交历史，发现其中约三分之二的存储库存在依赖冲突。我们手动检查了883个冲突，其中约89个 % 真正的积极因素，表明需要可靠地跟踪跨技术配置依赖关系并防止其错误配置。,配置依赖项，配置冲突，服务和组件,,,
L2R3QZH9,2023,https://doi.org/10.1109/TSE.2022.3176725,TSE 2023,Construct Validity in Software Engineering,"Empirical research aims to establish generalizable claims from data. Such claims may involve concepts that must be measured indirectly by using indicators. Construct validity is concerned with whether one can justifiably make claims at the conceptual level that are supported by results at the operational level. We report a quantitative analysis of the awareness of construct validity in the software engineering literature between 2000 and 2019 and a qualitative review of 83 articles about human-centric experiments published in five high-quality journals between 2015 and 2019. Over the two decades, the appearance in the literature of the term construct validity increased sevenfold. Some of the reviewed articles we reviewed employed various ways to ensure that the indicators span the concept in an unbiased manner. We also found articles that reuse formerly validated constructs. However, the articles disagree about how to define construct validity. Several interpret construct validity excessively by including threats to internal, external, or statistical conclusion validity. A few articles also include fundamental challenges of a study, such as cheating and misunderstanding of experiment material. The diversity of topics included as threats to construct validity calls for a more minimalist approach. Based on the review, we propose seven guidelines to improve how construct validity is handled and reported in software engineering.","Measurement,research quality,empirical research,systematic review,guidelines",软件工程中的结构有效性,实证研究旨在从数据中建立可推广的主张。这种主张可能涉及必须通过使用指标来间接衡量的概念。构念有效性是指一个人是否能够在概念层面提出有理由的主张，并得到操作层面结果的支持。我们报告了对2000年至2019年软件工程文献中结构有效性意识的定量分析，并对2015年至2019年间发表在五本高质量期刊上的83篇关于以人为中心的实验的文章进行了定性综述。在过去的二十年里，术语结构有效性在文献中的出现增加了七倍。我们审查的一些审查文章采用了各种方式，以确保指标以公正的方式跨越概念。我们还发现了重用以前验证过的构造的文章。然而，关于如何定义结构有效性，这两篇文章意见不一。一些人通过包括对内部、外部或统计结论有效性的威胁来过度解释结构有效性。一些文章还包括研究的基本挑战，如作弊和对实验材料的误解。作为建构有效性的威胁而包含的主题的多样性要求采取更为简约的方法。在回顾的基础上，我们提出了七条指导方针，以改进软件工程中构造有效性的处理和报告方式。,测量，研究质量，实证研究，系统评价，指南,,,
7UCD72WL,2023,https://doi.org/10.1109/TSE.2022.3172654,TSE 2023,Refactoring Test Smells With JUnit 5: Why Should Developers Keep Up-to-Date?,"Test smells are symptoms in the test code that indicate possible design or implementation problems. Previous research demonstrated their harmfulness and the developers’ acknowledgment of test smells’ effects, prevention, and refactoring strategies. Test automation frameworks are constantly evolving, and the JUnit, one of the most used ones for Java projects, has its version 5 available since late 2017. However, we do not know the extent to which developers use the newly introduced features and whether such features indeed help refactor existing test code to remove test smells. This article conducts a mixed-method study investigation to minimize these knowledge gaps. Our study consists of three parts. First, we evaluate the usage of this framework and its features by analyzing the source code of 485 popular Java open-source projects on GitHub that use JUnit. We found that 15.9% of these projects use the JUnit 5 library. We also found that, from 17 new features detected in use, only 3 (i.e., 17.6%) are responsible for more than 70% of usages, limiting optimized propositions to test code creation and maintenance. Second, after identifying features in the JUnit 5 framework that could be considered to test smells removal and prevention, we use these features to propose novel refactorings. In particular, we present refactorings based on 7 introduced JUnit 5 features that help to remove 13 test smells, such as Assertion Roulette, Test Code Duplication, and Conditional Test Logic. Third, to evaluate our refactorings with the opinions of experienced developers, we (i) survey 212 developers for their preferences and comments about our refactorings, corroborating the benefits of our proposals and raising community feedback on JUnit 5 features, and (ii) we refactor actual test code from popular GitHub Java projects and submit 38 Pull Requests, reaching a 94% acceptance rate among respondents. As implications of our study, we alert the software testing community (i.e., practitioners and researchers) to the need to study the JUnit 5 features to effectively remove and prevent test smells. To better assist this process, we give directions on how test smells can be refactored using such features.","Software/Program verification,test design,testing strategies",重构测试嗅到JUnit5的味道：为什么开发人员应该跟上时代？,测试气味是测试代码中的症状，表明可能存在设计或实现问题。先前的研究证明了它们的危害性，以及开发人员对测试气味的影响、预防和重构策略的认可。测试自动化框架在不断发展，JUnit是Java项目中最常用的框架之一，自2017年底以来，它的版本5已经可用。然而，我们不知道开发人员在多大程度上使用了新引入的功能，也不知道这些功能是否真的有助于重构现有的测试代码以消除测试气味。本文进行了一项混合方法研究调查，以最大限度地减少这些知识差距。我们的研究包括三个部分。首先，我们通过分析GitHub上485个使用JUnit的流行Java开源项目的源代码，评估了该框架的使用情况及其功能。我们发现15.9%的项目使用JUnit 5库。我们还发现，在使用中检测到的17个新功能中，只有3个（即17.6%）负责70%以上的使用，这将优化命题限制在测试代码创建和维护上。其次，在确定了JUnit5框架中可以用来测试气味去除和预防的特性之后，我们使用这些特性来提出新的重构。特别是，我们介绍了基于7个引入的JUnit 5功能的重构，这些功能有助于去除13种测试气味，如断言轮盘、测试代码重复和条件测试逻辑。第三，为了根据经验丰富的开发人员的意见来评估我们的重构，我们（i）调查了212名开发人员对我们重构的偏好和评论，证实了我们的建议的好处，并提出了社区对JUnit 5功能的反馈；（ii）我们从流行的GitHub Java项目中重构了实际的测试代码，并提交了38个Pull Request，受访者接受率达到94%。作为我们研究的启示，我们提醒软件测试社区（即从业者和研究人员）需要研究JUnit 5功能，以有效地消除和防止测试气味。为了更好地帮助这个过程，我们给出了如何使用这些功能重构测试气味的指导。,软件/程序验证，测试设计，测试策略,,,
8ZGTS9CB,2023,https://doi.org/10.1109/TSE.2022.3158831,TSE 2023,A Procedure to Continuously Evaluate Predictive Performance of Just-In-Time Software Defect Prediction Models During Software Development,"Just-In-Time Software Defect Prediction (JIT-SDP) uses machine learning to predict whether software changes are defect-inducing or clean. When adopting JIT-SDP, changes in the underlying defect generating process may significantly affect the predictive performance of JIT-SDP models over time. Therefore, being able to continuously track the predictive performance of JIT-SDP models during the software development process is of utmost importance for software companies to decide whether or not to trust the predictions provided by such models over time. However, there has been little discussion on how to continuously evaluate predictive performance in practice, and such evaluation is not straightforward. In particular, labeled software changes that can be used for evaluation arrive over time with a delay, which in part corresponds to the time we have to wait to label software changes as ‘clean’ (waiting time). A clean label assigned based on a given waiting time may not correspond to the true label of the software changes. This can potentially hinder the validity of any continuous predictive performance evaluation procedure for JIT-SDP models. This paper provides the first discussion of how to continuously evaluate predictive performance of JIT-SDP models over time during the software development process, and the first investigation of whether and to what extent waiting time affects the validity of such continuous performance evaluation procedure in JIT-SDP. Based on 13 GitHub projects, we found that waiting time had a significant impact on the validity. Though typically small, the differences in estimated predicted performance were sometimes large, and thus inappropriate choices of waiting time can lead to misleading estimations of predictive performance over time. Such impact did not normally change the ranking between JIT-SDP models, and thus conclusions in terms of which JIT-SDP model performs better are likely reliable independent of the choice of waiting time, especially when considered across projects.","Just-in-time software defect prediction,performance evaluation procedure,concept drift,data stream learning,online learning,verification latency,and label noise",软件开发过程中实时软件缺陷预测模型预测性能的连续评估过程,实时软件缺陷预测（JIT-SDP）使用机器学习来预测软件更改是导致缺陷还是清除缺陷。当采用JIT-SDP时，随着时间的推移，潜在缺陷生成过程的变化可能会显著影响JIT-SDP模型的预测性能。因此，能够在软件开发过程中持续跟踪JIT-SDP模型的预测性能对于软件公司决定是否相信这些模型随时间提供的预测至关重要。然而，关于如何在实践中持续评估预测性能的讨论很少，而且这种评估并不简单。特别是，可用于评估的标记软件更改会随着时间的推移而延迟到达，这在一定程度上对应于我们必须等待的时间，才能将软件更改标记为“干净”（等待时间）。基于给定等待时间分配的干净标签可能与软件更改的真实标签不对应。这可能会潜在地阻碍JIT-SDP模型的任何连续预测性能评估程序的有效性。本文首次讨论了在软件开发过程中如何随着时间的推移不断评估JIT-SDP模型的预测性能，并首次研究了等待时间是否以及在多大程度上影响JIT-SDP中这种连续性能评估过程的有效性。基于13个GitHub项目，我们发现等待时间对有效性有显著影响。虽然通常很小，但估计的预测性能差异有时很大，因此，对等待时间的不适当选择可能会导致对预测性能随时间推移的误导性估计。这种影响通常不会改变JIT-SDP模型之间的排名，因此，与等待时间的选择无关，尤其是在跨项目考虑时，JIT-SDP模式表现更好的结论可能是可靠的。,即时软件缺陷预测，效能评估程序，概念漂移，资料流学习，线上学习，验证延迟与标签杂讯,,,
SN5YXDII,2023,https://doi.org/10.1109/TSE.2023.3282981,TSE 2023,\textdollar\textbackslashmathtt {SIEGE}\textdollarSIEGE: A Semantics-Guided Safety Enhancement Framework for AI-Enabled Cyber-Physical Systems,"Cyber-Physical Systems (CPSs) have been widely adopted in various industry domains to support many important tasks that impact our daily lives, such as automotive vehicles, robotics manufacturing, and energy systems. As Artificial Intelligence (AI) has demonstrated its promising abilities in diverse tasks like decision-making, prediction, and optimization, a growing number of CPSs adopt AI components in the loop to further extend their efficiency and performance. However, these modern AI-enabled CPSs have to tackle pivotal problems that the AI-enabled control systems might need to compensate the balance across multiple operation requirements and avoid possible defections in advance to safeguard human lives and properties. Modular redundancy and ensemble method are two widely adopted solutions in the traditional CPSs and AI communities to enhance the functionality and flexibility of a system. Nevertheless, there is a lack of deep understanding of the effectiveness of such ensemble design on AI-CPSs across diverse industrial applications. Considering the complexity of AI-CPSs, existing ensemble methods fall short of handling such huge state space and sophisticated system dynamics. Furthermore, an ideal control solution should consider the multiple system specifications in real-time and avoid erroneous behaviors beforehand. Such that, a new specification-oriented ensemble control system is of urgent need for AI-CPSs. In this paper, we propose $\mathtt {SIEGE}$, a semantics-guided ensemble control framework to initiate an early exploratory study of ensemble methods on AI-CPSs and aim to construct an efficient, robust, and reliable control solution for multi-tasks AI-CPSs. We first utilize a semantic-based abstraction to decompose the large state space, capture the ongoing system status and predict future conditions in terms of the satisfaction of specifications. We propose a series of new semantics-aware ensemble strategies and an end-to-end Deep Reinforcement Learning (DRL) hierarchical ensemble method to improve the flexibility and reliability of the control systems. Our large-scale, comprehensive evaluations over five subject CPSs show that 1) the semantics abstraction can efficiently narrow the large state space and predict the semantics of incoming states, 2) our semantics-guided methods outperform state-of-the-art individual controllers and traditional ensemble methods, and 3) the DRL hierarchical ensemble approach shows promising capabilities to deliver a more robust, efficient, and safety-assured control system. To enable further research along this direction to build better AI-enabled CPS, we made all of the code and experimental results data publicly. (https://sites.google.com/view/ai-cps-siege/home).","Cyber-physical systems,reinforcement learning,state abstraction,AI controllers,ensemble methods",\textdollar\textrashmatt｛SIEGE｝\textdollarSIEGE：人工智能网络物理系统的语义导向安全增强框架,网络物理系统（CPS）已被广泛应用于各个行业领域，以支持影响我们日常生活的许多重要任务，如汽车、机器人制造和能源系统。随着人工智能（AI）在决策、预测和优化等多种任务中显示出其有前途的能力，越来越多的CPS采用人工智能组件来进一步提高其效率和性能。然而，这些现代人工智能控制系统必须解决关键问题，即人工智能控制的系统可能需要补偿多种操作要求之间的平衡，并提前避免可能的故障，以保护人类的生命和财产。模块冗余和集成方法是传统CPSs和AI社区中广泛采用的两种解决方案，用于增强系统的功能性和灵活性。尽管如此，人们对这种集成设计在不同工业应用中对人工智能消费品的有效性缺乏深入的了解。考虑到人工智能CPSs的复杂性，现有的集成方法无法处理如此巨大的状态空间和复杂的系统动力学。此外，理想的控制解决方案应该实时考虑多个系统规范，并事先避免错误行为。因此，迫切需要一种新的面向规范的集成控制系统。在本文中，我们提出了$\mathtt｛SIEGE｝$，这是一个语义引导的集成控制框架，以启动对AI CPSs集成方法的早期探索性研究，并旨在为多任务AI CPSs构建一个高效、稳健和可靠的控制解决方案。我们首先利用基于语义的抽象来分解大的状态空间，捕捉正在进行的系统状态，并根据规范的满足程度预测未来的条件。我们提出了一系列新的语义感知集成策略和端到端的深度强化学习（DRL）分层集成方法，以提高控制系统的灵活性和可靠性。我们对五个主题CPSs的大规模、全面的评估表明，1）语义抽象可以有效地缩小大的状态空间并预测进入状态的语义，2）我们的语义引导方法优于最先进的单个控制器和传统的集成方法，和3）DRL分层集成方法显示出提供更稳健、高效和安全保证的控制系统的有希望的能力。为了能够沿着这个方向进行进一步的研究，以构建更好的人工智能CPS，我们公开了所有的代码和实验结果数据。(https://sites.google.com/view/ai-cps-siege/home)。,网络物理系统，强化学习，状态抽象，人工智能控制器，集成方法,,,
LUL4MRKK,2023,https://doi.org/10.1109/TSE.2023.3301443,TSE 2023,DeepManeuver: Adversarial Test Generation for Trajectory Manipulation of Autonomous Vehicles,"Adversarial test generation techniques aim to produce input perturbations that cause a DNN to compute incorrect outputs. For autonomous vehicles driven by a DNN, however, the effect of such perturbations are attenuated by other parts of the system and are less effective as vehicle state evolves. In this work we argue that for adversarial testing perturbations to be effective on autonomous vehicles, they must account for the subtle interplay between the DNN and vehicle states. Building on that insight, we develop DeepManeuver, an automated framework that interleaves adversarial test generation with vehicle trajectory physics simulation. Thus, as the vehicle moves along a trajectory, DeepManeuver enables the refinement of candidate perturbations to: (1) account for changes in the state of the vehicle that may affect how the perturbation is perceived by the system; (2) retain the effect of the perturbation on previous states so that the current state is still reachable and past trajectory is preserved; and (3) result in multi-target maneuvers that require fulfillment of vehicle state sequences (e.g. reaching locations in a road to navigate a tight turn). Our assessment reveals that DeepManeuver can generate perturbations to force maneuvers more effectively and consistently than state-of-the-art techniques by 20.7 percentage points on average. We also show DeepManeuver's effectiveness at disrupting vehicle behavior to achieve multi-target maneuvers with a minimum 52% rate of success.","Test generation,adversarial testing,autonomous systems",深度机动：自动驾驶汽车轨迹操纵的对抗性测试生成,对抗性测试生成技术旨在产生输入扰动，导致DNN计算不正确的输出。然而，对于由DNN驱动的自动驾驶车辆，这种扰动的影响会被系统的其他部分减弱，并且随着车辆状态的发展，效果会降低。在这项工作中，我们认为，为了使对抗性测试扰动在自动驾驶汽车上有效，它们必须考虑DNN和车辆状态之间的微妙相互作用。基于这一见解，我们开发了DeepManivement，这是一个将对抗性测试生成与车辆轨迹物理模拟交织在一起的自动化框架。因此，当车辆沿着轨迹移动时，DeepManivement能够细化候选扰动，以：（1）考虑可能影响系统如何感知扰动的车辆状态变化；（2） 保留扰动对先前状态的影响，使得当前状态仍然是可到达的并且保留过去的轨迹；以及（3）导致需要完成车辆状态序列的多目标机动（例如到达道路中的位置以导航急转弯）。我们的评估表明，与最先进的技术相比，深度机动可以产生平均20.7个百分点的扰动，从而更有效、更一致地迫使机动。我们还展示了DeepManivement在干扰车辆行为以实现多目标机动方面的有效性，成功率至少为52%。,测试生成，对抗性测试，自主系统,,,
98VANWVI,2023,https://doi.org/10.1109/TSE.2022.3202311,TSE 2023,Mind the Gap! A Study on the Transferability of Virtual Versus Physical-World Testing of Autonomous Driving Systems,"Safe deployment of self-driving cars (SDC) necessitates thorough simulated and in-field testing. Most testing techniques consider virtualized SDCs within a simulation environment, whereas less effort has been directed towards assessing whether such techniques transfer to and are effective with a physical real-world vehicle. In this paper, we shed light on the problem of generalizing testing results obtained in a driving simulator to a physical platform and provide a characterization and quantification of the sim2real gap affecting SDC testing. In our empirical study, we compare SDC testing when deployed on a physical small-scale vehicle versus its digital twin. Due to the unavailability of driving quality indicators from the physical platform, we use neural rendering to estimate them through visual odometry, hence allowing full comparability with the digital twin. Then, we investigate the transferability of behavior and failure exposure between virtual and real-world environments, targeting both unintended abnormal test data and intended adversarial examples. Our study shows that, despite the usage of a faithful digital twin, there are still critical shortcomings that contribute to the reality gap between the virtual and physical world, threatening existing testing solutions that only consider virtual SDCs. On the positive side, our results present the test configurations for which physical testing can be avoided, either because their outcome does transfer between virtual and physical environments, or because the uncertainty profiles in the simulator can help predict their outcome in the real world.","AI testing,self-driving cars,simulated testing,real-world testing,deep neural networks,autonomous vehicles",小心缺口！自动驾驶系统虚拟与物理世界测试的可移植性研究,自动驾驶汽车（SDC）的安全部署需要进行彻底的模拟和现场测试。大多数测试技术都考虑在模拟环境中使用虚拟SDC，而较少致力于评估此类技术是否转移到物理真实世界车辆并对其有效。在本文中，我们阐明了将驾驶模拟器中获得的测试结果推广到物理平台的问题，并对影响SDC测试的模拟间隙进行了表征和量化。在我们的实证研究中，我们比较了部署在物理小型车辆上的SDC测试与其数字孪生车辆。由于无法从物理平台获得驾驶质量指标，我们使用神经渲染通过视觉里程计对其进行估计，因此可以与数字孪生进行完全可比性。然后，我们针对意外的异常测试数据和预期的对抗性示例，研究了行为和失败暴露在虚拟环境和现实世界环境之间的可转移性。我们的研究表明，尽管使用了忠实的数字孪生，但仍然存在严重的缺陷，导致虚拟世界和物理世界之间的现实差距，威胁到只考虑虚拟SDCs的现有测试解决方案。从积极的方面来看，我们的结果提供了可以避免物理测试的测试配置，要么是因为它们的结果确实在虚拟和物理环境之间转移，要么是模拟器中的不确定性曲线可以帮助预测它们在现实世界中的结果。,人工智能测试，自动驾驶汽车，模拟测试，真实世界测试，深度神经网络，自动驾驶汽车,,,
W8Y96PJF,2023,https://doi.org/10.1109/TSE.2023.3310874,TSE 2023,"DexBERT: Effective, Task-Agnostic and Fine-Grained Representation Learning of Android Bytecode","The automation of an increasingly large number of software engineering tasks is becoming possible thanks to Machine Learning (ML). One foundational building block in the application of ML to software artifacts is the representation of these artifacts (e.g., source code or executable code) into a form that is suitable for learning. Traditionally, researchers and practitioners have relied on manually selected features, based on expert knowledge, for the task at hand. Such knowledge is sometimes imprecise and generally incomplete. To overcome this limitation, many studies have leveraged representation learning, delegating to ML itself the job of automatically devising suitable representations and selections of the most relevant features. Yet, in the context of Android problems, existing models are either limited to coarse-grained whole-app level (e.g., apk2vec) or conducted for one specific downstream task (e.g., smali2vec). Thus, the produced representation may turn out to be unsuitable for fine-grained tasks or cannot generalize beyond the task that they have been trained on. Our work is part of a new line of research that investigates effective, task-agnostic, and fine-grained universal representations of bytecode to mitigate both of these two limitations. Such representations aim to capture information relevant to various low-level downstream tasks (e.g., at the class-level). We are inspired by the field of Natural Language Processing, where the problem of universal representation was addressed by building Universal Language Models, such as BERT, whose goal is to capture abstract semantic information about sentences, in a way that is reusable for a variety of tasks. We propose DexBERT, a BERT-like Language Model dedicated to representing chunks of DEX bytecode, the main binary format used in Android applications. We empirically assess whether DexBERT is able to model the DEX language and evaluate the suitability of our model in three distinct class-level software engineering tasks: Malicious Code Localization, Defect Prediction, and Component Type Classification. We also experiment with strategies to deal with the problem of catering to apps having vastly different sizes, and we demonstrate one example of using our technique to investigate what information is relevant to a given task.","Representation learning,Android app analysis,code representation,malicious code localization,defect prediction",DexBERT：Android字节码的高效、任务不可知和细粒度表示学习,由于机器学习（ML），越来越多的软件工程任务的自动化成为可能。ML应用于软件工件的一个基本构建块是将这些工件（例如，源代码或可执行代码）表示为适合学习的形式。传统上，研究人员和从业者依靠基于专家知识手动选择的特征来完成手头的任务。这种知识有时是不精确的，而且通常是不完整的。为了克服这一限制，许多研究利用表示学习，将自动设计合适的表示和选择最相关特征的工作委托给ML本身。然而，在Android问题的背景下，现有的模型要么局限于粗粒度的整个应用程序级别（例如，apk2vec），要么针对一个特定的下游任务（例如，smali2vec）进行。因此，生成的表示可能不适合细粒度任务，或者无法在训练过的任务之外进行推广。我们的工作是一项新研究的一部分，该研究旨在调查字节码的有效、任务不可知和细粒度通用表示，以减轻这两个限制。这样的表示旨在捕获与各种低级下游任务相关的信息（例如，在类级别）。我们受到了自然语言处理领域的启发，在该领域，通过构建通用语言模型来解决通用表示问题，如BERT，其目标是以可重复用于各种任务的方式捕获关于句子的抽象语义信息。我们提出了DexBERT，这是一种类似BERT的语言模型，专门表示DEX字节码的块，DEX字节代码是Android应用程序中使用的主要二进制格式。我们根据经验评估DexBERT是否能够为DEX语言建模，并评估我们的模型在三个不同的类级软件工程任务中的适用性：恶意代码本地化、缺陷预测和组件类型分类。我们还尝试了一些策略来处理适应大小相差悬殊的应用程序的问题，并展示了一个使用我们的技术来调查哪些信息与给定任务相关的例子。,表示学习，Android应用程序分析，代码表示，恶意代码本地化，缺陷预测,,,
57IP44RZ,2023,https://doi.org/10.1109/TSE.2023.3266324,TSE 2023,Taming Android Fragmentation Through Lightweight Crowdsourced Testing,"Android fragmentation refers to the overwhelming diversity of Android devices and OS versions. These lead to the impossibility of testing an app on every supported device, leaving a number of compatibility bugs scattered in the community and thereby resulting in poor user experiences. To mitigate this, our fellow researchers have designed various works to automatically detect such compatibility issues. However, the current state-of-the-art tools can only be used to detect specific kinds of compatibility issues (i.e., compatibility issues caused by API signature evolution), i.e., many other essential types of compatibility issues are still unrevealed. For example, customized OS versions on real devices and semantic changes of OS could lead to serious compatibility issues, which are non-trivial to be detected statically. To this end, we propose a novel, lightweight, crowdsourced testing approach, LazyCow, to fill this research gap and enable the possibility of taming Android fragmentation through crowdsourced efforts. Specifically, crowdsourced testing is an emerging alternative to conventional mobile testing mechanisms that allow developers to test their products on real devices to pinpoint platform-specific issues. Experimental results on thousands of test cases on real-world Android devices show that LazyCow is effective in automatically identifying and verifying API-induced compatibility issues. Also, after investigating the user experience through qualitative metrics, users’ satisfaction provides strong evidence that LazyCow is useful and welcome in practice.","Software testing,crowd-based software enginee ring,android fragmentation,compatibility issues",通过轻量级众包测试驯服Android碎片化,安卓碎片化指的是安卓设备和操作系统版本的巨大多样性。这导致无法在每一台受支持的设备上测试应用程序，从而在社区中留下大量兼容性漏洞，从而导致用户体验不佳。为了缓解这种情况，我们的研究伙伴设计了各种工作来自动检测此类兼容性问题。然而，当前最先进的工具只能用于检测特定类型的兼容性问题（即，由API签名演变引起的兼容性问题），即，许多其他基本类型的兼容性仍然无法解决。例如，真实设备上的定制操作系统版本和操作系统的语义变化可能会导致严重的兼容性问题，而这些问题对于静态检测来说是非常重要的。为此，我们提出了一种新颖、轻量级、众包的测试方法LazyCow，以填补这一研究空白，并通过众包努力驯服Android碎片化的可能性。具体而言，众包测试是传统移动测试机制的一种新兴替代方案，该机制允许开发人员在真实设备上测试他们的产品，以确定特定平台的问题。在现实世界的Android设备上对数千个测试案例的实验结果表明，LazyCo在自动识别和验证API引起的兼容性问题方面是有效的。此外，在通过定性指标调查用户体验后，用户的满意度提供了强有力的证据，证明LazyCow在实践中是有用和受欢迎的。,软件测试，基于人群的软件工程，Android碎片化，兼容性问题,,,
FXT89YFW,2023,https://doi.org/10.1109/TSE.2023.3263509,TSE 2023,Evolutionary Generation of Test Suites for Multi-Path Coverage of MPI Programs With Non-Determinism,"When a large number of target paths in a sequential program need to be covered, we can divide similar target paths into the same group, and generate a test suite covering the same group of target paths at the same time, so as to reduce the testing cost. However, different communication edges may be run under a same test input when executing a Message-Passing Interface (MPI) program with non-determinism, which cause different code fragments may be traversed, indicating the difficulty of generating a test suite to cover each group of target paths. This paper proposes an approach to evolutionary generation of test suites for multi-path coverage of MPI programs with non-determinism, which can significantly reduce the testing cost and difficulty. We first design an indicator for evaluating each traversal set of communication edges, which is used to form a relation matrix between each target path and each traversal set of communication edges, so as to divide all the target paths into a certain amount of groups. Then, we construct an optimization model for test suite generation associated with each group. Finally, an evolutionary optimization algorithm is extended to solve each model, and used to generate a test suite covering each group of target paths. The proposed approach is utilized and compared with several state-of-the-art approaches to seven benchmark MPI programs, as well as the experimental results illustrate that the proposed approach can efficiently generate a test suite, thus supporting the superiority of the proposed approach.","A large number of target paths,evolutionary optimization algorithm,multi-path coverage,MPI program with non-determinism,test suite generation",具有非确定性的MPI程序多路径覆盖测试集的进化生成,当序列程序中需要覆盖大量目标路径时，我们可以将相似的目标路径划分为同一组，并生成一个同时覆盖同一组目标路径的测试套件，以降低测试成本。然而，当以非确定性执行消息传递接口（MPI）程序时，不同的通信边缘可能在相同的测试输入下运行，这导致可能遍历不同的代码片段，这表明难以生成覆盖每组目标路径的测试套件。本文提出了一种进化生成MPI程序多路径覆盖测试套件的方法，该方法可以显著降低测试成本和难度。我们首先设计了一个指标来评估每个通信边缘的遍历集，用于在每个目标路径和每个通信边缘遍历集之间形成关系矩阵，从而将所有目标路径划分为一定数量的组。然后，我们构建了一个与每组相关联的测试套件生成优化模型。最后，将进化优化算法扩展到求解每个模型，并用于生成覆盖每组目标路径的测试套件。将所提出的方法应用于七个基准MPI程序，并与几种最先进的方法进行了比较，实验结果表明，该方法可以有效地生成测试套件，从而支持了所提出方法的优越性。,大量目标路径，进化优化算法，多路径覆盖，非确定性MPI程序，测试集生成,,,
Z4PQKCUB,2023,https://doi.org/10.1109/TSE.2023.3236449,TSE 2023,Characterizing and Finding System Setting-Related Defects in Android Apps,"Android, the most popular mobile system, offers a number of user-configurable system settings (e.g., network, location, and permission) for controlling devices and apps. Even popular, well-tested apps may fail to properly adapt their behaviors to diverse setting changes, thus frustrating their users. However, there exists no effort to systematically investigate such defects. To this end, we conduct the first large-scale empirical study to understand and characterize these system setting-related defects (in short as “setting defects”), which reside in apps and are triggered by system setting changes. We devote substantial manual effort (over four person-months) to analyze 1,074 setting defects from 180 popular apps on GitHub. We investigate the impact, root causes, and consequences of these setting defects and their correlations. We find that (1) setting defects have a wide impact on apps’ correctness with diverse root causes, (2) the majority of these defects ($\approx$70.7%) cause non-crashing (logic) failures, and (3) some correlations exist between the setting categories, root causes, and consequences. Motivated and informed by these findings, we propose two bug-finding techniques that can synergistically detect setting defects from both the GUI and code levels. Specifically, at the GUI level, we design and introduce setting-wise metamorphic fuzzing, the first automated dynamic testing technique to detect setting defects (causing crash and non-crashing failures, respectively) for Android apps. We implement this technique as an end-to-end, automated GUI testing tool named SetDroid. At the code level, we distill two major fault patterns and implement a static analysis tool named SetChecker to identify potential setting defects. We evaluate SetDroid and SetChecker on 26 popular, open-source Android apps, and they find 48 unique, previously-unknown setting defects. To date, 35 have been confirmed and 21 have been fixed by app developers. We also apply SetDroid and SetChecker on five highly popular industrial apps, namely WeChat, QQMail, TikTok, CapCut, and AlipayHK, all of which each have billions of monthly active users. SetDroid successfully detects 17 previously unknown setting defects in these apps’ latest releases, and all defects have been confirmed and fixed by the app vendors. After that, we collaborate with ByteDance and deploy these two bug-finding techniques internally to stress-test TikTok, one of its major app products. Within a two-month testing campaign, SetDroid successfully finds 53 setting defects, and SetChecker finds 22 ones. So far, 59 have been confirmed and 31 have been fixed. All these defects escaped from prior developer testing. By now, SetDroid has been integrated into ByteDance's official app testing infrastructure named FastBot for daily testing. These results demonstrate the strong effectiveness and practicality of our proposed techniques.","Empirical study,system settings,Android apps,GUI testing,static analysis",Android应用程序中与系统设置相关的缺陷的描述和查找,最受欢迎的移动系统Android提供了许多用户可配置的系统设置（例如，网络、位置和权限），用于控制设备和应用程序。即使是流行的、经过良好测试的应用程序，也可能无法使其行为适应不同的设置变化，从而让用户感到沮丧。然而，目前还没有系统地调查此类缺陷的努力。为此，我们进行了第一次大规模的实证研究，以了解和表征这些与系统设置相关的缺陷（简称“设置缺陷”），这些缺陷存在于应用程序中，并由系统设置变化触发。我们投入了大量的人工工作（超过四人月）来分析GitHub上180个流行应用程序中的1074个设置缺陷。我们调查了这些设置缺陷的影响、根本原因和后果及其相关性。我们发现（1）设置缺陷对应用程序的正确性有着广泛的影响，其根本原因多种多样；（2）这些缺陷中的大多数（$\approxy$70.7%）导致非崩溃（逻辑）故障；（3）设置类别、根本原因和后果之间存在一些相关性。受这些发现的启发和启发，我们提出了两种错误发现技术，可以从GUI和代码级别协同检测设置缺陷。具体来说，在GUI级别，我们设计并引入了设置方面的变形模糊，这是第一种自动动态测试技术，用于检测Android应用程序的设置缺陷（分别导致崩溃和非崩溃故障）。我们将此技术实现为一个名为SetDroid的端到端自动化GUI测试工具。在代码级别，我们提取了两种主要的故障模式，并实现了一个名为SetChecker的静态分析工具来识别潜在的设置缺陷。我们在26个流行的开源安卓应用程序上评估了SetDroid和SetChecker，发现了48个独特的、以前未知的设置缺陷。到目前为止，应用程序开发人员已经确认了35个，修复了21个。我们还在微信、QQMail、TikTok、CapCut和AlipayHK这五款广受欢迎的工业应用上应用了SetDroid和SetChecker，这些应用的月活跃用户都达到了数十亿。SetDroid在这些应用程序的最新版本中成功检测到17个以前未知的设置缺陷，所有缺陷都已由应用程序供应商确认并修复。之后，我们与字节跳动合作，在内部部署这两种漏洞发现技术，对其主要应用产品之一TikTok进行压力测试。在两个月的测试活动中，SetDroid成功发现了53个设置缺陷，SetChecker发现了22个。到目前为止，已经确认了59例，修复了31例。所有这些缺陷都是先前开发人员测试中遗漏的。到目前为止，SetDroid已集成到字节跳动的官方应用程序测试基础设施FastBot中，用于日常测试。这些结果证明了我们提出的技术的强大有效性和实用性。,实证研究，系统设置，Android应用程序，图形用户界面测试，静态分析,,,
7GKY34WT,2023,https://doi.org/10.1109/TSE.2022.3150153,TSE 2023,Cross-Project Online Just-In-Time Software Defect Prediction,"Cross-Project (CP) Just-In-Time Software Defect Prediction (JIT-SDP) makes use of CP data to overcome the lack of data necessary to train well performing JIT-SDP classifiers at the beginning of software projects. However, such approaches have never been investigated in realistic online learning scenarios, where Within-Project (WP) software changes naturally arrive over time and can be used to automatically update the classifiers. We provide the first investigation of when and to what extent CP data are useful for JIT-SDP in such realistic scenarios. For that, we propose three different online CP JIT-SDP approaches that can be updated with incoming CP and WP training examples over time. We also collect data on 9 proprietary software projects and use 10 open source software projects to analyse these approaches. We find that training classifiers with incoming CP+WP data can lead to absolute improvements in G-mean of up to 53.89% and up to 35.02% at the initial stage of the projects compared to classifiers using WP-only and CP-only data, respectively. Using CP+WP data was also shown to be beneficial after a large number of WP data were received. Using CP data to supplement WP data helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to absolute G-Mean improvements of up to 37.35% and 48.16% compared to WP-only and CP-only data during such periods, respectively. During periods of stable predictive performance, absolute improvements were of up to 29.03% and up to 41.25% compared to WP-only and CP-only classifiers, respectively. Our results highlight the importance of using both CP and WP data together in realistic online JIT-SDP scenarios.","Software defect prediction,cross-project learning,transfer learning,online learning,verification latency,concept drift",跨项目在线实时软件缺陷预测,跨项目（CP）实时软件缺陷预测（JIT-SDP）利用CP数据来克服在软件项目开始时训练性能良好的JIT-SDP分类器所需的数据不足的问题。然而，这种方法从未在现实的在线学习场景中进行过研究，在这种场景中，项目内（WP）软件的变化会随着时间的推移自然发生，并可用于自动更新分类器。我们首次调查了CP数据在这种现实场景中何时以及在多大程度上对JIT-SDP有用。为此，我们提出了三种不同的在线CP JIT-SDP方法，这些方法可以随着时间的推移用传入的CP和WP训练示例进行更新。我们还收集了9个专有软件项目的数据，并使用10个开源软件项目来分析这些方法。我们发现，与仅使用WP和仅使用CP的分类器相比，在项目的初始阶段，使用传入的CP+WP数据训练分类器可以使G-均值分别提高53.89%和35.02%。在接收到大量WP数据后，使用CP+WP数据也被证明是有益的。使用CP数据来补充WP数据有助于分类器减少或防止预测性能随着时间的推移而出现大幅下降，与在此期间仅使用WP和仅使用CP的数据相比，G-Mean的绝对值分别提高了37.35%和48.16%。在预测性能稳定的时期，与仅WP和仅CP分类器相比，绝对改进分别高达29.03%和41.25%。我们的研究结果强调了在现实的在线JIT-SDP场景中同时使用CP和WP数据的重要性。,软件缺陷预测，跨项目学习，迁移学习，在线学习，验证延迟，概念漂移,,,
LE7IQSWB,2023,https://doi.org/10.1109/TSE.2022.3150415,TSE 2023,On the Relationship Between Organizational Structure Patterns and Architecture in Agile Teams,"Forming members of an organisation into coherent groups or teams is an important issue in any large-scale software engineering endeavour, especially so in agile software development where teams rely heavily on self-organisation and organisational flexibility. But is there a recurrent organisational structure pattern in agile software engineering teams? and if so what does that pattern imply, in terms of software architecture quality? We address these questions using mixed-methods research in industry featuring interviews, surveys, and Delphi studies of real agile teams. In our study of 30 agile software teams we found that, out of seven organisational structure patterns that recur across our dataset, a single organisational pattern occurs over 37% of the time. This pattern: (a) reflects young communities (1-12 months old); (b) disappears in established ones (13+ months); and (c) reflects the highest number of architecture smells reported. Finally, we observe a negative correlation between a proposed organisational measure and architecture smells. On the one hand, these insights may serve to aid architects in designing not only their architectures but also their communities to best support their co-evolution. On the other hand, we observe that organisational structures in software engineering influence much more than simply software architectures, and we expect our results to lay the foundations of more structured and rigorous approaches to organisational structure studies and use in software engineering research and practice.","Software organisational structures,human aspects in software engineering,social software engineering,empirical software engineering,industrial mixed-methods research",敏捷团队中组织结构模式与体系结构的关系,在任何大规模的软件工程工作中，将一个组织的成员组成连贯的小组或团队都是一个重要问题，尤其是在敏捷软件开发中，团队在很大程度上依赖于自组织和组织灵活性。但是，在敏捷软件工程团队中，是否存在重复出现的组织结构模式？如果是这样的话，就软件体系结构质量而言，这种模式意味着什么？我们使用行业中的混合方法研究来解决这些问题，包括对真实敏捷团队的访谈、调查和Delphi研究。在我们对30个敏捷软件团队的研究中，我们发现，在数据集中重复出现的七种组织结构模式中，一种组织模式的发生率超过37%。这种模式：（a）反映年轻社区（1-12个月大）；（b） 在已建立的病例中消失（13个月以上）；以及（c）反映了所报告的最高数量的建筑气味。最后，我们观察到所提出的组织措施和建筑气味之间存在负相关。一方面，这些见解可能有助于建筑师不仅设计他们的建筑，而且设计他们的社区，以最好地支持他们的共同进化。另一方面，我们观察到软件工程中的组织结构的影响远不止是软件架构，我们希望我们的研究结果能为组织结构研究以及在软件工程研究和实践中的使用奠定更结构化和严格的方法的基础。,软件组织结构，软件工程中的人的方面，社会软件工程，经验软件工程，工业混合方法研究,,,
HRHT43AI,2023,https://doi.org/10.1109/TSE.2023.3242415,TSE 2023,Understanding Mentors' Engagement in OSS Communities via Google Summer of Code,"A constant influx of newcomers is essential for the sustainability and success of open source software (OSS) projects. However, successful onboarding is always challenging because newcomers face various initial contributing barriers. To support newcomer onboarding, OSS communities widely adopt the mentoring approach. Despite its significance, previous mentoring studies tend to focus on the newcomer's perspective, leaving the mentor's perspective relatively under-studied. To better support mentoring, we study the popular Google Summer of Code (GSoC). It is a well-established global program that offers stipends and mentors to students aiming to bring more student developers into OSS development. We combine online data analysis, an email survey, and semi-structured interviews with the GSoC mentors to understand their motivations, challenges, strategies, and gains. We propose a taxonomy of GSoC mentors’ engagement with four themes, ten categories, 34 sub-categories, and 118 codes, as well as the mentors’ attitudes toward the codes. In particular, we find that mentors participating in GSoC are primarily intrinsically motivated, and some new motivators emerge adapting to the contemporary challenges, e.g., sustainability and advertisement of projects. Forty-one challenges and 52 strategies associated with the program timeline are identified, most of which are first time revealed. Although almost all the challenges are agreed upon by specific mentors, some mentors believe that several challenges are reasonable and even have a positive effect. For example, the cognitive differences between mentors and mentees can stimulate new perspectives. Most of the mentors agreed that they had adopted these strategies during the mentoring process, but a few strategies recommended by the GSoC administration were not agreed upon. Self-satisfaction, different skills, and peer recognition are the main gains of mentors to participate in GSoC. Eventually, we discuss practical implications for mentors, students, OSS communities, GSoC programs, and researchers.","Mentor,mentoring process,newcomer onboarding,open source communities,summer of code,taxonomy",通过谷歌代码之夏了解导师对OSS社区的参与,不断涌入的新来者对开放源码软件项目的可持续性和成功至关重要。然而，成功的入职总是具有挑战性的，因为新员工面临着各种最初的贡献障碍。为了支持新人入职，OSS社区广泛采用辅导方法。尽管它很重要，但以前的辅导研究往往侧重于新人的视角，使导师的视角相对研究不足。为了更好地支持辅导，我们研究了流行的谷歌代码之夏（GSoC）。这是一个完善的全球项目，为学生提供津贴和导师，旨在将更多的学生开发人员引入OSS开发。我们结合在线数据分析、电子邮件调查和对GSoC导师的半结构化采访，了解他们的动机、挑战、策略和收获。我们提出了一个GSoC导师参与度的分类法，包括四个主题、十个类别、34个子类别和118个代码，以及导师对代码的态度。特别是，我们发现参与GSoC的导师主要是有内在动机的，并且出现了一些新的动机来适应当代的挑战，例如项目的可持续性和广告。确定了与项目时间表相关的41项挑战和52项策略，其中大部分是首次披露的。尽管几乎所有的挑战都是由特定的导师达成一致的，但一些导师认为，一些挑战是合理的，甚至具有积极的影响。例如，导师和学员之间的认知差异可以激发新的视角。大多数导师都同意，他们在指导过程中采用了这些策略，但GSoC管理部门建议的一些策略没有达成一致。自我满足、不同的技能和同伴认可是导师参与GSoC的主要收获。最后，我们讨论了对导师、学生、OSS社区、GSoC项目和研究人员的实际影响。,指导，指导流程，新人入门，开源社区，代码之夏，分类,,,
YCQENABX,2023,https://doi.org/10.1109/TSE.2023.3285743,TSE 2023,STRE: An Automated Approach to Suggesting App Developers When to Stop Reading Reviews,"It is well known that user feedback (i.e., reviews) plays an essential role in mobile app maintenance. Users upload their troubles, app issues, or praises, to help developers refine their apps. However, reading tremendous amounts of reviews to retrieve useful information is a challenging job. According to our manual studies, reviews are full of repetitive opinions, thus developers could stop reading reviews when no more new helpful information appears. Developers can extract useful information from partial reviews to ameliorate their app and then develop a new version. However, it is tough to have a good trade-off between getting enough useful feedback and saving more time. In this paper, we propose a novel approach, named STRE, which utilizes historical reviews to suggest the time when most of the useful information appears in reviews of a certain version. We evaluate STRE on 62 recent versions of five apps from Apple's App Store. Study results demonstrate that our approach can help developers save their time by up to 98.33% and reserve enough useful reviews before stopping to read reviews such that developers do not spend additional time in reading redundant reviews over the suggested stopping time. At the same time, STRE can complement existing review categorization approaches that categorize reviews to further assist developers. In addition, we find that the missed top-word-related reviews appearing after the suggested stopping time contain limited useful information for developers. Finally, we find that 12 out of 13 of the emerging bugs from the studied versions appear before the suggested stopping time. Our approach demonstrates the value of automatically refining information from reviews.","App reviews,maintenance,mobile applications",STRE：一种自动提示应用程序开发人员何时停止阅读评论的方法,众所周知，用户反馈（即评论）在手机应用程序维护中起着至关重要的作用。用户上传他们的烦恼、应用程序问题或赞扬，以帮助开发人员完善他们的应用程序。然而，阅读大量的评论来检索有用的信息是一项具有挑战性的工作。根据我们的手动研究，评论充满了重复的意见，因此当没有更多新的有用信息出现时，开发人员可能会停止阅读评论。开发人员可以从部分评论中提取有用的信息来改进他们的应用程序，然后开发新版本。然而，很难在获得足够有用的反馈和节省更多时间之间做出良好的权衡。在本文中，我们提出了一种新的方法，称为STRE，它利用历史评论来建议大多数有用信息出现在某个版本的评论中的时间。我们对苹果应用商店中五款应用程序的62个最新版本进行了STRE评估。研究结果表明，我们的方法可以帮助开发人员节省高达98.33%的时间，并在停止阅读评论之前保留足够的有用评论，这样开发人员就不会在建议的停止时间内花费额外的时间阅读多余的评论。同时，STRE可以补充现有的评审分类方法，对评审进行分类，以进一步帮助开发人员。此外，我们发现，在建议的停止时间之后出现的遗漏的热门评论对开发人员的有用信息有限。最后，我们发现，在研究版本中出现的13个bug中，有12个出现在建议的停止时间之前。我们的方法展示了从评论中自动提炼信息的价值。,应用程序审核，维护，移动应用程序,,,
JFU6AG2P,2023,https://doi.org/10.1109/TSE.2022.3228739,TSE 2023,Agile Effort Estimation: Have We Solved the Problem Yet? Insights From a Replication Study,"In the last decade, several studies have explored automated techniques to estimate the effort of agile software development. We perform a close replication and extension of a seminal work proposing the use of Deep Learning for Agile Effort Estimation (namely Deep-SE), which has set the state-of-the-art since. Specifically, we replicate three of the original research questions aiming at investigating the effectiveness of Deep-SE for both within-project and cross-project effort estimation. We benchmark Deep-SE against three baselines (i.e., Random, Mean and Median effort estimators) and a previously proposed method to estimate agile software project development effort (dubbed TF/IDF-SVM), as done in the original study. To this end, we use the data from the original study and an additional dataset of 31,960 issues mined from TAWOS, as using more data allows us to strengthen the confidence in the results, and to further mitigate external validity threats. The results of our replication show that Deep-SE outperforms the Median baseline estimator and TF/IDF-SVM in only very few cases with statistical significance (8/42 and 9/32 cases, respectively), thus confounding previous findings on the efficacy of Deep-SE. The two additional RQs revealed that neither augmenting the training set nor pre-training Deep-SE play lead to an improvement of its accuracy and convergence speed. These results suggest that using semantic similarity is not enough to differentiate user stories with respect to their story points; thus, future work has yet to explore and find new techniques and features that obtain accurate agile software development estimates.","Software effort estimation,story point estimation,deep learning",敏捷工作量估计：我们已经解决了这个问题吗？复制研究的见解,在过去的十年里，一些研究探索了自动化技术来评估敏捷软件开发的努力。我们对一项开创性的工作进行了密切的复制和扩展，该工作提出了将深度学习用于敏捷工作量估计（即Deep SE），自那以来，该工作已经达到了最先进的水平。具体而言，我们复制了三个原始研究问题，旨在调查Deep SE在项目内和跨项目工作量估计中的有效性。我们将Deep SE与三个基线（即随机、均值和中值工作量估计量）和先前提出的估计敏捷软件项目开发工作量的方法（称为TF/IDF-SVM）进行比较，如原始研究中所做的那样。为此，我们使用了原始研究的数据和从TAWOS中挖掘的31960个问题的额外数据集，因为使用更多的数据可以增强我们对结果的信心，并进一步减轻外部有效性威胁。我们的复制结果表明，Deep SE仅在极少数具有统计学意义的病例（分别为8/42和9/32例）中优于中位数基线估计器和TF/IDF-SVM，从而混淆了先前关于Deep SE疗效的研究结果。两个额外的RQ表明，无论是增强训练集还是预训练Deep SE play，都不会提高其准确性和收敛速度。这些结果表明，使用语义相似性不足以区分用户故事的故事点；因此，未来的工作还需要探索和发现新的技术和功能，以获得准确的敏捷软件开发估计。,软件工作量估计，故事点估计，深度学习,,,
RVCLRS4H,2023,https://doi.org/10.1109/TSE.2022.3174408,TSE 2023,Towards Scalable Model Checking of Reflective Systems via Labeled Transition Systems,"Reflection is a technique that enables a system to inspect or change its structure and/or behavior at runtime. It is a key enabler of many techniques for developing systems that have to function despite rapidly changing requirements and environments. A crucial issue in developing reflective systems is to ensure the correctness of their behaviors, because object-level behaviors are affected by metalevel behaviors. In this paper, we present an extended labeled transition system (LTS), which we call a metalevel LTS (MLTS), that supports data representation of another LTS for use in modeling a reflective tower. We show that two of the existing state reduction techniques for an LTS (symmetry reduction and divergence-sensitive stutter bisimulation) are also applicable to an MLTS. Then, we introduce two strategies for implementing an MLTS model in Promela, thereby enabling verification with the SPIN model checker. We also present case studies of applying MLTSs to two reflection applications: self-adaptation of a reconnaissance robot system, and dynamic evolution of an Internet-of-things (IoT) system. The case studies demonstrate the applicability of our approach and its scalability improvement through the state reduction techniques.","Model checking,labeled transition system,reflection,SPIN model checker",通过标记过渡系统实现反射系统的可扩展模型检查,反射是一种使系统能够在运行时检查或更改其结构和/或行为的技术。它是开发系统的许多技术的关键推动者，这些系统必须在需求和环境快速变化的情况下发挥作用。开发反射系统的一个关键问题是确保其行为的正确性，因为对象级行为受到元级行为的影响。在本文中，我们提出了一个扩展的标记转换系统（LTS），我们称之为金属级LTS（MLTS），它支持另一个LTS的数据表示，用于对反射塔建模。我们证明了LTS的两种现有状态约简技术（对称约简和发散敏感的口吃互模拟）也适用于MLTS。然后，我们介绍了在Promela中实现MLTS模型的两种策略，从而能够使用SPIN模型检查器进行验证。我们还介绍了将MLTS应用于两个反射应用的案例研究：侦察机器人系统的自适应和物联网（IoT）系统的动态进化。案例研究证明了我们的方法的适用性及其通过状态约简技术提高的可扩展性。,模型检查，标记转换系统，反射，旋转模型检查器,,,
MIYDXXVB,2023,https://doi.org/10.1109/TSE.2022.3219458,TSE 2023,"A Software Requirements Ecosystem: Linking Forum, Issue Tracker, and FAQs for Requirements Management","User feedback is an important resource in modern software development, often containing requirements that help address user concerns and desires for a software product. The feedback in online channels is a recent focus for software engineering researchers, with multiple studies proposing automatic analysis tools. In this work, we investigate the product forums of two large open source software projects. Through a quantitative analysis, we show that forum feedback is often manually linked to related issue tracker entries and product documentation. By linking feedback to their existing documentation, development teams enhance their understanding of known issues, and direct their users to known solutions. We discuss how the links between forum, issue tracker, and product documentation form a requirements ecosystem that has not been identified in the previous literature. We apply state-of-the-art deep-learning to automatically match forum posts with related issue tracker entries. Our approach identifies requirement matches with a mean average precision of 58.9% and hit ratio of 82.2%. Additionally, we apply deep-learning using an innovative clustering technique, achieving promising performance when matching forum posts to related product documentation. We discuss the possible applications of these automated techniques to support the flow of requirements between forum, issue tracker, and product documentation.","Requirements engineering,machine learning,natural language processing,deep learning,open source software,user feedback,software engineering",软件需求生态系统：用于需求管理的链接论坛、问题跟踪器和常见问题解答,用户反馈是现代软件开发中的一种重要资源，通常包含有助于解决用户对软件产品的关注和渴望的需求。在线渠道中的反馈是软件工程研究人员最近关注的焦点，多项研究提出了自动分析工具。在这项工作中，我们调查了两个大型开源软件项目的产品论坛。通过定量分析，我们发现论坛反馈通常手动链接到相关的问题跟踪条目和产品文档。通过将反馈链接到现有文档，开发团队可以增强对已知问题的理解，并引导用户找到已知的解决方案。我们讨论了论坛、问题跟踪器和产品文档之间的联系如何形成一个在以前的文献中没有发现的需求生态系统。我们应用最先进的深度学习，自动将论坛帖子与相关问题跟踪条目进行匹配。我们的方法以58.9%的平均精度和82.2%的命中率识别需求匹配。此外，我们使用创新的聚类技术应用深度学习，在将论坛帖子与相关产品文档匹配时实现了良好的性能。我们讨论了这些自动化技术的可能应用，以支持论坛、问题跟踪器和产品文档之间的需求流。,需求工程，机器学习，自然语言处理，深度学习，开源软件，用户反馈，软件工程,,,
VBPGDGSW,2023,https://doi.org/10.1109/TSE.2022.3176674,TSE 2023,Software Updates Strategies: A Quantitative Evaluation Against Advanced Persistent Threats,"Software updates reduce the opportunity for exploitation. However, since updates can also introduce breaking changes, enterprises face the problem of balancing the need to secure software with updates with the need to support operations. We propose a methodology to quantitatively investigate the effectiveness of software updates strategies against attacks of Advanced Persistent Threats (APTs). We consider strategies where the vendor updates are the only limiting factors to cases in which enterprises delay updates from 1 to 7 months based on SANS data. Our manually curated dataset of APT attacks covers 86 APTs and 350 campaigns from 2008 to 2020. It includes information about attack vectors, exploited vulnerabilities (e.g., 0-days versus public vulnerabilities), and affected software and versions. Contrary to common belief, most APT campaigns employed publicly known vulnerabilities. If an enterprise could theoretically update as soon as an update is released, it would face lower odds of being compromised than those waiting one (4.9x) or three (9.1x) months. However, if attacked, it could still be compromised from 14% to 33% of the times. As in practice enterprises must do regression testing before applying an update, our major finding is that one could perform 12% of all possible updates restricting oneself only to versions fixing publicly known vulnerabilities without significant changes to the odds of being compromised compared to a company that updates for all versions.","Advanced persistent threats,software vulnerabilities,software updates",软件更新策略：针对高级持续威胁的定量评估,软件更新减少了利用的机会。然而，由于更新也可能带来突破性的变化，企业面临着平衡安全软件和更新的需求与支持运营的需求的问题。我们提出了一种方法来定量研究软件更新策略对高级持久威胁（APT）攻击的有效性。我们考虑的策略是，供应商更新是企业根据SANS数据将更新延迟1到7个月的唯一限制因素。我们手动策划的APT攻击数据集涵盖了2008年至2020年的86次APT和350次活动。它包括有关攻击载体、被利用的漏洞（例如，0天与公共漏洞）以及受影响的软件和版本的信息。与普遍的看法相反，大多数APT活动都使用了众所周知的漏洞。如果一个企业理论上可以在更新发布后立即进行更新，那么它面临的风险将低于等待一个月（4.9倍）或三个月（9.1倍）的企业。然而，如果受到攻击，它仍然可能在14%到33%的时间内受到攻击。在实践中，企业在应用更新之前必须进行回归测试，我们的主要发现是，与为所有版本更新的公司相比，一个人可以执行所有可能更新的12%，将自己限制在修复已知漏洞的版本上，而不会显着改变被泄露的几率。,高级持续性威胁，软件漏洞，软件更新,,,
FLZHSRN2,2023,https://doi.org/10.1109/TSE.2023.3301660,TSE 2023,Runtime Verification of Crypto APIs: An Empirical Study,"Misuse of cryptographic (crypto) APIs is a noteworthy cause of security vulnerabilities. For this reason, static analyzers were recently proposed for detecting crypto API misuses. They differ in strengths and weaknesses, and they might miss bugs. Motivated by the inherent limitations of static analyzers, this article reports on a study of runtime verification (RV) as a dynamic-analysis-based alternative for crypto API misuse detection. RV monitors program runs against formal specifications; it was shown to be effective and efficient for amplifying the bug-finding ability of software tests. We focus on the popular JCA crypto API and write 22 RV specifications based on expert-validated rules in a static analyzer. We monitor these specifications while running tests in five benchmarks. Lastly, we compare the accuracy of our RV-based approach, RVSec, with those of three state-of-the-art crypto API misuses detectors: CogniCrypt, CryptoGuard, and CryLogger. Results show that RVSec has higher accuracy in four benchmarks and is on par with CryptoGuard in the fifth. Overall, RVSec achieves an average ${\boldsymbol{F}}_{1}$ measure of 95%, compared with 83%, 78%, and 86% for CogniCrypt, CryptoGuard, and CryLogger, respectively. We highlight the strengths and limitations of these tools and show that RV is effective for detecting crypto API misuses. We also discuss how static and dynamic analysis can complement each other for detecting crypto API misuses.","Security vulnerability,crypto API misuse,runtime verification",加密API的运行时验证：一项实证研究,滥用加密API是造成安全漏洞的一个值得注意的原因。出于这个原因，最近提出了用于检测加密API误用的静态分析器。它们的长处和短处各不相同，可能会错过bug。受静态分析器固有局限性的激励，本文报告了运行时验证（RV）作为加密API误用检测的一种基于动态分析的替代方案的研究。RV监测器程序按照正式规范运行；结果表明，该方法能够有效地增强软件测试的错误发现能力。我们专注于流行的JCA crypto API，并在静态分析器中编写了22个基于专家验证规则的RV规范。我们在五个基准测试中运行测试，同时监视这些规范。最后，我们将基于RV的方法RVSec的准确性与三种最先进的加密API误用检测器（CogniCrypt、CryptoGuard和CryLogger）的准确性进行了比较。结果表明，RVSec在四个基准测试中具有更高的准确性，与第五个基准测试的CryptoGuard不相上下。总体而言，RVSec实现了95%的平均${\boldsymbol｛F｝}_｛1｝$度量，而CogniCrypt、CryptoGuard和CryLogger分别为83%、78%和86%。我们强调了这些工具的优势和局限性，并表明RV对于检测加密API滥用是有效的。我们还讨论了静态和动态分析如何在检测加密API误用方面相互补充。,安全漏洞，加密API滥用，运行时验证,,,
X4H2KZ2H,2023,https://doi.org/10.1109/TSE.2023.3266041,TSE 2023,DeLag: Using Multi-Objective Optimization to Enhance the Detection of Latency Degradation Patterns in Service-Based Systems,"Performance debugging in production is a fundamental activity in modern service-based systems. The diagnosis of performance issues is often time-consuming, since it requires thorough inspection of large volumes of traces and performance indices. In this paper we present DeLag, a novel automated search-based approach for diagnosing performance issues in service-based systems. DeLag identifies subsets of requests that show, in the combination of their Remote Procedure Call execution times, symptoms of potentially relevant performance issues. We call such symptoms Latency Degradation Patterns. DeLag simultaneously searches for multiple latency degradation patterns while optimizing precision, recall and latency dissimilarity. Experimentation on 700 datasets of requests generated from two microservice-based systems shows that our approach provides better and more stable effectiveness than three state-of-the-art approaches and general purpose machine learning clustering algorithms. DeLag is more effective than all baseline techniques in at least one case study (with $p\leq 0.05$ and non-negligible effect size). Moreover, DeLag outperforms in terms of efficiency the second and the third most effective baseline techniques on the largest datasets used in our evaluation (up to 22%).","AIOps,anomaly correlation,automated diagnosis,microservices,Performance issues",DeLag：使用多目标优化增强对基于服务系统中潜在降级模式的检测,生产中的性能调试是现代基于服务的系统中的一项基本活动。性能问题的诊断通常很耗时，因为它需要对大量的痕迹和性能指标进行彻底检查。在本文中，我们提出了DeLag，这是一种新的基于自动搜索的方法，用于诊断基于服务的系统中的性能问题。DeLag识别请求的子集，这些子集在其远程过程调用执行时间的组合中显示潜在相关性能问题的症状。我们称这种症状为潜伏性退化模式。DeLag同时搜索多种延迟退化模式，同时优化精度、召回率和延迟差异性。对两个基于微服务的系统生成的700个请求数据集的实验表明，与三种最先进的方法和通用机器学习聚类算法相比，我们的方法提供了更好、更稳定的有效性。在至少一个案例研究中，DeLag比所有基线技术更有效（$p\leq0.05$，影响大小不可忽略）。此外，在我们评估中使用的最大数据集上，DeLag在效率方面优于第二和第三有效的基线技术（高达22%）。,AIOps，异常关联，自动诊断，微服务，性能问题,,,
C9SNSL5H,2023,https://doi.org/10.1109/TSE.2023.3234321,TSE 2023,Automated Detection of Software Performance Antipatterns in Java-Based Applications,"The detection of performance issues in Java-based applications is not trivial since many factors concur to poor performance, and software engineers are not sufficiently supported for this task. The goal of this manuscript is the automated detection of performance problems in running systems to guarantee that no quality-based hinders prevent their successful usage. Starting from software performance antipatterns, i.e., bad practices (e.g., extensive interaction between software methods) expressing both the problem and the solution with the purpose of identifying shortcomings and promptly fixing them, we develop a framework that automatically detects seven software antipatterns capturing a variety of performance issues in Java-based applications. Our approach is applied to real-world case studies from different domains, and it captures four real-life performance issues of Hadoop and Cassandra that were not predicted by state-of-the-art approaches. As empirical evidence, we calculate the accuracy of the proposed detection rules, we show that code commits inducing and fixing real-life performance issues present interesting variations in the number of detected antipattern instances, and solving one of the detected antipatterns improves the system performance up to 50%.","Dynamic analysis,Java-based applications,software performance antipatterns",基于Java的应用程序中软件性能反模式的自动检测,在基于Java的应用程序中检测性能问题并非易事，因为许多因素都会导致性能不佳，而且软件工程师没有得到足够的支持来完成这项任务。这份手稿的目标是自动检测运行系统中的性能问题，以确保没有基于质量的障碍阻碍其成功使用。从软件性能反模式开始，即表达问题和解决方案的不良做法（例如，软件方法之间的广泛交互），目的是识别缺陷并及时修复它们，我们开发了一个框架，可以自动检测七个软件反模式，捕获基于Java的应用程序中的各种性能问题。我们的方法应用于不同领域的真实案例研究，它捕捉到了Hadoop和Cassandra的四个真实性能问题，这些问题是最先进的方法无法预测的。作为经验证据，我们计算了所提出的检测规则的准确性，我们表明，引发和修复实际性能问题的代码提交在检测到的反模式实例数量上存在有趣的变化，解决其中一个检测到的逆模式可将系统性能提高50%。,动态分析，基于Java的应用程序，软件性能反模式,,,
TYZEKQUC,2023,https://doi.org/10.1109/TSE.2023.3291137,TSE 2023,Incomplete Adaptive Distinguishing Sequences for Non-Deterministic FSMs,"The increasing complexity and criticality of software systems have led to growing interest in automated test generation. One of the most promising approaches is to use model-based testing (MBT), in which test automation is based on a model of the implementation under test (IUT), with much of the work concerning finite state machine (FSM) models. Many FSM-based test generation techniques use, possibly adaptive, sequences to check the state of the IUT. Of particular interest are adaptive distinguishing sequences (ADSs) because their use can lead to relatively small tests. However, not all systems possess an ADS. In this work, we generalise the notion of incomplete ADSs to non-deterministic partial and observable FSMs. We show that the problem of checking the existence of a set of $k$ incomplete ADSs that separates every pair of states is PSPACE-hard. Further, we generalise the notion of invertible sequences to non-deterministic partial and observable FSMs and show how invertible sequences can be used to derive additional incomplete ADSs. We propose a novel algorithm to generate incomplete ADSs and describe the results of experiments that evaluated its performance. The results indicate that the proposed method can generate sequences to identify states of the IUT and is faster and can process larger FSMs than other existing methods.","Adaptive distinguishing sequences/tests,non- deterministic finite state machines,software engineering/ software/program verification,software engineering/test design,software engineering/testing and debugging",非确定性有限状态机的不完全自适应判别序列,软件系统的复杂性和关键性不断增加，导致人们对自动化测试生成越来越感兴趣。最有前途的方法之一是使用基于模型的测试（MBT），其中测试自动化基于在测实现的模型（IUT），大部分工作涉及有限状态机（FSM）模型。许多基于FSM的测试生成技术使用（可能是自适应的）序列来检查IUT的状态。特别令人感兴趣的是自适应区分序列（ADS），因为它们的使用可以导致相对较小的测试。然而，并不是所有的系统都具有ADS。在这项工作中，我们将不完全ADS的概念推广到不确定的部分和可观察的FSM。我们证明了检查分离每对状态的一组$k$不完全ADS的存在性的问题是PSPACE困难的。此外，我们将可逆序列的概念推广到非确定性部分和可观察FSM，并展示了如何使用可逆序列来导出额外的不完全ADS。我们提出了一种生成不完全ADS的新算法，并描述了评估其性能的实验结果。结果表明，与其他现有方法相比，该方法可以生成序列来识别IUT的状态，并且速度更快，可以处理更大的FSM。,自适应区分序列/测试，非确定性有限状态机，软件工程/软件/程序验证，软件工程/测试设计，软件工程/测试和调试,,,
XAGNLN7A,2023,https://doi.org/10.1109/TSE.2023.3235684,TSE 2023,DASP: A Framework for Driving the Adoption of Software Security Practices,"Implementing software security practices is a critical concern in modern software development. Industry practitioners, security tool providers, and researchers have provided standard security guidelines and sophisticated security development tools to ensure a secure software development pipeline. But despite these efforts, there continues to be an increase in the number of vulnerabilities that can be exploited by malicious hackers. There is thus an urgent need to understand why developers still introduce security vulnerabilities into their applications and to understand what can be done to motivate them to write more secure code. To understand and address this problem further, we propose DASP, a framework for diagnosing and driving the adoption of software security practices among developers. DASP was conceived by combining behavioral science theories to shape a cross-sectional interview study with 28 software practitioners. Our interviews lead to a framework that consists of a comprehensive set of 33 drivers grouped into 7 higher-level categories that represent what needs to happen or change so that the adoption of software security practices occurs. Using the DASP framework, organizations can design interventions suitable for developers’ specific development contexts that will motivate them to write more secure code.","Behavior change,developer-centric security,software security,software security practices",DASP：推动软件安全实践采用的框架,实现软件安全实践是现代软件开发中的一个关键问题。行业从业者、安全工具提供商和研究人员提供了标准的安全指南和复杂的安全开发工具，以确保软件开发管道的安全。但尽管做出了这些努力，可被恶意黑客利用的漏洞数量仍在增加。因此，迫切需要了解为什么开发人员仍然在他们的应用程序中引入安全漏洞，并了解如何激励他们编写更安全的代码。为了进一步理解和解决这个问题，我们提出了DASP，这是一个用于诊断和推动开发人员采用软件安全实践的框架。DASP是通过结合行为科学理论对28名软件从业者进行横断面访谈研究而构思的。我们的访谈得出了一个框架，该框架由33个驱动因素组成，分为7个更高级别的类别，代表了需要发生或改变的事情，以便采用软件安全实践。使用DASP框架，组织可以设计适合开发人员特定开发环境的干预措施，从而激励他们编写更安全的代码。,行为改变，以开发人员为中心的安全，软件安全，软件安全实践,,,
K25W8PWR,2023,https://doi.org/10.1109/TSE.2022.3213041,TSE 2023,Data-Driven Mutation Analysis for Cyber-Physical Systems,"Cyber-physical systems (CPSs) typically consist of a wide set of integrated, heterogeneous components; consequently, most of their critical failures relate to the interoperability of such components. Unfortunately, most CPS test automation techniques are preliminary and industry still heavily relies on manual testing. With potentially incomplete, manually-generated test suites, it is of paramount importance to assess their quality. Though mutation analysis has demonstrated to be an effective means to assess test suite quality in some specific contexts, we lack approaches for CPSs. Indeed, existing approaches do not target interoperability problems and cannot be executed in the presence of black-box or simulated components, a typical situation with CPSs. In this article, we introduce data-driven mutation analysis, an approach that consists in assessing test suite quality by verifying if it detects interoperability faults simulated by mutating the data exchanged by software components. To this end, we describe a data-driven mutation analysis technique (DaMAT) that automatically alters the data exchanged through data buffers. Our technique is driven by fault models in tabular form where engineers specify how to mutate data items by selecting and configuring a set of mutation operators. We have evaluated DaMAT with CPSs in the space domain; specifically, the test suites for the software systems of a microsatellite and nanosatellites launched on orbit last year. Our results show that the approach effectively detects test suite shortcomings, is not affected by equivalent and redundant mutants, and entails acceptable costs.","Cyber-physical systems,CPS interoperability,integration testing,mutation analysis",网络物理系统的数据驱动突变分析,网络物理系统（CPS）通常由一组广泛的集成、异构组件组成；因此，它们的大多数关键故障都与这些组件的互操作性有关。不幸的是，大多数CPS测试自动化技术都是初步的，行业仍然严重依赖手动测试。对于可能不完整的手动生成的测试套件，评估其质量至关重要。尽管突变分析已被证明是在某些特定情况下评估测试套件质量的有效手段，但我们缺乏CPSs的方法。事实上，现有的方法不针对互操作性问题，并且不能在存在黑盒或模拟组件的情况下执行，这是CPSs的典型情况。在本文中，我们介绍了数据驱动的突变分析，这是一种通过验证测试套件是否检测到互操作性故障来评估测试套件质量的方法，该故障是通过对软件组件交换的数据进行突变来模拟的。为此，我们描述了一种数据驱动的突变分析技术（DaMAT），该技术可以自动更改通过数据缓冲区交换的数据。我们的技术是由表格形式的故障模型驱动的，工程师通过选择和配置一组变异运算符来指定如何变异数据项。我们已经在空间域中用CPSs评估了DaMAT；特别是去年在轨道上发射的微卫星和纳米卫星的软件系统测试套件。我们的结果表明，该方法有效地检测了测试套件的缺陷，不受等效和冗余突变体的影响，并且成本可接受。,网络物理系统，CPS互操作性，集成测试，突变分析,,,
P9P3HQM5,2023,https://doi.org/10.1109/TSE.2022.3170272,TSE 2023,Identifying Similar Test Cases That Are Specified in Natural Language,"Software testing is still a manual process in many industries, despite the recent improvements in automated testing techniques. As a result, test cases (which consist of one or more test steps that need to be executed manually by the tester) are often specified in natural language by different employees and many redundant test cases might exist in the test suite. This increases the (already high) cost of test execution. Manually identifying similar test cases is a time-consuming and error-prone task. Therefore, in this paper, we propose an unsupervised approach to identify similar test cases. Our approach uses a combination of text embedding, text similarity and clustering techniques to identify similar test cases. We evaluate five different text embedding techniques, two text similarity metrics, and two clustering techniques to cluster similar test steps and three techniques to identify similar test cases from the test step clusters. Through an evaluation in an industrial setting, we showed that our approach achieves a high performance to cluster test steps (an F-score of 87.39%) and identify similar test cases (an F-score of 86.13%). Furthermore, a validation with developers indicates several different practical usages of our approach (such as identifying redundant test cases), which help to reduce the testing manual effort and time.","Software testing,test case similarity,clustering",识别自然语言中指定的类似测试用例,尽管最近自动化测试技术有所改进，但在许多行业中，软件测试仍然是一个手动过程。因此，测试用例（由一个或多个需要测试人员手动执行的测试步骤组成）通常由不同的员工用自然语言指定，并且测试套件中可能存在许多冗余的测试用例。这增加了（已经很高的）测试执行成本。手动识别类似的测试用例是一项耗时且容易出错的任务。因此，在本文中，我们提出了一种无监督的方法来识别类似的测试用例。我们的方法结合了文本嵌入、文本相似性和聚类技术来识别相似的测试用例。我们评估了五种不同的文本嵌入技术、两种文本相似性度量、两种对相似测试步骤进行聚类的聚类技术和三种从测试步骤聚类中识别相似测试用例的技术。通过在工业环境中的评估，我们表明我们的方法实现了对集群测试步骤的高性能（F分为87.39%）和对相似测试用例的识别（F得分为86.13%）。此外，与开发人员的验证表明了我们方法的几种不同的实际用途（如识别冗余测试用例），这有助于减少测试手动工作量和时间。,软件测试，测试用例相似度，聚类,,,
RMIM2XSI,2023,https://doi.org/10.1109/TSE.2023.3250835,TSE 2023,Modelling Second-Order Uncertainty in State Machines,"Modelling the behaviour of state-based systems can be challenging, especially when the modeller is not entirely certain about its intended interactions with the user or the environment. Currently, it is possible to associate a stated level of uncertainty with a given event by attaching probabilities to transitions (producing ‘Probabilistic State Machines’). This captures the ‘First-order uncertainty’ - the (un-)certainty that a given event will occur. However, this does not permit the modeller to capture their own uncertainty (or lack thereof) about that stated probability - also known as ‘Second-order uncertainty’. In this article we introduce a generalisation of probabilistic finite state machines that makes it possible to incorporate this important additional dimension of uncertainty. For this we adopt a formalism for reasoning about uncertainty called Subjective Logic. We present an algorithm to create these enhanced state machines automatically from a conventional state machine and a set of observed sequences. We show how this approach can be used for reverse-engineering predictive state machines from traces.","State machine,second order uncertainty,subjective logic,inference,test prioritization",状态机中的二阶不确定性建模,对基于状态的系统的行为建模可能具有挑战性，尤其是当建模者不完全确定其与用户或环境的预期交互时。目前，通过将概率附加到转换（产生“概率状态机”），可以将规定的不确定性水平与给定事件相关联。这捕捉到了“一阶不确定性”，即给定事件将发生的（不）确定性。然而，这不允许建模者捕捉他们自己对所述概率的不确定性（或缺乏不确定性），也称为“二阶不确定性”。在这篇文章中，我们介绍了概率有限状态机的一种推广，它使我们有可能结合这一重要的不确定性的额外维度。为此，我们采用了一种形式主义来推理不确定性，称为主观逻辑。我们提出了一种算法，从传统的状态机和一组观察到的序列中自动创建这些增强的状态机。我们展示了这种方法如何从轨迹中用于逆向工程预测状态机。,状态机，二阶不确定性，主观逻辑，推理，测试优先级,,,
ERPUZRKT,2023,https://doi.org/10.1109/TSE.2023.3250029,TSE 2023,Beyond Literal Meaning: Uncover and Explain Implicit Knowledge in Code Through Wikipedia-Based Concept Linking,"When reusing or modifying code, developers need to understand the implicit knowledge behind a piece of code in addition to the literal meaning of code. Such implicit knowledge involves related concepts and their explanations. Uncovering and understanding the implicit knowledge in code are challenging due to the extensive use of abbreviations, scattered expressions of concepts, and ambiguity of concept mentions. In this paper, we propose an automatic approach (called CoLiCo) that can uncover implicit concepts in code and link the uncovered concepts to Wikipedia. Based on a trained identifier embedding model, CoLiCo identifies Wikipedia concepts mentioned in a given code snippet and excerpts a paragraph-level explanation from Wikipedia for each concept. During the process, CoLiCo resolves identifier abbreviation (i.e., concepts mentioned in the form of abbreviations) and identifier aggregation (i.e., concepts mentioned by an aggregation of multiple identifiers) based on identifier embedding and mining of identifier abbreviation/aggregation relations. Experimental study shows that CoLiCo outperforms a general entity linking approach by 38.7% in the correctness of concept linking and identifies 96.7% more correct concept linkings on a dataset with 629 code snippets. The concept linking is significant for program understanding in 54% code snippets. Our user study shows that CoLiCo can significantly shorten the time and improve the correctness in code comprehension tasks that intensively involve implicit knowledge.","Concept,code semantics,knowledge,program comprehension",超越字面意义：通过基于维基百科的概念链接揭示和解释代码中的隐性知识,在重用或修改代码时，开发人员除了需要了解代码的字面含义外，还需要了解代码背后的隐含知识。这种隐性知识涉及相关概念及其解释。由于缩写词的广泛使用、概念的零散表达以及概念提及的模糊性，揭示和理解代码中的隐含知识具有挑战性。在本文中，我们提出了一种自动方法（称为CoLiCo），可以揭示代码中的隐含概念，并将未揭示的概念链接到维基百科。基于经过训练的标识符嵌入模型，CoLiCo识别给定代码片段中提到的维基百科概念，并从维基百科中摘录每个概念的段落级解释。在此过程中，CoLiCo基于标识符嵌入和标识符缩写/聚合关系的挖掘，解析标识符缩写（即以缩写形式提及的概念）和标识符聚合（即通过多个标识符的聚合提及的概念。实验研究表明，在629个代码片段的数据集上，CoLiCo在概念链接的正确性方面比一般实体链接方法高38.7%，并识别出96.7%的正确概念链接。在54%的代码片段中，概念链接对于程序理解具有重要意义。我们的用户研究表明，在涉及隐含知识的代码理解任务中，CoLiCo可以显著缩短时间并提高正确性。,概念，代码语义，知识，程序理解,,,
XNYXZ4DT,2023,https://doi.org/10.1109/TSE.2022.3173346,TSE 2023,Machine/Deep Learning for Software Engineering: A Systematic Literature Review,"Since 2009, the deep learning revolution, which was triggered by the introduction of ImageNet, has stimulated the synergy between Software Engineering (SE) and Machine Learning (ML)/Deep Learning (DL). Meanwhile, critical reviews have emerged that suggest that ML/DL should be used cautiously. To improve the applicability and generalizability of ML/DL-related SE studies, we conducted a 12-year Systematic Literature Review (SLR) on 1,428 ML/DL-related SE papers published between 2009 and 2020. Our trend analysis demonstrated the impacts that ML/DL brought to SE. We examined the complexity of applying ML/DL solutions to SE problems and how such complexity led to issues concerning the reproducibility and replicability of ML/DL studies in SE. Specifically, we investigated how ML and DL differ in data preprocessing, model training, and evaluation when applied to SE tasks, and what details need to be provided to ensure that a study can be reproduced or replicated. By categorizing the rationales behind the selection of ML/DL techniques into five themes, we analyzed how model performance, robustness, interpretability, complexity, and data simplicity affected the choices of ML/DL models.","Software engineering,machine learning,deep learning",软件工程的机器/深度学习：系统文献综述,自2009年以来，由ImageNet的引入引发的深度学习革命激发了软件工程（SE）和机器学习（ML）/深度学习（DL）之间的协同作用。与此同时，出现了一些批判性评论，认为应谨慎使用ML/DL。为了提高ML/DL相关SE研究的适用性和可推广性，我们对2009年至2020年间发表的1428篇ML/DL有关SE论文进行了为期12年的系统文献综述（SLR）。我们的趋势分析证明了ML/DL给SE带来的影响。我们研究了将ML/DL解决方案应用于SE问题的复杂性，以及这种复杂性如何导致SE中ML/DL研究的再现性和可复制性问题。具体而言，我们研究了当应用于SE任务时，ML和DL在数据预处理、模型训练和评估方面的差异，以及需要提供哪些细节以确保研究可以复制或复制。通过将ML/DL技术选择背后的理由分为五个主题，我们分析了模型性能、稳健性、可解释性、复杂性和数据简单性如何影响ML/DL模型的选择。,软件工程，机器学习，深度学习,,,
QC4QK8GU,2023,https://doi.org/10.1109/TSE.2023.3294971,TSE 2023,Animation2API: API Recommendation for the Implementation of Android UI Animations,"UI animations, such as card movement and menu slide in/out, provide appealing user experience and enhance the usability of mobile applications. In the process of UI animation implementation, it is difficult for developers to identify suitable APIs for the animation to be implemented from a large number of APIs. Fortunately, the huge app market contains millions of apps, and they can provide valuable data resources for solving this problem. By summarizing the API usage for the same or similar animations in apps, reusable knowledge can be mined for the API recommendation. In this paper, we propose a novel method Animation2API, which mines the knowledge about APIs from existing apps and recommends APIs for UI animations. Different from existing text-based API recommendation approaches, Animation2API takes the UI animation in GIF/video format as query input. Firstly, we construct a database containing mappings between UI animations and APIs by analyzing a broad set of apps. Then, we build a UI animation feature extractor, which can be used to gain temporal-spatial feature vectors of UI animations. By comparing the temporal-spatial feature vectors between UI animations, we identify animations that are similar to the query animation from the database. Finally, we summarize the APIs used for implementing these animations and recommend a list of APIs for developers. The empirical evaluation results show that our method can achieve 82.66% Success rate and outperform the baseline Guru by 230.77% and 184.95% in terms of Precision and Recall when considering twenty APIs. In the user study, we take the scenarios of using web search and ChatGPT to implement animations as baselines, and the results show that participants can complete animations faster (14.54%) after using Animation2API. Furthermore, participants’ positive feedbacks on the questionnaire indicate the usefulness of Animation2API.","User interface,UI animation,API recommendation",动画2API:API推荐Android用户界面动画的实现,UI动画，如卡片移动和菜单滑入/滑出，提供了吸引人的用户体验，并增强了移动应用程序的可用性。在UI动画实现过程中，开发人员很难从大量的API中为要实现的动画确定合适的API。幸运的是，庞大的应用程序市场包含了数百万个应用程序，它们可以为解决这个问题提供宝贵的数据资源。通过总结应用程序中相同或相似动画的API使用情况，可以为API推荐挖掘可重用知识。在本文中，我们提出了一种新的方法Animation2API，该方法从现有的应用程序中挖掘有关API的知识，并为UI动画推荐API。与现有的基于文本的API推荐方法不同，Animation2API采用GIF/视频格式的UI动画作为查询输入。首先，我们通过分析一组广泛的应用程序，构建了一个包含UI动画和API之间映射的数据库。然后，我们构建了一个UI动画特征提取器，用于获取UI动画的时空特征向量。通过比较UI动画之间的时间-空间特征向量，我们从数据库中识别出与查询动画相似的动画。最后，我们总结了用于实现这些动画的API，并向开发人员推荐了一个API列表。实证评估结果表明，在考虑20种原料药的情况下，我们的方法可以获得82.66%的成功率，在精度和召回率方面分别优于基线Guru 230.77%和184.95%。在用户研究中，我们以使用网络搜索和ChatGPT实现动画的场景为基线，结果显示，参与者在使用Animation2API后可以更快地完成动画（14.54%）。此外，参与者对问卷的积极反馈表明了Animation2API的有用性。,用户界面，UI动画，API推荐,,,
6MTR5MLH,2023,https://doi.org/10.1109/TSE.2023.3237123,TSE 2023,Towards Saving Blockchain Fees via Secure and Cost-Effective Batching of Smart-Contract Invocations,"This paper presents iBatch, a middleware system running on top of an operational Ethereum network to enable secure batching of smart-contract invocations against an untrusted relay server off-chain. iBatch does so at a low overhead by validating the server's batched invocations in smart contracts without additional states of user nonces. The iBatch mechanism supports a variety of policies, ranging from conservative to aggressive batching, and can be configured adaptively to the current workloads. iBatch automatically rewrites smart contracts to integrate with legacy applications and support large-scale deployment. We built an evaluation platform for fast and cost-accurate transaction replaying and constructed real transaction benchmarks on popular Ethereum applications. With a functional prototype of iBatch, we conduct extensive cost evaluations, which shows iBatch saves $14.6\%\sim {}59.1\%$ Gas cost per invocation with a moderate 2-minute delay and $19.06\%\sim {}31.52\%$ Ether cost per invocation with a delay of $0.26\sim {}1.66$ blocks.","Blockchains,cost effectiveness,DeFi,replay attacks,smart contracts",通过安全且经济高效的批量智能合约调用节省区块链费用,本文介绍了iBatch，这是一个运行在运行中的以太坊网络上的中间件系统，可以在链下不受信任的中继服务器上安全地批量处理智能合约调用。iBatch通过在智能合约中验证服务器的批处理调用而无需额外的用户随机数状态，从而以较低的开销实现了这一点。iBatch机制支持多种策略，从保守批处理到激进批处理，并且可以根据当前工作负载进行自适应配置。iBatch自动重写智能合约，以与传统应用程序集成并支持大规模部署。我们建立了一个快速、成本准确的交易回放评估平台，并在流行的以太坊应用程序上构建了真实的交易基准。使用iBatch的功能原型，我们进行了广泛的成本评估，结果显示，iBatch在中等2分钟延迟的情况下，每次调用节省14.6\%\sim｛｝59.1\%%$天然气成本，在0.26\sim｛｝1.66$块延迟的情况中，每次调用节约19.06\%\sim｛｝31.52\%%$以太币成本。,区块链，成本效益，Defi，重放攻击，智能合约,,,
7XJY7MPT,2023,https://doi.org/10.1109/TSE.2022.3149240,TSE 2023,Enhancing DNN-Based Binary Code Function Search With Low-Cost Equivalence Checking,"Binary code function search has been used as the core basis of various security and software engineering applications, including malware clustering, code clone detection, and vulnerability audits. Recognizing logically similar assembly functions, however, remains a challenge. Most binary code search tools rely on program structure-level information, such as control flow and data flow graphs, that is extracted using program analysis techniques or deep neural networks (DNNs). However, DNN-based techniques capture lexical-, control structure-, or data flow-level information of binary code for representation learning, which is often too coarse-grained and does not accurately denote program functionality. Additionally, it may exhibit low robustness to a variety of challenging settings, such as compiler optimizations and obfuscations. This paper proposes a general solution for enhancing the top-$k$ ranked candidates in DNN-based binary code function search. The key idea is to design a low-cost and comprehensive equivalence check that quickly exposes functionality deviations between the target function and its top-$k$ matched functions. Functions that fail this equivalence check can be shaved from the top-$k$ list, and functions that pass the check can be revisited to move ahead on the top-$k$ ranked candidates, in a deliberate way. We design a practical and efficient equivalence check, named BinUSE, using under-constrained symbolic execution (USE). USE, a variant of symbolic execution, improves scalability by initiating symbolic execution directly from function entry points and relaxing constraints on function parameters. It eliminates the overhead incurred by path explosion and costly constraints. BinUSE is specifically designed to deliver an assembly function-level equivalence check, enhancing DNN-based binary code search by reducing its false alarms with low cost. Our evaluation shows that BinUSE can enable a general and effective enhancement of four state-of-the-art DNN-based binary code search tools when confronted with challenges posed by different compilers, optimizations, obfuscations, and architectures.","Reverse engineering,symbolic execution,software similarity,deep learning",用低成本等价性检查增强基于DNN的二进制码函数搜索,二进制代码功能搜索已被用作各种安全和软件工程应用程序的核心基础，包括恶意软件群集、代码克隆检测和漏洞审计。然而，识别逻辑上相似的组装功能仍然是一个挑战。大多数二进制代码搜索工具依赖于程序结构级别的信息，如控制流和数据流图，这些信息是使用程序分析技术或深度神经网络（DNN）提取的。然而，基于DNN的技术捕获二进制代码的词法、控制结构或数据流级别的信息用于表示学习，这通常过于粗粒度，并且不能准确地表示程序功能。此外，它可能对各种具有挑战性的设置（如编译器优化和模糊处理）表现出较低的鲁棒性。本文提出了一种通用的解决方案，用于增强基于DNN的二进制代码函数搜索中排名前-$k$的候选者。关键思想是设计一种低成本、全面的等价性检查，快速暴露目标函数与其匹配的顶级函数之间的功能偏差。通过等价性检查的函数可以从排名前-$k$的列表中删除，通过检查的函数也可以重新访问，以一种深思熟虑的方式在排名前-$k$的候选函数中继续前进。我们使用欠约束符号执行（USE）设计了一个实用高效的等价检查，名为BinUSE。USE是符号执行的一种变体，它通过直接从函数入口点启动符号执行并放松对函数参数的约束来提高可伸缩性。它消除了路径爆炸和成本高昂的约束所带来的开销。BinUSE专门设计用于提供汇编函数级等效检查，通过以低成本减少误报来增强基于DNN的二进制代码搜索。我们的评估表明，当面临不同编译器、优化、模糊处理和架构带来的挑战时，BinUSE可以全面有效地增强四种最先进的基于DNN的二进制代码搜索工具。,逆向工程，符号执行，软件相似性，深度学习,,,
VM6RZCZG,2023,https://doi.org/10.1109/TSE.2023.3315292,TSE 2023,K-ST: A Formal Executable Semantics of the Structured Text Language for PLCs,"Programmable Logic Controllers (PLCs) are responsible for automating process control in many industrial systems (e.g. in manufacturing and public infrastructure), and thus it is critical to ensure that they operate correctly and safely. The majority of PLCs are programmed in languages such as Structured Text (ST). However, a lack of formal semantics makes it difficult to ascertain the correctness of their translators and compilers, which vary from vendor-to-vendor. In this work, we develop K-ST, a formal executable semantics for ST in the $\boldsymbol{\mathbb{K}}$ framework. Defined with respect to the IEC 61131-3 standard and PLC vendor manuals, K-ST is a high-level reference semantics that can be used to evaluate the correctness and consistency of different ST implementations. We validate K-ST by executing 567 ST programs extracted from GitHub and comparing the results against existing commercial compilers (i.e., CODESYS, CX-Programmer, and GX Works2). We then apply K-ST to validate the implementation of the open source OpenPLC platform, comparing the executions of several test programs to uncover five bugs and nine functional defects in the compiler.","Formal executable semantics,PLC programming,Structured text, $\boldsymbol{\mathbb{K}}$    K      framework,OpenPLC",K-ST:PLC结构化文本语言的形式可执行语义,可编程逻辑控制器（PLC）负责许多工业系统（如制造业和公共基础设施）中的自动化过程控制，因此确保其正确安全运行至关重要。大多数PLC都是用结构化文本（ST）等语言编程的。然而，由于缺乏形式语义，很难确定翻译器和编译器的正确性，因为供应商不同。在这项工作中，我们开发了K-ST，这是$\boldsymbol｛\mathbb｛K｝｝$框架中ST的一种正式可执行语义。根据IEC 61131-3标准和PLC供应商手册定义，K-ST是一种高级参考语义，可用于评估不同ST实现的正确性和一致性。我们通过执行从GitHub中提取的567个ST程序来验证K-ST，并将结果与现有的商业编译器（即CODESYS、CX Programmer和GX Works2）进行比较。然后，我们应用K-ST验证了开源OpenPLC平台的实现，比较了几个测试程序的执行情况，发现了编译器中的五个错误和九个功能缺陷。,形式可执行语义，PLC编程，结构化文本，$\boldsign{\mathbb{K}}$K框架，OpenPLC,,,
33U5IYLP,2023,https://doi.org/10.1109/TSE.2022.3148258,TSE 2023,"Runtime Permission Issues in Android Apps: Taxonomy, Practices, and Ways Forward","Android introduces a new permission model that allows apps to request permissions at runtime rather than at the installation time since 6.0 (Marshmallow, API level 23). While this runtime permission model provides users with greater flexibility in controlling an app's access to sensitive data and system features, it brings new challenges to app development. First, as users may grant or revoke permissions at any time while they are using an app, developers need to ensure that the app properly checks and requests required permissions before invoking any permission-protected APIs. Second, Android's permission mechanism keeps evolving and getting customized by device manufacturers. Developers are expected to comprehensively test their apps on different Android versions and device models to make sure permissions are properly requested in all situations. Unfortunately, these requirements are often impractical for developers. In practice, many Android apps suffer from various runtime permission issues (ARP issues). While existing studies have explored ARP issues, the understanding of such issues is still preliminary. To better characterize ARP issues, we performed an empirical study using 135 Stack Overflow posts that discuss ARP issues and 199 real ARP issues archived in popular open-source Android projects on GitHub. Via analyzing the data, we observed 11 types of ARP issues that commonly occur in Android apps. For each type of issues, we systematically studied: (1) how they can be manifested, (2) how pervasive and serious they are in real-world apps, and (3) how they can be fixed. We also analyzed the evolution trend of different types of issues from 2015 to 2020 to understand their impact on the Android ecosystem. Furthermore, we conducted a field survey and in-depth interviews among the practitioners from open-source community and industry, to gain insights from practitioners’ practices and learn their requirements of tools that can help combat ARP issues. Finally, to understand the strengths and weaknesses of the existing tools that can detect ARP issues, we built ARPBench, an open benchmark consisting of 94 real ARP issues, and evaluated the performance of three available tools. The experimental results indicate that the existing tools have very limited supports for detecting our observed issue types and report a large number of false alarms. We further analyzed the tools’ limitations and summarized the challenges of designing an effective ARP issue detection technique. We hope that our findings can shed light on future research and provide useful guidance to practitioners.","Runtime permission,Android apps,empirical study",Android应用程序中的运行时权限问题：分类、实践和前进方向,Android引入了一种新的权限模型，允许应用程序在运行时请求权限，而不是从6.0开始在安装时请求权限（Marshmallow，API 23级）。虽然这种运行时权限模型为用户提供了更大的灵活性来控制应用程序对敏感数据和系统功能的访问，但它给应用程序开发带来了新的挑战。首先，由于用户在使用应用程序时可以随时授予或撤销权限，开发人员需要确保应用程序在调用任何受权限保护的API之前正确检查和请求所需的权限。其次，安卓的许可机制不断发展，并被设备制造商定制。开发者将在不同的安卓版本和设备型号上全面测试他们的应用程序，以确保在所有情况下都能正确请求权限。不幸的是，这些要求对于开发人员来说往往不切实际。在实践中，许多安卓应用程序都会遇到各种运行时权限问题（ARP问题）。虽然现有的研究已经探讨了ARP问题，但对这些问题的理解仍然是初步的。为了更好地描述ARP问题，我们使用135篇Stack Overflow帖子进行了一项实证研究，这些帖子讨论了ARP问题，并在GitHub上流行的开源Android项目中存档了199个真实的ARP问题。通过分析数据，我们观察到安卓应用程序中常见的11种ARP问题。对于每种类型的问题，我们都系统地研究了：（1）它们如何表现出来，（2）它们在现实世界的应用程序中有多普遍和严重，以及（3）如何解决它们。我们还分析了2015年至2020年不同类型问题的演变趋势，以了解它们对安卓生态系统的影响。此外，我们对开源社区和行业的从业者进行了实地调查和深入采访，以从从业者的实践中获得见解，并了解他们对有助于解决ARP问题的工具的需求。最后，为了了解现有可以检测ARP问题的工具的优势和劣势，我们构建了ARPBench，这是一个由94个真实ARP问题组成的开放基准，并评估了三个可用工具的性能。实验结果表明，现有工具对检测我们观察到的问题类型的支持非常有限，并报告了大量的假警报。我们进一步分析了这些工具的局限性，并总结了设计一种有效的ARP问题检测技术所面临的挑战。我们希望我们的发现能为未来的研究提供线索，并为从业者提供有用的指导。,运行时权限，Android应用程序，经验研究,,,
XT383J8S,2023,https://doi.org/10.1109/TSE.2022.3231850,TSE 2023,From Inheritance to Mockito: An Automatic Refactoring Approach,"Unit testing focuses on verifying the functions of individual units of a software system. It is challenging due to the high inter dependencies among software units. Developers address this by mocking—replacing the dependency by a “fake” object. Despite the existence of powerful, dedicated mocking frameworks, developers often turn to a “hand-rolled” approach—inheritance. That is, they create a subclass of the dependent class and mock its behavior through method overriding. However, this requires tedious implementation and compromises the design quality of unit tests. This work contributes a fully automated refactoring framework to identify and replace the usage of inheritance by using Mockito—a well received mocking framework. Our approach is built upon the empirical experience from five open source projects that use inheritance for mocking. We evaluate our approach on nine other projects. Results show that our framework is efficient, generally applicable to new datasets, mostly preserves test case behaviors in detecting defects (in the form of mutants), and decouples test code from production code. The qualitative evaluation by experienced developers suggests that the auto-refactoring solutions generated by our framework improve the quality of the unit test cases in various aspects, such as making test conditions more explicit, as well as improved cohesion, readability, understandability, and maintainability with test cases. Finally, we submit 23 pull requests containing our refactoring solutions to the open source projects. It turns our that, 9 requests are accepted/merged, 6 requests are rejected, the remaining requests are pending (5 requests), with unexpected exceptions (2 requests), or undecided (1 request). In particular, among the 21 open source developers that are involved in the reviewing process, 81% give positive votes. This indicates that our refactoring solutions are quite well received by the open source projects and developers.","Software refactoring,software testing,mocking",从继承到Mockito：一种自动重构方法,单元测试的重点是验证软件系统各个单元的功能。由于软件单元之间的高度相互依赖性，这是具有挑战性的。开发人员通过嘲讽来解决这个问题——用“假”对象替换依赖关系。尽管存在强大的、专门的嘲讽框架，但开发人员经常转向“手动”方法——继承。也就是说，他们创建依赖类的子类，并通过方法重写来模拟其行为。然而，这需要繁琐的实现，并且会影响单元测试的设计质量。这项工作提供了一个完全自动化的重构框架，通过使用Mockito来识别和替换继承的使用，Mockito是一个广受欢迎的mocking框架。我们的方法建立在五个使用继承进行嘲讽的开源项目的经验基础上。我们对其他九个项目的方法进行了评估。结果表明，我们的框架是有效的，通常适用于新的数据集，在检测缺陷（以突变体的形式）时大多保留了测试用例行为，并将测试代码与生产代码解耦。经验丰富的开发人员的定性评估表明，我们的框架生成的自动重构解决方案在各个方面提高了单元测试用例的质量，例如使测试条件更加明确，以及提高了测试用例的内聚性、可读性、可理解性和可维护性。最后，我们向开源项目提交了23个包含重构解决方案的pull请求。结果发现，9个请求被接受/合并，6个请求被拒绝，其余请求处于挂起状态（5个请求），有意外异常（2个请求）或未决定状态（1个请求）。特别是，在参与审查过程的21名开源开发人员中，81%的人投了赞成票。这表明我们的重构解决方案受到了开源项目和开发人员的好评。,软件重构，软件测试，模仿,,,
FD5PLLVL,2023,https://doi.org/10.1109/TSE.2022.3225197,TSE 2023,Giving Back: Contributions Congruent to Library Dependency Changes in a Software Ecosystem,"The widespread adoption of third-party libraries for contemporary software development has led to the creation of large inter-dependency networks, where sustainability issues of a single library can have widespread network effects. Maintainers of these libraries are often overworked, relying on the contributions of volunteers to sustain these libraries. To understand these contributions, in this work, we leverage socio-technical techniques to introduce and formalise dependency-contribution congruence (DC congruence) at both ecosystem and library level, i.e., to understand the degree and origins of contributions congruent to dependency changes, analyze whether they contribute to library dormancy (i.e., a lack of activity), and investigate similarities between these congruent contributions compared to typical contributions. We conduct a large-scale empirical study to measure the DC congruence for the npm ecosystem using 1.7 million issues, 970 thousand pull requests (PRs), and over 5.3 million commits belonging to 107,242 npm libraries. We find that the most congruent contributions originate from contributors who can only submit (not commit) to both a client and a library. At the project level, we find that DC congruence shares an inverse relationship with the likelihood that a library becomes dormant. Specifically, a library is less likely to become dormant if the contributions are congruent with upgrading dependencies. Finally, by comparing the source code of contributions, we find statistical differences in the file path and added lines in the source code of congruent contributions when compared to typical contributions. Our work has implications to encourage dependency contributions, especially to support library maintainers in sustaining their projects.","Software ecosystem,dependency changes,npm ecosystem",回馈：软件生态系统中图书馆依赖性变化的贡献,第三方库在当代软件开发中的广泛采用导致了大型相互依赖网络的创建，其中单个库的可持续性问题可能会产生广泛的网络影响。这些图书馆的维护人员经常超负荷工作，依靠志愿者的贡献来维持这些图书馆。为了理解这些贡献，在这项工作中，我们利用社会技术技术在生态系统和图书馆层面引入并正式化依赖-贡献一致性（DC一致性），即了解与依赖变化一致的贡献的程度和来源，分析它们是否有助于图书馆休眠（即缺乏活动），并研究这些一致贡献与典型贡献之间的相似性。我们进行了一项大规模的实证研究，使用属于107242个npm库的170万个问题、97万个拉取请求（PR）和530多万个提交来衡量npm生态系统的DC一致性。我们发现，最一致的贡献源于那些只能向客户端和库提交（而不能提交）的贡献者。在项目级别，我们发现DC同余与库休眠的可能性呈反比。具体来说，如果贡献与升级依赖性一致，那么库就不太可能处于休眠状态。最后，通过比较贡献源代码，我们发现与典型贡献相比，一致贡献源代码的文件路径和添加行存在统计差异。我们的工作有助于鼓励依赖性贡献，特别是支持库维护人员维持他们的项目。,软件生态系统，依赖性变化，NPM生态系统,,,
6FBB9JEN,2023,https://doi.org/10.1109/TSE.2023.3252442,TSE 2023,"Automatically Tagging the ""AAA"" Pattern in Unit Test Cases Using Machine Learning Models","The AAA pattern (i.e., Arrange-Act-Assert) is a common and natural layout to create a test case. Following this pattern in test cases may benefit comprehension, debugging, and maintenance. The AAA structure of real-life test cases, however, may not be clear due to their high complexity. Manually labeling AAA statements in test cases is tedious. Thus, we envision that an automated approach for labeling AAA statements in existing test cases could benefit new developers and projects that practice collective code ownership and test-driven development. This paper contributes an automatic approach based on machine learning models. The “secret sauce” of this approach is a set of three learning features that are based on the semantic, syntax, and context information in test cases, derived from the manual tagging process. Thus, our approach mimics how developers may manually tag the AAA pattern of a test case. We assess the precision, recall, and F-1 score of our approach based on 449 test cases, containing about 16,612 statements, across 4 Apache open source projects. To achieve the best performance in our approach, we explore the usage of six machine learning models; the contribution of the SMOTE data balancing technique; the comparison of the three learning features; and the comparison of five different methods for calculating the semantic feature. The results show our approach is able to identify Arrangement, Action, and Assertion statements with a precision upwards of 92%, and recall up to 74%. We also summarize some experience based on our experiments—regarding the choice of machine learning models, data balancing algorithm, and feature engineering methods—which could potentially provide some reference to related future research.","AAA pattern,feature engineering,machine learning,natural language processing,software testing,unit testing",使用机器学习模型自动标记单元测试用例中的“AAA”模式,AAA模式（即Arrange-Act-Assert）是创建测试用例的常见且自然的布局。在测试用例中遵循此模式可能有利于理解、调试和维护。然而，现实测试用例的AAA结构可能并不清楚，因为它们的复杂性很高。在测试用例中手动标记AAA语句是乏味的。因此，我们设想，在现有测试用例中标记AAA语句的自动化方法可以使新的开发人员和实践集体代码所有权和测试驱动开发的项目受益。本文提出了一种基于机器学习模型的自动方法。这种方法的“秘密酱汁”是一组基于测试用例中的语义、语法和上下文信息的三个学习特征，源于手动标记过程。因此，我们的方法模拟了开发人员如何手动标记测试用例的AAA模式。我们基于4个Apache开源项目中的449个测试用例（包含约16612条语句）来评估方法的准确性、召回率和F-1分数。为了在我们的方法中获得最佳性能，我们探索了六个机器学习模型的使用；SMOTE数据平衡技术的贡献；三种学习特征的比较；以及五种不同的语义特征计算方法的比较。结果表明，我们的方法能够识别安排、行动和断言语句，准确率高达92%，召回率高达74%。我们还总结了一些基于实验的经验——关于机器学习模型、数据平衡算法和特征工程方法的选择——这些经验可能为未来的相关研究提供一些参考。,AAA模式，特征工程，机器学习，自然语言处理，软件测试，单元测试,,,
ARTSQFEH,2023,https://doi.org/10.1109/TSE.2022.3209590,TSE 2023,Effective Isolation of Fault-Correlated Variables via Statistical and Mutation Analysis,"It is a widely-adopted strategy for developers to monitor the values of program variables when debugging in practice. In particular, developers often set breakpoints at specific locations or execute the program step by step in the debugging mode to inspect if abnormal values or status will be observed for concerned variables. Such a practical debugging strategy can facilitate developers in understanding and localizing the target fault. This study aims to identify suspicious program variables of a given fault (i.e., denoted as fault-correlated variables) automatically, thus facilitating the debugging activities for developers. To the best of our knowledge, this is the finest granularity in fault localization (FL) so far, which can address the limitations of being coarse-grained as faced by existing FL techniques. However, isolating fault-correlated variables precisely is challenging since there are usually substantially different variables used or defined in a program, and plenty of them are in the same basic block which cannot be well discriminated from each other since they will be either executed or not against the given test suite. To address such challenges, this study presents IsoVar, a two-phase model to isolate fault-correlated variables. Specifically, IsoVar first performs statistical analysis based on variable execution matrices, which is a novel concept proposed in this study, to identify a set of suspicious variables. It then observes the impacts of those variables on the program dynamically after applying subtle mutations at the bytecode level, to further isolate fault-correlated variables. Extensive experiments on Defects4J and Bears demonstrate that IsoVar can outperform state-of-the-art techniques significantly ($13.0\%$ for MAP and $19.3\%$ for MRR). More importantly, we incorporated IsoVar into 11 existing FL techniques as well as 14 automated program repair techniques, and found that IsoVar can significantly boost their performance.","Fault localization,program variables,debugging",通过统计和突变分析有效隔离故障相关变量,在实际调试时，监视程序变量的值是开发人员广泛采用的策略。特别是，开发人员经常在特定位置设置断点，或者在调试模式下一步一步地执行程序，以检查是否会观察到相关变量的异常值或状态。这种实用的调试策略可以帮助开发人员理解和定位目标故障。本研究旨在自动识别给定故障的可疑程序变量（即表示为故障相关变量），从而为开发人员的调试活动提供便利。据我们所知，这是迄今为止故障定位（FL）中最精细的粒度，可以解决现有FL技术所面临的粗粒度限制。然而，精确地隔离故障相关变量是具有挑战性的，因为程序中通常使用或定义有实质上不同的变量，并且其中许多变量位于同一基本块中，不能很好地相互区分，因为它们将针对给定的测试套件执行或不执行。为了应对这些挑战，本研究提出了IsoVar，这是一个分离故障相关变量的两阶段模型。具体来说，IsoVar首先基于变量执行矩阵进行统计分析，这是本研究中提出的一个新概念，以识别一组可疑变量。然后，它在字节码级别应用细微的突变后，动态观察这些变量对程序的影响，以进一步隔离故障相关变量。对Defects4J和Bears的大量实验表明，IsoVar可以显著优于最先进的技术（MAP为13.0\%$，MRR为19.3\%$）。更重要的是，我们将IsoVar纳入了11种现有的FL技术以及14种自动程序修复技术中，发现IsoVar可以显著提高它们的性能。,故障定位，程序变量，调试,,,
QPZAA4U8,2023,https://doi.org/10.1109/TSE.2022.3194188,TSE 2023,How do Developers Really Feel About Bug Fixing? Directions for Automatic Program Repair,"Automatic program repair (APR) is a rapidly advancing field of software engineering that aims to supplement or replace manual bug fixing with an automated tool. For APR to be successfully adopted in industry, it is vital that APR tools respond to developer needs and preferences. However, very little research has considered developers’ general attitudes to APR or developers’ current bug fixing practices (the activity APR aims to replace). This article responds to this gap by reporting on a survey of 386 software developers about their bug finding and fixing practices and experiences, and their instinctive attitudes towards APR. We find that bug finding and fixing is not necessarily as onerous for developers as has often been suggested, being rated as more satisfying than developers’ general work. The fact that developers derive satisfaction and benefit from bug fixing indicates that APR adoption is not as simple as APR replacing an unwanted activity. When it comes to potential APR approaches, we find a strong preference for developers being kept in the loop (for example, choosing between different fixes or validating fixes) as opposed to a fully automated process. This suggests that advances in APR should be careful to consider the agency of the developer, as well as what information is presented to developers alongside fixes. It also indicates that there are key barriers related to trust that would need to be overcome for full scale APR adoption, supported by the fact that even those developers who stated that they were positive about APR listed several caveats and concerns. We find very few statistically significant relationships between particular demographic variables (for example, developer experience, age, education) and key attitudinal variables, suggesting that developers’ instinctive attitudes towards APR are little influenced by experience level but are held widely across the developer community.","Computer bugs,Software,Debugging,Automation,Maintenance engineering,Task analysis,Manuals",开发人员对Bug修复的真实感受如何？程序自动修复说明,自动程序修复（APR）是一个快速发展的软件工程领域，旨在用自动化工具补充或取代手动错误修复。为了在行业中成功采用APR，APR工具响应开发人员的需求和偏好至关重要。然而，很少有研究考虑开发人员对APR的总体态度或开发人员当前的错误修复实践（APR旨在取代的活动）。本文通过对386名软件开发人员的调查来回应这一差距，了解他们的错误发现和修复实践和经验，以及他们对APR的本能态度。我们发现，错误发现和解决对开发人员来说并不一定像人们常说的那样繁重，被认为比开发人员的一般工作更令人满意。开发人员从错误修复中获得满足感和收益的事实表明，采用APR并不像APR取代不需要的活动那么简单。当谈到潜在的APR方法时，我们发现开发人员强烈倾向于保持循环（例如，在不同的修复程序或验证修复程序之间进行选择），而不是完全自动化的过程。这表明，APR的进步应该小心考虑开发人员的代理，以及在修复的同时向开发人员提供哪些信息。它还表明，全面采用APR需要克服与信任有关的关键障碍，即使是那些表示对APR持积极态度的开发商也列出了一些警告和担忧。我们发现，特定的人口统计变量（例如，开发人员经验、年龄、教育）与关键态度变量之间几乎没有统计学上的显著关系，这表明开发人员对APR的本能态度几乎不受经验水平的影响，但在开发人员社区中广泛存在。,计算机错误，软件，调试，自动化，维护工程，任务分析，手册,,,
CVCAWSK6,2023,https://doi.org/10.1109/TSE.2022.3152089,TSE 2023,"Let's Talk With Developers, Not About Developers: A Review of Automatic Program Repair Research","Automatic program repair (APR) offers significant potential for automating some coding tasks. Using APR could reduce the high costs historically associated with fixing code faults and deliver significant benefits to software engineering. Adopting APR could also have profound implications for software developers’ daily activities, transforming their work practices. To realise the benefits of APR it is vital that we consider how developers feel about APR and the impact APR may have on developers’ work. Developing APR tools without consideration of the developer is likely to undermine the success of APR deployment. In this paper, we critically review how developers are considered in APR research by analysing how human factors are treated in 260 studies from Monperrus’s Living Review of APR. Over half of the 260 studies in our review were motivated by a problem faced by developers (e.g., the difficulty associated with fixing faults). Despite these human-oriented motivations, fewer than 7% of the 260 studies included a human study. We looked in detail at these human studies and found their quality mixed (for example, one human study was based on input from only one developer). Our results suggest that software developers are often talked about in APR studies, but are rarely talked with. A more comprehensive and reliable understanding of developer human factors in relation to APR is needed. Without this understanding, it will be difficult to develop APR tools and techniques which integrate effectively into developers’ workflows. We recommend a future research agenda to advance the study of human factors in APR.","Human factors,software development,automatic program repair",让我们与开发者对话，而不是与开发者对话：自动程序修复研究综述,自动程序修复（APR）为一些编码任务的自动化提供了巨大的潜力。使用APR可以减少历史上与修复代码错误相关的高成本，并为软件工程带来显著好处。采用APR也可能对软件开发人员的日常活动产生深远影响，从而改变他们的工作实践。为了实现APR的好处，我们必须考虑开发商对APR的感受以及APR可能对开发商工作产生的影响。在不考虑开发人员的情况下开发APR工具可能会破坏APR部署的成功。在这篇论文中，我们通过分析Monperrus的《APR生活评论》中260项研究中如何处理人为因素，批判性地回顾了在APR研究中如何考虑开发人员。在我们的综述中，超过一半的260项研究是由开发人员面临的问题（例如，修复故障的困难）引起的。尽管有这些以人为本的动机，但在260项研究中，只有不到7%的研究包含了人类研究。我们详细研究了这些人类研究，发现它们的质量参差不齐（例如，一项人类研究仅基于一名开发人员的输入）。我们的研究结果表明，在APR研究中，软件开发人员经常被谈论，但很少被谈论。需要更全面、更可靠地了解与APR相关的开发人员人为因素。如果没有这种理解，就很难开发出能够有效集成到开发人员工作流程中的APR工具和技术。我们建议未来的研究议程，以推进对APR中人为因素的研究。,人为因素，软件开发，自动程序修复,,,
3ZKI6IZL,2023,https://doi.org/10.1109/TSE.2023.3285357,TSE 2023,Enhancing Fault Injection Testing of Service Systems via Fault-Tolerance Bottleneck,"Modern large-scale service systems are usually deployed with redundant components to ensure high dependability in distributed and volatile environments. Fault Injection Testing (FIT) is a popular technique for testing such systems, while the application of FIT to validating the correctness of redundant components remains a challenging task, especially when the system's structural information is unavailable when testing starts. In this study, we refer to a minimum set of faults that, when injected, will cut off all execution paths in a service system as a fault-tolerance bottleneck, and we propose a novel Fault-tolerance Bottleneck driven Fault Injection (FBFI) approach to the exploration and validation of redundant components without prior knowledge of the system's business structure. The core idea of FBFI is to iteratively infer and inject bottlenecks of the business structure constructed so far. In this way, FBFI is able to discover and test redundant components by repeatedly triggering new system behaviors. The effectiveness and efficiency of FBFI is evaluated using two microservice benchmark systems with different deployment scales. The results reveal that FBFI is more practical and cost-effective than random and lineage-driven FIT approaches in testing service systems of high redundancy levels.","Fault injection testing,fault-tolerance bottleneck,redundant component,service system",通过容错瓶颈增强服务系统的故障注入测试,现代大型服务系统通常部署有冗余组件，以确保在分布式和易失性环境中的高可靠性。故障注入测试（FIT）是测试此类系统的一种流行技术，而将FIT应用于验证冗余组件的正确性仍然是一项具有挑战性的任务，尤其是当测试开始时系统的结构信息不可用时。在这项研究中，我们将注入时会切断服务系统中所有执行路径的最小故障集称为容错瓶颈，并提出了一种新的容错瓶颈驱动的故障注入（FBFI）方法，用于在不事先了解系统业务结构的情况下探索和验证冗余组件。FBFI的核心思想是迭代地推断和注入迄今为止构建的业务结构的瓶颈。通过这种方式，FBFI能够通过反复触发新的系统行为来发现和测试冗余组件。使用两个不同部署规模的微服务基准系统来评估FBFI的有效性和效率。结果表明，在测试高冗余级别的服务系统时，FBFI比随机和谱系驱动的FIT方法更实用、更具成本效益。,故障注入测试，容错瓶颈，冗余组件，服务系统,,,
TSH2DM76,2023,https://doi.org/10.1109/TSE.2023.3265362,TSE 2023,Specializing Neural Networks for Cryptographic Code Completion Applications,"Similarities between natural languages and programming languages have prompted researchers to apply neural network models to software problems, such as code generation and repair. However, program-specific characteristics pose unique prediction challenges that require the design of new and specialized neural network solutions. In this work, we identify new prediction challenges in application programming interface (API) completion tasks and find that existing solutions are unable to capture complex program dependencies in program semantics and structures. We design a new neural network model Multi-HyLSTM to overcome the newly identified challenges and comprehend complex dependencies between API calls. Our neural network is empowered with a specialized dataflow analysis to extract multiple global API dependence paths for neural network predictions. We evaluate Multi-HyLSTM on 64,478 Android Apps and predict 774,460 Java cryptographic API calls that are usually challenging for developers to use correctly. Our Multi-HyLSTM achieves an excellent top-1 API completion accuracy at 98.99%. Moreover, we show the effectiveness of our design choices through an ablation study and have released our dataset.","API completion,neural networks,program dependencies",专门用于密码完成应用的神经网络,自然语言和编程语言之间的相似性促使研究人员将神经网络模型应用于软件问题，如代码生成和修复。然而，程序特定的特性带来了独特的预测挑战，需要设计新的专门的神经网络解决方案。在这项工作中，我们确定了应用程序编程接口（API）完成任务中的新预测挑战，并发现现有的解决方案无法捕获程序语义和结构中复杂的程序依赖性。我们设计了一个新的神经网络模型Multi-HyLSTM，以克服新发现的挑战，并理解API调用之间的复杂依赖关系。我们的神经网络通过专门的数据流分析来提取用于神经网络预测的多个全局API依赖路径。我们在64478个Android应用程序上评估了Multi-HyLSTM，并预测了774460个Java加密API调用，这些调用通常对开发人员的正确使用具有挑战性。我们的Multi-HyLSTM在API完成精度方面达到了98.99%的前1名。此外，我们通过消融研究展示了我们设计选择的有效性，并发布了我们的数据集。,API完成，神经网络，程序依赖,,,
CZPY2H5Z,2023,https://doi.org/10.1109/TSE.2022.3171295,TSE 2023,Fragment-Based Test Generation for Web Apps,"Automated model-based test generation presents a viable alternative to the costly manual test creation currently employed for regression testing of web apps. However, existing model inference techniques rely on threshold-based whole-page comparison to establish state equivalence, which cannot reliably identify near-duplicate web pages in modern web apps. Consequently, existing techniques produce inadequate models for dynamic web apps, and fragile test oracles, rendering the generated regression test suites ineffective. We propose a model-based test generation technique, FragGen, that eliminates the need for thresholds, by employing a novel state abstraction based on page fragmentation to establish state equivalence. FragGen also uses fine-grained page fragment analysis to diversify state exploration and generate reliable test oracles. Our evaluation shows that FragGen outperforms existing whole-page techniques by detecting more near-duplicates, inferring better web app models and generating test suites that are better suited for regression testing. On a dataset of 86,165 state-pairs, FragGen detected 123% more near-duplicates on average compared to whole-page techniques. The crawl models inferred by FragGen have 62% more precision and 70% more recall on average. FragGen also generates reliable regression test suites with test actions that have nearly 100% success rate on the same version of the web app even if the execution environment is varied. The test oracles generated by FragGen can detect 98.7% of the visible changes in web pages while being highly robust, making them suitable for regression testing.","Automatic web app exploration,software testing,state abstraction,test generation,web application model inference,web application crawling,web page - state abstraction and equivalence,web testing",基于片段的Web应用程序测试生成,基于模型的自动测试生成为目前用于web应用程序回归测试的昂贵的手动测试创建提供了一种可行的替代方案。然而，现有的模型推理技术依赖于基于阈值的整页比较来建立状态等价，这不能可靠地识别现代网络应用程序中接近重复的网页。因此，现有技术为动态web应用程序和脆弱的测试预言库生成了不充分的模型，导致生成的回归测试套件无效。我们提出了一种基于模型的测试生成技术FragGen，它通过使用一种基于页面碎片的新状态抽象来建立状态等价性，从而消除了对阈值的需求。FragGen还使用细粒度的页面碎片分析来实现状态探索的多样化，并生成可靠的测试预言器。我们的评估表明，FragGen通过检测更多的近似重复、推断更好的web应用程序模型以及生成更适合回归测试的测试套件，优于现有的整页技术。在由86165个状态对组成的数据集上，FragGen检测到的近似重复比整页技术平均多123%。FragGen推断的爬行模型平均精度高出62%，召回率高出70%。FragGen还生成了可靠的回归测试套件，即使执行环境不同，测试操作在同一版本的web应用程序上的成功率也接近100%。FragGen生成的测试预言器可以检测网页中98.7%的可见变化，同时具有高度的鲁棒性，适合进行回归测试。,自动Web应用探索，软件测试，状态抽象，测试生成，Web应用模型推理，Web应用爬行，Web页面状态抽象和等价性，Web测试,,,
6PUPYA5X,2023,https://doi.org/10.1109/TSE.2022.3184842,TSE 2023,Scalable and Accurate Test Case Prioritization in Continuous Integration Contexts,"Continuous Integration (CI) requires efficient regression testing to ensure software quality without significantly delaying its CI builds. This warrants the need for techniques to reduce regression testing time, such as Test Case Prioritization (TCP) techniques that prioritize the execution of test cases to detect faults as early as possible. Many recent TCP studies employ various Machine Learning (ML) techniques to deal with the dynamic and complex nature of CI. However, most of them use a limited number of features for training ML models and evaluate the models on subjects for which the application of TCP makes little practical sense, due to their small regression testing time and low number of failed builds. In this work, we first define, at a conceptual level, a data model that captures data sources and their relations in a typical CI environment. Second, based on this data model, we define a comprehensive set of features that covers all features previously used by related studies. Third, we develop methods and tools to collect the defined features for 25 open-source software systems with enough failed builds and whose regression testing takes at least five minutes. Fourth, relying on the collected dataset containing a comprehensive feature set, we answer four research questions concerning data collection time, the effectiveness of ML-based TCP, the impact of the features on effectiveness, the decay of ML-based TCP models over time, and the trade-off between data collection time and the effectiveness of ML-based TCP techniques.","Machine learning,software testing,test case prioritization,test case selection,continuous integration",连续集成环境中可扩展且准确的测试用例优先级,持续集成（CI）需要有效的回归测试来确保软件质量，而不会显著延迟其CI构建。这就保证了对减少回归测试时间的技术的需求，例如测试用例优先级排序（TCP）技术，该技术对测试用例的执行进行优先级排序，以尽早检测故障。最近的许多TCP研究都使用了各种机器学习（ML）技术来处理CI的动态和复杂性质。然而，由于其回归测试时间短，失败构建的次数少，大多数研究都使用有限的特征来训练ML模型，并在TCP应用几乎没有实际意义的对象上评估模型。在这项工作中，我们首先在概念层面定义了一个数据模型，该模型在典型的CI环境中捕获数据源及其关系。其次，基于这个数据模型，我们定义了一组全面的特征，涵盖了相关研究之前使用的所有特征。第三，我们开发了方法和工具来收集25个开源软件系统的定义特征，这些系统有足够多的失败构建，其回归测试至少需要5分钟。第四，基于收集的包含综合特征集的数据集，我们回答了四个研究问题，涉及数据收集时间、基于ML的TCP的有效性、特征对有效性的影响、基于ML TCP模型随时间的衰减，以及数据收集时间和基于ML TCP技术的有效性之间的权衡。,机器学习，软件测试，测试用例优先排序，测试用例选择，持续集成,,,
39529AB7,2023,https://doi.org/10.1109/TSE.2023.3234206,TSE 2023,How to Find Actionable Static Analysis Warnings: A Case Study With FindBugs,"Automatically generated static code warnings suffer from a large number of false alarms. Hence, developers only take action on a small percent of those warnings. To better predict which static code warnings should not be ignored, we suggest that analysts need to look deeper into their algorithms to find choices that better improve the particulars of their specific problem. Specifically, we show here that effective predictors of such warnings can be created by methods that locally adjust the decision boundary (between actionable warnings and others). These methods yield a new high water-mark for recognizing actionable static code warnings. For eight open-source Java projects (cassandra, jmeter, commons, lucene-solr, maven, ant, tomcat, derby) we achieve perfect test results on 4/8 datasets and, overall, a median AUC (area under the true negatives, true positives curve) of 92%.","Software analytics,static analysis,false alarms,locality,hyperparameter optimization",如何查找可操作的静态分析警告：以FindBugs为例,自动生成的静态代码警告会出现大量错误警报。因此，开发人员只对这些警告中的一小部分采取行动。为了更好地预测哪些静态代码警告不应被忽视，我们建议分析师需要深入研究他们的算法，以找到更好地改善特定问题细节的选择。具体来说，我们在这里表明，可以通过局部调整决策边界（可操作警告和其他警告之间）的方法来创建此类警告的有效预测因子。这些方法为识别可操作的静态代码警告提供了一个新的高水位线。对于八个开源Java项目（cassandra、jmeter、commons、lucene solr、maven、ant、tomcat、derby），我们在4/8数据集上获得了完美的测试结果，总体而言，AUC（真阴性、真阳性曲线下的面积）中值为92%。,软件分析，静态分析，错误警报，局部性，超参数优化,,,
LV8W4FGA,2023,https://doi.org/10.1109/TSE.2023.3285787,TSE 2023,Mobile App Crowdsourced Test Report Consistency Detection via Deep Image-and-Text Fusion Understanding,"Crowdsourced testing, as a distinct testing paradigm, has attracted much attention in software testing, especially in mobile application (app) testing field. Compared with in-house testing, crowdsourced testing shows superiority with the diverse testing environments when faced with the mobile testing fragmentation problem. However, crowdsourced testing also encounters the low-quality test report problem caused by unprofessional crowdworkers involved with different expertise. In order to handle the submitted reports of uneven quality, app developers have to distinguish high-quality reports from low-quality ones to help the bug inspection. One kind of typical low-quality test report is inconsistent test reports, which means the textual descriptions are not focusing on the attached bug-occurring screenshots. According to our empirical survey, only 18.07% crowdsourced test reports are consistent. Inconsistent reports cause waste on mobile app testing. To solve the inconsistency problem, we propose ReCoDe to detect the consistency of crowdsourced test reports via deep image-and-text fusion understanding. ReCoDe is a two-stage approach that first classifies the reports based on textual descriptions into different categories according to the bug feature. In the second stage, ReCoDe has a deep understanding of the GUI image features of the app screenshots and then applies different strategies to handle different types of bugs to detect the consistency of the crowdsourced test reports. We conduct an experiment on a dataset with over 22 k test reports to evaluate ReCoDe, and the results show the effectiveness of ReCoDe in detecting the consistency of crowdsourced test reports. Besides, a user study is conducted to prove the practical value of ReCoDe in effectively helping app developers improve the efficiency of reviewing the crowdsourced test reports.","Crowdsourced testing,image-and-text fusion understanding,report consistency detection",通过深度图像和文本融合理解移动应用程序众包测试报告一致性检测,众包测试作为一种独特的测试模式，在软件测试领域，尤其是在移动应用程序测试领域受到了广泛关注。与内部测试相比，当面临移动测试碎片化问题时，众包测试在多样化的测试环境中显示出优势。然而，众包测试也遇到了由不专业的众包人员参与不同专业知识所导致的低质量测试报告问题。为了处理提交的质量参差不齐的报告，应用程序开发人员必须区分高质量报告和低质量报告，以帮助检查错误。一种典型的低质量测试报告是不一致的测试报告，这意味着文本描述没有集中在所附的错误发生屏幕截图上。根据我们的实证调查，只有18.07%的众包测试报告是一致的。不一致的报告导致移动应用程序测试浪费。为了解决不一致性问题，我们提出了ReCoDe，通过深度的图像和文本融合理解来检测众包测试报告的一致性。ReCoDe是一种分两阶段的方法，首先根据错误特征将基于文本描述的报告分类为不同的类别。在第二阶段，ReCoDe深入了解了应用程序屏幕截图的GUI图像功能，然后应用不同的策略来处理不同类型的错误，以检测众包测试报告的一致性。我们在拥有超过22k测试报告的数据集上进行了一项实验，以评估ReCoDe，结果显示了ReCoDe在检测众包测试报告一致性方面的有效性。此外，还进行了一项用户研究，以证明ReCoDe在有效帮助应用程序开发人员提高众包测试报告审查效率方面的实际价值。,众包测试，图文融合理解，报告一致性检测,,,
F3ECDRKY,2023,https://doi.org/10.1109/TSE.2023.3275380,TSE 2023,Learning the Relation Between Code Features and Code Transforms With Structured Prediction,"To effectively guide the exploration of the code transform space for automated code evolution techniques, we present in this article the first approach for structurally predicting code transforms at the level of AST nodes using conditional random fields (CRFs). Our approach first learns offline a probabilistic model that captures how certain code transforms are applied to certain AST nodes, and then uses the learned model to predict transforms for arbitrary new, unseen code snippets. Our approach involves a novel representation of both programs and code transforms. Specifically, we introduce the formal framework for defining the so-called AST-level code transforms and we demonstrate how the CRF model can be accordingly designed, learned, and used for prediction. We instantiate our approach in the context of repair transform prediction for Java programs. Our instantiation contains a set of carefully designed code features, deals with the training data imbalance issue, and comprises transform constraints that are specific to code. We conduct a large-scale experimental evaluation based on a dataset of bug fixing commits from real-world Java projects. The results show that when the popular evaluation metric top-3 is used, our approach predicts the code transforms with an accuracy varying from 41% to 53% depending on the transforms. Our model outperforms two baselines based on history probability and neural machine translation (NMT), suggesting the importance of considering code structure in achieving good prediction accuracy. In addition, a proof-of-concept synthesizer is implemented to concretize some repair transforms to get the final patches. The evaluation of the synthesizer on the Defects4j benchmark confirms the usefulness of the predicted AST-level repair transforms in producing high-quality patches.","Code transform,big code,machine learning,program repair",用结构化预测学习代码特征与代码变换之间的关系,为了有效地指导自动代码进化技术对代码转换空间的探索，我们在本文中提出了第一种使用条件随机场（CRFs）在AST节点级别结构预测代码转换的方法。我们的方法首先离线学习一个概率模型，该模型捕捉特定代码转换如何应用于特定AST节点，然后使用学习的模型预测任意新的、看不见的代码片段的转换。我们的方法涉及程序和代码转换的新颖表示。具体来说，我们介绍了定义所谓AST级代码转换的正式框架，并演示了如何相应地设计、学习CRF模型并将其用于预测。我们在Java程序的修复转换预测的上下文中实例化我们的方法。我们的实例化包含一组精心设计的代码特性，处理训练数据不平衡问题，并包括特定于代码的转换约束。我们基于真实世界Java项目的错误修复提交数据集进行了大规模的实验评估。结果表明，当使用流行的评估度量top-3时，我们的方法预测代码转换的准确率从41%到53%不等，具体取决于转换。我们的模型优于基于历史概率和神经机器翻译（NMT）的两个基线，表明考虑代码结构对于实现良好的预测精度的重要性。此外，还实现了一个概念验证合成器，以具体化一些修复转换，从而获得最终的补丁。合成器在Defects4j基准上的评估证实了预测的AST水平修复变换在产生高质量补丁中的有用性。,代码转换，大代码，机器学习，程序修复,,,
JJRFHGGM,2023,https://doi.org/10.1109/TSE.2022.3178096,TSE 2023,Towards Automatically Localizing Function Errors in Mobile Apps With User Reviews,"Removing all function errors is critical for making successful mobile apps. Since app testing may miss some function errors given limited time and resource, the user reviews of mobile apps are very important to developers for learning the uncaught errors. Unfortunately, manually handling each review is time-consuming and even error-prone. Existing studies on mobile apps’ reviews could not help developers effectively locate the problematic code according to the reviews, because the majority of such research focus on review classification, requirements engineering, sentiment analysis, and summarization [1]. They do not localize the function errors described in user reviews in apps’ code. Moreover, recent studies on mapping reviews to problematic source files look for the matching between the words in reviews and that in source code, bug reports, commit messages, and stack traces, thus may result in false positives and false negatives since they do not consider the semantic meaning and part of speech tag of each word. In this paper, we propose a novel approach to localize function errors in mobile apps by exploiting the context information in user reviews and correlating the reviews and bytecode through their semantic meanings. We realize our new approach as a tool named ReviewSolver, and carefully evaluate it with reviews of real apps. The experimental result shows that ReviewSolver has much better performance than the state-of-the-art tools (i.e., ChangeAdvisor and Where2Change).","Function error localization,user reviews,mobile apps",基于用户评价的移动应用程序中功能错误的自动定位,删除所有功能错误对于制作成功的移动应用程序至关重要。由于在时间和资源有限的情况下，应用程序测试可能会错过一些功能错误，因此移动应用程序的用户评论对于开发人员学习未捕获的错误非常重要。不幸的是，手动处理每次审查都很耗时，甚至容易出错。现有的关于移动应用程序评论的研究无法帮助开发人员根据评论有效地定位有问题的代码，因为大多数此类研究都集中在评论分类、需求工程、情绪分析和总结[1]。他们不会本地化应用程序代码中用户评论中描述的功能错误。此外，最近关于将评论映射到有问题的源文件的研究寻找评论中的单词与源代码、错误报告、提交消息和堆栈跟踪中的单词之间的匹配，因此可能会导致误报和误报，因为它们没有考虑每个单词的语义和词性标签。在本文中，我们提出了一种新的方法来定位移动应用程序中的功能错误，方法是利用用户评论中的上下文信息，并通过其语义将评论和字节码关联起来。我们将我们的新方法作为一个名为ReviewSolver的工具来实现，并通过对真实应用程序的评论来仔细评估它。实验结果表明，ReviewSolver比最先进的工具（即ChangeAdvisor和Where2Change）具有更好的性能。,功能错误本地化，用户评论，移动应用程序,,,
WS9GFYZK,2023,https://doi.org/10.1109/TSE.2022.3156071,TSE 2023,Off to a Good Start: Dynamic Contribution Patterns and Technical Success in an OSS Newcomer's Early Career,"Attracting and retaining newcomers are critical aspects for OSS projects, as such projects rely on newcomers’ sustainable contributions. Considerable effort has been made to help newcomers by identifying and overcoming the barriers during the onboarding process. However, most newcomers eventually fail and drop out of their projects even after successful onboarding. Meanwhile, it has been long known that individuals’ early career stages profoundly impact their long-term career success. However, newcomers’ early careers are less investigated in SE research. In this paper, we sought to develop an empirical understanding of the relationships between newcomers’ dynamic contribution patterns in their early careers and their technical success. To achieve this goal, we compiled a dataset of newcomers’ contribution data from 54 large OSS projects under three different ecosystems and analyzed it with time series analysis and other statistical analysis techniques. Our analyses yield rich findings. The correlations between several contribution patterns and technical success were identified. In general, being consistent and persistent in newcomers’ early careers is positively associated with their technical success. While these correlations generally hold in all three ecosystems, we observed some differences in detailed contribution patterns correlated with technical success across ecosystems. In addition, we performed a case study to investigate whether another type of contributions, i.e., documentation contribution, could potentially have positive correlations with newcomers’ technical success. We discussed the implications and summarized practical recommendations to OSS newcomers. The insights gained from this work demonstrated the necessity of extending the focus of research and practice to newcomers’ early careers and hence shed light on future research in this direction.","Dynamic contribution pattern,early career,newcomer,open source,technical success",良好开端：OSS新人早期职业生涯中的动态贡献模式与技术成功,吸引和留住新来者是开放源码软件项目的关键方面，因为这些项目依赖于新来者的可持续贡献。已经做出了相当大的努力，通过识别和克服入职过程中的障碍来帮助新来者。然而，大多数新人最终都会失败，甚至在成功入职后就退出了他们的项目。同时，人们早就知道，个人职业生涯的早期阶段会对他们的长期职业成功产生深远影响。然而，在SE研究中，对新人早期职业生涯的调查较少。在这篇论文中，我们试图从经验上理解新来者在早期职业生涯中的动态贡献模式与他们的技术成功之间的关系。为了实现这一目标，我们汇编了来自三个不同生态系统下54个大型OSS项目的新来者贡献数据集，并用时间序列分析和其他统计分析技术对其进行了分析。我们的分析得出了丰富的结论。确定了几种贡献模式与技术成功之间的相关性。总的来说，在新人的早期职业生涯中保持一致和坚持不懈与他们的技术成功呈正相关。虽然这些相关性通常适用于所有三个生态系统，但我们观察到，与生态系统技术成功相关的详细贡献模式存在一些差异。此外，我们进行了一项案例研究，以调查另一种类型的贡献，即文档贡献，是否可能与新来者的技术成功呈正相关。我们讨论了这些影响，并总结了对开放源码软件新成员的实际建议。从这项工作中获得的见解表明，有必要将研究和实践的重点扩展到新来者的早期职业生涯，从而为未来这一方向的研究提供线索。,动态贡献模式，职业早期，新人，开源，技术成功,,,
4LDJ3N84,2023,https://doi.org/10.1109/TSE.2022.3217544,TSE 2023,Dynamic Human-in-the-Loop Assertion Generation,"Test cases use assertions to check program behaviour. While these assertions may not be complex, they are themselves code that must be written correctly in order to determine whether a test case should pass or fail. We claim that most test assertions are relatively repetitive and straight-forward, making their construction well suited to automation and that this automation can reduce developer effort while improving assertion quality. Examining 33,873 assertions from 105 projects revealed that developer-written assertions fall into twelve high-level categories, confirming that the vast majority ($&gt;$90%) of test assertions are fairly simple in practice. We created AutoAssert, a human-in-the-loop tool to fit naturally into a developer's test-writing workflow by automatically generating assertions for JavaScript and TypeScript test cases. A developer invokes AutoAssert by identifying the variable they want validated; AutoAssert uses dynamic analysis to generate assertions relevant for this variable and its runtime values, injecting the assertions into the test case for the developer to accept, modify, delete. Comparing AutoAssert's assertions to those written by developers, we found that the assertions generated by AutoAssert are the same kind of assertion as was written by developers 84% of the time in a sample of over 1,000 assertions. Additionally we validated the utility of AutoAssert-generated assertions with 17 developers who found the majority of generated assertions to be useful and expressed considerable interest in using such a tool for their own projects.","Complexity theory,Codes,Semantics,Human in the loop,Testing,Runtime,Libraries",动态人在环断言生成,测试用例使用断言来检查程序行为。虽然这些断言可能并不复杂，但它们本身就是必须正确编写的代码，以便确定测试用例应该通过还是失败。我们声称，大多数测试断言都是相对重复和直接的，这使得它们的构建非常适合自动化，这种自动化可以减少开发人员的工作量，同时提高断言质量。通过检查105个项目中的33873个断言，发现开发人员编写的断言分为12个高级类别，证实了绝大多数（$&gt；$90%）测试断言在实践中相当简单。我们创建了AutoAssert，这是一个循环中的人工工具，通过自动生成JavaScript和TypeScript测试用例的断言，可以自然地融入开发人员的测试编写工作流程。开发人员通过识别他们想要验证的变量来调用AutoAssert；AutoAssert使用动态分析生成与该变量及其运行时值相关的断言，将断言注入测试用例中，供开发人员接受、修改和删除。将AutoAssert的断言与开发人员编写的断言进行比较，我们发现，在1000多个断言的样本中，AutoAssert生成的断言与开发者编写的断言在84%的时间内是相同的。此外，我们还与17名开发人员一起验证了AutoAssert生成断言的实用性，他们发现大多数生成断言都很有用，并表示有兴趣在自己的项目中使用这种工具。,复杂性理论，代码，语义学，循环中的人，测试，运行时，库,,,
YGE9B8KB,2023,https://doi.org/10.1109/TSE.2022.3160155,TSE 2023,ErrHunter: Detecting Error-Handling Bugs in the Linux Kernel Through Systematic Static Analysis,"Error handling is essential for operating systems, thus, there are many bugs in error-handling code, which could result in serious consequences. In this paper, we revisit the problem of error miss-handling bugs and analyze the root cause of the most common ones in the Linux kernel. Based on the analysis, we propose a systematic static taint-analysis-based approach, ErrHunter, to detect multiple kinds of error miss-handling bugs in the Linux kernel. An automated critical variable identification approach is proposed to identify critical variables in the error-handling paths. A static cross-control-flow taint analysis approach is proposed to construct critical-variable control flow graphs (CCFGs), which describe the processing of critical variables in separate control flows. Based on the CCFGs, ErrHunter can target the root cause of the most common error miss-handling bugs and detect the bugs in a systematic way. ErrHunter is designed for kernel bug detection, so it can handle many specific features of the Linux kernel, such as memory management mechanisms, etc.","Error handling bugs,static taint analysis,cross-control-flow analysis,bug detection",ErrHunter：通过系统静态分析检测Linux内核中的错误处理错误,错误处理对操作系统至关重要，因此，错误处理代码中存在许多错误，可能会导致严重后果。在本文中，我们重新审视了错误遗漏处理错误的问题，并分析了Linux内核中最常见错误的根本原因。在此基础上，我们提出了一种基于静态污染分析的系统方法ErrHunter，用于检测Linux内核中的多种错误遗漏处理错误。提出了一种自动关键变量识别方法来识别错误处理路径中的关键变量。提出了一种静态交叉控制流污染分析方法来构造临界变量控制流图（CCFGs），该图描述了临界变量在单独控制流中的处理。基于CCFG，ErrHunter可以针对最常见的错误处理错误的根本原因，并以系统的方式检测错误。ErrHunter是为内核错误检测而设计的，因此它可以处理Linux内核的许多特定功能，例如内存管理机制等。,错误处理错误，静态污点分析，交叉控制流分析，错误检测,,,
JP7DWMHD,2023,https://doi.org/10.1109/TSE.2023.3240118,TSE 2023,Challenging Machine Learning-Based Clone Detectors via Semantic-Preserving Code Transformations,"Software clone detection identifies similar or identical code snippets. It has been an active research topic that attracts extensive attention over the last two decades. In recent years, machine learning (ML) based detectors, especially deep learning-based ones, have demonstrated impressive capability on clone detection. It seems that this longstanding problem has already been tamed owing to the advances in ML techniques. In this work, we would like to challenge the robustness of the recent ML-based clone detectors through code semantic-preserving transformations. We first utilize fifteen simple code transformation operators combined with commonly-used heuristics (i.e., Random Search, Genetic Algorithm, and Markov Chain Monte Carlo) to perform equivalent program transformation. Furthermore, we propose a deep reinforcement learning-based sequence generation (DRLSG) strategy to effectively guide the search process of generating clones that could escape from the detection. We then evaluate the ML-based detectors with the pairs of original and generated clones. We realize our method in a framework named CloneGen (stands for Clone Generator). CloneGen In evaluation, we challenge the three state-of-the-art ML-based detectors and four traditional detectors with the code clones after semantic-preserving transformations via the aid of CloneGen. Surprisingly, our experiments show that, despite the notable successes achieved by existing clone detectors, the ML models inside these detectors still cannot distinguish numerous clones produced by the code transformations in CloneGen. In addition, adversarial training of ML-based clone detectors using clones generated by CloneGen can improve their robustness and accuracy. Meanwhile, compared with the commonly-used heuristics, the DRLSG strategy has shown the best effectiveness in generating code clones to decrease the detection accuracy of the ML-based detectors. Our investigation reveals an explicable but always ignored robustness issue of the latest ML-based detectors. Therefore, we call for more attention to the robustness of these new ML-based detectors.","Clone detection,code transformaiton,semantic clone,machinae learning",通过语义保持代码转换挑战基于机器学习的克隆检测器,软件克隆检测识别相似或相同的代码片段。在过去的二十年里，它一直是一个活跃的研究课题，引起了广泛的关注。近年来，基于机器学习（ML）的检测器，特别是基于深度学习的检测器，在克隆检测方面表现出了令人印象深刻的能力。由于ML技术的进步，这个长期存在的问题似乎已经得到了解决。在这项工作中，我们想通过代码语义保留转换来挑战最近基于ML的克隆检测器的稳健性。我们首先利用十五个简单的代码转换算子与常用的启发式算法（即随机搜索、遗传算法和马尔可夫链蒙特卡罗）相结合来执行等效程序转换。此外，我们提出了一种基于深度强化学习的序列生成（DRLSG）策略，以有效地指导生成可以逃避检测的克隆的搜索过程。然后，我们用原始和生成的克隆对来评估基于ML的检测器。我们在一个名为CloneGen（代表Clone Generator）的框架中实现了我们的方法。CloneGen在评估中，我们通过CloneGen的帮助，对三个最先进的基于ML的检测器和四个传统检测器进行语义保留转换后的代码克隆提出了挑战。令人惊讶的是，我们的实验表明，尽管现有的克隆检测器取得了显著的成功，但这些检测器中的ML模型仍然无法区分CloneGen中代码转换产生的大量克隆。此外，使用CloneGen生成的克隆对基于ML的克隆检测器进行对抗性训练可以提高其鲁棒性和准确性。同时，与常用的启发式算法相比，DRLSG策略在生成代码克隆以降低基于ML的检测器的检测精度方面表现出了最佳的有效性。我们的研究揭示了最新的基于ML的检测器的一个可解释但总是被忽视的鲁棒性问题。因此，我们呼吁更多地关注这些新的基于ML的检测器的鲁棒性。,克隆检测，代码转换，语义克隆，机器学习,,,
SWTWE7FN,2023,https://doi.org/10.1109/TSE.2022.3150302,TSE 2023,Automatic Detection of Java Cryptographic API Misuses: Are We There Yet?,"The Java platform provides various cryptographic APIs to facilitate secure coding. However, correctly using these APIs is challenging for developers who lack cybersecurity training. Prior work shows that many developers misused APIs and consequently introduced vulnerabilities into their software. To eliminate such vulnerabilities, people created tools to detect and/or fix cryptographic API misuses. However, it is still unknown (1) how current tools are designed to detect cryptographic API misuses, (2) how effectively the tools work to locate API misuses, and (3) how developers perceive the usefulness of tools’ outputs. For this paper, we conducted an empirical study to investigate the research questions mentioned above. Specifically, we first conducted a literature survey on existing tools and compared their approach design from different angles. Then we applied six of the tools to three popularly used benchmarks to measure tools’ effectiveness of API-misuse detection. Next, we applied the tools to 200 Apache projects and sent 57 vulnerability reports to developers for their feedback. Our study revealed interesting phenomena. For instance, none of the six tools was found universally better than the others; however, CogniCrypt, CogniGuard, and Xanitizer outperformed SonarQube. More developers rejected tools’ reports than those who accepted reports (30 versus 9) due to their concerns on tools’ capabilities, the correctness of suggested fixes, and the exploitability of reported issues. This study reveals a significant gap between the state-of-the-art tools and developers’ expectations; it sheds light on future research in vulnerability detection.","Detection of cryptographic API misuses,developers’ feedback,empirical",Java密码API误用的自动检测：我们还存在吗？,Java平台提供了各种加密API来促进安全编码。然而，对于缺乏网络安全培训的开发人员来说，正确使用这些API是一项挑战。先前的工作表明，许多开发人员滥用API，从而在软件中引入漏洞。为了消除此类漏洞，人们创建了检测和/或修复加密API误用的工具。然而，目前尚不清楚（1）当前工具如何设计来检测API密码滥用，（2）工具如何有效地定位API滥用，以及（3）开发人员如何感知工具输出的有用性。在本文中，我们对上述研究问题进行了实证研究。具体而言，我们首先对现有工具进行了文献调查，并从不同角度对其方法设计进行了比较。然后，我们将其中六种工具应用于三个常用的基准测试，以衡量工具对API敏感性检测的有效性。接下来，我们将这些工具应用于200个Apache项目，并向开发人员发送了57份漏洞报告，以征求他们的反馈。我们的研究揭示了有趣的现象。例如，这六种工具中没有一种普遍比其他工具更好；然而，CogniCrypt、CogniGuard和Xanitzer的表现优于SonarQube。由于担心工具的功能、建议修复的正确性以及报告问题的可利用性，拒绝工具报告的开发者比接受报告的开发者多（30比9）。这项研究揭示了最先进的工具与开发人员的期望之间的巨大差距；它为漏洞检测的未来研究提供了启示。,检测加密API滥用，开发人员反馈，经验,,,
HMBDHCR4,2023,https://doi.org/10.1109/TSE.2022.3194640,TSE 2023,FalsifAI: Falsification of AI-Enabled Hybrid Control Systems Guided by Time-Aware Coverage Criteria,"Modern Cyber-Physical Systems (CPSs) that need to perform complex control tasks (e.g., autonomous driving) are increasingly using AI-enabled controllers, mainly based on deep neural networks (DNNs). The quality assurance of such types of systems is of vital importance. However, their verification can be extremely challenging, due to their complexity and uninterpretable decision logic. Falsification is an established approach for CPS quality assurance, which, instead of attempting to prove the system correctness, aims at finding a time-variant input signal violating a formal specification describing the desired behavior; it often employs a search-based testing approach that tries to minimize the robustness of the specification, given by its quantitative semantics. However, guidance provided by robustness is mostly black-box and only related to the system output, but does not allow to understand whether the temporal internal behavior determined by multiple consecutive executions of the neural network controller has been explored sufficiently. To bridge this gap, in this paper, we make an early attempt at exploring the temporal behavior determined by the repeated executions of the neural network controllers in hybrid control systems and first propose eight time-aware coverage criteria specifically designed for neural network controllers in the context of CPS, which consider different features by design: the simple temporal activation of a neuron, the continuous activation of a neuron for a given duration, and the differential neuron activation behavior over time. Second, we introduce a falsification framework, named $\mathtt {FalsifAI}$, that exploits the coverage information for better falsification guidance. Namely, inputs of the controller that increase the coverage (so improving the exploration of the DNN behaviors), are prioritized in the exploitation phase of robustness minimization. Our large-scale evaluation over a total of 3 typical CPS tasks, 6 system specifications, 18 DNN models and more than 12,000 experiment runs, demonstrates 1) the advantage of our proposed technique in outperforming two state-of-the-art falsification approaches, and 2) the usefulness of our proposed time-aware coverage criteria for effective falsification guidance.","Search-based testing,falsification,neural network controllers,coverage criteria,cyber-physical systems",伪造AI：基于时间感知覆盖准则的人工智能混合控制系统的伪造,需要执行复杂控制任务（如自动驾驶）的现代网络物理系统（CPS）越来越多地使用人工智能控制器，主要基于深度神经网络（DNN）。这类系统的质量保证至关重要。然而，由于它们的复杂性和难以解释的决策逻辑，它们的验证可能极具挑战性。伪造是CPS质量保证的一种既定方法，其目的不是试图证明系统的正确性，而是发现违反描述所需行为的形式规范的时变输入信号；它通常采用基于搜索的测试方法，试图最大限度地降低规范的稳健性，这是由其定量语义给出的。然而，鲁棒性提供的指导大多是黑箱的，仅与系统输出有关，但不允许理解由神经网络控制器的多次连续执行确定的时间内部行为是否已经被充分探索。为了弥补这一差距，在本文中，我们早期尝试探索混合控制系统中由神经网络控制器的重复执行决定的时间行为，并首先提出了八个专门为CPS背景下的神经网络控制器设计的时间感知覆盖标准，其通过设计考虑了不同的特征：神经元的简单时间激活、神经元在给定持续时间内的连续激活以及神经元随时间的不同激活行为。其次，我们引入了一个名为$\mathtt｛FalsifAI｝$的伪造框架，该框架利用覆盖信息来更好地指导伪造。也就是说，在鲁棒性最小化的利用阶段，增加覆盖范围（从而改进对DNN行为的探索）的控制器的输入被优先考虑。我们对总共3个典型的CPS任务、6个系统规范、18个DNN模型和12000多个实验运行进行了大规模评估，证明了1）我们提出的技术在优于两种最先进的伪造方法方面的优势，以及2）我们提出了时间感知覆盖标准对有效伪造指导的有用性。,基于搜索的测试，证伪，神经网络控制器，覆盖标准，网络物理系统,,,
R3FFD2II,2023,https://doi.org/10.1109/TSE.2022.3227559,TSE 2023,BEQAIN: An Effective and Efficient Identifier Normalization Approach With BERT and the Question Answering System,"As one of the most important resources to express the semantics of source code, identifiers are usually composed of several common or domain-specific terms and abbreviations, thus heavily hindering developers from analyzing and comprehending source code. Hence, it is very necessary to normalize identifiers, which aims to align the vocabulary found in identifiers with natural language words found in other software artifacts. Even though researchers have proposed several identifier normalization approaches in the literature, these approaches only rely on the lexical information in identifiers and related source code entities to normalize identifiers, suffering from the lack of deep semantic understanding of identifiers. In this paper, we propose an effective and efficient identifier normalization approach BEQAIN to split identifiers into their composing words and expand the enclosed abbreviations. Specifically, BEQAIN employs a deep learning model, which is mainly composed of a Bidirectional Encoder Representation from Transformers (BERT) layer and a Conditional Random Fields (CRF) layer to embed identifiers into low-level vectors and learn the identifier splitting patterns. The BERT-CRF network is also combined with a pre-processing component and a post-processing component to resolve the problems of over-splitting and under-splitting so as to improve the identifier splitting performance. Furthermore, BEQAIN also employs a Question Answering (Q&A) system to learn the abbreviation expansion mappings and leverages the current programming context to determine the exactly correct expansion when there are multiple expansions for specific abbreviations. After BEQAIN is fully trained, it can be used to normalize identifiers. We conduct extensive experiments to validate the effectiveness and efficiency of BEQAIN over two publicly available datasets with nine projects. Experimental results show that BEQAIN achieves the overall average Accuracy of 80.20% and outperforms the existing state-of-the-art approach by 9.88% in normalizing identifiers. The pre-processing and post-processing components could improve the Accuracy of BEQAIN in identifier splitting by 11.70%. Employing the programming context information could improve the Accuracy of BEQAIN in abbreviation expansion by 11.15% on average. In addition, the average normalization time of BEQAIN is less than one second. Finally, we also discuss some observations for the road ahead for identifier normalization to inspire other researchers.","Identifier expansion,identifier normalization,identifier splitting,source code comprehension",BEQAIN:一种有效的基于BERT和问答系统的标识符规范化方法,标识符作为表示源代码语义的最重要资源之一，通常由几个常见或特定领域的术语和缩写组成，从而严重阻碍了开发人员分析和理解源代码。因此，非常有必要规范标识符，目的是将标识符中的词汇表与其他软件工件中的自然语言单词对齐。尽管研究人员在文献中提出了几种标识符规范化方法，但这些方法仅依赖于标识符和相关源代码实体中的词汇信息来规范标识符，缺乏对标识符的深层语义理解。在本文中，我们提出了一种有效的标识符规范化方法BEQAIN，将标识符拆分为其组成词，并扩展所包含的缩写。具体而言，BEQAIN采用了深度学习模型，该模型主要由来自变换器的双向编码器表示（BERT）层和条件随机场（CRF）层组成，以将标识符嵌入到低级向量中并学习标识符分割模式。BERT-CRF网络还与预处理组件和后处理组件相结合，解决了过分割和欠分割的问题，从而提高了标识符分割性能。此外，BEQAIN还采用问答系统来学习缩写扩展映射，并在特定缩写有多个扩展时利用当前编程上下文来确定准确的扩展。BEQAIN经过充分训练后，可以用于规范标识符。我们在九个项目的两个公开可用数据集上进行了大量实验，以验证BEQAIN的有效性和效率。实验结果表明，BEQAIN实现了80.20%的总体平均准确率，在规范标识符方面比现有的最先进方法高9.88%。预处理和后处理组件可以将BEQAIN在标识符拆分中的准确率提高11.70%。使用编程上下文信息可以将BEQA in在缩写扩展中的准确度平均提高11.15%。此外，BEQAIN的平均归一化时间小于1秒。最后，我们还讨论了标识符规范化的一些观察结果，以启发其他研究人员。,标识符扩展，标识符归一化，标识符拆分，源代码理解,,,
J9N3I2EV,2023,https://doi.org/10.1109/TSE.2023.3286586,TSE 2023,Vulnerability Detection by Learning From Syntax-Based Execution Paths of Code,"Vulnerability detection is essential to protect software systems. Various approaches based on deep learning have been proposed to learn the pattern of vulnerabilities and identify them. Although these approaches have shown vast potential in this task, they still suffer from the following issues: (1) It is difficult for them to distinguish vulnerability-related information from a large amount of irrelevant information, which hinders their effectiveness in capturing vulnerability features. (2) They are less effective in handling long code because many neural models would limit the input length, which hinders their ability to represent the long vulnerable code snippets. To mitigate these two issues, in this work, we proposed to decompose the syntax-based Control Flow Graph (CFG) of the code snippet into multiple execution paths to detect the vulnerability. Specifically, given a code snippet, we first build its CFG based on its Abstract Syntax Tree (AST), refer to such CFG as syntax-based CFG, and decompose the CFG into multiple paths from an entry node to its exit node. Next, we adopt a pre-trained code model and a convolutional neural network to learn the path representations with intra- and inter-path attention. The feature vectors of the paths are combined as the representation of the code snippet and fed into the classifier to detect the vulnerability. Decomposing the code snippet into multiple paths can filter out some redundant information unrelated to the vulnerability and help the model focus on the vulnerability features. Besides, since the decomposed paths are usually shorter than the code snippet, the information located in the tail of the long code is more likely to be processed and learned. To evaluate the effectiveness of our model, we build a dataset with over 231 k code snippets, in which there are 24 k vulnerabilities. Experimental results demonstrate that the proposed approach outperforms state-of-the-art baselines by at least 22.30%, 42.92%, and 32.58% in terms of Precision, Recall, and F1-Score, respectively. Our further analysis investigates the reason for the proposed approach's superiority.","Vulnerability detection,deep learning,control flow graph,pre-trained model",通过学习基于语法的代码执行路径进行漏洞检测,漏洞检测对于保护软件系统至关重要。已经提出了基于深度学习的各种方法来学习漏洞模式并识别它们。尽管这些方法在这项任务中显示出了巨大的潜力，但它们仍然存在以下问题：（1）它们很难将与漏洞相关的信息与大量无关的信息区分开来，这阻碍了它们捕捉漏洞特征的有效性。（2） 它们在处理长代码方面效果较差，因为许多神经模型会限制输入长度，这阻碍了它们表示长时间易受攻击的代码片段的能力。为了缓解这两个问题，在这项工作中，我们提出将代码片段的基于语法的控制流图（CFG）分解为多个执行路径来检测漏洞。具体来说，给定一个代码片段，我们首先基于它的抽象语法树（AST）构建它的CFG，将这种CFG称为基于语法的CFG，并将CFG分解为从入口节点到出口节点的多个路径。接下来，我们采用预先训练的代码模型和卷积神经网络来学习具有路径内和路径间注意力的路径表示。路径的特征向量被组合为代码片段的表示，并被输入分类器以检测漏洞。将代码片段分解为多个路径可以过滤掉一些与漏洞无关的冗余信息，并帮助模型关注漏洞特征。此外，由于分解的路径通常比代码片段短，位于长代码尾部的信息更有可能被处理和学习。为了评估我们模型的有效性，我们构建了一个包含超过231k个代码片段的数据集，其中有24k个漏洞。实验结果表明，所提出的方法在精度、召回率和F1分数方面分别优于最先进的基线至少22.30%、42.92%和32.58%。我们的进一步分析调查了所提出的方法优越性的原因。,漏洞检测，深度学习，控制流图，预训练模型,,,
RYJZ7K9M,2023,https://doi.org/10.1109/TSE.2023.3279125,TSE 2023,Context-Aware Neural Fault Localization,"Numerous fault localization techniques identify suspicious statements potentially responsible for program failures by discovering the statistical correlation between test results (i.e., failing or passing) and the executions of the different statements of a program (i.e., covered or not covered). They rarely incorporate a failure context into their suspiciousness evaluation despite the fact that a failure context showing how a failure is produced is useful for analyzing and locating faults. Since a failure context usually contains the transitive relationships among the statements of causing a failure, its relationship complexity becomes one major obstacle for the context incorporation in suspiciousness evaluation of fault localization. To overcome the obstacle, our insight is that leveraging the promising learning ability may be a candidate solution to learn a feasible model for incorporating a failure context into fault localization. Thus, we propose a context-aware neural fault localization approach (CAN). Specifically, CAN represents the failure context by constructing a program dependency graph, which shows how a set of statements interact with each other (i.e., data and control dependencies) to cause a failure. Then, CAN utilizes graph neural networks to analyze and incorporate the context (e.g., the dependencies among the statements) into suspiciousness evaluation. Our empirical results on the 12 large-sized programs show that CAN achieves promising results (e.g., 29.23% faults are ranked within top 5), and it significantly improves the state-of-the-art baselines with a substantial margin.","Fault localization,graph neural networks,program dependecy graphs,suspiciousness",上下文感知神经故障定位,许多故障定位技术通过发现测试结果（即失败或通过）与程序不同语句的执行（即覆盖或未覆盖）之间的统计相关性来识别可能导致程序故障的可疑语句。尽管显示故障是如何产生的故障上下文对分析和定位故障很有用，但他们很少将故障上下文纳入可疑性评估中。由于故障上下文通常包含导致故障的语句之间的传递关系，其关系复杂性成为故障定位可疑性评估中上下文合并的主要障碍之一。为了克服这一障碍，我们的见解是，利用有前景的学习能力可能是学习将故障上下文纳入故障定位的可行模型的候选解决方案。因此，我们提出了一种上下文感知的神经故障定位方法（CAN）。具体而言，CAN通过构建程序依赖关系图来表示故障上下文，该图显示了一组语句如何相互作用（即数据和控制依赖关系）以导致故障。然后，CAN利用图神经网络来分析上下文（例如，语句之间的依赖关系）并将其纳入可疑性评估中。我们对12个大型项目的实证结果表明，CAN取得了很好的结果（例如，29.23%的故障排在前5位），并以相当大的优势显著提高了最先进的基线。,故障定位，图神经网络，程序依赖图，可疑性,,,
V7CXJSBS,2023,https://doi.org/10.1109/TSE.2022.3223875,TSE 2023,New Reliability-Driven Bounds for Architecture-Based Multi-Objective Testing Resource Allocation,"The multi-objective testing resource allocation problem (MOTRAP) aims at seeking a good trade-off between system reliability, testing cost, and testing time, which is of significant importance to facilitate the testing planning. Yet most studies focus on the time constraint but rarely consider the practical reliability requirement. In this work, we address MOTRAP on an architecture-based model (ABM) with the personalized preference over reliability. More specifically, we first present a reliability-constrained MOTRAP model on the basis of ABM and illustrate how to use this model for real-world systems. Then, to leverage the problem's knowledge, we develop new lower and upper bounds on testing time invested in different components from both theoretical and algorithmic perspectives on the basis of the Lagrange multiplier and half-interval search. Importantly, these new derived bounds have strong implications due to the fact that they can be easily employed by optimizers as the limits of variables to prune the search space to the region of interests of the decision maker and locate feasible solutions with the expected reliability. Finally, we evaluate the proposed bounds in popular multi-objective optimizers for MOTRAP on application and empirical cases. Experimental results demonstrate that our new bounds practically improve the search performance of optimizers, and decision makers can easily combine these new bounds with off-the-shelf optimizers to find higher-quality solutions that they are interested in, which greatly soothes away stress on optimizer and solution selections of decision makers.","Multi-objective testing resource allocation,architecture-based model,preference over reliability,region of interest,lower and upper bounds",基于体系结构的多目标测试资源分配的新的可靠性驱动边界,多目标测试资源分配问题（MOTRAP）旨在寻求系统可靠性、测试成本和测试时间之间的良好权衡，这对促进测试规划具有重要意义。然而，大多数研究都集中在时间约束上，很少考虑实际的可靠性要求。在这项工作中，我们在一个基于体系结构的模型（ABM）上解决了MOTRAP，该模型具有个性化偏好而非可靠性。更具体地说，我们首先在ABM的基础上提出了一个可靠性约束的MOTRAP模型，并说明了如何将该模型用于真实世界的系统。然后，为了利用问题的知识，我们在拉格朗日乘子和半区间搜索的基础上，从理论和算法的角度，开发了不同组件的测试时间的新的下限和上限。重要的是，这些新的推导边界具有很强的含义，因为优化器可以很容易地将它们用作变量的极限，以将搜索空间修剪到决策者感兴趣的区域，并定位具有预期可靠性的可行解决方案。最后，我们在应用和经验案例中评估了MOTRAP的流行多目标优化器中提出的边界。实验结果表明，我们的新边界实际上提高了优化器的搜索性能，决策者可以很容易地将这些新边界与现成的优化器相结合，以找到他们感兴趣的更高质量的解决方案，这大大减轻了优化器和决策者选择解决方案的压力。,多目标测试资源分配，基于体系结构的模型，对可靠性的偏好，感兴趣的区域，上下限,,,
4D5B8STX,2023,https://doi.org/10.1109/TSE.2023.3285910,TSE 2023,CPVD: Cross Project Vulnerability Detection Based on Graph Attention Network and Domain Adaptation,"Code vulnerability detection is critical for software security prevention. Vulnerability annotation in large-scale software code is quite tedious and challenging, which requires domain experts to spend a lot of time annotating. This work offers CPVD, a cross-domain vulnerability detection approach based on the challenge of ”learning to predict the vulnerability labels of another item quickly using one item with rich vulnerability labels.” CPVD uses the code property graph to represent the code and uses the Graph Attention Network and Convolution Pooling Network to extract the graph feature vector. It reduces the distribution between the source domain and target domain data in the Domain Adaptation Representation Learning stage for cross-domain vulnerability detection. In this paper, we test each other on different real-world project codes. Compared with methods without domain adaptation and domain adaptation methods based on natural language processing, CPVD is more general and performs better in cross-domain vulnerability detection tasks. Specifically, for the four datasets of chr_deb, qemu, libav, and sard, they achieved the best results of 70.2%, 81.1%, 59.7%, and 78.1% respectively on the F1-Score, and 88.4%,86.3%, 85.2%, and 88.6% on the AUC.","Code property graph,cross-domain vulnerability detection,domain adaptation representation learning,graph attention network",CPVD：基于图注意力网络和领域自适应的跨项目漏洞检测,代码漏洞检测对于软件安全预防至关重要。大规模软件代码中的漏洞注释非常繁琐且具有挑战性，这需要领域专家花费大量时间进行注释。这项工作提供了CPVD，这是一种跨领域的漏洞检测方法，基于“学习使用一个具有丰富漏洞标签的项目快速预测另一个项目的漏洞标签”的挑战CPVD使用代码属性图来表示代码，并使用图注意力网络和卷积池网络来提取图特征向量。在跨域漏洞检测的域自适应表示学习阶段，它减少了源域和目标域数据之间的分布。在本文中，我们在不同的真实世界项目代码上相互测试。与没有域自适应的方法和基于自然语言处理的域自适应方法相比，CPVD更通用，在跨域漏洞检测任务中表现更好。具体而言，对于chr_deb、qemu、libav和sard这四个数据集，它们在F1分数上分别获得了70.2%、81.1%、59.7%和78.1%的最佳结果，在AUC上分别获得88.4%、86.3%、85.2%和88.6%的最佳结果。,代码属性图，跨域漏洞检测，域自适应表示学习，图关注网络,,,
A5Y74RBX,2023,https://doi.org/10.1109/TSE.2023.3278055,TSE 2023,"Designing, Modeling and Analysis of GALS Software Systems","Designing software systems underpinned by a formal model of computation (MoC) is crucial for safety-critical, real-time and all industrial applications as it allows formal analysis of those designs and support for correct by design systems. In this paper, we focus on Globally Asynchronous Locally Synchronous (GALS) software systems and Coloured Petri Nets (CPNs) based approach to formally model and analyse GALS software systems specified in SystemJ GALS programming language. The approach translates SystemJ constructs into CPN modules and composes them into CPN GALS model based on control flow and concurrency specified in the SystemJ program. It preserves GALS MoC by automatically integrating synchronizer modules, asynchronous channel interface modules, and scheduling modules to result in the execution model of SystemJ program equivalent CPN. The created CPN GALS model allows system developers to verify the properties of the design formally with the use of Computation Tree Logic (CTL). An industrial automation example is provided as a use case.","Formal models,model checking,models of computation,petri nets,systems and software",GALS软件系统的设计、建模与分析,设计以形式化计算模型（MoC）为基础的软件系统对于安全关键、实时和所有工业应用至关重要，因为它允许对这些设计进行形式化分析，并支持正确的设计系统。在本文中，我们重点研究了全局异步局部同步（GALS）软件系统和基于有色Petri网（CPNs）的方法，以对SystemJ GALS编程语言中指定的GALS软件系统进行形式化建模和分析。该方法基于SystemJ程序中指定的控制流和并发性，将SystemJ构造转换为CPN模块，并将其组合为CPN-GALS模型。它通过自动集成同步器模块、异步通道接口模块和调度模块来保留GALS MoC，从而产生SystemJ程序等效CPN的执行模型。创建的CPN GALS模型允许系统开发人员使用计算树逻辑（CTL）正式验证设计的属性。提供了一个工业自动化示例作为用例。,形式模型，模型检查，计算模型，Petri网，系统和软件,,,
GTJZJUFM,2023,https://doi.org/10.1109/TSE.2022.3170122,TSE 2023,Finding Critical Scenarios for Automated Driving Systems: A Systematic Mapping Study,"Scenario-based approaches have been receiving a huge amount of attention in research and engineering of automated driving systems. Due to the complexity and uncertainty of the driving environment, and the complexity of the driving task itself, the number of possible driving scenarios that an Automated Driving System or Advanced Driving-Assistance System may encounter is virtually infinite. Therefore it is essential to be able to reason about the identification of scenarios and in particular critical ones that may impose unacceptable risk if not considered. Critical scenarios are particularly important to support design, verification and validation efforts, and as a basis for a safety case. In this paper, we present the results of a systematic mapping study in the context of autonomous driving. The main contributions are: (i) introducing a comprehensive taxonomy for critical scenario identification methods; (ii) giving an overview of the state-of-the-art research based on the taxonomy encompassing 86 papers between 2017 and 2020; and (iii) identifying open issues and directions for further research. The provided taxonomy comprises three main perspectives encompassing the problem definition (the why), the solution (the methods to derive scenarios), and the assessment of the established scenarios. In addition, we discuss open research issues considering the perspectives of coverage, practicability, and scenario space explosion.","Critical scenario,automated driving,systematic mapping study",寻找自动驾驶系统的关键场景：一项系统地图研究,基于场景的方法在自动驾驶系统的研究和工程中受到了极大的关注。由于驾驶环境的复杂性和不确定性，以及驾驶任务本身的复杂性，自动驾驶系统或高级驾驶辅助系统可能遇到的驾驶场景数量几乎是无限的。因此，至关重要的是，能够对确定情景，特别是如果不考虑可能带来不可接受风险的关键情景进行推理。关键场景对于支持设计、验证和验证工作以及作为安全案例的基础尤为重要。在本文中，我们介绍了在自动驾驶背景下进行的系统映射研究的结果。主要贡献是：（i）为关键场景识别方法引入了一个全面的分类法；（ii）概述2017年至2020年间基于分类法的最新研究，包括86篇论文；以及（三）确定有待进一步研究的未决问题和方向。所提供的分类法包括三个主要视角，包括问题定义（原因）、解决方案（推导场景的方法）和对已建立场景的评估。此外，我们还从覆盖率、实用性和场景空间爆炸的角度讨论了开放性研究问题。,关键场景，自动驾驶，系统测绘研究,,,
BJPXIM3P,2023,https://doi.org/10.1109/TSE.2023.3270117,TSE 2023,"Combatting Front-Running in Smart Contracts: Attack Mining, Benchmark Construction and Vulnerability Detector Evaluation","Front-running attacks have been a major concern on the blockchain. Attackers launch front-running attacks by inserting additional transactions before upcoming victim transactions to manipulate victim transaction executions and make profits. Recent studies have shown that front-running attacks are prevalent on the Ethereum blockchain and have caused millions of US dollars loss. It is the vulnerabilities in smart contracts, which are blockchain programs invoked by transactions, that enable the front-running attack opportunities. Although techniques to detect front-running vulnerabilities have been proposed, their performance on real-world vulnerable contracts is unclear. There is no large-scale benchmark based on real attacks to evaluate their capabilities. We make four contributions in this paper. First, we design an effective algorithm to mine real-world attacks in the blockchain history. The evaluation shows that our mining algorithm is more effective and comprehensive, achieving higher recall in finding real attacks than the previous study. Second, we propose an automated and scalable vulnerability localization approach to localize code snippets in smart contracts that enable front-running attacks. The evaluation also shows that our localization approaches are effective in achieving higher precision in pinpointing vulnerabilities compared to the baseline technique. Third, we build a benchmark consisting of 513 real-world attacks with vulnerable code labeled in 235 distinct smart contracts, which is useful to help understand the nature of front-running attacks, vulnerabilities in smart contracts, and evaluate vulnerability detection techniques. Last but not least, we conduct an empirical evaluation of seven state-of-the-art vulnerability detection techniques on our benchmark. The evaluation experiment reveals the inadequacy of existing techniques in detecting front-running vulnerabilities, with a low recall of $\leq$ 6.04%. Our further analysis identifies four common limitations in existing techniques: lack of support for inter-contract analysis, inefficient constraint solving for cryptographic operations, improper vulnerability patterns, and lack of token support.","Benchmark,dataset,empirical study,ethereum,front-running,smart contract,vulnerability,blockchain",智能合约中的前沿作战：攻击挖掘、基准构建和漏洞检测器评估,前沿攻击一直是区块链上的一个主要问题。攻击者通过在即将到来的受害者交易之前插入额外的交易来启动前端攻击，以操纵受害者交易的执行并获利。最近的研究表明，前端攻击在以太坊区块链上很普遍，并造成了数百万美元的损失。智能合约是由交易调用的区块链程序，正是智能合约中的漏洞才有可能引发前端攻击。尽管已经提出了检测前端漏洞的技术，但它们在现实世界中易受攻击的合同中的性能尚不清楚。没有基于真实攻击的大规模基准来评估其能力。我们在这篇论文中有四点贡献。首先，我们设计了一种有效的算法来挖掘区块链历史上的真实世界攻击。评估表明，我们的挖掘算法比以前的研究更有效、更全面，在发现真实攻击方面实现了更高的召回率。其次，我们提出了一种自动化和可扩展的漏洞定位方法来定位智能合约中的代码片段，从而实现前端攻击。评估还表明，与基线技术相比，我们的定位方法在精确定位漏洞方面是有效的。第三，我们建立了一个由513个真实世界的攻击组成的基准，其中235个不同的智能合约中标记了易受攻击的代码，这有助于了解前端攻击的性质、智能合约中的漏洞，并评估漏洞检测技术。最后但并非最不重要的是，我们在基准上对七种最先进的漏洞检测技术进行了实证评估。评估实验揭示了现有技术在检测前端运行漏洞方面的不足，召回率低至$\leq$6.04%。我们的进一步分析确定了现有技术中的四个常见局限性：缺乏对契约间分析的支持、加密操作的低效约束解决、不正确的漏洞模式和缺乏令牌支持。,基准，数据集，实证研究，以太，领先，智能合同，漏洞，区块链,,,
B433LZMY,2023,https://doi.org/10.1109/TSE.2022.3168373,TSE 2023,Demystifying Performance Regressions in String Solvers,"Over the past few years, SMT string solvers have found their applications in an increasing number of domains, such as program analyses in mobile and Web applications, which require the ability to reason about string values. A series of research has been carried out to find quality issues of string solvers in terms of its correctness and performance. Yet, none of them has considered the performance regressions happening across multiple versions of a string solver. To fill this gap, in this paper, we focus on solver performance regressions (SPRs), i.e., unintended slowdowns introduced during the evolution of string solvers. To this end, we develop SPRFinderto not only generate test cases demonstrating SPRs, but also localize the probable causes of them, in terms of commits. We evaluated the effectiveness of SPRFinderon three state-of-the-art string solvers, i.e., Z3Seq, Z3Str3, and CVC4. The results demonstrate that SPRFinderis effective in generating SPR-inducing test cases and also able to accurately locate the responsible commits. Specifically, the average running time on the target versions is 13.2× slower than that of the reference versions. Besides, we also conducted the first empirical study to peek into the characteristics of SPRs, including the impact of random seed configuration for SPR detection, understanding the root causes of SPRs, and characterizing the regression test cases through case studies. Finally, we highlight that 149 unique SPR-inducing commits were discovered in total by SPRFinder, and 27of them have been confirmed by the corresponding developers.","SMT string solver,performance regression,SPRFinder",字符串求解器中性能回归的解密,在过去的几年里，SMT字符串求解器已经在越来越多的领域中找到了它们的应用，例如移动和Web应用程序中的程序分析，这些领域需要对字符串值进行推理。为了发现字符串求解器在正确性和性能方面的质量问题，已经进行了一系列研究。然而，他们中没有一个考虑到在多个版本的字符串解算器中发生的性能回归。为了填补这一空白，在本文中，我们重点关注解算器性能回归（SPRs），即字符串解算器发展过程中引入的意外减速。为此，我们开发了SPRFinderto，不仅生成了演示SPR的测试用例，还从提交的角度定位了它们的可能原因。我们评估了SPRFinderon三种最先进的字符串求解器的有效性，即Z3Seq、Z3Str3和CVC4。结果表明，SPRFinderis在生成SPR诱导测试用例方面是有效的，并且能够准确定位负责的提交。具体而言，目标版本的平均运行时间比参考版本慢13.2倍。此外，我们还进行了第一次实证研究，以深入了解SPR的特征，包括随机种子配置对SPR检测的影响，了解SPR的根本原因，并通过案例研究来表征回归测试案例。最后，我们强调，SPRFinder总共发现了149个独特的SPR诱导提交，其中27个已得到相应开发人员的确认。,SMT字符串求解器，性能回归，SPRTOWN,,,
DHRBJPAJ,2023,https://doi.org/10.1109/TSE.2022.3165056,TSE 2023,Pull Request Decisions Explained: An Empirical Overview,"Context: The pull-based development model is widely used in open source projects, leading to the emergence of trends in distributed software development. One aspect that has garnered significant attention concerning pull request decisions is the identification of explanatory factors. Objective: This study builds on a decade of research on pull request decisions and provides further insights. We empirically investigate how factors influence pull request decisions and the scenarios that change the influence of such factors. Method: We identify factors influencing pull request decisions on GitHub through a systematic literature review and infer them by mining archival data. We collect a total of 3,347,937 pull requests with 95 features from 11,230 diverse projects on GitHub. Using these data, we explore the relations among the factors and build mixed effects logistic regression models to empirically explain pull request decisions. Results: Our study shows that a small number of factors explain pull request decisions, with that concerning whether the integrator is the same as or different from the submitter being the most important factor. We also note that the influence of factors on pull request decisions change with a change in context; e.g., the area hotness of pull request is important only in the early stage of project development, however it becomes unimportant for pull request decisions as projects become mature.","Pull-based development,pull request decision,distributed software development,GitHub",拉动请求决策解释：实证综述,上下文：基于拉的开发模型在开源项目中被广泛使用，导致了分布式软件开发趋势的出现。关于拉取请求决定，引起人们极大关注的一个方面是确定解释因素。目的：本研究建立在十年来对拉取请求决策的研究基础上，并提供进一步的见解。我们实证研究了因素如何影响拉取请求决策，以及改变这些因素影响的场景。方法：通过系统的文献综述，我们在GitHub上确定了影响拉取请求决策的因素，并通过挖掘档案数据进行推断。我们在GitHub上从11230个不同的项目中收集了总共3347937个具有95个功能的拉取请求。利用这些数据，我们探索了各因素之间的关系，并建立了混合效应逻辑回归模型来实证解释拉取请求决策。结果：我们的研究表明，有少数因素可以解释拉取请求决策，其中最重要的因素是集成商与提交者是否相同或不同。我们还注意到，因素对拉取请求决策的影响随着上下文的变化而变化；例如，拉请求的区域热度仅在项目开发的早期阶段很重要，但随着项目的成熟，它对拉请求决策变得不重要。,基于Pull的开发，Pull请求决策，分布式软件开发，GitHub,,,
K8LQX78M,2023,https://doi.org/10.1109/TSE.2023.3298609,TSE 2023,BiAn: Smart Contract Source Code Obfuscation,"With the rising prominence of smart contracts, security attacks targeting them have increased, posing severe threats to their security and intellectual property rights. Existing simplistic datasets hinder effective vulnerability detection, raising security concerns. To address these challenges, we propose BiAn, a source code level smart contract obfuscation method that generates complex vulnerability test datasets. BiAn protects contracts by obfuscating data flows, control flows, and code layouts, increasing complexity and making it harder for attackers to discover vulnerabilities. Our experiments with buggy contracts showed an average complexity enhancement of approximately 174% after obfuscation. Decompilers Vandal and Gigahorse had total failure rate increments of 38.8% and 40.5% respectively. Obfuscated contracts also decreased vulnerability detection rates in more than 50% of cases for ten widely-used static analysis detection tools.","Blockchain,Ethereum,smart contract,source code,obfuscation",BiAn:智能合约源代码混乱,随着智能合约的日益突出，针对它们的安全攻击也在增加，对它们的安全和知识产权构成了严重威胁。现有过于简单的数据集阻碍了有效的漏洞检测，引发了安全问题。为了应对这些挑战，我们提出了BiAn，这是一种源代码级别的智能合约模糊方法，可以生成复杂的漏洞测试数据集。BiAn通过混淆数据流、控制流和代码布局来保护合同，增加了复杂性，使攻击者更难发现漏洞。我们对有缺陷合约的实验表明，在模糊处理后，平均复杂度提高了约174%。分解器Vandal和Gigahorse的总故障率分别增加了38.8%和40.5%。对于十种广泛使用的静态分析检测工具，混乱的合同也降低了50%以上的漏洞检测率。,区块链，以太，智能合约，源代码，混淆,,,
QQNURYHK,2023,https://doi.org/10.1109/TSE.2022.3216279,TSE 2023,Towards the Analysis and Completion of Syntactic Structure Ellipsis for Inline Comments,"The ellipsis of the syntactic structure is a common phenomenon in ordinary textual documents. Existing studies have found that despite syntactic ellipsis could help avoid repetition of normative documents, it could also, for example, lead to ambiguity and hamper the understandability of document contents. As a fundamental component of software, code comments are generally written by developers in a non-structured way just like normative documents. This naturally inspires us to explore whether syntactic ellipsis is also a common phenomenon in code comments and what potential negative effects would such ellipsis have on software tasks such as code/comments comprehension activities. Such explorations, in our opinion, are expected to facilitate the research on code comments and comments-related software tasks. To this end, we conduct the first large-scale study to explore the syntactic structure ellipsis problem of code comments, with a focus on Java inline comments. Specifically, we construct a data set of 1,000 Java projects with 1,307,457 inline comments and associated codes. Based on this data set, we first study the prevalence of syntactic structure ellipsis in inline comments. We find that syntactic structure ellipsis is quite common in inline comments where 83.6% comments have structure ellipsis (such as subject/predicate omissions). Then, we investigate the effects of syntactic structure ellipsis on code/comment understanding activities. As a result, we find that there indeed exists a negative relationship between them, with a medium effect size. Based on these findings, we further propose neural network based approaches to complete the ellipsis parts for the inline comments. With our approach, we could achieve: 1) a medium improvement in assisting code/comment understanding activities, and 2) a substantial improvement of 11.3% in comment-assisted code abbreviation extension task.","Inline comments,syntactic structure,ellipsis analysis,ellipsis completion",联机评论句法结构省略的分析与完善,句法结构的省略是普通文本文件中常见的现象。现有研究发现，尽管句法省略有助于避免规范性文件的重复，但它也可能导致歧义，阻碍文件内容的可理解性。作为软件的基本组成部分，代码注释通常由开发人员以非结构化的方式编写，就像规范性文档一样。这自然启发我们探索句法省略是否也是代码注释中的常见现象，以及这种省略对软件任务（如代码/注释理解活动）会产生什么潜在的负面影响。我们认为，这样的探索有望促进代码注释和注释相关软件任务的研究。为此，我们首次对代码注释的句法结构省略问题进行了大规模研究，重点是Java内联注释。具体来说，我们构建了一个由1000个Java项目组成的数据集，其中包含1307457个内联注释和相关代码。基于这一数据集，我们首先研究了内联评论中句法结构省略的普遍性。我们发现句法结构省略在内联评论中非常常见，83.6%的评论具有结构省略（如主谓省略）。然后，我们研究了句法结构省略对代码/注释理解活动的影响。因此，我们发现它们之间确实存在负相关，具有中等的效应大小。基于这些发现，我们进一步提出了基于神经网络的方法来完成内联评论的省略部分。使用我们的方法，我们可以实现：1）在辅助代码/注释理解活动方面有中等程度的改进，2）在注释辅助代码缩写扩展任务方面有11.3%的实质性改进。,内嵌注释，句法结构，省略分析，省略补全,,,
TNXW77T9,2023,https://doi.org/10.1109/TSE.2022.3154769,TSE 2023,"Web APIs: Features, Issues, and Expectations - A Large-Scale Empirical Study of Web APIs From Two Publicly Accessible Registries Using Stack Overflow and a User Survey","With the increasing adoption of services-oriented computing and cloud computing technologies, web APIs have become the fundamental building blocks for constructing software applications. Web APIs are developed and published on the internet. The functionality of web APIs can be used to facilitate the development of software applications. There are numerous studies on retrieving and recommending candidate web APIs based on user requirements from a large set of web APIs. However, there are very limited studies on the features of web APIs that make them more likely to be used and the issues of using web APIs in practice. Moreover, users’ expectations on the development and management of web APIs are rarely investigated. In this paper, we conduct a large-scale empirical study of 20,047 web APIs published at two popular and publicly accessible web API registries: ProgrammableWeb and APIs.guru. We first extract the questions posted in Stack Overflow (SO) that are relevant to the web APIs. We then manually analyze 1,885 randomly sampled SO questions and identify 24 web API issue types (e.g., authorization error) that are encountered by users. Afterwards, we conduct a user survey to investigate the features of web APIs that users often consider when shortlisting a web API for testing before they adopt it, validate the identified types of web API issues, and understand users’ expectations on the development and management of web APIs. From the 191 received responses, we extract 14 important features for users to decide whether to use a web API (e.g., well-organized documentation). We also gain a better understanding of web API issue types and summarize 11 categories of user expectations on web APIs (e.g., documentation and SDK/library). As the result of our study, we provide guidelines for web API developers and registry managers to improve web APIs and promote the use of web APIs.","Web APIs,empirical study,user survey,stack overflow",Web API：特性、问题和期望-使用堆栈溢出和用户调查对两个公共可访问注册表的Web API的大规模实证研究,随着面向服务的计算和云计算技术的日益普及，web API已成为构建软件应用程序的基本构建块。Web API是在互联网上开发和发布的。web API的功能可以用于促进软件应用程序的开发。有许多研究基于用户需求从大量的web API中检索和推荐候选web API。然而，关于使其更有可能被使用的web API的特性以及在实践中使用web API的问题的研究非常有限。此外，用户对web API的开发和管理的期望很少被调查。在这篇论文中，我们对在两个流行的和可公开访问的web API注册中心ProgrammableWeb和APIs.guru发布的20047个web API进行了大规模的实证研究。我们首先提取了Stack Overflow（SO）中发布的与web API相关的问题。然后，我们手动分析1885个随机抽样的SO问题，并识别用户遇到的24个web API问题类型（例如，授权错误）。之后，我们进行了一项用户调查，以调查用户在选择web API进行测试之前经常考虑的web API的特性，验证所识别的web API问题类型，并了解用户对web API开发和管理的期望。从191个收到的回复中，我们提取了14个重要特征，供用户决定是否使用web API（例如，组织良好的文档）。我们还对web API问题类型有了更好的了解，并总结了11类用户对web API的期望（例如文档和SDK/库）。根据我们的研究结果，我们为web API开发人员和注册管理人员提供了改进web API和促进web API使用的指导方针。,Web API，经验研究，用户调查，堆栈溢出,,,
SB9FDNWD,2023,https://doi.org/10.1109/TSE.2022.3146831,TSE 2023,APIMatchmaker: Matching the Right APIs for Supporting the Development of Android Apps,"Android developers are often faced with the need to learn how to use different APIs suitable for their projects. Automated API recommendation approaches have been invented to help fill this gap, and these have been demonstrated to be useful to some extent. Unfortunately, most state-of-the-art works are not proposed for Android developers, and the ones dedicated to Android app development often suffer from high redundancy and poor run-time performance, or do not target the problem of recommending API usage patterns. To address this gap we propose to the community a new tool, namely APIMatchmaker, to recommend API usages by learning directly from similar real-world Android apps. Unlike existing recommendation approaches, which leverage a single context to find similar projects, we innovatively introduce a multi-dimensional, context-aware, collaborative filtering approach to better achieve the purpose. Specifically, in addition to code similarity, we also take app descriptions (or topics) into consideration to ensure that similar apps also provide similar functions. We evaluate APIMatchmaker on a large number of real-world Android apps and observe that APIMatchmaker yields a high success rate in recommending APIs for Android apps under development, and it is also able to outperform the state-of-the-art.","Android,API,recommendation,collaborative filtering,apimatchmaker",APIMatchmaker:匹配正确的API以支持Android应用程序的开发,Android开发人员经常需要学习如何使用适合其项目的不同API。自动API推荐方法已经被发明来帮助填补这一空白，并且这些方法在一定程度上被证明是有用的。不幸的是，大多数最先进的作品都不是为Android开发人员提出的，而专门用于Android应用程序开发的作品往往存在高冗余和运行时性能差的问题，或者不针对推荐API使用模式的问题。为了解决这一差距，我们向社区提出了一种新的工具，即APIMatchmaker，通过直接学习现实世界中类似的Android应用程序来推荐API的使用。与现有的建议方法不同，现有的推荐方法利用单个上下文来寻找类似的项目，我们创新性地引入了一种多维度、上下文感知的协作过滤方法，以更好地实现目的。具体来说，除了代码相似性，我们还考虑应用程序描述（或主题），以确保类似的应用程序也提供类似的功能。我们在大量真实世界的Android应用程序上评估了APIMatchmaker，并观察到APIMatchmakeer在为正在开发的Android应用推荐API方面取得了很高的成功率，而且它也能够超越最先进的应用程序。,Android，API，推荐，协作过滤，apimatchaker,,,
UF595UR2,2023,https://doi.org/10.1109/TSE.2022.3144480,TSE 2023,Achieving High MAP-Coverage Through Pattern Constraint Reduction,"Testing multi-threaded programs is challenging due to the enormous space of thread interleavings. Recently, a code coverage criterion for multi-threaded programs called MAP-coverage has been proposed and shown to be effective for testing concurrent programs. Existing approaches for achieving high MAP-coverage are based on random testing with simple heuristics, which is ineffective in systematically triggering rare thread interleavings. In this study, we propose a novel approach called pattern constraint reduction (PCR), which employs optimized constraint solving to generate thread interleavings for high MAP-coverage. The idea is to iteratively encode and solve path conditions to generate thread interleavings which are guaranteed to improve MAP-coverage. Furthermore, we effectively apply interpolation techniques to reduce the efforts of constraint solving by avoiding solving infeasible constraints. The experiment results on 20 benchmark programs show that our approach complements existing random testing based approaches when there are rare failure-inducing interleaving in the whole search space. Specifically, PCR finds concurrency bugs faster in 18 out of 20 programs, with an average speedup of 4.2x and a maximum speedup of 11.4x.","Concurrency bug detection,constraint solving,coverage criteria,thread-safe class",通过减少模式约束实现高MAP覆盖,由于线程间隔的巨大空间，测试多线程程序具有挑战性。最近，一种称为MAP覆盖的多线程程序代码覆盖标准被提出，并被证明对测试并发程序是有效的。现有的实现高MAP覆盖率的方法是基于简单启发式的随机测试，这在系统地触发罕见的线程穿插方面是无效的。在这项研究中，我们提出了一种称为模式约束减少（PCR）的新方法，该方法采用优化的约束求解来生成线程间插，以实现高MAP覆盖率。其思想是迭代地编码和求解路径条件，以生成线程交织，保证提高MAP覆盖率。此外，我们通过避免求解不可行约束，有效地应用插值技术来减少约束求解的工作量。在20个基准程序上的实验结果表明，当在整个搜索空间中出现罕见的故障交织时，我们的方法补充了现有的基于随机测试的方法。具体来说，PCR在20个程序中的18个程序中发现并发错误的速度更快，平均速度提高了4.2倍，最大速度提高了11.4倍。,并发错误检测，约束求解，覆盖标准，线程安全类,,,
VB8HMXL3,2023,https://doi.org/10.1109/TSE.2022.3167628,TSE 2023,A Large-Scale Empirical Study of Real-Life Performance Issues in Open Source Projects,"Software performance is a critical quality attribute that determines the success of a software system. However, practitioners lack comprehensive and holistic understanding of how real-life performance issues are caused and resolved in practice from the technical, engineering, and economic perspectives. This paper presents a large-scale empirical study of 570 real-life performance issues from 13 open source projects from various problem domains, and implemented in three popular programming languages, Java (192 issues), C/C++ (162 issues), and Python (216 issues). From the technical perspective, we summarize eight general types of performance issues with corresponding root causes and resolutions that apply for all three languages. We also identify available tools for detecting and resolving different types of issues from the literature. In addition, we found that 27% of the 570 issues are resolved by design-level optimization—coordinated revision of a group of related source files and their design structure. We reveal four typical design-level optimization patterns, including classic design patterns, change propagation, optimization clone, and parallel optimization that practitioners should be aware of in resolving performance issues. From the engineering perspective, this study analyzes how test code changes in performance optimization. We found that only 15% of the 570 performance issues involve revision of test code. In most cases, the revised test cases focus on the functional logic of the performance optimization, rather than directly evaluate the performance improvement. This finding points to the potential lack of engineering standard for formally verifying performance optimization in regression testing. Finally, from the economic perspective, we analyze the “Return On Investment” of performance optimization. We found that design-level optimization usually requires more investment, but not always yields to higher performance improvement. However, developers tend to use design-level optimization when they concern about other quality attributes, such as maintainability and readability.","Design patterns,software design structure,software performance,testing code",开源项目中现实生活中性能问题的大规模实证研究,软件性能是决定软件系统成功与否的关键质量属性。然而，从业者缺乏从技术、工程和经济角度对现实生活中的性能问题是如何在实践中引起和解决的全面和全面的理解。本文对来自不同问题领域的13个开源项目中的570个实际性能问题进行了大规模的实证研究，并用三种流行的编程语言Java（192期）、C/C++（162期）和Python（216期）实现。从技术角度来看，我们总结了八种常见类型的性能问题，以及适用于所有三种语言的相应根本原因和解决方案。我们还从文献中确定了用于检测和解决不同类型问题的可用工具。此外，我们发现570个问题中有27%是通过设计级优化解决的，即协调修改一组相关的源文件及其设计结构。我们揭示了四种典型的设计级优化模式，包括经典设计模式、更改传播、优化克隆和并行优化，从业者在解决性能问题时应该注意这些模式。从工程的角度，本研究分析了测试代码在性能优化中的变化。我们发现，在570个性能问题中，只有15%涉及测试代码的修订。在大多数情况下，修订后的测试用例侧重于性能优化的功能逻辑，而不是直接评估性能改进。这一发现表明，回归测试中可能缺乏正式验证性能优化的工程标准。最后，从经济学角度分析了绩效优化的“投资回报率”。我们发现，设计级别的优化通常需要更多的投资，但并不总是能带来更高的性能改进。然而，当开发人员关心其他质量属性（如可维护性和可读性）时，他们倾向于使用设计级别的优化。,设计模式，软件设计结构，软件性能，测试代码,,,
BB55F2TV,2023,https://doi.org/10.1109/TSE.2022.3195640,TSE 2023,Neural Network Guided Evolutionary Fuzzing for Finding Traffic Violations of Autonomous Vehicles,"Self-driving cars and trucks, autonomous vehicles (avs), should not be accepted by regulatory bodies and the public until they have much higher confidence in their safety and reliability — which can most practically and convincingly be achieved by testing. But existing testing methods are inadequate for checking the end-to-end behaviors of av controllers against complex, real-world corner cases involving interactions with multiple independent agents such as pedestrians and human-driven vehicles. While test-driving avs on streets and highways fails to capture many rare events, existing simulation-based testing methods mainly focus on simple scenarios and do not scale well for complex driving situations that require sophisticated awareness of the surroundings. To address these limitations, we propose a new fuzz testing technique, called AutoFuzz, which can leverage widely-used av simulators’ API grammars to generate semantically and temporally valid complex driving scenarios (sequences of scenes). To efficiently search for traffic violations-inducing scenarios in a large search space, we propose a constrained neural network (NN) evolutionary search method to optimize AutoFuzz. Evaluation of our prototype on one state-of-the-art learning-based controller, two rule-based controllers, and one industrial-grade controller in five scenarios shows that AutoFuzz efficiently finds hundreds of traffic violationsin high-fidelity simulation environments. For each scenario, AutoFuzz can find on average 10-39% more unique traffic violationsthan the best-performing baseline method. Further, fine-tuning the learning-based controller with the traffic violationsfound by AutoFuzz successfully reduced the traffic violationsfound in the new version of the av controller software.","Search-based software engineering,evolutionary algorithms,neural networks,software testing,test generation,autonomous vehicles",基于神经网络的进化模糊识别自动驾驶汽车交通违法行为,自动驾驶汽车和卡车，即自动驾驶汽车（avs），不应该被监管机构和公众接受，除非他们对其安全性和可靠性有更高的信心——这可以通过测试最实际、最令人信服地实现。但现有的测试方法不足以根据复杂的、真实世界的角落案例来检查av控制器的端到端行为，这些案例涉及与行人和人类驾驶的车辆等多个独立主体的交互。虽然在街道和高速公路上测试驾驶avs无法捕捉到许多罕见的事件，但现有的基于模拟的测试方法主要集中在简单的场景上，对于需要对周围环境进行复杂感知的复杂驾驶情况，无法很好地扩展。为了解决这些限制，我们提出了一种新的模糊测试技术，称为AutoFuzz，它可以利用广泛使用的av模拟器的API语法来生成语义和时间有效的复杂驾驶场景（场景序列）。为了在大搜索空间中有效地搜索交通违法诱导场景，我们提出了一种约束神经网络（NN）进化搜索方法来优化AutoFuzz。在五个场景中，对我们的原型在一个最先进的基于学习的控制器、两个基于规则的控制器和一个工业级控制器上的评估表明，AutoFuzz在高保真模拟环境中有效地发现了数百个交通违法行为。对于每种情况，AutoFuzz可以发现比性能最好的基线方法平均多10-39%的唯一流量冲突。此外，利用AutoFuzz发现的交通违规对基于学习的控制器进行微调，成功地减少了新版av控制器软件中发现的交通违法。,基于搜索的软件工程，进化算法，神经网络，软件测试，测试生成，自动驾驶车辆,,,
4PI4D74K,2023,https://doi.org/10.1109/TSE.2023.3308755,TSE 2023,Multi-Misconfiguration Diagnosis via Identifying Correlated Configuration Parameters,"Software configuration requires that the user sets appropriate values to specified variables, known as configuration parameters, which potentially affect the behaviors of software system. It is an essential means for software reliability, but how to ensure correct configurations remains a great challenge, especially when a large number of parameter settings are involved. Existing studies on misconfiguration diagnosis treat all configurations independently, ignoring the constraints and correlations among different configurations. In this article, we reveal the phenomenon of multi-misconfigurations and present a tool, MMD, for multi-misconfigurations diagnosis. Specifically, MMD consists of two modules: Correlated Configurations Analysis and Primary Misconfigurations Diagnosis. The former determines the correlation among each pair of configurations by analyzing the control and data flows related to each configuration. The latter is responsible for collecting a list of configurations ranked according to their suspiciousness. Combining the outputs of two modules, MMD is able to assist the user in multi-misconfigurations diagnosis. We evaluate MMD on seven popular Java projects: Randoop, Soot, Synoptic, Hdfs, Hbase, Yarn, and Zookeeper. MMD identifies 510 configuration correlations with a 4.9% false positive rate. Furthermore, it effectively diagnoses 22 multi-misconfigurations collected from StackOverflow, outperforming two state-of-the-art baselines.","Configuration,correlation,multi-misconfiguration,parameters,diagnosis",通过识别相关配置参数进行多配置错误诊断,软件配置要求用户为指定的变量设置适当的值，即配置参数，这些变量可能会影响软件系统的行为。它是软件可靠性的重要手段，但如何确保正确的配置仍然是一个巨大的挑战，尤其是当涉及大量参数设置时。现有的错误配置诊断研究独立处理所有配置，忽略了不同配置之间的约束和相关性。在这篇文章中，我们揭示了多重错误配置的现象，并提出了一种用于多重错误配置诊断的工具MMD。具体来说，MMD由两个模块组成：相关配置分析和主要错误配置诊断。前者通过分析与每个配置相关的控制和数据流来确定每对配置之间的相关性。后者负责收集根据可疑程度排列的配置列表。MMD结合了两个模块的输出，能够帮助用户进行多个错误配置的诊断。我们在七个流行的Java项目上评估MMD：Randoop、Soot、Synoptic、Hdfs、Hbase、Yarn和Zookeeper。MMD识别510个配置相关性，假阳性率为4.9%。此外，它有效地诊断了从StackOverflow收集的22个多重错误配置，优于两个最先进的基线。,配置，关联，多个错误配置，参数，诊断,,,
YWE6IASI,2023,https://doi.org/10.1109/TSE.2023.3254142,TSE 2023,Specification-Based Autonomous Driving System Testing,"Autonomous vehicle (AV) systems must be comprehensively tested and evaluated before they can be deployed. High-fidelity simulators such as CARLA or LGSVL allow this to be done safely in very realistic and highly customizable environments. Existing testing approaches, however, fail to test simulated AVs systematically, as they focus on specific scenarios and oracles (e.g., lane following scenario with the “no collision” requirement) and lack any coverage criteria measures. In this paper, we propose $\mathtt {AVUnit}$, a framework for systematically testing AV systems against customizable correctness specifications. Designed modularly to support different simulators, $\mathtt {AVUnit}$ consists of two new languages for specifying dynamic properties of scenes (e.g., changing pedestrian behaviour after waypoints) and fine-grained assertions about the AV's journey. $\mathtt {AVUnit}$ further supports multiple fuzzing algorithms that automatically search for test cases that violate these assertions, using robustness and coverage measures as fitness metrics. We evaluated the implementation of $\mathtt {AVUnit}$ for the LGSVL+Apollo simulation environment, finding 19 kinds of issues in Apollo, which indicate that the open-source Apollo does not perform well in complex intersections and lane-changing related scenarios.","Autonomous driving system,coverage criteria,fuzzing,specification languages,testing",基于规范的自动驾驶系统测试,在部署自动驾驶汽车（AV）系统之前，必须对其进行全面的测试和评估。像CARLA或LGSVL这样的高保真度模拟器可以在非常逼真和高度可定制的环境中安全地完成这项工作。然而，现有的测试方法无法系统地测试模拟AV，因为它们专注于特定场景和预言（例如，具有“无碰撞”要求的车道跟随场景），并且缺乏任何覆盖标准措施。在本文中，我们提出了$\mathtt｛AVUnit｝$，这是一个根据可定制的正确性规范对AV系统进行系统测试的框架。模块化设计以支持不同的模拟器，$\mathtt｛AVUnit｝$由两种新语言组成，用于指定场景的动态属性（例如，在路标后改变行人行为）和关于AV旅程的细粒度断言$\mattt{AVUnit}$进一步支持多个模糊算法，这些算法使用稳健性和覆盖率度量作为适应度度量，自动搜索违反这些断言的测试用例。我们评估了$\mathtt｛AVUnit｝$在LGSVL+Apollo模拟环境中的实现，发现Apollo中存在19种问题，这表明开源Apollo在复杂的十字路口和变道相关场景中表现不佳。,自动驾驶系统，覆盖标准，模糊，规范语言，测试,,,
FDRZJBGQ,2023,https://doi.org/10.1109/TSE.2022.3219520,TSE 2023,UltraFuzz: Towards Resource-Saving in Distributed Fuzzing,"Recent research has sought to improve fuzzing performance via parallel computing. However, researchers focus on improving efficiency while ignoring the increasing cost of testing resources. Parallel fuzzing in the distributed environment amplifies the resource-wasting problem caused by the random nature of fuzzing. In the parallel mode, owing to the lack of an appropriate task dispatching scheme and timely fuzzing status synchronization among different fuzzing instances, task conflicts and workload imbalance occur, making the resource-wasting problem severe. In this paper, we design UltraFuzz, a fuzzer for resource-saving in distributed fuzzing. Based on centralized dynamic scheduling, UltraFuzz can dispatch tasks and schedule power globally and reasonably to avoid resource-wasting. Besides, UltraFuzz can elastically allocate computing power for fuzzing and seed evaluation, thereby avoiding the potential bottleneck of seed evaluation that blocks the fuzzing process. UltraFuzz was evaluated using real-world programs, and the results show that with the same testing resource, UltraFuzz outperforms state-of-the-art tools, such as AFL, AFL-P, PAFL, and EnFuzz. Most importantly, the experiment reveals certain results that seem counter-intuitive, namely that parallel fuzzing can achieve “super-linear acceleration” when compared with single-core fuzzing. We conduct additional experiments to reveal the deep reasons behind this phenomenon and dig deep into the inherent advantages of parallel fuzzing over serial fuzzing, including the global optimization of seed energy scheduling and the escape of local optimal seed. Additionally, 24 real-world vulnerabilities were discovered using UltraFuzz.","parallel fuzzing,distributed fuzzing,resource saving in fuzzing,super-linear acceleration",超引信：面向分布式引信资源节约,最近的研究试图通过并行计算来提高模糊性能。然而，研究人员专注于提高效率，而忽略了不断增加的测试资源成本。分布式环境中的并行模糊处理放大了模糊处理的随机性造成的资源浪费问题。在并行模式下，由于缺乏合适的任务调度方案和不同模糊实例之间及时的模糊状态同步，导致任务冲突和工作负载失衡，资源浪费问题严重。在本文中，我们设计了UltraFuzz，一个在分布式模糊处理中节省资源的模糊器。基于集中式动态调度，UltraFuzz可以全局合理地调度任务和功率，避免资源浪费。此外，UltraFuzz可以弹性分配模糊化和种子评估的计算能力，从而避免了种子评估阻碍模糊化过程的潜在瓶颈。使用真实世界的程序对UltraFuzz进行了评估，结果表明，在相同的测试资源下，UltraFuzz的性能优于最先进的工具，如AFL、AFL-P、PAFL和EnFuzz。最重要的是，实验揭示了一些似乎违背直觉的结果，即与单核模糊相比，并行模糊可以实现“超线性加速”。我们进行了额外的实验来揭示这一现象背后的深层原因，并深入挖掘并行模糊化相对于串行模糊化的内在优势，包括种子能量调度的全局优化和局部最优种子的逃逸。此外，使用UltraFuzz还发现了24个真实世界中的漏洞。,并行模糊，分布式模糊，模糊中的资源节约，超线性加速,,,
RZRZ8NGB,2023,https://doi.org/10.1109/TSE.2023.3238161,TSE 2023,Towards Retrieval-Based Neural Code Summarization: A Meta-Learning Approach,"Code summarization aims to generate code summaries automatically, and has attracted a lot of research interest lately. Recent approaches to it commonly adopt neural machine translation techniques, which train a Seq2Seq model on a large corpus and assume it could work on various new code snippets. However, codes are highly varied in practice due to different domains, businesses or programming styles. Therefore, it is challenging to learn such a variety of patterns into a single model. In this paper, we propose a brand-new framework for code summarization based on meta-learning and code retrieval, named MLCS to tackle this issue. In this framework, the summarization of each target code is formalized as a few-shot learning task, where its similar examples are used as training data and the testing example is itself. We retrieve examples similar to the target code in a rank-and-filter manner. Given a neural code summarizer, we optimize it into a meta-learner via Model-Agnostic Meta-Learning (MAML). During inference, the meta-learner first adapts to the retrieved examples and yields an exclusive model for the target code, and then generates its summary. Extensive experiments on real-world datasets show: (1) Utilizing MLCS, a standard Seq2Seq model is able to outperform previous state-of-the-art approaches, including both neural models and retrieval-based neural models; (2) MLCS can flexibly adapt to existing neural code summarizers without modifying their architecture, and could significantly improve their performance with the relative gain of up to 112.7% on BLEU-4, 23.2% on ROUGE-L, and 31.5% on METEOR; (3) Compared to the existing retrieval-based neural approaches, MLCS can better leverage multiple similar examples, and shows better generalization ability on different retrievers, unseen retrieval corpus and low-frequency words.","Code summarization,deep learning,information retrieval,meta-learning",基于检索的神经代码汇总：一种元学习方法,代码摘要旨在自动生成代码摘要，近年来引起了人们的广泛研究。最近的方法通常采用神经机器翻译技术，在大型语料库上训练Seq2Seq模型，并假设它可以处理各种新的代码片段。然而，由于不同的领域、业务或编程风格，代码在实践中差异很大。因此，将如此多样的模式学习到一个单一的模型中是具有挑战性的。在本文中，我们提出了一个全新的基于元学习和代码检索的代码摘要框架，称为MLCS来解决这个问题。在该框架中，每个目标代码的摘要被形式化为少量的学习任务，其中其相似的示例被用作训练数据，测试示例本身就是测试示例。我们以排序和筛选的方式检索与目标代码相似的示例。给定一个神经代码汇总器，我们通过模型不可知元学习（MAML）将其优化为元学习器。在推理过程中，元学习器首先适应检索到的示例，并为目标代码生成排他性模型，然后生成其摘要。在真实世界数据集上的大量实验表明：（1）利用MLCS，标准Seq2Seq模型能够优于以前最先进的方法，包括神经模型和基于检索的神经模型；（2） MLCS可以在不修改其架构的情况下灵活地适应现有的神经代码汇总器，并且可以显著提高其性能，在BLEU-4上的相对增益高达112.7%，在ROUGE-L上的相对收益高达23.2%，在METEOR上的相对增益高达31.5%；（3） 与现有的基于检索的神经方法相比，MLCS可以更好地利用多个相似的例子，并对不同的检索者、看不见的检索语料库和低频词表现出更好的泛化能力。,代码摘要，深度学习，信息检索，元学习,,,
2TVJ6L8J,2023,https://doi.org/10.1109/TSE.2022.3229221,TSE 2023,Containerization for High Performance Computing Systems: Survey and Prospects,"Containers improve the efficiency in application deployment and thus have been widely utilised on Cloud and lately in High Performance Computing (HPC) environments. Containers encapsulate complex programs with their dependencies in isolated environments making applications more compatible and portable. Often HPC systems have higher security levels compared to Cloud systems, which restrict users’ ability to customise environments. Therefore, containers on HPC need to include a heavy package of libraries making their size relatively large. These libraries usually are specifically optimised for the hardware, which compromises portability of containers. Per contra, a Cloud container has smaller volume and is more portable. Furthermore, containers would benefit from orchestrators that facilitate deployment and management of containers at a large scale. Cloud systems in practice usually incorporate sophisticated container orchestration mechanisms as opposed to HPC systems. Nevertheless, some solutions to enable container orchestration on HPC systems have been proposed in state of the art. This paper gives a survey and taxonomy of efforts in both containerisation and its orchestration strategies on HPC systems. It highlights differences thereof between Cloud and HPC. Lastly, challenges are discussed and the potentials for research and engineering are envisioned.","AI,cloud computing,container,HPC,job scheduling,orchestration,resource management",高性能计算系统的容器化：综述与展望,容器提高了应用程序部署的效率，因此在云上以及最近的高性能计算（HPC）环境中得到了广泛应用。容器将复杂的程序及其依赖关系封装在隔离的环境中，使应用程序更加兼容和可移植。与云系统相比，HPC系统通常具有更高的安全级别，这限制了用户定制环境的能力。因此，HPC上的容器需要包含一个沉重的库包，使它们的大小相对较大。这些库通常是专门针对硬件进行优化的，这会影响容器的可移植性。根据contra的说法，云容器体积更小，更便于移植。此外，容器将受益于有助于大规模部署和管理容器的协调器。与HPC系统相比，云系统在实践中通常包含复杂的容器编排机制。然而，在现有技术中，已经提出了一些在HPC系统上实现容器编排的解决方案。本文对HPC系统上的容器化及其编排策略进行了调查和分类。它强调了Cloud和HPC之间的差异。最后，讨论了挑战，并展望了研究和工程的潜力。,AI，云计算，容器，HPC，作业调度，编排，资源管理,,,
BCBFHY75,2023,https://doi.org/10.1109/TSE.2023.3269804,TSE 2023,A Search-Based Testing Approach for Deep Reinforcement Learning Agents,"Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving, trading decisions, and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One of the ways to assess the safety of DRL agents is to test them to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Furthermore, their main goal is to test the robustness of DRL agents rather than testing the compliance of the agents’ policies with respect to requirements. Due to the huge state space of DRL environments, the high cost of test execution, and the black-box nature of DRL algorithms, exhaustive testing of DRL agents is impossible. In this paper, we propose a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL agent by effectively searching for failing executions of the agent within a limited testing budget. We rely on machine learning models and a dedicated genetic algorithm to narrow the search toward faulty episodes (i.e., sequences of states and actions produced by the DRL agent). We apply STARLA on Deep-Q-Learning agents trained on two different RL problems widely used as benchmarks and show that STARLA significantly outperforms Random Testing by detecting more faults related to the agent's policy. We also investigate how to extract rules that characterize faulty episodes of the DRL agent using our search results. Such rules can be used to understand the conditions under which the agent fails and thus assess the risks of deploying it.","Genetic algorithm,machine learning,reinforcement learning,state abstraction,testing",一种基于搜索的深度强化学习Agent测试方法,在过去的十年里，深度强化学习（DRL）算法越来越多地被用于解决各种决策问题，如自动驾驶、交易决策和机器人。然而，这些算法在安全关键环境中部署时面临着巨大的挑战，因为它们经常表现出可能导致潜在关键错误的错误行为。评估DRL代理安全性的方法之一是对其进行测试，以检测在执行过程中可能导致关键故障的故障。这就提出了一个问题，即我们如何有效地测试DRL政策，以确保其正确性和符合安全要求。大多数现有的DRL代理测试工作都使用干扰代理状态或动作的对抗性攻击。然而，这种攻击往往会导致不现实的环境状况。此外，他们的主要目标是测试DRL代理的稳健性，而不是测试代理的策略是否符合要求。由于DRL环境的巨大状态空间、高测试执行成本以及DRL算法的黑匣子性质，DRL代理的详尽测试是不可能的。在本文中，我们提出了一种基于搜索的强化学习代理测试方法（STARLA），通过在有限的测试预算内有效搜索代理的失败执行来测试DRL代理的策略。我们依靠机器学习模型和专用的遗传算法来将搜索范围缩小到故障事件（即DRL代理产生的状态和动作序列）。我们将STARLA应用于针对两个不同RL问题训练的深度学习代理，这两个问题被广泛用作基准，并表明STARLA通过检测更多与代理策略相关的故障，显著优于随机测试。我们还研究了如何使用搜索结果提取表征DRL代理故障事件的规则。这些规则可用于了解代理失败的条件，从而评估部署代理的风险。,遗传算法，机器学习，强化学习，状态抽象，测试,,,
V8PHN4WI,2023,https://doi.org/10.1109/TSE.2022.3222318,TSE 2023,Forecasting the Principal of Code Technical Debt in JavaScript Applications,"JavaScript (JS) is one of the most popular programming languages for developing client-side applications mainly due to allowing the adoption of different programming styles, not having strict syntax rules, and supporting a plethora of frameworks. The flexibility that the language provides may accelerate the development of application, but also pose threats to the quality of the final software product, e.g., introducing Technical Debt (TD). TD reflects the additional cost of software maintenance activities to implement new features, occurring due to poorly developed solutions. Being able to forecast the levels of TD in the future can be extremely valuable in managing TD, since it can contribute to informed decision making when designating future repayments and refactoring budget among a company's projects. Despite the popularity of JS and the undoubtful benefits of accurate TD forecasting, in the literature, there is available only a limited number of tools and methodologies that are able to: (a) forecast TD during software evolution, (b) provide a ground-truth TD quantifications to train forecasting, since TD tools that are available are based on different rulesets and none is recognized as a state-of-the-art solution, (c) take into consideration the language-specific characteristics of JS. As a main contribution for this study, we propose a methodology (along with a supporting tool) that supports the aforementioned goals based on the Backward Stepwise Regression and Auto-Regressive Integrated Moving Average (ARIMA). We evaluate the proposed approach through a case study on 19,636 releases of 105 open-source applications. The results point out that: (a) the proposed model can lead to an accurate prediction of TD, and (b) the Number of appearances of the “new” and “eval” keyword along with the number of “anonymous” and “arrow” functions are among the features of JavaScript language that are related to high levels of TD.","Code technical debt,JavaScript,software quality,source code quality",JavaScript应用程序中代码技术债务本金的预测,JavaScript（JS）是开发客户端应用程序最流行的编程语言之一，主要是因为它允许采用不同的编程风格，没有严格的语法规则，并且支持过多的框架。该语言提供的灵活性可能会加速应用程序的开发，但也会对最终软件产品的质量构成威胁，例如引入技术债务（TD）。TD反映了为实现新功能而进行的软件维护活动的额外成本，这些成本是由于开发不足的解决方案而产生的。能够预测未来TD的水平对管理TD非常有价值，因为在指定公司项目的未来还款和重构预算时，这有助于做出明智的决策。尽管JS很受欢迎，准确的TD预测也有着毋庸置疑的好处，但在文献中，只有有限数量的工具和方法能够：（a）在软件进化过程中预测TD，（b）提供真实的TD量化来训练预测，由于可用的TD工具基于不同的规则集，并且没有一个被认为是最先进的解决方案，（c）考虑到JS的特定语言特性。作为本研究的主要贡献，我们提出了一种基于后向逐步回归和自回归综合移动平均（ARIMA）的方法论（以及支持工具），以支持上述目标。我们通过对105个开源应用程序的19636个版本的案例研究来评估所提出的方法。结果指出：（a）所提出的模型可以准确地预测TD，（b）“new”和“eval”关键字的出现次数以及“匿名”和“箭头”函数的数量是JavaScript语言与高TD水平相关的特征之一。,代码技术债务，Java脚本，软件质量，源代码质量,,,
NPTK74TV,2023,https://doi.org/10.1145/3571848,TOSEM 2023,On the Discoverability of npm Vulnerabilities in Node.js Projects,"The reliance on vulnerable dependencies is a major threat to software systems. Dependency vulnerabilities are common and remain undisclosed for years. However, once the vulnerability is discovered and publicly known to the community, the risk of exploitation reaches its peak, and developers have to work fast to remediate the problem. While there has been a lot of research to characterize vulnerabilities in software ecosystems, none have explored the problem taking the discoverability into account. ","Software and its engineering,Software notations and tools,Software libraries and repositories",Node.js项目中npm漏洞的可发现性研究,对易受攻击的依赖性是对软件系统的主要威胁。依赖漏洞很常见，多年来一直未公开。然而，一旦漏洞被发现并为社区所知，利用的风险就会达到顶峰，开发人员必须迅速修复问题。虽然已经有很多研究来描述软件生态系统中的漏洞，但没有一项研究考虑到可发现性来探讨这个问题。,软件及其工程，软件符号和工具，软件库和存储库,,,
NDZ2K5AW,2023,https://doi.org/10.1145/3549541,TOSEM 2023,On Proving the Correctness of Refactoring Class Diagrams of MDE Metamodels,"Model Driven Engineering (MDE) is a general-purpose engineering methodology to elevate system design, maintenance, and analysis to corresponding activities on models. Models (graphical and/or textual) of a target application are automatically transformed into source code, performance models, Promela files (for model checking), and so on for system analysis and construction.","Software and its engineering,Software organization and properties,Software functional properties,Correctness,Functionality,Theory of computation,Semantics and reasoning,Program semantics",关于MDE元模型重构类图正确性的证明,模型驱动工程（MDE）是一种通用的工程方法，用于将系统设计、维护和分析提升到相应的模型活动中。目标应用程序的模型（图形和/或文本）会自动转换为源代码、性能模型、Promela文件（用于模型检查）等，用于系统分析和构建。,软件及其工程，软件组织和属性，软件功能属性，正确性，功能，计算理论，语义和推理，程序语义学,,,
UFVGDDCM,2023,https://doi.org/10.1145/3542945,TOSEM 2023,"Testing, Validation, and Verification of Robotic and Autonomous Systems: A Systematic Review","We perform a systematic literature review on testing, validation, and verification of robotic and autonomous systems (RAS). The scope of this review covers peer-reviewed research papers proposing, improving, or evaluating testing techniques, processes, or tools that address the system-level qualities of RAS. ","Computer systems organization,Embedded and cyber-physical systems,Robotics,Robotic autonomy,General and reference,Document types,Surveys and overviews,Software and its engineering,Software organization and properties,Software system structures",机器人和自主系统的测试、验证和验证：系统综述,我们对机器人和自主系统（RAS）的测试、验证和验证进行了系统的文献综述。本综述的范围包括提出、改进或评估解决RAS系统级质量的测试技术、过程或工具的同行评审研究论文。,计算机系统组织，嵌入式和网络物理系统，机器人学，机器人自主性，概论和参考，文件类型，综述和概述，软件及其工程，软件组织和属性，软件系统结构,,,
DXUI5HC5,2023,https://doi.org/10.1145/3532182,TOSEM 2023,Some Seeds Are Strong: Seeding Strategies for Search-based Test Case Selection,"The time it takes software systems to be tested is usually long. Search-based test selection has been a widely investigated technique to optimize the testing process. In this article, we propose a set of seeding strategies for the test case selection problem that generates the initial population of Pareto-based multi-objective algorithms, with the goals of (1) helping to find an overall better set of solutions and (2) enhancing the convergence of the algorithms. The seeding strategies were integrated with four state-of-the-art multi-objective search algorithms and applied into two contexts where regression-testing is paramount: (1) Simulation-based testing of Cyber-physical Systems and (2) Continuous Integration. For the first context, we evaluated our approach by using six fitness function combinations and six independent case studies, whereas in the second context, we derived a total of six fitness function combinations and employed four case studies. Our evaluation suggests that some of the proposed seeding strategies are indeed helpful for solving the multi-objective test case selection problem. Specifically, the proposed seeding strategies provided a higher convergence of the algorithms towards optimal solutions in 96% of the studied scenarios and an overall cost-effectiveness with a standard search budget in 85% of the studied scenarios.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",一些种子很强：基于搜索的测试用例选择的种子策略,测试软件系统所花费的时间通常很长。基于搜索的测试选择是一种广泛研究的优化测试过程的技术。在本文中，我们为测试用例选择问题提出了一组种子策略，该策略生成了基于Pareto的多目标算法的初始种群，目标是（1）帮助找到一组整体更好的解决方案，以及（2）增强算法的收敛性。种子策略与四种最先进的多目标搜索算法相结合，并应用于回归测试至关重要的两种情况：（1）网络物理系统的模拟测试和（2）连续集成。对于第一种情况，我们通过使用六个适应度函数组合和六个独立的案例研究来评估我们的方法，而在第二种情况下，我们总共推导了六个适合度函数组合，并使用了四个案例研究。我们的评估表明，所提出的一些种子策略确实有助于解决多目标测试用例选择问题。具体而言，所提出的种子策略在96%的研究场景中提供了更高的算法收敛性，并在85%的研究场景下提供了标准搜索预算的总体成本效益。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
65M38W32,2023,https://doi.org/10.1145/3593802,TOSEM 2023,Predicting the Change Impact of Resolving Defects by Leveraging the Topics of Issue Reports in Open Source Software Systems,"Upon receiving a new issue report, practitioners start by investigating the defect type, the potential fixing effort needed to resolve the defect and the change impact. Moreover, issue reports contain valuable information, such as, the title, description and severity, and researchers leverage the topics of issue reports as a collective metric portraying similar characteristics of a defect. Nonetheless, none of the existing studies leverage the defect topic, i.e., a semantic cluster of defects of the same nature, such as Performance, GUI, and Database, to estimate the change impact that represents the amount of change needed in terms of code churn and the number of files changed. To this end, in this article, we conduct an empirical study on 298,548 issue reports belonging to three large-scale open-source systems, i.e., Mozilla, Apache, and Eclipse, to estimate the change impact in terms of code churn or the number of files changed while leveraging the topics of issue reports. First, we adopt the Embedded Topic Model (ETM), a state-of-the-art topic modelling algorithm, to identify the topics. Second, we investigate the feasibility of predicting the change impact using the identified topics and other information extracted from the issue reports by building eight prediction models that classify issue reports requiring small or large change impact along two dimensions, i.e., the code churn size and the number of files changed. Our results suggest that XGBoost is the best-performing algorithm for predicting the change impact, with an AUC of 0.84, 0.76, and 0.73 for the code churn and 0.82, 0.71, and 0.73 for the number of files changed metric for Mozilla, Apache, and Eclipse, respectively. Our results also demonstrate that the topics of issue reports improve the recall of the prediction model by up to 45%.","Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Software organization and properties,Software functional properties",利用开源软件系统中的问题报告主题预测解决缺陷的变化影响,在收到新的问题报告后，从业者首先调查缺陷类型、解决缺陷所需的潜在修复工作以及更改影响。此外，问题报告包含有价值的信息，如标题、描述和严重程度，研究人员将问题报告的主题作为描述缺陷类似特征的集体指标。尽管如此，现有的研究都没有利用缺陷主题，即相同性质的缺陷的语义集群，如性能、GUI和数据库，来估计更改影响，该影响代表了代码流失和更改文件数量方面所需的更改量。为此，在本文中，我们对Mozilla、Apache和Eclipse三个大型开源系统的298548份问题报告进行了实证研究，以评估在利用问题报告主题的同时，从代码流失或更改文件数量方面的更改影响。首先，我们采用最先进的主题建模算法嵌入式主题模型（ETM）来识别主题。其次，我们通过构建八个预测模型来研究使用从问题报告中提取的已识别主题和其他信息来预测更改影响的可行性，这些模型将需要较小或较大更改影响的问题报告分为两个维度，即代码流失大小和更改的文件数量。我们的结果表明，XGBoost是预测更改影响的最佳算法，Mozilla、Apache和Eclipse的代码流失AUC分别为0.84、0.76和0.73，文件更改数量AUC分别是0.82、0.71和0.73。我们的结果还表明，问题报告的主题将预测模型的召回率提高了45%。,软件及其工程，软件创建和管理，软件开发协作，开放源码模型，软件组织和属性，软件功能属性,,,
5PC4Y7QF,2023,https://doi.org/10.1145/3550271,TOSEM 2023,Black-box Safety Analysis and Retraining of DNNs based on Feature Extraction and Clustering,"Deep neural networks (DNNs) have demonstrated superior performance over classical machine learning to support many features in safety-critical systems. Although DNNs are now widely used in such systems (e.g., self driving cars), there is limited progress regarding automated support for functional safety analysis in DNN-based systems. For example, the identification of root causes of errors, to enable both risk analysis and DNN retraining, remains an open problem. In this article, we propose SAFE, a black-box approach to automatically characterize the root causes of DNN errors. SAFE relies on a transfer learning model pre-trained on ImageNet to extract the features from error-inducing images. It then applies a density-based clustering algorithm to detect arbitrary shaped clusters of images modeling plausible causes of error. Last, clusters are used to effectively retrain and improve the DNN. The black-box nature of SAFE is motivated by our objective not to require changes or even access to the DNN internals to facilitate adoption. ","Computing methodologies,Machine learning,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis",基于特征提取和聚类的DNN黑盒安全性分析与再训练,深度神经网络（DNN）在支持安全关键系统的许多功能方面，已经证明了其优于经典机器学习的性能。尽管DNN现在被广泛用于此类系统（例如，自动驾驶汽车），但在基于DNN的系统中对功能安全分析的自动化支持方面进展有限。例如，识别错误的根本原因，以实现风险分析和DNN再培训，仍然是一个悬而未决的问题。在本文中，我们提出了SAFE，这是一种黑盒方法，可以自动表征DNN错误的根本原因。SAFE依赖于在ImageNet上预先训练的迁移学习模型来从引起错误的图像中提取特征。然后，它应用基于密度的聚类算法来检测任意形状的图像聚类，为可能的错误原因建模。最后，使用集群来有效地重新训练和改进DNN。SAFE的黑匣子性质是出于我们的目标，即不需要更改甚至不需要访问DNN内部以促进采用。,计算方法论，机器学习，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析,,,
AD2IZXC2,2023,https://doi.org/10.1145/3548683,TOSEM 2023,Storage State Analysis and Extraction of Ethereum Blockchain Smart Contracts,"In migrating and upgrading an Ethereum smart contract, it is necessary to transfer both the code as well as the stored data. Various methods attempt to migrate or upgrade a smart contract, but they are mostly manual, error-prone, and applicable only before deployment. Further, they have challenges in extracting the storage state of complex mapping data structures along with their keys. In this work, we present Smartmuv as an automatic source-code-based static analysis tool to analyze and extract the state from the storage-trie of smart contracts. Based on the abstract syntax tree and the control flow graphs of the Solidity source code, the tool analyzes each state variable including mapping types along the inheritance hierarchy. It also provides the upgrade algorithm that initializes the extracted state in the constructor of new smart contract. Smartmuv safely approximates the origin of the keys used in the mapping to extract values and has been able to extract the mapping state of 23,673 smart contracts with 95.7% overall precision. Moreover, we also validate the Smartmuv’s extracted state with the third-party tool Etherscan.","Software and its engineering,Software creation and management,Software development techniques,Software prototyping,Software notations and tools,Software maintenance tools",以太坊区块链智能合约的存储状态分析与提取,在迁移和升级以太坊智能合约时，有必要传输代码和存储的数据。各种方法试图迁移或升级智能合约，但它们大多是手动的、容易出错的，并且仅在部署之前适用。此外，他们在提取复杂映射数据结构的存储状态及其密钥方面面临挑战。在这项工作中，我们提出了Smartmuv作为一种基于源代码的自动静态分析工具，用于分析和提取智能合约的存储trie中的状态。基于Solidity源代码的抽象语法树和控制流图，该工具分析每个状态变量，包括继承层次结构中的映射类型。它还提供了在新智能合约的构造函数中初始化提取状态的升级算法。Smartmuv安全地近似映射中用于提取值的密钥的来源，并能够以95.7%的总体精度提取23673个智能合约的映射状态。此外，我们还使用第三方工具Etherscan验证Smartmuv的提取状态。,软件及其工程，软件创建和管理，软件开发技术，软件原型，软件符号和工具，软件维护工具,,,
Q5FA77SN,2023,https://doi.org/10.1145/3585004,TOSEM 2023,Modern Code Reviews - Survey of Literature and Practice,"Background: Modern Code Review (MCR) is a lightweight alternative to traditional code inspections. While secondary studies on MCR exist, it is uanknown whether the research community has targeted themes that practitioners consider important.","General and reference,Document types,Surveys and overviews,Software and its engineering,Software creation and management",现代法典评论——文献与实践综述,背景：现代代码审查（MCR）是传统代码检查的轻量级替代方案。虽然存在关于MCR的二级研究，但尚不清楚研究界是否有从业者认为重要的目标主题。,概述和参考，文件类型，综述和概述，软件及其工程，软件创建和管理,,,
2K6PN9ZX,2023,https://doi.org/10.1145/3534116,TOSEM 2023,"Graded Refinement, Retrenchment, and Simulation","Refinement of formal system models towards implementation has been a mainstay of system development since the inception of formal and Correct by Construction approaches to system development. However, pure refinement approaches do not always deal fluently with all desirable system requirements. This prompted the development of alternatives and generalizations, such as retrenchment. The crucial concept of simulation is key to judging the quality of the conformance between abstract and more concrete system models. Reformulations of these theoretical approaches are reprised and are embedded in a graded framework. The added flexibility this offers is intended to deal more effectively with the needs of applications in which the relationship between different levels of abstraction is not straightforward, and in which behavior can oscillate between conforming quite closely to an idealized abstraction and deviating quite far from it. The framework developed is confronted with an intentionally demanding case study: a model active control system for the protection of buildings during earthquakes. This offers many challenges: it is hybrid/cyber-physical; it has to respond to rather unpredictable inputs; and it has to straddle the gap between continuous behavior and discretized/quantized/numerical implementation.","Computer systems organization,Embedded and cyber-physical systems,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Software organization and properties,Software functional properties,Formal methods",分级细化、再强化和模拟,自系统开发的正式方法和“按构造正确”方法问世以来，对正式系统模型的细化一直是系统开发的支柱。然而，纯粹的精化方法并不总是能够流利地处理所有期望的系统需求。这促使人们制定了替代方案和概括办法，如裁员。仿真的关键概念是判断抽象和更具体的系统模型之间一致性的质量的关键。这些理论方法的改革是重复的，并嵌入分级框架中。它提供的额外灵活性旨在更有效地处理应用程序的需求，在这些应用程序中，不同抽象级别之间的关系并不简单，并且行为可以在非常接近理想化抽象和远离理想化抽象之间摇摆。所开发的框架面临着一个有意要求的案例研究：一个用于地震期间建筑物保护的模型主动控制系统。这带来了许多挑战：它是混合/网络物理的；它必须对相当不可预测的投入作出反应；并且它必须跨越连续行为和离散化/量化/数值实现之间的差距。,计算机系统组织，嵌入式和信息物理系统，软件及其工程，软件创建和管理，软件验证和确认，正式软件验证，软件组织和属性，软件功能属性，正式方法,,,
L9FJQFJX,2023,https://doi.org/10.1145/3597208,TOSEM 2023,An Empirical Study on GitHub Pull Requests' Reactions,"The pull request mechanism is commonly used to propose source code modifications and get feedback from the community before merging them into a software repository. On GitHub, practitioners can provide feedback on a pull request by either commenting on the pull request or simply reacting to it using a set of pre-defined GitHub reactions, i.e., “Thumbs-up”, “Laugh”, “Hooray”, “Heart”, “Rocket”, “Thumbs-down”, “Confused”, and “Eyes”. While a large number of prior studies investigated how to improve different software engineering activities (e.g., code review and integration) by investigating the feedback on pull requests, they focused only on pull requests’ comments as a source of feedback. However, the GitHub reactions, according to our preliminary study, contain feedback that is not manifested within the comments of pull requests. In fact, our preliminary analysis of six popular projects shows that a median of 100% of the practitioners who reacted to a pull request did not leave any comment suggesting that reactions can be a unique source of feedback to further improve the code review and integration process.","Software and its engineering,Software creation and management,Collaboration in software development",GitHub拉取请求反应的实证研究,拉请求机制通常用于在将源代码合并到软件存储库之前提出修改源代码的建议并从社区获得反馈。在GitHub上，从业者可以通过评论拉取请求或简单地使用一组预定义的GitHub反应来提供对拉取请求的反馈，即“大拇指向上”、“大笑”、“Hooray”、“心脏”、“火箭”、“大拇指向下”、“困惑”和“眼睛”。尽管大量先前的研究调查了如何通过调查拉取请求的反馈来改进不同的软件工程活动（例如，代码审查和集成），但它们只关注拉取请求作为反馈来源的评论。然而，根据我们的初步研究，GitHub的反应包含的反馈并没有体现在拉取请求的评论中。事实上，我们对六个流行项目的初步分析表明，对撤回请求做出反应的从业者中，有100%的人没有留下任何评论，这表明反应可以成为进一步改进代码审查和集成过程的独特反馈来源。,软件及其工程，软件创建和管理，软件开发中的协作,,,
GA4YAFWI,2023,https://doi.org/10.1145/3533021,TOSEM 2023,A Machine Learning Approach for Automated Filling of Categorical Fields in Data Entry Forms,"Users frequently interact with software systems through data entry forms. However, form filling is time-consuming and error-prone. Although several techniques have been proposed to auto-complete or pre-fill fields in the forms, they provide limited support to help users fill categorical fields, i.e., fields that require users to choose the right value among a large set of options.","Computing methodologies,Machine learning,Machine learning approaches,Learning in probabilistic graphical models,Bayesian network models,Information systems,Information retrieval,Retrieval tasks and goals,Recommender systems,Software and its engineering,Software organization and properties,Extra-functional properties,Software usability",一种用于自动填充数据输入表单中类别字段的机器学习方法,用户经常通过数据输入表单与软件系统进行交互。然而，表单填写耗时且容易出错。尽管已经提出了几种技术来自动填写或预填写表单中的字段，但它们在帮助用户填写分类字段方面提供的支持有限，即要求用户在一大组选项中选择正确值的字段。,计算方法，机器学习，机器学习方法，概率图形模型学习，贝叶斯网络模型，信息系统，信息检索，检索任务和目标，推荐系统，软件及其工程，软件组织和属性，额外功能属性，软件可用性,,,
BPAJTZMU,2023,https://doi.org/10.1145/3514232,TOSEM 2023,Fold2Vec: Towards a Statement-Based Representation of Code for Code Comprehension,"We introduce a novel approach to source code representation to be used in combination with neural networks. Such a representation is designed to permit the production of a continuous vector for each code statement. In particular, we present how the representation is produced in the case of Java source code. We test our representation for three tasks: code summarization, statement separation, and code search. We compare with the state-of-the-art non-autoregressive and end-to-end models for these tasks. We conclude that all tasks benefit from the proposed representation to boost their performance in terms of F1-score, accuracy, and mean reciprocal rank, respectively. Moreover, we show how models trained on code summarization and models trained on statement separation can be combined to address methods with tangled responsibilities, meaning that these models can be used to detect code misconduct.","Computing methodologies,Machine learning,Machine learning approaches,Neural networks",Fold2Vec：面向代码理解的基于语句的代码表示,我们介绍了一种与神经网络结合使用的源代码表示的新方法。这样的表示被设计为允许为每个代码语句产生连续向量。特别是，我们介绍了在Java源代码的情况下如何生成表示。我们测试了三个任务的表示：代码摘要、语句分离和代码搜索。我们将这些任务与最先进的非自回归和端到端模型进行了比较。我们得出的结论是，所有任务都受益于所提出的表示，以分别提高其在F1级、准确性和平均倒数排名方面的性能。此外，我们还展示了如何将受过代码摘要训练的模型和受过语句分离训练的模型结合起来，以解决职责复杂的方法，这意味着这些模型可以用于检测代码不当行为。,计算方法，机器学习，机器学习方法，神经网络,,,
7IXKN3Y5,2023,https://doi.org/10.1145/3533818,TOSEM 2023,Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments,"Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints.","Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation,Software defect analysis,Software testing and debugging",虚拟环境中自动驾驶汽车的单目标和多目标测试用例优先级,模拟环境测试有助于识别自动驾驶汽车（SDCs）的关键故障场景。基于模拟的测试比现场操作测试更安全，并且允许在部署前检测软件缺陷。然而，这些测试非常昂贵，而且数量太多，无法在有限的时间限制内频繁运行。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认，软件缺陷分析，软件测试和调试,,,
QWYX55NK,2023,https://doi.org/10.1145/3593803,TOSEM 2023,Automatic Core-Developer Identification on GitHub: A Validation Study,"Many open-source software projects are self-organized and do not maintain official lists with information on developer roles. So, knowing which developers take core and maintainer roles is, despite being relevant, often tacit knowledge. We propose a method to automatically identify core developers based on role permissions of privileged events triggered in GitHub issues and pull requests. In an empirical study on 25/GitHub projects, (1) we validate the set of automatically identified core developers with a sample of project-reported developer lists, and (2) we use our set of identified core developers to assess the accuracy of state-of-the-art unsupervised developer classification methods. Our results indicate that the set of core developers, which we extracted from privileged issue events, is sound and the accuracy of state-of-the-art unsupervised classification methods depends mainly on the data source (commit data versus issue data) rather than the network-construction method (directed versus undirected, etc.). In perspective, our results shall guide research and practice to choose appropriate unsupervised classification methods, and our method can help create reliable ground-truth data for training supervised classification methods.","Software and its engineering,Software creation and management,Collaboration in software development,Open source model",GitHub上的核心开发者自动识别：一项验证研究,许多开源软件项目都是自组织的，不维护包含开发者角色信息的官方列表。因此，知道哪些开发人员扮演核心和维护人员的角色，尽管是相关的，但往往是隐性的知识。我们提出了一种基于GitHub问题和拉取请求中触发的特权事件的角色权限自动识别核心开发人员的方法。在一项针对25/GitHub项目的实证研究中，（1）我们用项目报告的开发者列表样本验证了一组自动识别的核心开发者，（2）我们使用一组识别的核心开发人员来评估最先进的无监督开发者分类方法的准确性。我们的结果表明，我们从特权问题事件中提取的核心开发人员集合是健全的，最先进的无监督分类方法的准确性主要取决于数据源（提交数据与问题数据），而不是网络构建方法（有向与无向等），我们的研究结果将指导研究和实践选择合适的无监督分类方法，我们的方法可以帮助创建可靠的地面实况数据来训练有监督的分类方法。,软件及其工程，软件创建和管理，软件开发协作，开放源码模型,,,
JNAZYXAG,2023,https://doi.org/10.1145/3529318,TOSEM 2023,Testing Feedforward Neural Networks Training Programs,"At present, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. ","Mathematics of computing,Probability and statistics,Probabilistic algorithms,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation,Theory and algorithms for application domains,Machine learning theory,Models of learning",测试前馈神经网络训练程序,目前，我们看到越来越多的人致力于提高深度神经网络（DNN）的性能和可信度，目的是使其能够应用于自动驾驶汽车或飞机防撞系统等安全关键系统。提出了多种测试技术来生成测试用例，这些测试用例可以暴露DNN模型行为中的不一致性。这些技术隐含地假设训练程序是无错误的并且配置适当。然而，对于一个新问题来说，满足这一假设需要大量的工程工作来准备数据、设计DNN、实施训练程序和调整超参数，以产生当前自动测试数据生成器搜索角落案例行为的模型。所有这些模型训练步骤都可能容易出错。因此，在基于DNN的软件系统的所有工程步骤中检测和纠正错误是至关重要的，而不仅仅是在生成的DNN模型上。,计算数学，概率统计，概率算法，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论，应用领域的理论和算法，机器学习理论，学习模型,,,
Z64G38YV,2023,https://doi.org/10.1145/3571850,TOSEM 2023,Blindspots in Python and Java APIs Result in Vulnerable Code,"Blindspots in APIs can cause software engineers to introduce vulnerabilities, but such blindspots are, unfortunately, common. We study the effect APIs with blindspots have on developers in two languages by replicating a 109-developer, 24-Java-API controlled experiment. Our replication applies to Python and involves 129 new developers and 22 new APIs. We find that using APIs with blindspots statistically significantly reduces the developers’ ability to correctly reason about the APIs in both languages, but that the effect is more pronounced for Python. Interestingly, for Java, the effect increased with complexity of the code relying on the API, whereas for Python, the opposite was true. This suggests that Python developers are less likely to notice potential for vulnerabilities in complex code than in simple code, whereas Java developers are more likely to recognize the extra complexity and apply more care, but are more careless with simple code. Whether the developers considered API uses to be more difficult, less clear, and less familiar did not have an effect on their ability to correctly reason about them. Developers with better long-term memory recall were more likely to correctly reason about APIs with blindspots, but short-term memory, processing speed, episodic memory, and memory span had no effect. Surprisingly, professional experience and expertise did not improve the developers’ ability to reason about APIs with blindspots across both languages, with long-term professionals with many years of experience making mistakes as often as relative novices. Finally, personality traits did not significantly affect the Python developers’ ability to reason about APIs with blindspots, but less extroverted and more open developers were better at reasoning about Java APIs with blindspots. Overall, our findings suggest that blindspots in APIs are a serious problem across languages, and that experience and education alone do not overcome that problem, suggesting that tools are needed to help developers recognize blindspots in APIs as they write code that uses those APIs.","Human-centered computing,Human computer interaction (HCI),HCI design and evaluation methods,User studies,Security and privacy,Software and application security,Software security engineering,Software and its engineering",Python和Java API中的盲点导致易受攻击的代码,API中的盲点可能会导致软件工程师引入漏洞，但不幸的是，这种盲点很常见。我们通过复制109开发人员24-Java-API控制的实验，研究了带有盲点的API对两种语言的开发人员的影响。我们的复制适用于Python，涉及129名新开发人员和22个新API。我们发现，在统计上，使用带有盲点的API会显著降低开发人员对两种语言中的API进行正确推理的能力，但对Python来说，这种影响更为明显。有趣的是，对于Java，这种效果随着依赖API的代码的复杂性而增加，而对于Python，情况正好相反。这表明，与简单代码相比，Python开发人员不太可能注意到复杂代码中潜在的漏洞，而Java开发人员更有可能意识到额外的复杂性并更加谨慎，但对简单代码更为粗心。开发人员是否认为API使用更难、更不清楚、更不熟悉，并没有影响他们正确推理的能力。具有更好的长期内存回忆的开发人员更有可能正确地推断出有盲点的API，但短期内存、处理速度、情节内存和内存跨度没有影响。令人惊讶的是，专业经验和专业知识并没有提高开发人员对两种语言中存在盲点的API进行推理的能力，具有多年经验的长期专业人员犯错误的频率与相对新手一样高。最后，性格特征并没有显著影响Python开发人员对有盲点的API进行推理的能力，但不那么外向、更开放的开发人员更善于对有盲点地Java API进行推理。总的来说，我们的研究结果表明，API中的盲点是跨语言的一个严重问题，仅靠经验和教育并不能解决这个问题，这表明需要一些工具来帮助开发人员在编写使用这些API的代码时识别API中的盲区。,以人为中心的计算，人机交互，人机交互设计和评估方法，用户研究，安全和隐私，软件和应用安全，软件安全工程，软件及其工程,,,
DIZ282BU,2023,https://doi.org/10.1145/3542947,TOSEM 2023,Dealing with Belief Uncertainty in Domain Models,"There are numerous domains in which information systems need to deal with uncertain information. These uncertainties may originate from different reasons such as vagueness, imprecision, incompleteness, or inconsistencies, and in many cases, they cannot be neglected. In this article, we are interested in representing and processing uncertain information in domain models, considering the stakeholders’ beliefs (opinions). We show how to associate beliefs to model elements and how to propagate and operate with their associated uncertainty so that domain experts can individually reason about their models enriched with their personal opinions. In addition, we address the challenge of combining the opinions of different domain experts on the same model elements, with the goal to come up with informed collective decisions. We provide different strategies and a methodology to optimally merge individual opinions.","Software and its engineering,Software notations and tools,System description languages,Software organization and properties,Software system structures,Software system models,Model-driven software engineering",处理领域模型中的置信不确定性,信息系统需要处理不确定信息的领域很多。这些不确定性可能源于不同的原因，如模糊、不精确、不完整或不一致，在许多情况下，它们不能被忽视。在本文中，我们感兴趣的是在领域模型中表示和处理不确定信息，并考虑利益相关者的信念（意见）。我们展示了如何将信念与模型元素相关联，以及如何传播和操作与之相关的不确定性，以便领域专家能够对他们的模型进行个性化推理，丰富他们的个人观点。此外，我们还应对了将不同领域专家对同一模型元素的意见结合起来的挑战，目标是做出知情的集体决定。我们提供不同的策略和方法，以最佳地合并个人意见。,软件及其工程，软件符号和工具，系统描述语言，软件组织和属性，软件系统结构，软件系统模型，模型驱动软件工程,,,
AWLRET9Q,2023,https://doi.org/10.1145/3583563,TOSEM 2023,Actor-Driven Decomposition of Microservices through Multi-level Scalability Assessment,"The microservices architectural style has gained widespread acceptance. However, designing applications according to this style is still challenging. Common difficulties concern finding clear boundaries that guide decomposition while ensuring performance and scalability. With the aim of providing software architects and engineers with a systematic methodology, we introduce a novel actor-driven decomposition strategy to complement the domain-driven design and overcome some of its limitations by reaching a finer modularization yet enforcing performance and scalability improvements. The methodology uses a multi-level scalability assessment framework that supports decision-making over iterative steps. At each iteration, architecture alternatives are quantitatively evaluated at multiple granularity levels. The assessment helps architects to understand the extent to which architecture alternatives increase or decrease performance and scalability. We applied the methodology to drive further decomposition of the core microservices of a real data-intensive smart mobility application and an existing open-source benchmark in the e-commerce domain. The results of an in-depth evaluation show that the approach can effectively support engineers in (i) decomposing monoliths or coarse-grained microservices into more scalable microservices and (ii) comparing among alternative architectures to guide decision-making for their deployment in modern infrastructures that orchestrate lightweight virtualized execution units.","Software and its engineering,Software creation and management,Designing software,Software organization and properties,Extra-functional properties,Software performance,Software system structures,Distributed systems organizing principles,Cloud computing,Software architectures",通过多级可扩展性评估实现微服务的行动者驱动分解,微服务的架构风格已经得到广泛接受。然而，根据这种风格设计应用程序仍然具有挑战性。常见的困难是在确保性能和可扩展性的同时，找到指导分解的清晰边界。为了向软件架构师和工程师提供系统的方法论，我们引入了一种新的行动者驱动的分解策略，以补充领域驱动的设计，并通过实现更精细的模块化来克服其一些局限性，同时加强性能和可扩展性的改进。该方法使用多级可扩展性评估框架，支持对迭代步骤进行决策。在每次迭代中，体系结构备选方案都会在多个粒度级别上进行定量评估。该评估有助于架构师了解体系结构备选方案在多大程度上提高或降低了性能和可扩展性。我们应用该方法来推动对真实数据密集型智能移动应用程序和电子商务领域现有开源基准的核心微服务的进一步分解。深入评估的结果表明，该方法可以有效地支持工程师（i）将单块或粗粒度微服务分解为更具可扩展性的微服务，以及（ii）在替代架构之间进行比较，以指导其在编排轻量级虚拟化执行单元的现代基础设施中的部署决策。,软件及其工程，软件创建和管理，设计软件，软件组织和属性，额外功能属性，软件性能，软件系统结构，分布式系统组织原则，云计算，软件体系结构,,,
HFIHL85N,2023,https://doi.org/10.1145/3569932,TOSEM 2023,Exploring Better Black-Box Test Case Prioritization via Log Analysis,"Test case prioritization (TCP) has been widely studied in regression testing, which aims to optimize the execution order of test cases so as to detect more faults earlier. TCP has been divided into white-box test case prioritization (WTCP) and black-box test case prioritization (BTCP). WTCP can achieve better prioritization effectiveness by utilizing source code information, but is not applicable in many practical scenarios (where source code is unavailable, e.g., outsourced testing). BTCP has the benefit of not relying on source code information, but tends to be less effective than WTCP. That is, both WTCP and BTCP suffer from limitations in the practical use.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",通过日志分析探索更好的黑盒测试用例优先级,测试用例优先级（TCP）在回归测试中得到了广泛的研究，其目的是优化测试用例的执行顺序，以便更早地检测到更多的故障。TCP分为白盒测试用例优先级（WTCP）和黑盒测试用例优先（BTCP）。WTCP可以通过利用源代码信息来实现更好的优先级有效性，但不适用于许多实际场景（源代码不可用，例如外包测试）。BTCP具有不依赖源代码信息的优点，但往往不如WTCP有效。也就是说，WTCP和BTCP在实际使用中都受到限制。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
LTR66VXE,2023,https://doi.org/10.1145/3514233,TOSEM 2023,The Weights Can Be Harmful: Pareto Search versus Weighted Search in Multi-objective Search-based Software Engineering,"In presence of multiple objectives to be optimized in Search-Based Software Engineering (SBSE), Pareto search has been commonly adopted. It searches for a good approximation of the problem’s Pareto-optimal solutions, from which the stakeholders choose the most preferred solution according to their preferences. However, when clear preferences of the stakeholders (e.g., a set of weights that reflect relative importance between objectives) are available prior to the search, weighted search is believed to be the first choice, since it simplifies the search via converting the original multi-objective problem into a single-objective one and enables the search to focus on what only the stakeholders are interested in.","Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation,Empirical software validation,Software organization and properties,Extra-functional properties,Software performance",权重可能是有害的：基于多目标搜索的软件工程中的Pareto搜索与加权搜索,在基于搜索的软件工程（SBSE）中，存在多个需要优化的目标，Pareto搜索已被普遍采用。它搜索问题的Pareto最优解的良好近似，利益相关者根据自己的偏好从中选择最优选的解决方案。然而，当利益相关者的明确偏好（例如，反映目标之间相对重要性的一组权重）在搜索之前可用时，加权搜索被认为是第一选择，因为它通过将原始多目标问题转换为单目标问题来简化搜索，并使搜索能够专注于只有利益相关者感兴趣的内容。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认，经验软件验证，软件组织和属性，额外功能属性，软件性能,,,
TT4VBEYH,2023,https://doi.org/10.1145/3571853,TOSEM 2023,Do Performance Aspirations Matter for Guiding Software Configuration Tuning? An Empirical Investigation under Dual Performance Objectives,"Configurable software systems can be tuned for better performance. Leveraging on some Pareto optimizers, recent work has shifted from tuning for a single, time-related performance objective to two intrinsically different objectives that assess distinct performance aspects of the system, each with varying aspirations to be satisfied, e.g., “the latency is less than 10s” while “the memory usage is no more than 1GB”. Before we design better optimizers, a crucial engineering decision to make therein is how to handle the performance requirements with clear aspirations in the tuning process. For this, the community takes two alternative optimization models: either quantifying and incorporating the aspirations into the search objectives that guide the tuning, or not considering the aspirations during the search but purely using them in the later decision-making process only. However, despite being a crucial decision that determines how an optimizer can be designed and tailored, there is a rather limited understanding of which optimization model should be chosen under what particular circumstance, and why.","Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation,Empirical software validation,Software organization and properties,Extra-functional properties,Software performance",性能要求对指导软件配置调优有重要意义吗？双重绩效目标下的实证研究,可配置的软件系统可以进行调整以获得更好的性能。利用一些Pareto优化器，最近的工作已经从单一的、与时间相关的性能目标转变为两个本质上不同的目标，这两个目标评估系统的不同性能方面，每个目标都有不同的满足愿望，例如，“延迟小于10s”而“内存使用不超过1GB”。在我们设计更好的优化器之前，需要在其中做出的一个关键工程决策是如何在调优过程中以明确的愿望处理性能要求。为此，社区采用了两种可选的优化模型：要么量化愿望并将其纳入指导调整的搜索目标，要么在搜索过程中不考虑愿望，而仅在稍后的决策过程中使用它们。然而，尽管这是一个决定如何设计和定制优化器的关键决策，但对于在什么特定情况下应该选择哪个优化模型以及为什么选择，人们的理解相当有限。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认，经验软件验证，软件组织和属性，额外功能属性，软件性能,,,
SQL38G8P,2023,https://doi.org/10.1145/3587155,TOSEM 2023,Toward Understanding Deep Learning Framework Bugs,"DL frameworks are the basis of constructing all DL programs and models, and thus their bugs could lead to the unexpected behaviors of any DL program or model relying on them. Such a wide effect demonstrates the necessity and importance of guaranteeing DL frameworks’ quality. Understanding the characteristics of DL framework bugs is a fundamental step for this quality assurance task, facilitating designing effective bug detection and debugging approaches. Hence, in this work, we conduct the most large-scale study on 1,000 bugs from four popular and diverse DL frameworks (i.e., TensorFlow, PyTorch, MXNet, and DL4J). By analyzing the root causes and symptoms of DL framework bugs associated with five components decomposed from DL frameworks, as well as measuring test coverage achieved by three state-of-the-art testing techniques, we obtain 12 major findings for the comprehensive understanding of DL framework bugs and the current status of existing DL framework testing practice, and then provide a series of actionable guidelines for better DL framework bug detection and debugging. Finally, based on the guidelines, we design and implement a prototype DL-framework testing tool, called TenFuzz, which is evaluated to be effective and finds three unknown bugs on the latest TensorFlow framework in a preliminary study, indicating the significance of our guidelines.","General and reference,Cross-computing tools and techniques,Empirical studies,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software notations and tools,Software libraries and repositories",深入理解深度学习框架错误,DL框架是构建所有DL程序和模型的基础，因此它们的错误可能导致任何依赖它们的DL程序或模型的意外行为。这种广泛的影响表明了保证DL框架质量的必要性和重要性。了解DL框架错误的特征是这项质量保证任务的基本步骤，有助于设计有效的错误检测和调试方法。因此，在这项工作中，我们对来自四个流行且多样化的DL框架（即TensorFlow、PyTorch、MXNet和DL4J）的1000个bug进行了最大规模的研究。通过分析与从DL框架分解的五个组件相关的DL框架错误的根源和症状，以及测量三种最先进的测试技术实现的测试覆盖率，我们获得了12个主要发现，以全面理解DL框架错误和现有DL框架测试实践的现状，然后为更好的DL框架错误检测和调试提供一系列可操作的指导方针。最后，在指南的基础上，我们设计并实现了一个名为TenFuzz的DL框架测试工具原型，该工具被评估为有效的，并在初步研究中发现了最新的TensorFlow框架上的三个未知错误，表明了我们指南的意义。,通则和参考，交叉计算工具和技术，经验研究，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件符号和工具，软件库和存储库,,,
9PYX8PHM,2023,https://doi.org/10.1145/3582573,TOSEM 2023,QuoTe: Quality-oriented Testing for Deep Learning Systems,"Recently, there has been significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is DL testing—that is, given a property of test, defects of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the neuron coverage metrics, which are commonly used by most existing DL testing approaches, are not necessarily correlated with model quality (e.g., robustness, the most studied model property), and are also not an effective measurement on the confidence of the model quality after testing. In this work, we address this gap by proposing a novel testing framework called QuoTe (i.e., Quality-oriented Testing). A key part of QuoTe is a quantitative measurement on (1) the value of each test case in enhancing the model property of interest (often via retraining) and (2) the convergence quality of the model property improvement. QuoTe utilizes the proposed metric to automatically select or generate valuable test cases for improving model quality. The proposed metric is also a lightweight yet strong indicator of how well the improvement converged. Extensive experiments on both image and tabular datasets with a variety of model architectures confirm the effectiveness and efficiency of QuoTe in improving DL model quality—that is, robustness and fairness. As a generic quality-oriented testing framework, future adaptations can be made to other domains (e.g., text) as well as other model properties.","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",QuoTe：面向质量的深度学习系统测试,最近，人们对将软件工程技术应用于深度学习（DL）系统的质量保证的兴趣显著增长。一个流行的方向是DL测试——也就是说，给定测试的性质，DL系统的缺陷可以通过模糊化或在某些测试指标的帮助下进行引导搜索来发现。然而，最近的研究表明，大多数现有DL测试方法通常使用的神经元覆盖度量不一定与模型质量相关（例如，鲁棒性，研究最多的模型属性），并且也不是测试后模型质量置信度的有效测量。在这项工作中，我们通过提出一种新的测试框架QuoTe（即面向质量的测试）来解决这一差距。QuoTe的一个关键部分是对（1）每个测试用例在增强感兴趣的模型特性方面的价值（通常通过重新训练）和（2）模型特性改进的收敛质量的定量测量。QuoTe利用所提出的度量来自动选择或生成有价值的测试用例，以提高模型质量。所提出的指标也是一个轻量级但有力的指标，表明改进的收敛程度。在具有各种模型架构的图像和表格数据集上进行的大量实验证实了QuoTe在提高DL模型质量方面的有效性和效率，即鲁棒性和公平性。作为一个通用的面向质量的测试框架，未来可以对其他领域（例如文本）以及其他模型属性进行调整。,计算方法，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
FI6B3WNI,2023,https://doi.org/10.1145/3583561,TOSEM 2023,A Comprehensive Empirical Study of Bias Mitigation Methods for Machine Learning Classifiers,"Software bias is an increasingly important operational concern for software engineers. We present a large-scale, comprehensive empirical study of 17 representative bias mitigation methods for Machine Learning (ML) classifiers, evaluated with 11 ML performance metrics (e.g., accuracy), 4 fairness metrics, and 20 types of fairness-performance tradeoff assessment, applied to 8 widely-adopted software decision tasks. The empirical coverage is much more comprehensive, covering the largest numbers of bias mitigation methods, evaluation metrics, and fairness-performance tradeoff measures compared to previous work on this important software property. We find that (1) the bias mitigation methods significantly decrease ML performance in 53% of the studied scenarios (ranging between 42%∼66% according to different ML performance metrics); (2) the bias mitigation methods significantly improve fairness measured by the 4 used metrics in 46% of all the scenarios (ranging between 24%∼59% according to different fairness metrics); (3) the bias mitigation methods even lead to decrease in both fairness and ML performance in 25% of the scenarios; (4) the effectiveness of the bias mitigation methods depends on tasks, models, the choice of protected attributes, and the set of metrics used to assess fairness and ML performance; (5) there is no bias mitigation method that can achieve the best tradeoff in all the scenarios. The best method that we find outperforms other methods in 30% of the scenarios. Researchers and practitioners need to choose the bias mitigation method best suited to their intended application scenario(s).","Computing methodologies,Machine learning,Software and its engineering,Software creation and management",机器学习分类器偏差抑制方法的综合实证研究,对于软件工程师来说，软件偏见是一个越来越重要的操作问题。我们对机器学习（ML）分类器的17种代表性偏差缓解方法进行了大规模、全面的实证研究，用11种ML性能指标（如准确性）、4种公平性指标和20种类型的公平性-性能权衡评估进行了评估，应用于8个广泛采用的软件决策任务。与之前关于这一重要软件特性的工作相比，经验覆盖范围要全面得多，涵盖了数量最多的偏差缓解方法、评估指标和公平性能权衡措施。我们发现（1）在53%的研究场景中，偏差缓解方法显著降低了ML性能（根据不同的ML性能指标，范围在42%～66%之间）；（2） 在46%的所有场景中，偏差缓解方法显著提高了由4个使用的指标衡量的公平性（根据不同的公平性指标，范围在24%～59%之间）；（3） 在25%的场景中，偏差缓解方法甚至导致公平性和ML性能的降低；（4） 偏差缓解方法的有效性取决于任务、模型、受保护属性的选择以及用于评估公平性和ML性能的一组度量；（5） 没有一种可以在所有场景中实现最佳折衷的偏差缓解方法。我们发现的最佳方法在30%的场景中优于其他方法。研究人员和从业者需要选择最适合其预期应用场景的偏见缓解方法。,计算方法论，机器学习，软件及其工程，软件创建和管理,,,
T4287MYZ,2023,https://doi.org/10.1145/3528100,TOSEM 2023,HINNPerf: Hierarchical Interaction Neural Network for Performance Prediction of Configurable Systems,"Modern software systems are usually highly configurable, providing users with customized functionality through various configuration options. Understanding how system performance varies with different option combinations is important to determine optimal configurations that meet specific requirements. Due to the complex interactions among multiple options and the high cost of performance measurement under a huge configuration space, it is challenging to study how different configurations influence the system performance. To address these challenges, we propose HINNPerf, a novel hierarchical interaction neural network for performance prediction of configurable systems. HINNPerf employs the embedding method and hierarchic network blocks to model the complicated interplay between configuration options, which improves the prediction accuracy of the method. In addition, we devise a hierarchical regularization strategy to enhance the model robustness. Empirical results on 10 real-world configurable systems show that our method statistically significantly outperforms state-of-the-art approaches by achieving average 22.67% improvement in prediction accuracy. In addition, combined with the Integrated Gradients method, the designed hierarchical architecture provides some insights about the interaction complexity and the significance of configuration options, which might help users and developers better understand how the configurable system works and efficiently identify significant options affecting the performance.","Software and its engineering,Software organization and properties,Extra-functional properties,Software performance",HINNPerf:可配置系统性能预测的层次交互神经网络,现代软件系统通常是高度可配置的，通过各种配置选项为用户提供定制的功能。了解系统性能如何随不同选项组合而变化，对于确定满足特定要求的最佳配置非常重要。由于多个选项之间的复杂交互以及在巨大的配置空间下性能测量的高成本，研究不同的配置如何影响系统性能具有挑战性。为了应对这些挑战，我们提出了HINNPerf，这是一种用于可配置系统性能预测的新型层次交互神经网络。HINNPerf采用嵌入方法和分层网络块对配置选项之间复杂的相互作用进行建模，提高了该方法的预测精度。此外，我们还设计了一种层次正则化策略来增强模型的鲁棒性。在10个真实世界可配置系统上的经验结果表明，我们的方法在预测精度上平均提高了22.67%，在统计上显著优于最先进的方法。此外，结合集成梯度方法，所设计的分层架构提供了一些关于交互复杂性和配置选项重要性的见解，这可能有助于用户和开发人员更好地了解可配置系统的工作方式，并有效地识别影响性能的重要选项。,软件及其工程，软件组织和属性，额外功能属性，软件性能,,,
TPPX2VUU,2023,https://doi.org/10.1145/3546945,TOSEM 2023,Assessing the Alignment between the Information Needs of Developers and the Documentation of Programming Languages: A Case Study on Rust,"Programming language documentation refers to the set of technical documents that provide application developers with a description of the high-level concepts of a language (e.g., manuals, tutorials, and API references). Such documentation is essential to support application developers in effectively using a programming language. One of the challenges faced by documenters (i.e., personnel that design and produce documentation for a programming language) is to ensure that documentation has relevant information that aligns with the concrete needs of developers, defined as the missing knowledge that developers acquire via voluntary search. In this article, we present an automated approach to support documenters in evaluating the differences and similarities between the concrete information need of developers and the current state of documentation (a problem that we refer to as the topical alignment of a programming language documentation). Our approach leverages semi-supervised topic modelling that uses domain knowledge to guide the derivation of topics. We initially train a baseline topic model from a set of Rust-related Q&A posts. We then use this baseline model to determine the distribution of topic probabilities of each document of the official Rust documentation. Afterwards, we assess the similarities and differences between the topics of the Q&A posts and the official documentation. Our results show a relatively high level of topical alignment in Rust documentation. Still, information about specific topics is scarce in both the Q&A websites and the documentation, particularly related topics with programming niches such as network, game, and database development. For other topics (e.g., related topics with language features such as structs, patterns and matchings, and foreign function interface), information is only available on Q&A websites while lacking in the official documentation. Finally, we discuss implications for programming language documenters, particularly how to leverage our approach to prioritize topics that should be added to the documentation.","Software and its engineering,Software creation and management",评估开发人员的信息需求与编程语言文档之间的一致性：以Rust为例,编程语言文档是指为应用程序开发人员提供语言高级概念描述的一组技术文档（例如，手册、教程和API参考资料）。这样的文档对于支持应用程序开发人员有效地使用编程语言至关重要。文档编制人员（即为编程语言设计和制作文档的人员）面临的挑战之一是确保文档具有符合开发人员具体需求的相关信息，即开发人员通过自愿搜索获得的缺失知识。在本文中，我们提出了一种自动化的方法来支持文档管理员评估开发人员的具体信息需求与文档的当前状态之间的差异和相似性（我们称之为编程语言文档的主题对齐问题）。我们的方法利用半监督主题建模，该建模使用领域知识来指导主题的推导。我们最初从一组与Rust相关的问答帖子中训练一个基线主题模型。然后，我们使用这个基线模型来确定Rust官方文档的每个文档的主题概率分布。之后，我们评估了问答帖子和官方文件主题之间的异同。我们的研究结果显示，Rust文档中的主题对齐程度相对较高。尽管如此，问答网站和文档中关于特定主题的信息都很少，尤其是与网络、游戏和数据库开发等编程领域相关的主题。对于其他主题（例如，具有语言特征的相关主题，如结构、模式和匹配以及外语功能界面），信息仅在问答网站上提供，而在官方文档中缺乏。最后，我们讨论了对编程语言文档编制者的影响，特别是如何利用我们的方法来确定应该添加到文档中的主题的优先级。,软件及其工程，软件创建和管理,,,
I3PQRHYN,2023,https://doi.org/10.1145/3546942,TOSEM 2023,Hippodrome: Data Race Repair Using Static Analysis Summaries,"Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way. ","Computing methodologies,Concurrent computing methodologies,Concurrent programming languages,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,General programming languages,Software maintenance tools,Theory of computation,Semantics and reasoning,Program reasoning",竞技场：使用静态分析总结进行数据竞赛修复,在现代软件开发中，实现无缺陷并发程序是一项具有挑战性的任务。最先进的静态分析在生产代码中发现了数百个并发错误，可扩展到大型代码库。然而，在不断变化的代码库中修复这些错误对程序员来说是一项艰巨的工作，尤其是因为在并发代码中进行修复可能会以微妙的方式引入其他错误。,计算方法学，并发计算方法学，并发编程语言，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，通用编程语言，软件维护工具，计算理论，语义和推理，程序推理,,,
9NWBHNCD,2023,https://doi.org/10.1145/3548684,TOSEM 2023,Patching Locking Bugs Statically with Crayons,"The Linux Kernel is a world-class operating system controlling most of our computing infrastructure: mobile devices, Internet routers and services, and most of the supercomputers. Linux is also an example of low-level software with no comprehensive regression test suite (for good reasons). The kernel’s tremendous societal importance imposes strict stability and correctness requirements. These properties make Linux a challenging and relevant target for static automated program repair (APR). ","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis",用Crayons静态修补锁定Bug,Linux内核是一个世界级的操作系统，控制着我们的大部分计算基础设施：移动设备、互联网路由器和服务，以及大多数超级计算机。Linux也是一个没有全面回归测试套件的低级软件的例子（理由很充分）。内核在社会上的巨大重要性对稳定性和正确性提出了严格的要求。这些特性使Linux成为静态自动程序修复（APR）的一个具有挑战性和相关性的目标。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析,,,
AEKQCBJP,2023,https://doi.org/10.1145/3542948,TOSEM 2023,SafeDrop: Detecting Memory Deallocation Bugs of Rust Programs via Static Data-flow Analysis,"Rust is an emerging programming language that aims to prevent memory-safety bugs. However, the current design of Rust also brings side effects, which may increase the risk of memory-safety issues. In particular, it employs ownership-based resource management and enforces automatic deallocation of unused resources without using the garbage collector. It may therefore falsely deallocate reclaimed memory and lead to use-after-free or double-free issues. In this article, we study the problem of invalid memory deallocation and propose SafeDrop, a static path-sensitive data-flow analysis approach to detect such bugs. Our approach analyzes each function of a Rust crate iteratively in a flow-sensitive and field-sensitive way. It leverages a modified Tarjan algorithm to achieve scalable path-sensitive analysis and a cache-based strategy for efficient inter-procedural analysis. We have implemented our approach and integrated it into the Rust compiler. Experiment results show that the approach can successfully detect all such bugs in our experiments with a limited number of false positives and incurs a very small overhead compared to the original compilation time.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software organization and properties,Software functional properties,Formal methods,Automated static analysis",SafeDrop：通过静态数据流分析检测Rust程序的内存分配错误,Rust是一种新兴的编程语言，旨在防止内存安全漏洞。然而，Rust目前的设计也带来了副作用，这可能会增加内存安全问题的风险。特别是，它采用了基于所有权的资源管理，并在不使用垃圾收集器的情况下强制自动释放未使用的资源。因此，它可能会错误地释放回收的内存，并导致在空闲或两次空闲后使用。在本文中，我们研究了无效内存释放的问题，并提出了SafeDrop，一种静态路径敏感的数据流分析方法来检测此类错误。我们的方法以流敏感和场敏感的方式迭代分析Rust机箱的每个函数。它利用改进的Tarjan算法实现可扩展的路径敏感分析，并利用基于缓存的策略实现高效的过程间分析。我们已经实现了我们的方法，并将其集成到Rust编译器中。实验结果表明，在我们的实验中，该方法可以成功地检测到所有此类错误，误报数量有限，并且与原始编译时间相比，开销非常小。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件组织和属性，软件功能属性，形式化方法，自动静态分析,,,
SXBC8XUS,2023,https://doi.org/10.1145/3587157,TOSEM 2023,What's (Not) Working in Programmer User Studies?,"A key goal of software engineering research is to improve the environments, tools, languages, and techniques programmers use to efficiently create quality software. Successfully designing these tools and demonstrating their effectiveness involves engaging with tool users—software engineers. Researchers often want to conduct user studies of software engineers to collect direct evidence. However, running user studies can be difficult, and researchers may lack solution strategies to overcome the barriers, so they may avoid user studies. To understand the challenges researchers face when conducting programmer user studies, we interviewed 26 researchers. Based on the analysis of interview data, we contribute (i) a taxonomy of 18 barriers researchers encounter; (ii) 23 solution strategies some researchers use to address 8 of the 18 barriers in their own studies; and (iii) 4 design ideas, which we adapted from the behavioral science community, that may lower 8 additional barriers. To validate the design ideas, we held an in-person all-day focus group with 16 researchers.","Human-centered computing,Human computer interaction (HCI),HCI design and evaluation methods,User studies,Software and its engineering",程序员用户研究中什么（不）有效？,软件工程研究的一个关键目标是改进程序员用来高效创建高质量软件的环境、工具、语言和技术。成功设计这些工具并展示其有效性需要与工具用户——软件工程师——接触。研究人员经常希望对软件工程师进行用户研究，以收集直接证据。然而，进行用户研究可能很困难，研究人员可能缺乏克服障碍的解决策略，因此他们可能会避免进行用户研究。为了了解研究人员在进行程序员用户研究时面临的挑战，我们采访了26名研究人员。基于对访谈数据的分析，我们贡献了（i）研究人员遇到的18个障碍的分类；（ii）一些研究人员在自己的研究中使用23种解决策略来解决18个障碍中的8个；以及（iii）我们从行为科学界改编的4个设计理念，可能会降低8个额外的障碍。为了验证设计理念，我们举办了一个由16名研究人员组成的全天焦点小组。,以人为中心的计算，人机交互，人机交互设计和评估方法，用户研究，软件及其工程,,,
BL4JHS32,2023,https://doi.org/10.1145/3542944,TOSEM 2023,Towards Learning Generalizable Code Embeddings Using Task-agnostic Graph Convolutional Networks,"Code embeddings have seen increasing applications in software engineering (SE) research and practice recently. Despite the advances in embedding techniques applied in SE research, one of the main challenges is their generalizability. A recent study finds that code embeddings may not be readily leveraged for the downstream tasks that the embeddings are not particularly trained for. Therefore, in this article, we propose GraphCodeVec, which represents the source code as graphs and leverages the Graph Convolutional Networks to learn more generalizable code embeddings in a task-agnostic manner. The edges in the graph representation are automatically constructed from the paths in the abstract syntax trees, and the nodes from the tokens in the source code. To evaluate the effectiveness of GraphCodeVec , we consider three downstream benchmark tasks (i.e., code comment generation, code authorship identification, and code clones detection) that are used in a prior benchmarking of code embeddings and add three new downstream tasks (i.e., source code classification, logging statements prediction, and software defect prediction), resulting in a total of six downstream tasks that are considered in our evaluation. For each downstream task, we apply the embeddings learned by GraphCodeVec and the embeddings learned from four baseline approaches and compare their respective performance. We find that GraphCodeVec outperforms all the baselines in five out of the six downstream tasks, and its performance is relatively stable across different tasks and datasets. In addition, we perform ablation experiments to understand the impacts of the training context (i.e., the graph context extracted from the abstract syntax trees) and the training model (i.e., the Graph Convolutional Networks) on the effectiveness of the generated embeddings. The results show that both the graph context and the Graph Convolutional Networks can benefit GraphCodeVec in producing high-quality embeddings for the downstream tasks, while the improvement by Graph Convolutional Networks is more robust across different downstream tasks and datasets. Our findings suggest that future research and practice may consider using graph-based deep learning methods to capture the structural information of the source code for SE tasks.","Computing methodologies,Machine learning,Software and its engineering",使用任务不可知图卷积网络学习可泛化代码嵌入,近年来，代码嵌入在软件工程（SE）研究和实践中的应用越来越多。尽管嵌入技术在SE研究中的应用取得了进步，但其主要挑战之一是其可推广性。最近的一项研究发现，代码嵌入可能不容易用于嵌入没有经过特别训练的下游任务。因此，在本文中，我们提出了GraphCodeVec，它将源代码表示为图，并利用图卷积网络以任务不可知的方式学习更通用的代码嵌入。图表示中的边是根据抽象语法树中的路径自动构建的，节点是根据源代码中的标记自动构建的。为了评估GraphCodeVec的有效性，我们考虑了在代码嵌入的先前基准测试中使用的三个下游基准测试任务（即代码注释生成、代码作者身份识别和代码克隆检测），并添加了三个新的下游任务（即源代码分类、日志记录语句预测和软件缺陷预测），导致在我们的评估中总共考虑了六个下游任务。对于每个下游任务，我们应用GraphCodeVec学习的嵌入和从四种基线方法学习的嵌入，并比较它们各自的性能。我们发现GraphCodeVec在六个下游任务中的五个任务中优于所有基线，并且在不同的任务和数据集中其性能相对稳定。此外，我们进行了消融实验，以了解训练上下文（即从抽象语法树中提取的图上下文）和训练模型（即图卷积网络）对生成嵌入的有效性的影响。结果表明，图上下文和图卷积网络都有利于GraphCodeVec为下游任务生成高质量的嵌入，而图卷积网络的改进在不同的下游任务和数据集上更具鲁棒性。我们的研究结果表明，未来的研究和实践可能会考虑使用基于图的深度学习方法来捕获SE任务的源代码的结构信息。,计算方法论，机器学习，软件及其工程,,,
7H99KEDT,2023,https://doi.org/10.1145/3604610,TOSEM 2023,Optimization Techniques for Model Checking Leads-to Properties in a Stratified Way,"We devised the L+1-layer divide & conquer approach to leads-to model checking (L+1-DCA2L2MC) and its parallel version, and developed sequential and parallel tools for L+1-DCA2L2MC. In a temporal logic called UNITY, designed by Chandy and Misra, the leads-to temporal connective plays an important role and many case studies have been conducted in UNITY, demonstrating that many systems requirements can be expressed as leads-to properties. Hence, it is worth dedicating to these properties. Counterexample generation is one of the main tasks in the L+1-DCA2L2MC technique that can be optimized to improve its running performance. This article proposes a technique to find all counterexamples at once in model checking with a new model checker. Furthermore, layer configuration selection is essential to make the best use of the L+1-DCA2L2MC technique. This work also proposes an approach to finding a good layer configuration for the technique with an analysis tool. Some experiments are conducted to demonstrate the power and usefulness of the two optimization techniques, respectively. Moreover, our sequential and parallel tools are compared with SPIN and LTSmin model checkers, showing a promising way to mitigate the state space explosion and improve the running performance of model checking when dealing with large state spaces.","Computing methodologies,Concurrent computing methodologies,Software and its engineering,Software creation and management,Software verification and validation,Formal software verification,Theory of computation,Logic,Verification by model checking",模型检验的优化技术导致性能分层,我们设计了L+1-层分治方法来引导模型检查（L+1-DCA2L2MC）及其并行版本，并为L+1-DAC2L2MC开发了顺序和并行工具。在Chandy和Misra设计的名为UNITY的时态逻辑中，时态连接的引线起着重要作用，并且在UNITY中进行了许多案例研究，表明许多系统需求可以表示为引线属性。因此，对这些特性的投入是值得的。反例生成是L+1-DCA2L2MC技术的主要任务之一，可以对其进行优化以提高其运行性能。本文提出了一种使用新的模型检查器在模型检查中同时查找所有反例的技术。此外，层配置选择对于充分利用L+1-DCA2L2MC技术至关重要。这项工作还提出了一种使用分析工具为该技术找到良好层配置的方法。进行了一些实验，分别证明了这两种优化技术的强大性和实用性。此外，将我们的顺序和并行工具与SPIN和LTSmin模型检查器进行了比较，表明在处理大状态空间时，可以很好地缓解状态空间爆炸并提高模型检查的运行性能。,计算方法论，并行计算方法论，软件及其工程，软件创建和管理，软件验证和确认，形式软件验证，计算理论，逻辑，模型检验验证,,,
IUQS8I2U,2023,https://doi.org/10.1145/3576040,TOSEM 2023,Input Distribution Coverage: Measuring Feature Interaction Adequacy in Neural Network Testing,"Testing deep neural networks (DNNs) has garnered great interest in the recent years due to their use in many applications. Black-box test adequacy measures are useful for guiding the testing process in covering the input domain. However, the absence of input specifications makes it challenging to apply black-box test adequacy measures in DNN testing. The Input Distribution Coverage (IDC) framework addresses this challenge by using a variational autoencoder to learn a low dimensional latent representation of the input distribution, and then using that latent space as a coverage domain for testing. IDC applies combinatorial interaction testing on a partitioning of the latent space to measure test adequacy. Empirical evaluation demonstrates that IDC is cost-effective, capable of detecting feature diversity in test inputs, and more sensitive than prior work to test inputs generated using different DNN test generation methods. The findings demonstrate that IDC overcomes several limitations of white-box DNN coverage approaches by discounting coverage from unrealistic inputs and enabling the calculation of test adequacy metrics that capture the feature diversity present in the input space of DNNs.","Computing methodologies,Artificial intelligence,Machine learning,Software and its engineering,Software creation and management",输入分布覆盖率：在神经网络测试中测量特征交互充分性,近年来，由于深度神经网络在许多应用中的应用，测试深度神经网络引起了人们的极大兴趣。黑盒测试充分性度量有助于指导覆盖输入域的测试过程。然而，由于缺乏输入规范，在DNN测试中应用黑盒测试充分性措施具有挑战性。输入分布覆盖（IDC）框架通过使用变分自动编码器来学习输入分布的低维潜在表示，然后使用该潜在空间作为覆盖域进行测试，从而解决了这一挑战。IDC将组合交互测试应用于潜在空间的划分，以衡量测试的充分性。经验评估表明，IDC具有成本效益，能够检测测试输入中的特征多样性，并且比以前的工作对使用不同DNN测试生成方法生成的测试输入更敏感。研究结果表明，IDC克服了白盒DNN覆盖方法的几个局限性，通过对不切实际的输入的覆盖进行折现，并能够计算测试充分性指标，以捕捉DNN输入空间中存在的特征多样性。,计算方法论，人工智能，机器学习，软件及其工程，软件创建和管理,,,
E5LZ8JPN,2023,https://doi.org/10.1145/3517193,TOSEM 2023,"Bash in the Wild: Language Usage, Code Smells, and Bugs","The Bourne-again shell (Bash) is a prevalent scripting language for orchestrating shell commands and managing resources in Unix-like environments. It is one of the mainstream shell dialects that is available on most GNU Linux systems. However, the unique syntax and semantics of Bash could easily lead to unintended behaviors if carelessly used. Prior studies primarily focused on improving the reliability of Bash scripts or facilitating writing Bash scripts; there is yet no empirical study on the characteristics of Bash programs written in reality, e.g., frequently used language features, common code smells, and bugs. ","General and reference,Cross-computing tools and techniques,Empirical studies,Software and its engineering,Software notations and tools,General programming languages,Language features",野外狂欢：语言使用、代码气味和Bug,Bourne-reagain shell（Bash）是一种流行的脚本语言，用于在类Unix环境中协调shell命令和管理资源。它是大多数GNU Linux系统上可用的主流shell方言之一。然而，如果不小心使用，Bash独特的语法和语义很容易导致意外行为。先前的研究主要集中在提高Bash脚本的可靠性或促进编写Bash脚本；目前还没有对现实中编写的Bash程序的特征进行实证研究，例如，频繁使用的语言特征、常见的代码气味和bug。,通则和参考，交叉计算工具和技术，经验研究，软件及其工程，软件符号和工具，通用编程语言，语言特征,,,
5NRI4FS4,2023,https://doi.org/10.1145/3546946,TOSEM 2023,DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class,"The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE’s overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23% more frequently than the original DIRE model.","Security and privacy,Software and application security,Software reverse engineering,Software and its engineering,Software notations and tools,Software maintenance tools",DIRE及其数据：关于软件类的神经分解变量重命名,反编译器是在没有相应源代码的情况下检查可执行二进制文件的最常见工具之一。它将二进制文件转换为高级代码，从而逆转编译过程。不幸的是，由于反编译过程通常是不完整的，所以反编译器的输出是不可读的。最先进的技术使用机器学习来预测变量名等缺失信息。虽然这些方法通常能够在上下文中提出好的变量名称，但没有现有的工作研究训练数据的选择如何影响这些机器学习模型。我们研究了数据来源和训练数据的质量如何影响性能，以及训练模型在软件领域中的泛化能力。我们使用一个这样的机器学习模型DIRE来关注变量重命名问题。我们首先详细描述了DIRE以及用于从原始代码生成训练数据的附带技术。我们还评估了DIRE的整体性能，而不考虑数据质量。接下来，我们将展示如何在更流行、可能更高质量的代码上进行训练（使用GitHub星级进行测量），从而产生更具可推广性的模型，因为流行代码往往具有更多样的变量名称。最后，我们评估了DIRE预测领域特定标识符的效果，提出了一种结合领域信息的修改，并表明它在领域特定场景中预测标识符的频率比原始DIRE模型高23%。,安全和隐私，软件和应用程序安全，软件反向工程，软件及其工程，软件符号和工具，软件维护工具,,,
L6GXYXJG,2023,https://doi.org/10.1145/3569935,TOSEM 2023,Simulator-based Explanation and Debugging of Hazard-triggering Events in DNN-based Safety-critical Systems,"When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error prone.","Computing methodologies,Machine learning,Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software maintenance tools",基于DNN的安全关键系统中危险触发事件的模拟器解释与调试,当深度神经网络（DNN）用于安全关键系统时，工程师应确定与测试期间观察到的故障（即错误输出）相关的安全风险。对于DNN处理图像，工程师对所有引起故障的图像进行视觉检查，以确定它们之间的共同特征。这些特性对应于危险触发事件（例如，低照度），这些事件是安全分析的重要输入。尽管这类活动内容丰富，但成本高昂且容易出错。,计算方法，机器学习，软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件维护工具,,,
YPE6Y7JL,2023,https://doi.org/10.1145/3580596,TOSEM 2023,Dissecting American Fuzzy Lop: A FuzzBench Evaluation,"AFL is one of the most used and extended fuzzers, adopted by industry and academic researchers alike. Although the community agrees on AFL’s effectiveness at discovering new vulnerabilities and its outstanding usability, many of its internal design choices remain untested to date. Security practitioners often clone the project “as-is” and use it as a starting point to develop new techniques, usually taking everything under the hood for granted. Instead, we believe that a careful analysis of the different parameters could help modern fuzzers improve their performance and explain how each choice can affect the outcome of security testing, either negatively or positively. ","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",剖析美国模糊Lop：模糊基准评估,AFL是最常用和扩展的模糊器之一，被行业和学术研究人员所采用。尽管社区一致认为AFL在发现新漏洞方面的有效性及其出色的可用性，但其许多内部设计选择迄今尚未经过测试。安全从业者经常“照原样”克隆项目，并将其作为开发新技术的起点，通常认为一切都是理所当然的。相反，我们相信，对不同参数的仔细分析可以帮助现代模糊器提高其性能，并解释每种选择如何影响安全测试的结果，无论是负面的还是正面的。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
U6XY2DCF,2023,https://doi.org/10.1145/3580600,TOSEM 2023,Dissecting American Fuzzy Lop - A FuzzBench Evaluation - RCR Report,"This report describes the artifacts of the “Dissecting American Fuzzy Lop – A FuzzBench Evaluation” paper. The artifacts are available online at  https://github.com/eurecom-s3/dissecting_afl and archived at  https://doi.org/10.6084/m9.figshare.21401280. American Fuzzy Lop (AFL) consists of the produced code, the setup to run the experiments in FuzzBench, and the generated reports. We claim the Functional badge as the patches to AFL are easy to enable and the experiments are easy to run thanks to the FuzzBench service, but the evaluations are self-contained and the modifications to AFL are as is. For the purpose of reproducing the experiments, no particular skills are needed as the process is straightforward and described in  https://google.github.io/fuzzbench/getting-started/adding-a-new-fuzzer/#requesting-an-experiment.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",剖析美国模糊Lop-A模糊基准评估-RCR报告,本报告描述了“剖析美国模糊Lop——模糊基准评估”论文中的人工制品。工件可在线获取，网址为https://github.com/eurecom-s3/dissecting_afl并存档于https://doi.org/10.6084/m9.figshare.21401280.美国模糊Lop（AFL）由生成的代码、在FuzzBench中运行实验的设置和生成的报告组成。由于FuzzBench服务，我们声称功能徽章是因为AFL的补丁很容易启用，实验也很容易运行，但评估是独立的，对AFL的修改是原样的。为了复制实验，不需要特定的技能，因为过程很简单，如https://google.github.io/fuzzbench/getting-started/adding-a-new-fuzzer/#requesting-实验。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
R2C4KTFG,2023,https://doi.org/10.1145/3522674,TOSEM 2023,Code Structure-Guided Transformer for Source Code Summarization,"Code summaries help developers comprehend programs and reduce their time to infer the program functionalities during software maintenance. Recent efforts resort to deep learning techniques such as sequence-to-sequence models for generating accurate code summaries, among which Transformer-based approaches have achieved promising performance. However, effectively integrating the code structure information into the Transformer is under-explored in this task domain. In this article, we propose a novel approach named SG-Trans to incorporate code structural properties into Transformer. Specifically, we inject the local symbolic information (e.g., code tokens and statements) and global syntactic structure (e.g., dataflow graph) into the self-attention module of Transformer as inductive bias. To further capture the hierarchical characteristics of code, the local information and global structure are designed to distribute in the attention heads of lower layers and high layers of Transformer. Extensive evaluation shows the superior performance of SG-Trans over the state-of-the-art approaches. Compared with the best-performing baseline, SG-Trans still improves 1.4% and 2.0% on two benchmark datasets, respectively, in terms of METEOR score, a metric widely used for measuring generation quality.","Software and its engineering,Software creation and management,Software development techniques",用于源代码摘要的代码结构引导转换器,代码摘要有助于开发人员理解程序，并减少他们在软件维护期间推断程序功能的时间。最近的努力求助于深度学习技术，如序列到序列模型，以生成准确的代码摘要，其中基于Transformer的方法取得了很好的性能。然而，在这个任务领域中，有效地将代码结构信息集成到Transformer中的探索还不够。在本文中，我们提出了一种名为SG-Trans的新方法，将代码结构属性合并到Transformer中。具体来说，我们将局部符号信息（例如，代码标记和语句）和全局句法结构（例如，数据流图）作为归纳偏差注入Transformer的自注意模块。为了进一步捕捉代码的层次特征，设计了局部信息和全局结构，以分布在Transformer的低层和高层的注意力头中。广泛的评估表明，SG Trans的性能优于最先进的方法。与表现最好的基线相比，SG-Trans在两个基准数据集上的METEOR得分（一种广泛用于衡量发电质量的指标）仍分别提高了1.4%和2.0%。,软件及其工程，软件创建和管理，软件开发技术,,,
UC4KHTWI,2023,https://doi.org/10.1145/3550150,TOSEM 2023,I Know What You Are Searching for: Code Snippet Recommendation from Stack Overflow Posts,"Stack Overflow has been heavily used by software developers to seek programming-related information. More and more developers use Community Question and Answer forums, such as Stack Overflow, to search for code examples of how to accomplish a certain coding task. This is often considered to be more efficient than working from source documentation, tutorials, or full worked examples. However, due to the complexity of these online Question and Answer forums and the very large volume of information they contain, developers can be overwhelmed by the sheer volume of available information. This makes it hard to find and/or even be aware of the most relevant code examples to meet their needs. To alleviate this issue, in this work, we present a query-driven code recommendation tool, named Que2Code, that identifies the best code snippets for a user query from Stack Overflow posts. Our approach has two main stages: (i) semantically equivalent question retrieval and (ii) best code snippet recommendation. During the first stage, for a given query question formulated by a developer, we first generate paraphrase questions for the input query as a way of query boosting and then retrieve the relevant Stack Overflow posted questions based on these generated questions. In the second stage, we collect all of the code snippets within questions retrieved in the first stage and develop a novel scheme to rank code snippet candidates from Stack Overflow posts via pairwise comparisons. To evaluate the performance of our proposed model, we conduct a large-scale experiment to evaluate the effectiveness of the semantically equivalent question retrieval task and best code snippet recommendation task separately on Python and Java datasets in Stack Overflow. We also perform a human study to measure how real-world developers perceive the results generated by our model. Both the automatic and human evaluation results demonstrate the promising performance of our model, and we have released our code and data to assist other researchers. ","Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software evolution",我知道你在搜索什么：来自堆栈溢出帖子的代码段推荐,软件开发人员大量使用堆栈溢出来寻找与编程相关的信息。越来越多的开发人员使用社区问答论坛（如Stack Overflow）来搜索如何完成特定编码任务的代码示例。这通常被认为比使用源文档、教程或完整的示例更高效。然而，由于这些在线问答论坛的复杂性及其包含的大量信息，开发人员可能会被大量可用信息淹没。这使得很难找到和/或意识到最相关的代码示例来满足它们的需求。为了缓解这个问题，在这项工作中，我们提出了一个名为Que2Code的查询驱动代码推荐工具，该工具可以从Stack Overflow帖子中为用户查询确定最佳代码片段。我们的方法有两个主要阶段：（i）语义等价的问题检索和（ii）最佳代码片段推荐。在第一阶段，对于开发人员制定的给定查询问题，我们首先为输入查询生成转述问题，作为查询增强的一种方式，然后根据这些生成的问题检索相关的Stack Overflow发布的问题。在第二阶段，我们收集了第一阶段检索到的问题中的所有代码片段，并开发了一种新的方案，通过成对比较对Stack Overflow帖子中的候选代码片段进行排名。为了评估我们提出的模型的性能，我们进行了一个大规模的实验，分别在Stack Overflow中的Python和Java数据集上评估语义等价的问题检索任务和最佳代码片段推荐任务的有效性。我们还进行了一项人体研究，以衡量现实世界的开发人员如何感知我们的模型生成的结果。自动和人工评估结果都证明了我们模型的良好性能，我们已经发布了我们的代码和数据来帮助其他研究人员。,软件及其工程，软件创建和管理，软件开发后问题，软件维护，软件演化,,,
PUXSBZKB,2023,https://doi.org/10.1145/3563213,TOSEM 2023,DeltaDroid: Dynamic Delivery Testing in Android,"Android is a highly fragmented platform with a diverse set of devices and users. To support the deployment of apps in such a heterogeneous setting, Android has introduced dynamic delivery—a new model of software deployment in which optional, device- or user-specific functionalities of an app, called Dynamic Feature Modules (DFMs), can be installed, as needed, after the app’s initial installation. This model of app deployment, however, has exacerbated the challenges of properly testing Android apps. In this article, we first describe the results of an extensive study in which we formalized a defect model representing the various conditions under which DFM installations may fail. We then present DeltaDroid—a tool aimed at assisting the developers with validating dynamic delivery behavior in their apps by augmenting their existing test suite. Our experimental evaluation using real-world apps corroborates DeltaDroid’s ability to detect many crashes and unexpected behaviors that the existing automated testing tools cannot reveal.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",DeltaDroid：Android中的动态交付测试,安卓是一个高度分散的平台，拥有各种各样的设备和用户。为了支持在这种异构环境中部署应用程序，Android引入了动态交付——一种新的软件部署模式，在该模式中，应用程序的可选、设备或用户特定功能，称为动态功能模块（DFM），可以在应用程序初始安装后根据需要安装。然而，这种应用程序部署模式加剧了正确测试安卓应用程序的挑战。在本文中，我们首先描述了一项广泛研究的结果，在该研究中，我们形式化了一个缺陷模型，该模型表示DFM安装可能失败的各种条件。然后，我们介绍了DeltaDroid——一个旨在帮助开发人员通过增强现有测试套件来验证应用程序中的动态交付行为的工具。我们使用真实世界应用程序进行的实验评估证实了DeltaDroid检测许多现有自动测试工具无法揭示的崩溃和意外行为的能力。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
WARXQXGX,2023,https://doi.org/10.1145/3569934,TOSEM 2023,What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories,"There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card’s and FactSheet’s guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task’s type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories.","Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Designing software,Software design engineering",这个模型的预期用途是什么？不同模型库中预训练模型的探索性研究,研究人员和从业者倾向于直接应用预先训练的模型来解决他们的特定任务。例如，软件工程（SE）的研究人员已经成功地利用预先训练的语言模型来自动生成源代码和注释。然而，在不同的基准数据集中存在领域差距。在一个基准数据集上训练的这些数据驱动（或基于机器学习）模型在其他基准上可能无法顺利运行。因此，预训练模型的重用引入了大量成本和检查任意预训练模型是否适合特定任务重用的额外问题。据我们所知，软件工程师可以利用代码契约来最大限度地重用现有的软件组件或软件服务。类似于SE领域中的软件重用，重用SE可以扩展到预先训练的模型重用领域。因此，根据模型卡和FactSheet对预训练模型供应商应发布哪些信息的指导，我们提出了模型合同，包括预训练模型的前后条件，以实现更好的模型重用。此外，尽管许多预先训练的模型在模型存储库中很容易获得，但许多非琐碎但具有挑战性的问题尚未得到充分研究。根据我们的模型合同，我们在六个主流模型库（即TensorFlow Hub、PyTorch Hub、model Zoo、Wolfram Neural Net Repository、Nvidia和Hugging Face）上对1908个预先训练的模型进行了探索性研究，以调查必要的前后条件信息与实际规范之间的差距。我们的结果清楚地表明：（1）模型库倾向于提供预训练模型的令人困惑的信息，特别是关于任务类型、模型、训练集的信息，以及（2）模型库不能提供我们提出的所有预/后条件信息，尤其是预期用途、限制、性能和定量分析。基于我们的新发现，我们建议（1）模型库的开发人员应为每种任务类型中预训练模型的每种预/后条件提供一些必要的选择（例如，训练数据集、模型算法和性能度量）；（2）未来的研究人员和从业者提供更有效的指标来推荐合适的预训练模型，（3）预训练模型的供应商应严格按照我们提出的预/后条件报告他们的预训练模型，并根据模型库中报告的每个条件的特征报告他们的模型。,软件及其工程，软件创建和管理，软件开发协作，开放源码模型，软件设计，软件设计工程,,,
8FSFSG6E,2023,https://doi.org/10.1145/3582572,TOSEM 2023,"Code-line-level Bugginess Identification: How Far have We Come, and How Far have We Yet to Go?","Background. Code-line-level bugginess identification (CLBI) is a vital technique that can facilitate developers to identify buggy lines without expending a large amount of human effort. Most of the existing studies tried to mine the characteristics of source codes to train supervised prediction models, which have been reported to be able to discriminate buggy code lines amongst others in a target program.","Software and its engineering,Software notations and tools,Software maintenance tools",代码行级别的Bugginess标识：我们已经走了多远，还有多远要走？,背景代码行级错误识别（CLBI）是一种重要的技术，它可以帮助开发人员在不花费大量人力的情况下识别错误行。大多数现有的研究都试图挖掘源代码的特征来训练监督预测模型，据报道，该模型能够在目标程序中区分有缺陷的代码行。,软件及其工程，软件符号和工具，软件维护工具,,,
HMA6KPDW,2023,https://doi.org/10.1145/3576041,TOSEM 2023,Uncertainty-Aware Robustness Assessment of Industrial Elevator Systems,"Industrial elevator systems are commonly used software systems in our daily lives, which operate in uncertain environments such as unpredictable passenger traffic, uncertain passenger attributes and behaviors, and hardware delays. Understanding and assessing the robustness of such systems under various uncertainties enable system designers to reason about uncertainties, especially those leading to low system robustness, and consequently improve their designs and implementations in terms of handling uncertainties. To this end, we present a comprehensive empirical study conducted with industrial elevator systems provided by our industrial partner Orona, which focuses on assessing the robustness of a dispatcher—that is, a software component responsible for elevators’ optimal scheduling. In total, we studied 90 industrial dispatchers in our empirical study. Based on the experience gained from the study, we derived an uncertainty-aware robustness assessment method (named UncerRobua) comprising a set of guidelines on how to conduct the robustness assessment and a newly proposed ranking algorithm, for supporting the robustness assessment of industrial elevator systems against uncertainties.","Computer systems organization,Embedded and cyber-physical systems,Software and its engineering,Software creation and management,Software verification and validation,Empirical software validation,Software defect analysis,Software testing and debugging,Software organization and properties,Extra-functional properties,Software performance",工业电梯系统的不确定性鲁棒性评估,工业电梯系统是我们日常生活中常用的软件系统，在不确定的环境中运行，如不可预测的客运量、不确定的乘客属性和行为以及硬件延迟。了解和评估这些系统在各种不确定性下的稳健性，使系统设计者能够对不确定性进行推理，特别是那些导致系统稳健性低的不确定性，从而在处理不确定性方面改进其设计和实现。为此，我们对我们的工业合作伙伴Orona提供的工业电梯系统进行了全面的实证研究，重点评估调度员的稳健性，即负责电梯优化调度的软件组件。在我们的实证研究中，我们总共研究了90名工业调度员。基于从研究中获得的经验，我们推导了一种不确定性感知鲁棒性评估方法（名为UncerRobua），该方法包括一套关于如何进行鲁棒性评估的指南和一种新提出的排序算法，用于支持工业电梯系统对不确定性的鲁棒性评估。,计算机系统组织，嵌入式和网络物理系统，软件及其工程，软件创建和管理，软件验证和确认，经验软件验证，软件缺陷分析，软件测试和调试，软件组织和属性，额外功能属性，软件性能,,,
MS6CY278,2023,https://doi.org/10.1145/3593799,TOSEM 2023,UniLoc: Unified Fault Localization of Continuous Integration Failures,"Continuous integration (CI) practices encourage developers to frequently integrate code into a shared repository. Each integration is validated by automatic build and testing such that errors are revealed as early as possible. When CI failures or integration errors are reported, existing techniques are insufficient to automatically locate the root causes for two reasons. First, a CI failure may be triggered by faults in source code and/or build scripts, whereas current approaches consider only source code. Second, a tentative integration can fail because of build failures and/or test failures, whereas existing tools focus on test failures only. This article presents UniLoc, the first unified technique to localize faults in both source code and build scripts given a CI failure log, without assuming the failure’s location (source code or build scripts) and nature (a test failure or not). Adopting the information retrieval (IR) strategy, UniLoc locates buggy files by treating source code and build scripts as documents to search and by considering build logs as search queries. However, instead of naïvely applying an off-the-shelf IR technique to these software artifacts, for more accurate fault localization, UniLoc applies various domain-specific heuristics to optimize the search queries, search space, and ranking formulas. To evaluate UniLoc, we gathered 700 CI failure fixes in 72 open source projects that are built with Gradle. UniLoc could effectively locate bugs with the average mean reciprocal rank value as 0.49, mean average precision value as 0.36, and normalized discounted cumulative gain value as 0.54. UniLoc outperformed the state-of-the-art IR-based tool BLUiR and Locus. UniLoc has the potential to help developers diagnose root causes for CI failures more accurately and efficiently.","Software and its engineering,Software notations and tools,Software maintenance tools",UniLoc:连续集成故障的统一故障定位,持续集成（CI）实践鼓励开发人员经常将代码集成到共享存储库中。每个集成都通过自动构建和测试进行验证，以便尽早发现错误。当报告CI故障或集成错误时，现有技术不足以自动查找根本原因，原因有两个。首先，CI失败可能是由源代码和/或构建脚本中的错误触发的，而当前的方法只考虑源代码。其次，由于构建失败和/或测试失败，临时集成可能会失败，而现有工具只关注测试失败。本文介绍了UniLoc，这是第一种在给定CI故障日志的情况下，在源代码和构建脚本中定位故障的统一技术，而不假设故障的位置（源代码或构建脚本）和性质（是否为测试故障）。UniLoc采用信息检索（IR）策略，将源代码和构建脚本视为要搜索的文档，并将构建日志视为搜索查询，从而定位有缺陷的文件。然而，为了更准确地定位故障，UniLoc并没有天真地将现成的IR技术应用于这些软件工件，而是应用了各种特定领域的启发式方法来优化搜索查询、搜索空间和排名公式。为了评估UniLoc，我们在使用Gradle构建的72个开源项目中收集了700个CI故障修复。UniLoc可以有效定位错误，平均平均倒数排名值为0.49，平均精度值为0.36，归一化贴现累积增益值为0.54。UniLoc的性能优于最先进的基于IR的工具BLUiR和Locus。UniLoc有潜力帮助开发人员更准确、更高效地诊断CI故障的根本原因。,软件及其工程，软件符号和工具，软件维护工具,,,
V3NJ2MHN,2023,https://doi.org/10.1145/3579641,TOSEM 2023,IFDS-based Context Debloating for Object-Sensitive Pointer Analysis,"Object-sensitive pointer analysis, which separates the calling contexts of a method by its receiver objects, is known to achieve highly useful precision for object-oriented languages such as Java. Despite recent advances, all object-sensitive pointer analysis algorithms still suffer from the scalability problem due to the combinatorial explosion of contexts in large programs. In this article, we introduce a new approach, Conch, that can be applied to debloat contexts for all object-sensitive pointer analysis algorithms, thereby improving significantly their efficiency while incurring a negligible loss of precision. Our key insight is to approximate a recently proposed set of two necessary conditions for an object in a program to be context-sensitive, i.e., context-dependent (whose precise verification is undecidable) with a set of three linearly verifiable conditions in terms of the number of edges in the pointer assignment graph (PAG) representation of the program. These three linearly verifiable conditions, which turn out to be almost always necessary in practice, are synthesized from three key observations regarding context-dependability for the objects created and used in real-world object-oriented programs. To develop a practical implementation for Conch, we introduce an IFDS-based algorithm for reasoning about object reachability in the PAG of a program, which runs linearly in terms of the number of edges in the PAG. By debloating contexts for three representative object-sensitive pointer analysis algorithms, which are applied to a set of representative Java programs, Conch can speed up these three baseline algorithms substantially at only a negligible loss of precision (less than 0.1%) with respect to several commonly used precision metrics. In addition, Conch also improves their scalability by enabling them to analyze substantially more programs to completion than before (under a time budget of 12 hours). Conch has been open-sourced (http://www.cse.unsw.edu.au/~corg/tools/conch), opening up new opportunities for other researchers and practitioners to further improve this research. To demonstrate this, we introduce one extension of Conch to accelerate further the three baselines without losing any precision, providing further insights on extending Conch to make precision-efficiency tradeoffs in future research.","Software and its engineering,Software notations and tools,Software maintenance tools",用于对象敏感指针分析的基于IFDS的上下文去浮动,众所周知，对象敏感指针分析通过其接收器对象来分离方法的调用上下文，可以为Java等面向对象语言实现非常有用的精度。尽管最近取得了进展，但由于大型程序中上下文的组合爆炸，所有对象敏感的指针分析算法仍然存在可扩展性问题。在本文中，我们介绍了一种新的方法Conch，它可以应用于所有对象敏感指针分析算法的去块上下文，从而显著提高了它们的效率，同时导致了可忽略的精度损失。我们的关键见解是，根据程序的指针分配图（PAG）表示中的边数，用一组三个线性可验证条件来近似程序中对象对上下文敏感的两个必要条件，即上下文相关（其精确验证是不可确定的）。这三个线性可验证条件在实践中几乎总是必要的，它们是根据关于在现实世界面向对象程序中创建和使用的对象的上下文可靠性的三个关键观察结果综合而成的。为了开发Conch的实际实现，我们介绍了一种基于IFDS的算法，用于推理程序PAG中的对象可达性，该算法根据PAG中边的数量线性运行。通过对应用于一组代表性Java程序的三种代表性对象敏感指针分析算法的上下文进行去浮动，Conch可以在相对于几种常用精度指标而言仅可忽略的精度损失（小于0.1%）的情况下大大加快这三种基线算法的速度。此外，Conch还提高了他们的可扩展性，使他们能够分析比以前多得多的程序来完成（在12小时的时间预算下）。海螺一直是开源的(http://www.cse.unsw.edu.au/~corg/tools/const），为其他研究人员和从业者进一步改进这项研究开辟了新的机会。为了证明这一点，我们引入了海螺的一个扩展，以在不损失任何精度的情况下进一步加速三个基线，为扩展海螺在未来研究中进行精度-效率权衡提供了进一步的见解。,软件及其工程，软件符号和工具，软件维护工具,,,
SR5D3JI5,2023,https://doi.org/10.1145/3560263,TOSEM 2023,TokenAware: Accurate and Efficient Bookkeeping Recognition for Token Smart Contracts,"Tokens have become an essential part of blockchain ecosystem, so recognizing token transfer behaviors is crucial for applications depending on blockchain. Unfortunately, existing solutions cannot recognize token transfer behaviors accurately and efficiently because of their incomplete patterns and inefficient designs. This work proposes TokenAware, a novel online system for recognizing token transfer behaviors. To improve accuracy, TokenAware infers token transfer behaviors from modifications of internal bookkeeping of a token smart contract for recording the information of token holders (e.g., their addresses and shares). However, recognizing bookkeeping is challenging, because smart contract bytecode does not contain type information. TokenAware overcomes the challenge by first learning the instruction sequences for locating basic types and then deriving the instruction sequences for locating sophisticated types that are composed of basic types. To improve efficiency, TokenAware introduces four optimizations. We conduct extensive experiments to evaluate TokenAware with real blockchain data. Results show that TokenAware can automatically identify new types of bookkeeping and recognize 107,202 tokens with 98.7% precision. TokenAware with optimizations merely incurs 4% overhead, which is 1/345 of the overhead led by the counterpart with no optimization. Moreover, we develop an application based on TokenAware to demonstrate how it facilitates malicious behavior detection.","Security and privacy,Software and application security,Software security engineering",TokenAware：准确高效的代币智能合约记账识别,代币已成为区块链生态系统的重要组成部分，因此识别代币转移行为对于依赖区块链的应用至关重要。不幸的是，现有的解决方案由于其不完整的模式和低效的设计而无法准确有效地识别代币转移行为。这项工作提出了TokenAware，一种用于识别代币转移行为的新型在线系统。为了提高准确性，TokenAware通过修改代币智能合约的内部记账来推断代币转移行为，以记录代币持有者的信息（例如，他们的地址和份额）。然而，识别记账很有挑战性，因为智能合约字节码不包含类型信息。TokenAware通过首先学习用于定位基本类型的指令序列，然后推导用于定位由基本类型组成的复杂类型的指令顺序，克服了这一挑战。为了提高效率，TokenAware引入了四个优化。我们进行了大量实验，用真实的区块链数据评估TokenAware。结果表明，TokenAware可以自动识别新的记账类型，识别107202个代币，准确率为98.7%。经过优化的TokenAware只会产生4%的开销，这是未进行优化的对等方导致的开销的1/345。此外，我们开发了一个基于TokenAware的应用程序，以演示它如何促进恶意行为检测。,安全和隐私，软件和应用程序安全，软件安全工程,,,
9LFI6JJN,2023,https://doi.org/10.1145/3587156,TOSEM 2023,DatAFLow: Toward a Data-Flow-Guided Fuzzer,"Coverage-guided greybox fuzzers rely on control-flow coverage feedback to explore a target program and uncover bugs. Compared to control-flow coverage, data-flow coverage offers a more fine-grained approximation of program behavior. Data-flow coverage captures behaviors not visible as control flow and should intuitively discover more (or different) bugs. Despite this advantage, fuzzers guided by data-flow coverage have received relatively little attention, appearing mainly in combination with heavyweight program analyses (e.g., taint analysis, symbolic execution). Unfortunately, these more accurate analyses incur a high run-time penalty, impeding fuzzer throughput. Lightweight data-flow alternatives to control-flow fuzzing remain unexplored.","Software and its engineering,Software creation and management,Software verification and validation,Empirical software validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software maintenance tools",DatALow:面向数据流制导引信,覆盖引导的灰盒模糊器依靠控制流覆盖反馈来探索目标程序并发现错误。与控制流覆盖率相比，数据流覆盖率提供了对程序行为更细粒度的近似。数据流覆盖率捕获了作为控制流不可见的行为，并且应该直观地发现更多（或不同）错误。尽管有这一优势，但以数据流覆盖为导向的模糊器受到的关注相对较少，主要与重量级程序分析（如污点分析、符号执行）结合使用。不幸的是，这些更准确的分析会导致高运行时惩罚，阻碍模糊器的吞吐量。控制流模糊的轻量级数据流替代方案尚未探索。,软件及其工程，软件创建和管理，软件验证和确认，经验性软件确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件维护工具,,,
IUH7CXJJ,2023,https://doi.org/10.1145/3587159,TOSEM 2023,DatAFLow: Toward a Data-flow-guided Fuzzer,"This Replicating Computational Report (RCR) describes (a) our datAFLow fuzzer and (b) how to replicate the results in “datAFLow: Toward a Data-Flow-Guided Fuzzer.” Our primary artifact is the datAFLow fuzzer. Unlike traditional coverage-guided greybox fuzzers—which use control-flow coverage to drive program exploration—datAFLow uses data-flow coverage to drive exploration. This is achieved through a set of LLVM-based analyses and transformations. In addition to datAFLow, we also provide a set of tools, scripts, and patches for (a) statically analyzing data flows in a target program, (b) compiling a target program with the datAFLow instrumentation, (c) evaluating datAFLow on the Magma benchmark suite, and (d) evaluating datAFLow on the DDFuzz dataset. datAFLow is available at https://github.com/HexHive/datAFLow.","Software and its engineering,Software creation and management,Software verification and validation,Empirical software validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software maintenance tools",DatALow:迈向数据流制导引信,此复制计算报告（RCR）描述了（a）我们的datAFLow模糊器和（b）如何在“datAFLow:走向数据流引导的模糊器”中复制结果。我们的主要工件是datAFLow fuzzer。与传统的覆盖引导的灰盒模糊器不同，后者使用控制流覆盖来驱动程序探索，datAFLow使用数据流覆盖来推动探索。这是通过一组基于LLVM的分析和转换来实现的。除了datAFLow之外，我们还提供了一组工具、脚本和补丁，用于（a）静态分析目标程序中的数据流，（b）使用datAFLow仪器编译目标程序，（c）在Magma基准套件上评估datAFLow，以及（d）在DDFuzz数据集上评估datAFLow。datAFLow可在https://github.com/HexHive/datAFLow.,软件及其工程，软件创建和管理，软件验证和确认，经验软件验证，软件缺陷分析，软件测试和调试，软件符号和工具，软件维护工具,,,
CFK8XSRM,2023,https://doi.org/10.1145/3546943,TOSEM 2023,The Influence of Human Aspects on Requirements Engineering-related Activities: Software Practitioners' Perspective,"Requirements Engineering (RE)-related activities require high collaboration between various roles in software engineering (SE), such as requirements engineers, stakeholders, developers, and so on. Their demographics, views, understanding of technologies, working styles, communication and collaboration capabilities make RE highly human-dependent. Identifying how “human aspects”—such as motivation, domain knowledge, communication skills, personality, emotions, culture, and so on—might impact RE-related activities would help us improve RE and SE in general. This study aims at better understanding current industry perspectives on the influence of human aspects on RE-related activities, specifically focusing on motivation and personality, by targeting software practitioners involved in RE-related activities. Our findings indicate that software practitioners consider motivation, domain knowledge, attitude, communication skills and personality as highly important human aspects when involved in RE-related activities. A set of factors were identified as software practitioners’ key motivational factors when involved in RE-related activities, along with important personality characteristics to have when involved in RE. We also identified factors that made individuals less effective when involved in RE-related activities and obtained some feedback on measuring individuals’ performance when involved in RE. The findings from our study suggest various areas needing more investigation, and we summarise a set of key recommendations for further research.","Computer systems organization,Dependable and fault-tolerant systems and networks,Redundancy,Embedded and cyber-physical systems,Embedded systems,Robotics,Networks,Network properties,Network reliability",人的因素对需求工程相关活动的影响：软件从业者的视角,需求工程（RE）相关活动需要软件工程（SE）中各种角色之间的高度协作，如需求工程师、利益相关者、开发人员等。他们的人口统计、观点、对技术的理解、工作风格、沟通和协作能力使RE高度依赖于人。识别“人的方面”——如动机、领域知识、沟通技能、个性、情绪、文化等——可能会如何影响RE相关活动，将有助于我们总体上提高RE和SE。本研究旨在通过针对参与可再生能源相关活动的软件从业者，更好地理解当前行业对人类方面对可再生能源相关行为影响的看法，特别是关注动机和个性。我们的研究结果表明，软件从业者在参与RE相关活动时，将动机、领域知识、态度、沟通技能和个性视为人类高度重要的方面。一组因素被确定为软件从业者在参与RE相关活动时的关键动机因素，以及在参与RE时应具有的重要人格特征。我们还确定了在参与RE活动时使个人效率较低的因素，并获得了一些关于衡量个人在参与RE中表现的反馈。我们的研究结果表明，各个领域需要更多的调查，我们总结了一系列进一步研究的关键建议。,计算机系统组织，可靠和容错系统和网络，冗余，嵌入式和网络物理系统，嵌入式系统，机器人，网络，网络特性，网络可靠性,,,
9DG4TDZ8,2023,https://doi.org/10.1145/3580599,TOSEM 2023,NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing - RCR Report,"We provide artifacts to reproduce the evaluation results of our article: “NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing”. The provided artifacts can be downloaded from  https://zenodo.org/record/7134490. It includes 14 docker containers, several scripts for execution and analysis, one additional proof for the crash results, and six related documents for the running of experiments. We claim for all three badges, i.e., Available, Functional, and Reusable. This report gives instructions on how to reproduce the answers which mainly involve basic operations on the Ubuntu operating system.","Networks,Network protocols,Protocol correctness,Protocol testing and verification,Security and privacy,Software and application security",NSFuzz：实现高效和状态感知的网络服务模糊化-RCR报告,我们提供工件来重现我们文章的评估结果：“NSFuzz:走向高效和状态感知的网络服务模糊化”。提供的工件可以从https://zenodo.org/record/7134490.它包括14个docker容器、几个用于执行和分析的脚本、一个额外的崩溃结果证明，以及六个用于运行实验的相关文档。我们对所有三个徽章进行索赔，即可用、功能和可重复使用。本报告给出了如何重现答案的说明，这些答案主要涉及Ubuntu操作系统上的基本操作。,网络，网络协议，协议正确性，协议测试和验证，安全和隐私，软件和应用程序安全,,,
AFPDVQJG,2023,https://doi.org/10.1145/3582570,TOSEM 2023,A Comparative Study on Method Comment and Inline Comment,"Code comments are one of the important documents to help developers review and comprehend source code. In recent studies, researchers have proposed many deep learning models to generate the method header comments (i.e., method comment), which have achieved encouraging results. The comments in the method, which is called inline comment, are also important for program comprehension. Unfortunately, they have not received enough attention in automatic generation when comparing with the method comments. In this paper, we compare and analyze the similarities and differences between the method comments and the inline comments. By applying the existing models of generating method comments to the inline comment generation, we find that these existing models perform worse on the task of inline comment generation. We then further explore the possible reasons and obtain a number of new observations. For example, we find that there are a lot of templates (i.e., comments with the same or similar structures) in the method comment dataset, which makes the models perform better. Some terms were thought to be important (e.g., API calls) in the comment generation by previous study does not significantly affect the quality of the generated comments, which seems counter-intuitive. Our findings may give some implications for building the approaches of method comment or inline comment generation in the future.","Software and its engineering,Software creation and management,Software development techniques,Software post-development issues,Software evolution",方法评论与内联评论的比较研究,代码注释是帮助开发人员审查和理解源代码的重要文档之一。在最近的研究中，研究人员提出了许多深度学习模型来生成方法头注释（即方法注释），并取得了令人鼓舞的结果。该方法中的注释（称为内联注释）对程序理解也很重要。不幸的是，与方法注释相比，它们在自动生成方面没有得到足够的关注。在本文中，我们比较和分析了方法注释和内联注释之间的异同。通过将现有的生成方法注释的模型应用于内联注释生成，我们发现这些现有的模型在内联注释生成任务上的表现更差。然后，我们进一步探讨了可能的原因，并获得了一些新的观察结果。例如，我们发现方法注释数据集中有很多模板（即具有相同或相似结构的注释），这使得模型性能更好。以前的研究认为，在评论生成中，一些术语很重要（例如，API调用）不会显著影响生成评论的质量，这似乎是反直觉的。我们的发现可能会为未来构建方法注释或内联注释生成方法提供一些启示。,软件及其工程，软件创建和管理，软件开发技术，软件开发后问题，软件演化,,,
WYYREQ6U,2023,https://doi.org/10.1145/3597206,TOSEM 2023,Semantic-Enriched Code Knowledge Graph to Reveal Unknowns in Smart Contract Code Reuse,"Programmers who work with smart contract development often encounter challenges in reusing code from repositories. This is due to the presence of two unknowns that can lead to non-functional and functional failures. These unknowns are implicit collaborations between functions and subtle differences among similar functions. Current code mining methods can extract syntax and semantic knowledge (known knowledge), but they cannot uncover these unknowns due to a significant gap between the known and the unknown. To address this issue, we formulate knowledge acquisition as a knowledge deduction task and propose an analytic flow that uses the function clone as a bridge to gradually deduce the known knowledge into the problem-solving knowledge that can reveal the unknowns. This flow comprises five methods: clone detection, co-occurrence probability calculation, function usage frequency accumulation, description propagation, and control flow graph annotation. This provides a systematic and coherent approach to knowledge deduction. We then structure all of the knowledge into a semantic-enriched code Knowledge Graph (KG) and integrate this KG into two software engineering tasks: code recommendation and crowd-scaled coding practice checking. As a proof of concept, we apply our approach to 5,140 smart contract files available on Etherscan.io and confirm high accuracy of our KG construction steps. In our experiments, our code KG effectively improved code recommendation accuracy by 6% to 45%, increased diversity by 61% to 102%, and enhanced NDCG by 1% to 21%. Furthermore, compared to traditional analysis tools and the debugging-with-the-crowd method, our KG improved time efficiency by 30 to 380 seconds, vulnerability determination accuracy by 20% to 33%, and vulnerability fixing accuracy by 24% to 40% for novice developers who identified and fixed vulnerable smart contract functions.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Search-based software engineering,Software development techniques,Reusability,Software notations and tools,Formal language definitions,Semantics",语义丰富的代码知识图揭示智能合约代码重用中的未知,从事智能合约开发的程序员在重用存储库中的代码时经常遇到挑战。这是由于存在两个可能导致非功能性和功能性故障的未知因素。这些未知是函数之间隐含的协作，也是相似函数之间微妙的差异。当前的代码挖掘方法可以提取语法和语义知识（已知知识），但由于已知和未知之间存在显著差距，因此无法发现这些未知。为了解决这个问题，我们将知识获取公式化为知识推导任务，并提出了一种分析流程，该流程以函数克隆为桥梁，将已知知识逐步推导为能够揭示未知的解决问题的知识。该流程包括五种方法：克隆检测、同现概率计算、函数使用频率累积、描述传播和控制流图注释。这为知识推导提供了一种系统而连贯的方法。然后，我们将所有知识构造成一个语义丰富的代码知识图（KG），并将该知识图集成到两个软件工程任务中：代码推荐和大规模编码实践检查。作为概念验证，我们将我们的方法应用于Etherscan.io上提供的5140个智能合约文件，并确认我们KG构建步骤的高准确性。在我们的实验中，我们的代码KG有效地将代码推荐准确率提高了6%至45%，将多样性提高了61%至102%，并将NDCG提高了1%至21%。此外，与传统的分析工具和使用群组方法进行调试相比，我们的KG为识别和修复易受攻击的智能合约功能的新手开发人员提高了30至380秒的时间效率、20%至33%的漏洞确定准确率和24%至40%的漏洞修复准确率。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，基于搜索的软件工程，软件开发技术，可重用性，软件符号和工具，形式语言定义，语义,,,
N2BM9DYS,2023,https://doi.org/10.1145/3591870,TOSEM 2023,PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing,"In the past few years, Transformer has been widely adopted in many domains and applications because of its impressive performance. Vision Transformer (ViT), a successful and well-known variant, attracts considerable attention from both industry and academia thanks to its record-breaking performance in various vision tasks. However, ViT is also highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. How to improve the robustness of ViT is thus an urgent issue that needs to be addressed. Among all kinds of robustness, patch robustness is defined as giving a reliable output when a random patch in the input domain is perturbed. The perturbation could be natural corruption, such as part of the camera lens being blurred. It could also be a distribution shift, such as an object that does not exist in the training data suddenly appearing in the camera. And in the worst case, there could be a malicious adversarial patch attack that aims to fool the prediction of a machine learning model by arbitrarily modifying pixels within a restricted region of an input image. This kind of attack is also called physical attack, as it is believed to be more real than digital attack. Although there has been some work on patch robustness improvement of Convolutional Neural Network, related studies on its counterpart ViT are still at an early stage as ViT is usually much more complex with far more parameters. It is harder to assess and improve its robustness, not to mention to provide a provable guarantee. ","Computing methodologies,Artificial intelligence,Software and its engineering,Software organization and properties,Extra-functional properties,Software reliability",PatchCensor：通过详尽测试获得变压器的补丁鲁棒性认证,在过去的几年里，Transformer由于其令人印象深刻的性能而在许多领域和应用中被广泛采用。视觉转换器（ViT）是一种成功且知名的变体，由于其在各种视觉任务中的破纪录表现，吸引了业界和学术界的极大关注。然而，与其他经典神经网络一样，ViT也是高度非线性的，很容易被自然扰动和对抗性扰动所欺骗。这种限制可能会对ViT在实际工业环境中的部署构成威胁，尤其是在安全关键场景中。因此，如何提高ViT的健壮性是一个亟待解决的问题。在所有类型的鲁棒性中，补丁鲁棒性被定义为当输入域中的随机补丁受到扰动时给出可靠的输出。扰动可能是自然损坏，比如相机镜头的一部分被模糊了。它也可能是分布偏移，例如训练数据中不存在的对象突然出现在相机中。在最坏的情况下，可能会出现恶意对抗性补丁攻击，其目的是通过任意修改输入图像的受限区域内的像素来欺骗机器学习模型的预测。这种攻击也被称为物理攻击，因为它被认为比数字攻击更真实。尽管在卷积神经网络的补丁鲁棒性改进方面已经有了一些工作，但对其对应的ViT的相关研究仍处于早期阶段，因为ViT通常更复杂，参数要多得多。更难评估和提高其稳健性，更不用说提供可证明的保证了。,计算方法，人工智能，软件及其工程，软件组织和属性，额外功能属性，软件可靠性,,,
8YFUT3J6,2023,https://doi.org/10.1145/3591867,TOSEM 2023,"Tiny, Always-on, and Fragile: Bias Propagation through Design Choices in On-device Machine Learning Workflows","Billions of distributed, heterogeneous, and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast, and offline inference on personal data. On-device ML is highly context dependent and sensitive to user, usage, hardware, and environment attributes. This sensitivity and the propensity toward bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias. Our results validate that design choices made during model training, like the sample rate and input feature type, and choices made to optimize models, like light-weight architectures, the pruning learning rate, and pruning sparsity, can result in disparate predictive performance across male and female groups. Based on our findings, we suggest low effort strategies for engineers to mitigate bias in on-device ML.","Computing methodologies,Machine learning,General and reference,Cross-computing tools and techniques,Reliability,Hardware,Emerging technologies,Analysis and design of emerging devices and systems,Software and its engineering,Software creation and management",微小、始终开启和脆弱：通过设备上机器学习工作流中的设计选择传播偏见,数十亿分布式、异构和资源受限的物联网设备部署在设备上机器学习（ML），用于对个人数据进行私人、快速和离线推理。设备上ML高度依赖于上下文，并且对用户、使用、硬件和环境属性敏感。ML中的这种敏感性和偏向性使得研究设备上设置中的偏向性变得重要。我们的研究是这一新兴领域对偏见的首批研究之一，为建立更公平的设备ML奠定了重要基础。我们应用软件工程的视角，研究了偏见在设备ML工作流程中通过设计选择传播的情况。我们首先将可靠性偏差确定为不公平的来源，并提出了一种量化方法。然后，我们对关键词识别任务进行了实证实验，以表明复杂和相互作用的技术设计选择是如何放大和传播可靠性偏差的。我们的结果验证了在模型训练过程中做出的设计选择，如采样率和输入特征类型，以及优化模型的选择，如轻量级架构、修剪学习率和修剪稀疏性，可能会导致男性和女性群体的预测性能不同。基于我们的发现，我们建议工程师采取低成本策略来减轻设备上ML的偏见。,计算方法，机器学习，通则和参考，交叉计算工具和技术，可靠性，硬件，新兴技术，新兴设备和系统的分析和设计，软件及其工程，软件创建和管理,,,
LPPNMT8Y,2023,https://doi.org/10.1145/3603110,TOSEM 2023,Dependency Update Strategies and Package Characteristics,"Managing project dependencies is a key maintenance issue in software development. Developers need to choose an update strategy that allows them to receive important updates and fixes while protecting them from breaking changes. Semantic Versioning was proposed to address this dilemma, but many have opted for more restrictive or permissive alternatives. This empirical study explores the association between package characteristics and the dependency update strategy selected by its dependents to understand how developers select and change their update strategies. We study over 112,000 Node Package Manager (npm) packages and use 19 characteristics to build a prediction model that identifies the common dependency update strategy for each package. Our model achieves a minimum improvement of 72% over the baselines and is much better aligned with community decisions than the npm default strategy. We investigate how different package characteristics can influence the predicted update strategy and find that dependent count, age, and release status to be the highest influencing features. We complement the work with qualitative analyses of 160 packages to investigate the evolution of update strategies. While the common update strategy remains consistent for many packages, certain events such as the release of the 1.0.0 version or breaking changes influence the selected update strategy over time.","Software and its engineering,Software notations and tools,Software configuration management and version control systems,Software libraries and repositories",依赖项更新策略和包特性,管理项目依赖关系是软件开发中的一个关键维护问题。开发人员需要选择一种更新策略，允许他们接收重要的更新和修复，同时保护他们免受破坏性更改的影响。语义版本控制是为了解决这一困境而提出的，但许多人选择了更严格或更宽松的替代方案。这项实证研究探讨了包特征与其依赖项选择的依赖项更新策略之间的关联，以了解开发人员如何选择和改变他们的更新策略。我们研究了112000多个节点包管理器（npm）包，并使用19个特征构建了一个预测模型，该模型确定了每个包的通用依赖更新策略。我们的模型在基线基础上实现了72%的最低改进，并且比npm默认策略更符合社区决策。我们研究了不同的包特征如何影响预测的更新策略，发现依赖计数、年龄和发布状态是影响最大的特征。我们通过对160个包的定性分析来补充这项工作，以调查更新策略的演变。虽然许多包的通用更新策略保持一致，但随着时间的推移，某些事件（如1.0.0版本的发布或中断更改）会影响所选的更新策略。,软件及其工程，软件符号和工具，软件配置管理和版本控制系统，软件库和存储库,,,
N9TWS24X,2023,https://doi.org/10.1145/3561385,TOSEM 2023,1-to-1 or 1-to-n? Investigating the Effect of Function Inlining on Binary Similarity Analysis,"Binary similarity analysis is critical to many code-reuse-related issues, where function matching is its fundamental task. “1-to-1” mechanism has been applied in most binary similarity analysis works, in which one function in a binary file is matched against one function in a source file or binary file. However, we discover that the function mapping is a more complex problem of “1-to-n” (one binary function matches multiple source functions or binary functions) or even “n-to-n” (multiple binary functions match multiple binary functions) due to the existence of function inlining, different from traditional understanding. ","Security and privacy,Software and application security,Software and its engineering,Software creation and management,Search-based software engineering,Software post-development issues,Maintaining software",1对1还是1对n？函数嵌入对二元相似性分析的影响研究,二进制相似性分析是许多代码重用相关问题的关键，其中函数匹配是其基本任务。“1对1”机制已应用于大多数二进制相似性分析工作中，即二进制文件中的一个函数与源文件或二进制文件中一个函数相匹配。然而，我们发现，由于函数内联的存在，函数映射是一个更复杂的“1-to-n”（一个二进制函数与多个源函数或二进制函数匹配）甚至“n-to-n”的问题，这与传统的理解不同。,安全和隐私，软件和应用程序安全，软件及其工程，软件创建和管理，基于搜索的软件工程，软件开发后问题，维护软件,,,
9DHVVARJ,2023,https://doi.org/10.1145/3576038,TOSEM 2023,HybridCISave: A Combined Build and Test Selection Approach in Continuous Integration,"Continuous Integration (CI) is a popular practice in modern software engineering. Unfortunately, it is also a high-cost practice—Google and Mozilla estimate their CI systems in millions of dollars. To reduce the computational cost in CI, researchers developed approaches to selectively execute builds or tests that are likely to fail (and skip those likely to pass). In this article, we present a novel hybrid technique (HybridCISave) to improve on the limitations of existing techniques: to provide higher cost savings and higher safety. To provide higher cost savings, HybridCISave combines techniques to predict and skip executions of both full builds that are predicted to pass and partial ones (only the tests in them predicted to pass). To provide higher safety, HybridCISave combines the predictions of multiple techniques to obtain stronger certainty before it decides to skip a build or test. We evaluated HybridCISave by comparing its effectiveness with the existing build selection techniques over 100 projects and found that it provided higher cost savings at the highest safety. We also evaluated each design decision in HybridCISave and found that skipping both full and partial builds increased its cost savings and that combining multiple test selection techniques made it safer.","Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software verification and validation,Software defect analysis,Software testing and debugging",HybridCISave：一种连续集成中的构建和测试组合选择方法,连续集成（CI）是现代软件工程中一种流行的实践。不幸的是，这也是一种高成本的做法——谷歌和Mozilla估计他们的CI系统价值数百万美元。为了降低CI的计算成本，研究人员开发了有选择地执行可能失败的构建或测试（并跳过可能通过的测试）的方法。在本文中，我们提出了一种新的混合技术（HybridCISave），以改进现有技术的局限性：提供更高的成本节约和更高的安全性。为了提供更高的成本节约，HybridCISave结合了预测和跳过预测通过的完整构建和部分构建（只有其中的测试预测通过）执行的技术。为了提供更高的安全性，HybridCISave在决定跳过构建或测试之前，结合了多种技术的预测，以获得更强的确定性。我们通过将HybridCISave与100多个项目的现有构建选择技术进行比较来评估其有效性，发现它在最高安全性的情况下提供了更高的成本节约。我们还评估了HybridCISave中的每个设计决策，发现跳过完整和部分构建增加了成本节约，并且结合多种测试选择技术使其更安全。,软件及其工程，软件创建和管理，软件后开发问题，软件维护，软件验证和确认，软件缺陷分析，软件测试和调试,,,
47M7B2BS,2023,https://doi.org/10.1145/3569949,TOSEM 2023,Hierarchical and Hybrid Organizational Structures in Open-source Software Projects: A Longitudinal Study,"Despite the absence of a formal process and a central command-and-control structure, developer organization in open-source software (OSS) projects are far from being a purely random process. Prior work indicates that, over time, highly successful OSS projects develop a hybrid organizational structure that comprises a hierarchical part and a non-hierarchical part. This suggests that hierarchical organization is not necessarily a global organizing principle and that a fundamentally different principle is at play below the lowest positions in the hierarchy. Given the vast proportion of developers are in the non-hierarchical part, we seek to understand the interplay between these two fundamentally differently organized groups, how this hybrid structure evolves, and the trajectory individual developers take through these structures over the course of their participation. We conducted a longitudinal study of the full histories of 20 popular OSS projects, modeling their organizational structures as networks of developers connected by communication ties and characterizing developers’ positions in terms of hierarchical (sub)structures in these networks. We observed a number of notable trends and patterns in the subject projects: (1) hierarchy is a pervasive structural feature of developer networks of OSS projects; (2) OSS projects tend to form hybrid organizational structures, consisting of a hierarchical and a non-hierarchical part; and (3) the positional trajectory of a developer starts loosely connected in the non-hierarchical part and then tightly integrate into the hierarchical part, which is associated with the acquisition of experience (tenure), in addition to coordination and coding activities. Our study (a) provides a methodological basis for further investigations of hierarchy formation, (b) suggests a number of hypotheses on prevalent organizational patterns and trends in OSS projects to be addressed in further work, and (c) may ultimately guide the governance of organizational structures.","Social and professional topics,Professional topics,Management of computing and information systems,Project and people management,Software management,Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Programming teams,Software notations and tools,Software configuration management and version control systems",开源软件项目中的分层和混合组织结构：纵向研究,尽管没有正式的过程和中央指挥控制结构，开源软件项目中的开发人员组织远不是一个纯粹的随机过程。先前的工作表明，随着时间的推移，高度成功的开放源码软件项目形成了一种混合组织结构，包括分层部分和非分层部分。这表明，层级组织不一定是一个全球性的组织原则，在层级中的最低职位以下，一个根本不同的原则正在发挥作用。考虑到绝大多数开发人员属于非层级部分，我们试图了解这两个组织方式根本不同的群体之间的相互作用，这种混合结构是如何演变的，以及单个开发人员在参与过程中通过这些结构的轨迹。我们对20个流行的OSS项目的完整历史进行了纵向研究，将其组织结构建模为通过通信联系连接的开发人员网络，并根据这些网络中的层次（子）结构来描述开发人员的位置。我们在主题项目中观察到了一些显著的趋势和模式：（1）层次结构是OSS项目开发者网络普遍存在的结构特征；（2） 开放源码软件项目往往形成混合的组织结构，由分层和非分层部分组成；以及（3）开发人员的位置轨迹从非分层部分开始松散连接，然后紧密集成到分层部分，这与经验（任期）的获取以及协调和编码活动有关。我们的研究（a）为进一步研究层次结构提供了方法论基础，（b）提出了一些关于开放源码软件项目中普遍存在的组织模式和趋势的假设，有待于进一步的工作，（c）可能最终指导组织结构的治理。,社会和专业主题，专业主题，计算和信息系统管理，项目和人员管理，软件管理，软件及其工程，软件创建和管理，软件开发中的合作，开放源码模型，编程团队，软件符号和工具，软件配置管理和版本控制系统,,,
LIJHI82Y,2023,https://doi.org/10.1145/3569931,TOSEM 2023,Simulating Software Evolution to Evaluate the Reliability of Early Decision-making among Design Alternatives toward Maintainability,"Critical decisions among design altern seventh atives with regards to maintainability arise early in the software design cycle. Existing comparison models relayed on the structural evolution of the used design patterns are suitable to support such decisions. However, their effectiveness on predicting maintenance effort is usually verified on a limited number of case studies under heterogeneous metrics. In this article, a multi-variable simulation model for validating the decision-making reliability of the derived formal comparison models for the significant designing problem of recursive hierarchies of part-whole aggregations, proposed in our prior work, is introduced. In the absence of a strict validation, the simulation model has been thoroughly calibrated concerning its decision-making precision based on empirical distributions from time-series analysis, approximating the highly uncertain nature of actual maintenance process. The decision reliability of the formal models has been statistically validated on a sample of 1,000 instances of design attributes representing the entire design space of the problem. Despite the limited accuracy of measurements, the results show that the models demonstrate an increasing reliability in a long-term perspective, even under assumptions of high variability. Thus, the modeling theory discussed in our prior work delivers reliable models that significantly reduce decision-risk and relevant maintenance cost.","Computing methodologies,Modeling and simulation,Model development and analysis,Model verification and validation,Simulation support systems,Simulation tools,Software and its engineering,Software creation and management,Designing software,Software design tradeoffs,Software development process management,Software development methods,Design patterns,Software organization and properties,Software system structures,Software architectures,Object oriented architectures,Software system models,Model-driven software engineering",模拟软件进化评估可维护性设计方案中早期决策的可靠性,在软件设计周期的早期，就可维护性而言，设计备选方案之间的关键决策就出现了。基于所用设计模式的结构演变的现有比较模型适用于支持此类决策。然而，它们在预测维护工作方面的有效性通常在异构指标下的有限数量的案例研究中得到验证。本文介绍了一个多变量仿真模型，用于验证我们在先前工作中提出的部分-整体集合递归层次的重要设计问题的形式比较模型的决策可靠性。在缺乏严格验证的情况下，基于时间序列分析的经验分布，对模拟模型的决策精度进行了彻底校准，近似于实际维护过程的高度不确定性。形式模型的决策可靠性已经在代表问题的整个设计空间的1000个设计属性实例的样本上进行了统计验证。尽管测量的准确性有限，但结果表明，即使在高可变性的假设下，从长期角度来看，模型的可靠性也在提高。因此，我们先前工作中讨论的建模理论提供了可靠的模型，显著降低了决策风险和相关的维护成本。,计算方法，建模和仿真，模型开发和分析，模型验证和验证，仿真支持系统，仿真工具，软件及其工程，软件创建和管理，设计软件，软件设计权衡，软件开发过程管理，软件开发方法，设计模式，软件组织和属性，软件系统结构，软件体系结构，面向对象的体系结构，软件系统模型，模型驱动的软件工程,,,
XKHTSXG6,2023,https://doi.org/10.1145/3542946,TOSEM 2023,iBiR: Bug-report-driven Fault Injection,"Much research on software engineering relies on experimental studies based on fault injection. Fault injection, however, is not often relevant to emulate real-world software faults since it “blindly” injects large numbers of faults. It remains indeed challenging to inject few but realistic faults that target a particular functionality in a program. In this work, we introduce iBiR , a fault injection tool that addresses this challenge by exploring change patterns associated to user-reported faults. To inject realistic faults, we create mutants by re-targeting a bug-report-driven automated program repair system, i.e., reversing its code transformation templates. iBiR is further appealing in practice since it requires deep knowledge of neither code nor tests, just of the program’s relevant bug reports. Thus, our approach focuses the fault injection on the feature targeted by the bug report. We assess iBiR by considering the Defects4J dataset. Experimental results show that our approach outperforms the fault injection performed by traditional mutation testing in terms of semantic similarity with the original bug, when applied at either system or class levels of granularity, and provides better, statistically significant estimations of test effectiveness (fault detection). Additionally, when injecting 100 faults, iBiR injects faults that couple with the real ones in around 36% of the cases, while mutation testing achieves less than 4%.","Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software evolution,Software version control,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software maintenance tools,Software organization and properties,Extra-functional properties,Software fault tolerance,Software performance,Software reliability,Software safety,Software usability,Software functional properties,Correctness",iBiR:Bug报告驱动的故障注入,软件工程的许多研究都依赖于基于故障注入的实验研究。然而，故障注入通常与模拟真实世界的软件故障无关，因为它“盲目”地注入了大量的故障。在程序中注入针对特定功能的少量但现实的错误仍然是一项挑战。在这项工作中，我们介绍了iBiR，这是一种故障注入工具，通过探索与用户报告的故障相关的更改模式来解决这一挑战。为了注入现实的错误，我们通过重新定位错误报告驱动的自动程序修复系统来创建突变体，即反转其代码转换模板。iBiR在实践中更具吸引力，因为它既不需要对代码也不需要对测试有深入的了解，只需要对程序的相关错误报告有深入的认识。因此，我们的方法将故障注入的重点放在错误报告所针对的功能上。我们通过考虑Defects4J数据集来评估iBiR。实验结果表明，当应用于系统或类粒度级别时，我们的方法在与原始错误的语义相似性方面优于传统突变测试执行的故障注入，并提供了更好的、具有统计意义的测试有效性（故障检测）估计。此外，当注入100个故障时，iBiR在大约36%的情况下注入与真实故障耦合的故障，而突变测试的实现率不到4%。,软件及其工程，软件创建和管理，软件开发后问题，维护软件，软件演化，软件版本控制，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件维护工具，软件组织和属性，额外功能属性，软件容错，软件性能，软件可靠性，软件安全性，软件可用性，软件功能属性，正确性,,,
ICG7Q3D4,2023,https://doi.org/10.1145/3530785,TOSEM 2023,On Wasted Contributions: Understanding the Dynamics of Contributor-Abandoned Pull Requests-A Mixed-Methods Study of 10 Large Open-Source Projects,"Pull-based development has enabled numerous volunteers to contribute to open-source projects with fewer barriers. Nevertheless, a considerable amount of pull requests (PRs) with valid contributions are abandoned by their contributors, wasting the effort and time put in by both the contributors and maintainers. To better understand the underlying dynamics of contributor-abandoned PRs, we conduct a mixed-methods study using both quantitative and qualitative methods. We curate a dataset consisting of 265,325 PRs including 4,450 abandoned ones from ten popular and mature GitHub projects and measure 16 features characterizing PRs, contributors, review processes, and projects. Using statistical and machine learning techniques, we find that complex PRs, novice contributors, and lengthy reviews have a higher probability of abandonment and the rate of PR abandonment fluctuates alongside the projects’ maturity or workload. To identify why contributors abandon their PRs, we also manually examine a random sample of 354 abandoned PRs. We observe that the most frequent abandonment reasons are related to the obstacles faced by contributors, followed by the hurdles imposed by maintainers during the review process. Finally, we survey the top core maintainers of the studied projects to understand their perspectives on dealing with PR abandonment and on our findings.","Human-centered computing,Collaborative and social computing,Empirical studies in collaborative and social computing,Software and its engineering,Software creation and management,Collaboration in software development",论浪费贡献：理解贡献者放弃拉取请求的动态——对10个大型开源项目的混合方法研究,基于拉动的开发使许多志愿者能够以更少的障碍为开源项目做出贡献。然而，大量具有有效贡献的拉取请求（PR）被其贡献者放弃，浪费了贡献者和维护者投入的精力和时间。为了更好地了解贡献者放弃PR的潜在动态，我们使用定量和定性方法进行了一项混合方法研究。我们策划了一个由265325个PR组成的数据集，其中包括来自10个流行和成熟的GitHub项目的4450个废弃的PR，并测量了表征PR、贡献者、审查过程和项目的16个特征。使用统计和机器学习技术，我们发现复杂的PR、新手贡献者和冗长的评审有更高的放弃概率，并且PR放弃率随着项目的成熟度或工作量而波动。为了确定贡献者放弃PR的原因，我们还手动检查了354个被放弃PR的随机样本。我们观察到，最常见的放弃原因与贡献者面临的障碍有关，其次是维护者在审查过程中施加的障碍。最后，我们调查了所研究项目的顶级核心维护者，以了解他们对处理PR放弃和我们的发现的看法。,以人为中心的计算，协作和社会计算，协作和社会计算的实证研究，软件及其工程，软件创建和管理，软件开发中的协作,,,
IECBCIIT,2023,https://doi.org/10.1145/3546947,TOSEM 2023,Evaluating Surprise Adequacy for Deep Learning System Testing,"The rapid adoption of Deep Learning (DL) systems in safety critical domains such as medical imaging and autonomous driving urgently calls for ways to test their correctness and robustness. Borrowing from the concept of test adequacy in traditional software testing, existing work on testing of DL systems initially investigated DL systems from structural point of view, leading to a number of coverage metrics. Our lack of understanding of the internal mechanism of Deep Neural Networks (DNNs), however, means that coverage metrics defined on the Boolean dichotomy of coverage are hard to intuitively interpret and understand. We propose the degree of out-of-distribution-ness of a given input as its adequacy for testing: the more surprising a given input is to the DNN under test, the more likely the system will show unexpected behavior for the input. We develop the concept of surprise into a test adequacy criterion, called Surprise Adequacy (SA). Intuitively, SA measures the difference in the behavior of the DNN for the given input and its behavior for the training data. We posit that a good test input should be sufficiently, but not overtly, surprising compared to the training dataset. This article evaluates SA using a range of DL systems from simple image classifiers to autonomous driving car platforms, as well as both small and large data benchmarks ranging from MNIST to ImageNet. The results show that the SA value of an input can be a reliable predictor of the correctness of the mode behavior. We also show that SA can be used to detect adversarial examples, and also be efficiently computed against large training dataset such as ImageNet using sampling.","Computing methodologies,Machine learning,Software and its engineering,Software creation and management",评估深度学习系统测试的惊喜充分性,深度学习（DL）系统在医学成像和自动驾驶等安全关键领域的快速采用迫切需要测试其正确性和稳健性的方法。借鉴传统软件测试中测试充分性的概念，现有的DL系统测试工作最初从结构角度研究DL系统，得出了许多覆盖度量。然而，我们对深度神经网络（DNN）的内部机制缺乏了解，这意味着基于覆盖的布尔二分法定义的覆盖度量很难直观地解释和理解。我们提出给定输入的分布不均匀程度作为其测试的充分性：给定输入对被测DNN越令人惊讶，系统就越有可能对输入表现出意外行为。我们将惊奇的概念发展为一个测试充分性标准，称为惊奇充分性（SA）。直观地，SA测量给定输入的DNN的行为与其训练数据的行为之间的差异。我们假设，与训练数据集相比，一个好的测试输入应该足够令人惊讶，但不是公开的。本文使用一系列DL系统评估SA，从简单的图像分类器到自动驾驶汽车平台，以及从MNIST到ImageNet的小型和大型数据基准。结果表明，输入的SA值可以作为模式行为正确性的可靠预测因子。我们还表明，SA可以用于检测对抗性示例，也可以使用采样对大型训练数据集（如ImageNet）进行有效计算。,计算方法论，机器学习，软件及其工程，软件创建和管理,,,
AZ8E9L2A,2023,https://doi.org/10.1145/3550270,TOSEM 2023,Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty,"Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS’s final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS’s parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",不确定条件下自动驾驶系统测试的参数覆盖,自动驾驶系统（ADS）很有前途，但在采用之前必须证明其安全可靠。基于模拟的测试是一种广泛采用的方法，其中ADS在特定场景的模拟环境中运行。覆盖标准规定了需要覆盖的内容，以认为ADS经过了充分测试。然而，现有的标准并不能保证ADS可以做出不同的决定，这对评估其正确性至关重要。ADS通常使用参数化的基于规则的系统和成本函数（如成本成分或决策阈值）来计算其决策。在本文中，我们认为参数是决策过程的特征，因为它们的值会影响ADS的最终决策。因此，我们提出了参数覆盖率，这是一个需要覆盖ADS参数的标准。如果改变参数值会导致不同的模拟结果，则场景会覆盖某个参数，这意味着它与场景中做出的驾驶决策相关。由于ADS模拟器有点不确定，我们采用统计方法来评估多个模拟运行的执行差异和覆盖范围。使用自治ADS的实验表明，该标准可以区分不同的场景，并且可以通过适当的启发式方法来管理计算覆盖的成本。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
GMVAM88G,2023,https://doi.org/10.1145/3582575,TOSEM 2023,Estimating Software Functional Size via Machine Learning,"Measuring software functional size via standard Function Points Analysis (FPA) requires the availability of fully specified requirements and specific competencies. Most of the time, the need to measure software functional size occurs well in advance with respect to these ideal conditions, under the lack of complete information or skilled experts. To work around the constraints of the official measurement process, several estimation methods for FPA have been proposed and are commonly used. Among these, the International Function Points User Group (IFPUG) has adopted the “High-level FPA” method (also known as the NESMA method). This method avoids weighting each data and transaction function by using fixed weights instead. Applying High-level FPA, or similar estimation methods, is faster and easier than carrying out the official measurement process but inevitably yields an approximation in the measures. In this article, we contribute to the problem of estimating software functional size measures by using machine learning. To the best of our knowledge, machine learning methods were never applied to the early estimation of software functional size. Our goal is to understand whether machine learning techniques yield estimates of FPA measures that are more accurate than those obtained with High-level FPA or similar methods. An empirical study on a large dataset of functional size predictors was carried out to train and test three of the most popular and robust machine learning methods, namely Random Forests, Support Vector Regression , and Neural Networks. A systematic experimental phase, with cycles of dataset filtering and splitting, parameter tuning, and model training and validation, is presented. The estimation accuracy of the obtained models was then evaluated and compared to that of fixed-weight models (e.g., High-level FPA) and linear regression models, also using a second dataset as the test set. We found that Support Vector Regression yields quite accurate estimation models. However, the obtained level of accuracy does not appear significantly better with respect to High-level FPA or to models built via ordinary least squares regression. Noticeably, fairly good accuracy levels were obtained by models that do not even require discerning among different types of transactions and data.","General and reference,Cross-computing tools and techniques,Empirical studies,Estimation,Measurement,Software and its engineering,Software creation and management,Software development process management,Risk management",通过机器学习估计软件功能大小,通过标准功能点分析（FPA）测量软件功能大小需要具有完全指定的需求和特定能力。大多数情况下，在缺乏完整的信息或熟练的专家的情况下，相对于这些理想条件，测量软件功能大小的需求提前发生。为了解决官方测量过程的限制，已经提出了几种常用的FPA估计方法。其中，国际功能点用户组（IFPUG）采用了“高级FPA”方法（也称为NESMA方法）。该方法通过使用固定权重来避免对每个数据和事务函数进行加权。应用高水平FPA或类似的估计方法比执行官方测量过程更快、更容易，但不可避免地会在测量中产生近似值。在本文中，我们通过使用机器学习来估计软件功能大小度量的问题。据我们所知，机器学习方法从未应用于软件功能大小的早期估计。我们的目标是了解机器学习技术是否能产生比高水平FPA或类似方法更准确的FPA测量估计值。对函数大小预测因子的大型数据集进行了实证研究，以训练和测试三种最流行和最稳健的机器学习方法，即随机森林、支持向量回归和神经网络。提出了一个系统的实验阶段，包括数据集过滤和拆分、参数调整以及模型训练和验证的循环。然后评估所获得模型的估计精度，并将其与固定权重模型（例如，高水平FPA）和线性回归模型的估计准确性进行比较，同时使用第二个数据集作为测试集。我们发现支持向量回归产生了相当准确的估计模型。然而，相对于高水平FPA或通过普通最小二乘回归建立的模型，所获得的精度水平似乎并不明显更好。值得注意的是，模型甚至不需要区分不同类型的事务和数据，就可以获得相当好的准确性。,概论和参考，交叉计算工具和技术，实证研究，估计，度量，软件及其工程，软件创建和管理，软件开发过程管理，风险管理,,,
TGKGZ227,2023,https://doi.org/10.1145/3546941,TOSEM 2023,Estimating Probabilistic Safe WCET Ranges of Real-Time Systems at Design Stages,"Estimating worst-case execution time (WCET) is an important activity at early design stages of real-time systems. Based on WCET estimates, engineers make design and implementation decisions to ensure that task executions always complete before their specified deadlines. However, in practice, engineers often cannot provide precise point WCET estimates and prefer to provide plausible WCET ranges. Given a set of real-time tasks with such ranges, we provide an automated technique to determine for what WCET values the system is likely to meet its deadlines and, hence, operate safely with a probabilistic guarantee. Our approach combines a search algorithm for generating worst-case scheduling scenarios with polynomial logistic regression for inferring probabilistic safe WCET ranges. We evaluated our approach by applying it to three industrial systems from different domains and several synthetic systems. Our approach efficiently and accurately estimates probabilistic safe WCET ranges within which deadlines are likely to be satisfied with a high degree of confidence.","Computer systems organization,Real-time systems,Computing methodologies,Machine learning,Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation,Empirical software validation,Software organization and properties,Software functional properties,Correctness,Real-time schedulability",实时系统在设计阶段的概率安全WCET范围估计,估计最坏情况执行时间（WCET）是实时系统早期设计阶段的一项重要活动。根据WCET的估计，工程师做出设计和实施决策，以确保任务执行始终在指定的截止日期之前完成。然而，在实践中，工程师通常无法提供精确的点WCET估计，而倾向于提供合理的WCET范围。给定一组具有这样范围的实时任务，我们提供了一种自动技术来确定系统可能满足其截止日期的WCET值，从而在概率保证的情况下安全运行。我们的方法将用于生成最坏情况调度场景的搜索算法与用于推断概率安全WCET范围的多项式逻辑回归相结合。我们通过将其应用于来自不同领域的三个工业系统和几个合成系统来评估我们的方法。我们的方法有效而准确地估计了概率安全的WCET范围，在该范围内，截止日期可能会以高度的置信度得到满足。,计算机系统组织，实时系统，计算方法，机器学习，软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认，经验软件验证，软件组织和属性，软件功能属性，正确性，实时可重用性,,,
8ULXDZ6W,2023,https://doi.org/10.1145/3527851,TOSEM 2023,Combatting Energy Issues for Mobile Applications,"Energy efficiency is an important criterion to judge the quality of mobile apps, but one third of our arbitrarily sampled apps suffer from energy issues that can quickly drain battery power. To understand these issues, we conduct an empirical study on 36 well-maintained apps such as Chrome and Firefox, whose issue tracking systems are publicly accessible. Our study involves issue causes, manifestation, fixing efforts, detection techniques, reasons of no-fixes, and debugging techniques. Inspired by the empirical study, we propose a novel testing framework for detecting energy issues in real-world mobile apps. Our framework examines apps with well-designed input sequences and runtime context. We develop leading edge technologies, e.g., pre-designing input sequences with potential energy overuse and tuning tests on-the-fly, to achieve high efficacy in detecting energy issues. A large-scale evaluation shows that 90.4% of the detected issues in our experiments were previously unknown to developers. On average, these issues can double the energy consumption of the test cases where the issues were detected. And our test achieves a low number of false positives. Finally, we show how our test reports can help developers fix the issues.","Hardware,Power and energy,Power estimation and optimization,Platform power issues,Human-centered computing,Ubiquitous and mobile computing,Empirical studies in ubiquitous and mobile computing,Ubiquitous and mobile computing design and evaluation methods,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",为移动应用解决能源问题,能源效率是判断移动应用程序质量的重要标准，但我们任意采样的应用程序中有三分之一存在能源问题，可能会迅速耗尽电池电量。为了了解这些问题，我们对Chrome和Firefox等36款维护良好的应用程序进行了实证研究，这些应用程序的问题跟踪系统可以公开访问。我们的研究涉及问题的原因、表现形式、修复工作、检测技术、未修复的原因和调试技术。受实证研究的启发，我们提出了一个新的测试框架，用于检测现实世界移动应用程序中的能源问题。我们的框架通过精心设计的输入序列和运行时上下文来检查应用程序。我们开发了领先的技术，例如，预先设计具有潜在能量过度使用的输入序列和动态调整测试，以实现检测能量问题的高效性。一项大规模评估显示，在我们的实验中，90.4%的检测到的问题以前是开发人员未知的。平均而言，这些问题可能会使检测到问题的测试用例的能耗增加一倍。我们的测试实现了低数量的假阳性。最后，我们展示了我们的测试报告如何帮助开发人员解决问题。,硬件，电力和能源，电力评估和优化，平台电力问题，以人为中心的计算，泛在和移动计算的实证研究，泛在和移动计算的设计和评估方法，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
S74CR6DX,2023,https://doi.org/10.1145/3583566,TOSEM 2023,COMET: Coverage-guided Model Generation For Deep Learning Library Testing,"Recent deep learning (DL) applications are mostly built on top of DL libraries. The quality assurance of these libraries is critical to the dependable deployment of DL applications. Techniques have been proposed to generate various DL models and apply them to test these libraries. However, their test effectiveness is constrained by the diversity of layer API calls in their generated DL models. Our study reveals that these techniques can cover at most 34.1% layer inputs, 25.9% layer parameter values, and 15.6% layer sequences. As a result, we find that many bugs arising from specific layer API calls (i.e., specific layer inputs, parameter values, or layer sequences) can be missed by existing techniques.","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software libraries and repositories",COMET：深度学习库测试的覆盖引导模型生成,最近的深度学习（DL）应用程序大多建立在DL库之上。这些库的质量保证对于DL应用程序的可靠部署至关重要。已经提出了生成各种DL模型并将其应用于测试这些库的技术。然而，它们的测试有效性受到其生成的DL模型中层API调用的多样性的限制。我们的研究表明，这些技术最多可以覆盖34.1%的层输入、25.9%的层参数值和15.6%的层序列。因此，我们发现现有技术可能会错过特定层API调用（即特定层输入、参数值或层序列）引起的许多错误。,计算方法，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件库和存储库,,,
NQK9T5HF,2023,https://doi.org/10.1145/3597207,TOSEM 2023,CodeEditor: Learning to Edit Source Code with Pre-trained Models,"Developers often perform repetitive code editing activities (up to 70%) for various reasons (e.g., code refactoring) during software development. Many deep learning (DL) models have been proposed to automate code editing by learning from the code editing history. Among DL-based models, pre-trained code editing models have achieved the state-of-the-art (SOTA) results. Pre-trained models are first pre-trained with pre-training tasks and fine-tuned with the code editing task. Existing pre-training tasks mainly are code infilling tasks (e.g., masked language modeling), which are derived from the natural language processing field and are not designed for automatic code editing.","Computing methodologies,Artificial intelligence,Natural language processing,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software development techniques,Automatic programming",CodeEditor：学习使用预先训练的模型编辑源代码,在软件开发过程中，由于各种原因（例如代码重构），开发人员经常执行重复的代码编辑活动（高达70%）。已经提出了许多深度学习（DL）模型，以通过从代码编辑历史中学习来自动化代码编辑。在基于DL的模型中，预先训练的代码编辑模型已经获得了最先进的（SOTA）结果。经过预训练的模型首先通过预训练任务进行预训练，然后通过代码编辑任务进行微调。现有的预训练任务主要是代码填充任务（例如，掩蔽语言建模），这些任务源自自然语言处理领域，并不是为自动代码编辑而设计的。,计算方法论，人工智能，自然语言处理，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件开发技术，自动编程,,,
S8JJCZ7P,2023,https://doi.org/10.1145/3533700,TOSEM 2023,The Co-evolution of the WordPress Platform and Its Plugins,"One can extend the features of a software system by installing a set of additional components called plugins. WordPress, as a typical example of such plugin-based software ecosystems, is used by millions of websites and has a large number (i.e., 54,777) of available plugins. These plugin-based software ecosystems are different from traditional ecosystems (e.g., NPM dependencies) in the sense that there is high coupling between a platform and its plugins compared to traditional ecosystems for which components might not necessarily depend on each other (e.g., NPM libraries do not depend on a specific version of NPM or a specific version of a client software system). The high coupling between a plugin and its platform and other plugins causes incompatibility issues that occur during the co-evolution of a plugin and its platform as well as other plugins. In fact, incompatibility issues represent a major challenge when upgrading WordPress or its plugins. According to our study of the top 500 most-released WordPress plugins, we observe that incompatibility issues represent the third major cause for bad releases, which are rapidly (within the next 24 hours) fixed via urgent releases. Thirty-two percent of these incompatibilities are between a plugin and WordPress while 19% are between peer plugins. In this article, we study how plugins co-evolve with the underlying platform as well as other plugins, in an effort to understand the practices that are related support such co-evolution and reduce incompatibility issues. In particular, we investigate how plugins support the latest available versions of WordPress, as well as how plugins are related to each other, and how they co-evolve. We observe that a plugin’s support of new versions of WordPress with a large amount of code change is risky, as the releases that declare such support have a higher chance to be followed by an urgent release compared to ordinary releases. Although plugins support the latest WordPress version, plugin developers omit important changes such as deleting the use of removed WordPress APIs, which are removed a median of 873 days after the APIs have been removed from the source code of WordPress. Plugins introduce new releases that are made according to a median of five other plugins, which we refer to as peer-triggered releases. A median of 20% of the peer-triggered releases are urgent releases that fix problems in their previous releases. The most common goal of peer-triggered releases is the fixing of incompatibility issues that a plugin detects as late as after a median of 36 days since the last release of another plugin. Our work sheds light on the co-evolution of WordPress plugins with their platform as well as peer plugins in an effort to uncover the practices of plugin evolution, so WordPress can accordingly design approaches to avoid incompatibility issues.","Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software evolution",WordPress平台及其插件的共同发展,可以通过安装一组称为插件的附加组件来扩展软件系统的功能。WordPress作为这种基于插件的软件生态系统的典型例子，被数百万网站使用，并拥有大量（即54777）可用插件。这些基于插件的软件生态系统与传统生态系统（例如，NPM依赖性）的不同之处在于，与组件不一定相互依赖的传统生态系统相比，平台及其插件之间存在高度耦合（例如，NPTM库不依赖于特定版本的NPM或特定版本的客户端软件系统）。插件及其平台和其他插件之间的高度耦合导致了在插件及其平台以及其他插件的共同进化过程中出现的不兼容问题。事实上，在升级WordPress或其插件时，不兼容问题是一个主要挑战。根据我们对发布量最高的500个WordPress插件的研究，我们发现不兼容问题是不良发布的第三大原因，这些问题会通过紧急发布迅速（在接下来的24小时内）解决。32%的不兼容性发生在插件和WordPress之间，19%发生在对等插件之间。在本文中，我们研究了插件如何与底层平台以及其他插件协同进化，以了解支持这种协同进化的相关实践，并减少不兼容性问题。特别是，我们研究了插件如何支持最新版本的WordPress，以及插件如何相互关联，以及它们如何共同发展。我们观察到，一个插件对WordPress新版本的大量代码更改的支持是有风险的，因为与普通版本相比，声明支持这种支持的版本有更高的机会被紧急发布。尽管插件支持最新的WordPress版本，但插件开发人员省略了重要的更改，如删除已删除的WordPress API的使用，这些API在从WordPress源代码中删除后的平均873天内被删除。插件引入了根据其他五个插件的中位数制作的新版本，我们称之为对等触发版本。同行触发的版本中，有20%是紧急版本，用于修复以前版本中的问题。对等触发版本最常见的目标是修复不兼容问题，插件最晚在另一个插件上次发布后的36天内检测到这些问题。我们的工作揭示了WordPress插件与其平台以及对等插件的共同进化，以揭示插件进化的实践，因此WordPress可以相应地设计方法来避免不兼容问题。,软件及其工程，软件创建和管理，软件开发后问题，软件维护，软件演化,,,
RTEUVNDQ,2023,https://doi.org/10.1145/3571851,TOSEM 2023,Route: Roads Not Taken in UI Testing,"Core features (functionalities) of an app can often be accessed and invoked in several ways, i.e., through alternative sequences of user-interface (UI) interactions. Given the manual effort of writing tests, developers often only consider the typical way of invoking features when creating the tests (i.e., the “sunny day scenario”). However, the alternative ways of invoking a feature are as likely to be faulty. These faults would go undetected without proper tests. To reduce the manual effort of creating UI tests and help developers more thoroughly examine the features of apps, we present Route, an automated tool for feature-based UI test augmentation for Android apps. Route first takes a UI test and the app under test as input. It then applies novel heuristics to find additional high-quality UI tests, consisting of both inputs and assertions, that verify the same feature as the original test in alternative ways. Application of Route on several dozen tests for popular apps on Google Play shows that for 96% of the existing tests, Route was able to generate at least one alternative test. Moreover, the fault detection effectiveness of augmented test suites in our experiments showed substantial improvements of up to 39% over the original test suites.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",路线：UI测试中未采用的道路,应用程序的核心功能通常可以通过多种方式访问和调用，即通过用户界面（UI）交互的替代序列。考虑到编写测试的手动工作，开发人员在创建测试时通常只考虑调用特性的典型方式（即“晴天场景”）。然而，调用特性的其他方法也可能存在错误。如果没有适当的测试，这些故障将无法被发现。为了减少创建UI测试的手动工作量，并帮助开发人员更彻底地检查应用程序的功能，我们推出了Route，这是一款用于Android应用程序的基于功能的UI测试增强的自动化工具。Route首先将UI测试和测试中的应用程序作为输入。然后，它应用新的启发式方法来寻找额外的高质量UI测试，包括输入和断言，以其他方式验证与原始测试相同的功能。Route在Google Play上流行应用程序的几十项测试中的应用表明，在96%的现有测试中，Route能够生成至少一个替代测试。此外，在我们的实验中，增强测试套件的故障检测有效性比原始测试套件提高了39%。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
XC6XTVSN,2023,https://doi.org/10.1145/3597204,TOSEM 2023,Rise of Distributed Deep Learning Training in the Big Model Era: From a Software Engineering Perspective,"Deep learning (DL) has become a key component of modern software. In the “big model” era, the rich features of DL-based software (i.e., DL software) substantially rely on powerful DL models, e.g., BERT, GPT-3, and the recently emerging GPT-4, which are trained on the powerful cloud with large datasets. Hence, training effective DL models has become a vital stage in the whole software lifecycle. When training deep learning models, especially those big models, developers need to parallelize and distribute the computation and memory resources amongst multiple devices (e.g., a cluster of GPUs) in the training process, which is known as distributed deep learning training, or distributed training for short. However, the unique challenges that developers encounter in distributed training process have not been studied in the software engineering community. Given the increasingly heavy dependence of current DL-based software on distributed training, this paper aims to fill in the knowledge gap and presents the first comprehensive study on developers’ issues in distributed training. To this end, we focus on popular DL frameworks that support distributed training (including TensorFlow, PyTorch, Keras, and Horovod) and analyze 1,131 real-world developers’ issues about using these frameworks reported on Stack Overflow and GitHub. We construct a fine-grained taxonomy consisting of 30 categories regarding the fault symptoms and summarize common fix patterns for different symptoms. We find that: (1) many distributed-specific faults and non-distributed-specific faults inherently share the same fault symptoms, making it challenging to debug; (2) most of the fault symptoms have frequent fix patterns; (3) about half of the faults are related to system-level configurations. Based on the results, we suggest actionable implications on research avenues that can potentially facilitate the distributed training to develop DL-based software, such as focusing on the frequent and common fix patterns when designing testing or debugging tools, developing efficient testing and debugging techniques for communication configuration along with the synthesis of network configuration analysis, designing new multi-device checkpoint-and-replay techniques to help reproduction, and designing serverless APIs for cloud platforms.","Computing methodologies,Distributed computing methodologies,General and reference,Cross-computing tools and techniques,Empirical studies,Software and its engineering,Software creation and management",大模型时代分布式深度学习培训的兴起：从软件工程的角度,深度学习（DL）已成为现代软件的关键组成部分。在“大模型”时代，基于DL的软件（即DL软件）的丰富功能在很大程度上依赖于强大的DL模型，例如BERT、GPT-3和最近出现的GPT-4，它们在具有大型数据集的强大云上进行训练。因此，训练有效的DL模型已经成为整个软件生命周期中的一个重要阶段。当训练深度学习模型，特别是那些大模型时，开发人员需要在训练过程中在多个设备（例如GPU集群）之间并行化和分配计算和内存资源，这被称为分布式深度学习训练，简称分布式训练。然而，软件工程界尚未对开发人员在分布式培训过程中遇到的独特挑战进行研究。鉴于当前基于DL的软件越来越严重地依赖于分布式训练，本文旨在填补知识空白，并首次对分布式训练中的开发人员问题进行了全面研究。为此，我们重点关注支持分布式训练的流行DL框架（包括TensorFlow、PyTorch、Keras和Horovod），并分析了Stack Overflow和GitHub上报道的1131个真实世界开发人员关于使用这些框架的问题。我们构建了一个由30个关于故障症状的类别组成的细粒度分类法，并总结了不同症状的常见修复模式。我们发现：（1）许多分布式特定故障和非分布式特定故障本质上具有相同的故障症状，这使得调试具有挑战性；（2） 大多数故障症状具有频繁的修复模式；（3） 大约一半的故障与系统级配置有关。基于这些结果，我们对研究途径提出了可操作的启示，这些途径可能有助于分布式训练来开发基于DL的软件，例如在设计测试或调试工具时关注频繁和常见的修复模式，开发高效的通信配置测试和调试技术以及网络配置分析的综合，设计新的多设备检查点和回放技术以帮助复制，以及为云平台设计无服务器API。,计算方法论，分布式计算方法论，概论和参考，交叉计算工具和技术，实证研究，软件及其工程，软件创建和管理,,,
TQDJVFLL,2023,https://doi.org/10.1145/3576036,TOSEM 2023,Refactoring in Computational Notebooks,"Due to the exploratory nature of computational notebook development, a notebook can be extensively evolved even though it is small, potentially incurring substantial technical debt. Indeed, in interview studies notebook authors have attested to performing ongoing tidying and big cleanups. However, many notebook authors are not trained as software developers, and environments like JupyterLab possess few features to aid notebook maintenance. ","Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software evolution",计算笔记本中的重构,由于计算笔记本开发的探索性，即使笔记本很小，也可以进行广泛的开发，这可能会导致大量的技术债务。事实上，在访谈研究中，笔记本作者已经证明了他们正在进行整理和大扫除。然而，许多笔记本电脑作者并没有受过软件开发人员的培训，像JupyterLab这样的环境几乎没有帮助笔记本电脑维护的功能。,软件及其工程，软件创建和管理，软件开发后问题，软件维护，软件演化,,,
I5CG4IHG,2023,https://doi.org/10.1145/3585007,TOSEM 2023,\emphFaaSLight: General Application-level Cold-start Latency Optimization for Function-as-a-Service in Serverless Computing,"Serverless computing is a popular cloud computing paradigm that frees developers from server management. Function-as-a-Service (FaaS) is the most popular implementation of serverless computing, representing applications as event-driven and stateless functions. However, existing studies report that functions of FaaS applications severely suffer from cold-start latency.","Computer systems organization,Dependable and fault-tolerant systems and networks,Redundancy,Embedded and cyber-physical systems,Embedded systems,Robotics,Networks,Network properties,Network reliability",\emphFaaSLight：无服务器计算中功能即服务的通用应用程序级冷启动延迟优化,无服务器计算是一种流行的云计算模式，它将开发人员从服务器管理中解放出来。功能即服务（FaaS）是最流行的无服务器计算实现，将应用程序表示为事件驱动和无状态功能。然而，现有研究报告称，FaaS应用程序的功能严重受到冷启动延迟的影响。,计算机系统组织，可靠和容错系统和网络，冗余，嵌入式和网络物理系统，嵌入式系统，机器人，网络，网络特性，网络可靠性,,,
T9DS8AKD,2023,https://doi.org/10.1145/3579637,TOSEM 2023,Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair,"Fix pattern-based patch generation is a promising direction in automated program repair (APR). Notably, it has been demonstrated to produce more acceptable and correct patches than the patches obtained with mutation operators through genetic programming. The performance of pattern-based APR systems, however, depends on the fix ingredients mined from fix changes in development histories. Unfortunately, collecting a reliable set of bug fixes in repositories can be challenging. In this article, we propose investigating the possibility in an APR scenario of leveraging fix patterns inferred from code changes that address violations detected by static analysis tools. To that end, we build a fix pattern-based APR tool, Avatar, which exploits fix patterns of static analysis violations as ingredients for the patch generation of repairing semantic bugs. Evaluated on four benchmarks (i.e., Defects4J, Bugs.jar, BEARS, and QuixBugs), Avatar presents the potential feasibility of fixing semantic bugs with the fix patterns inferred from the patches for fixing static analysis violations and can correctly fix 26 semantic bugs when Avatar is implemented with the normal program repair pipeline. We also find that Avatar achieves performance metrics that are comparable to that of the closely related approaches in the literature. Compared with CoCoNut, Avatar can fix 18 new bugs in Defects4J and 3 new bugs in QuixBugs. When compared with HDRepair, JAID, and SketchFix, Avatar can newly fix 14 Defects4J bugs. In terms of the number of correctly fixed bugs, Avatar is also comparable to the program repair tools with the normal fault localization setting and presents better performance than most program repair tools. These results imply that Avatar is complementary to current program repair approaches. We further uncover that Avatar can present different bug-fixing performances when it is configured with different fault localization tools, and the stack trace information from the failed executions of test cases can be exploited to improve the bug-fixing performance of Avatar by fixing more bugs with fewer generated patch candidates. Overall, our study highlights the relevance of static bug-finding tools as indirect contributors of fix ingredients for addressing code defects identified with functional test cases (i.e., dynamic information).","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",从用于自动程序修复的静态校验器推断的可靠修复模式,基于修复模式的补丁生成是自动程序修复（APR）的一个很有前途的方向。值得注意的是，与通过遗传编程使用突变算子获得的补丁相比，它已经被证明产生了更可接受和正确的补丁。然而，基于模式的APR系统的性能取决于从开发历史中的修复变化中挖掘出的修复成分。不幸的是，在存储库中收集一组可靠的错误修复可能很有挑战性。在本文中，我们建议研究在APR场景中利用从代码更改中推断的修复模式的可能性，这些代码更改解决了静态分析工具检测到的违规行为。为此，我们构建了一个基于修复模式的APR工具Avatar，该工具利用静态分析违规的修复模式作为修复语义错误的补丁生成的组成部分。在Defects4J、Bugs.jar、BEARS和QuixBugs四个基准上进行评估后，Avatar展示了用从修复静态分析违规的补丁中推断出的修复模式修复语义错误的潜在可行性，并且当Avatar使用正常程序修复管道实现时，可以正确修复26个语义错误。我们还发现，《阿凡达》的性能指标与文献中密切相关的方法相当。与CoCoNut相比，Avatar可以修复Defects4J中的18个新错误和QuixBugs中的3个新错误。与HDRepair、JAID和SketchFix相比，Avatar可以修复14个Defects4J错误。就正确修复的错误数量而言，Avatar也可以与具有正常故障定位设置的程序修复工具相媲美，并且表现出比大多数程序修复工具更好的性能。这些结果表明，《阿凡达》是对当前程序修复方法的补充。我们进一步发现，当使用不同的故障定位工具配置Avatar时，它可以呈现不同的错误修复性能，并且可以利用测试用例执行失败的堆栈跟踪信息，通过用更少的生成候选补丁修复更多的错误来提高Avatar的错误修复能力。总的来说，我们的研究强调了静态错误查找工具作为修复成分的间接贡献者的相关性，这些修复成分用于解决功能测试用例（即动态信息）中确定的代码缺陷。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
QQFIVT3H,2023,https://doi.org/10.1145/3563211,TOSEM 2023,Security Responses in Software Development,"The pressure on software developers to produce secure software has never been greater. But what does security look like in environments that do not produce security-critical software? In answer to this question, this multi-sited ethnographic study characterizes security episodes and identifies five typical behaviors in software development. Using theory drawn from information security and motivation research in software engineering, this article characterizes key ways in which individual developers form security responses to meet the demands of particular circumstances, providing a framework managers and teams can use to recognize, understand, and alter security activity in their environments.","Human-centered computing,Collaborative and social computing,Empirical studies in collaborative and social computing,Security and privacy,Human and societal aspects of security and privacy,Software and its engineering",软件开发中的安全响应,软件开发人员生产安全软件的压力从未如此之大。但是，在不生产安全关键软件的环境中，安全性是什么样子的呢？为了回答这个问题，这项多站点的民族志研究描述了安全事件，并确定了软件开发中的五种典型行为。本文利用软件工程中信息安全和动机研究的理论，描述了个体开发人员形成安全响应以满足特定环境需求的关键方式，为管理者和团队提供了一个可以用来识别、理解和改变其环境中安全活动的框架。,以人为中心的计算，协作和社会计算，协作和社会计算的经验研究，安全和隐私，安全和隐私的人和社会方面，软件及其工程,,,
ZJAYU4ME,2023,https://doi.org/10.1145/3560264,TOSEM 2023,Pied-Piper: Revealing the Backdoor Threats in Ethereum ERC Token Contracts,"With the development of decentralized networks, smart contracts, especially those for ERC tokens, are attracting more and more Dapp users to implement their applications. There are some functions in ERC token contracts that only a specific group of accounts could invoke. Among those functions, some even can influence other accounts or the whole system without prior notice or permission. These functions are referred to as contract backdoors. Once exploited by an attacker, they can cause property losses and harm users’ privacy.","Security and privacy,Software and application security,Domain-specific security and privacy architectures",Pied Piper:揭示以太坊ERC代币合约中的后门威胁,随着去中心化网络的发展，智能合约，尤其是ERC代币的智能合约，正在吸引越来越多的Dapp用户来实现他们的应用。ERC代币合约中有一些功能，只有特定的一组帐户才能调用。在这些功能中，有些甚至可以在没有事先通知或许可的情况下影响其他帐户或整个系统。这些功能被称为合同后门。一旦被攻击者利用，它们可能会造成财产损失并损害用户隐私。,安全和隐私，软件和应用程序安全，特定于域的安全和隐私架构,,,
G6YRJYTN,2023,https://doi.org/10.1145/3544791,TOSEM 2023,Nudge: Accelerating Overdue Pull Requests toward Completion,"Pull requests are a key part of the collaborative software development and code review process today. However, pull requests can also slow down the software development process when the reviewer(s) or the author do not actively engage with the pull request. In this work, we design an end-to-end service, Nudge, for accelerating overdue pull requests toward completion by reminding the author or the reviewer(s) to engage with their overdue pull requests. First, we use models based on effort estimation and machine learning to predict the completion time for a given pull request. Second, we use activity detection to filter out pull requests that may be overdue but for which sufficient action is taking place nonetheless. Last, we use actor identification to understand who the blocker of the pull request is and nudge the appropriate actor (author or reviewer(s)). The key novelty of Nudge is that it succeeds in reducing pull request resolution time, while ensuring that developers perceive the notifications sent as useful, at the scale of thousands of repositories. In a randomized trial on 147 repositories in use at Microsoft, Nudge was able to reduce pull request resolution time by 60% for 8,500 pull requests, when compared to overdue pull requests for which Nudge did not send a notification. Furthermore, developers receiving Nudge notifications resolved 73% of these notifications as positive. We observed similar results when scaling up the deployment of Nudge to 8,000 repositories at Microsoft, for which Nudge sent 210,000 notifications during a full year. This demonstrates Nudge’s ability to scale to thousands of repositories. Last, our qualitative analysis of a selection of Nudge notifications indicates areas for future research, such as taking dependencies among pull requests and developer availability into account.","Software and its engineering,Software notations and tools,Development frameworks and environments,Integrated and visual development environments,Software configuration management and version control systems,Software maintenance tools",Nudge：加快逾期提取请求的完成速度,拉取请求是当今协同软件开发和代码审查过程中的一个关键部分。然而，当评审员或作者没有积极参与拉取请求时，拉取请求也会减慢软件开发过程。在这项工作中，我们设计了一个端到端的服务Nudge，通过提醒作者或审阅者处理他们的逾期拉取请求，来加速逾期拉取的完成。首先，我们使用基于努力估计和机器学习的模型来预测给定拉取请求的完成时间。其次，我们使用活动检测来过滤可能过期但仍在采取足够行动的拉取请求。最后，我们使用参与者识别来了解拉取请求的阻止者是谁，并推动适当的参与者（作者或审阅者）。Nudge的关键新颖之处在于，它成功地减少了拉取请求的解析时间，同时确保开发人员认为发送的通知是有用的，其规模相当于数千个存储库。在一项针对微软使用的147个存储库的随机试验中，与Nudge没有发送通知的逾期拉取请求相比，Nudge能够将8500个拉取请求的拉取请求解决时间缩短60%。此外，收到Nudge通知的开发人员将73%的通知解析为阳性。当我们在微软将Nudge的部署扩大到8000个存储库时，我们观察到了类似的结果，Nudge全年为此发送了210000个通知。这展示了Nudge扩展到数千个存储库的能力。最后，我们对Nudge通知的定性分析指出了未来研究的领域，例如考虑拉取请求和开发人员可用性之间的依赖性。,软件及其工程，软件符号和工具，开发框架和环境，集成和可视化开发环境，软件配置管理和版本控制系统，软件维护工具,,,
748EDHWQ,2023,https://doi.org/10.1145/3585006,TOSEM 2023,Fair Enough: Searching for Sufficient Measures of Fairness,"Testing machine learning software for ethical bias has become a pressing current concern. In response, recent research has proposed a plethora of new fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360 toolkit. This raises the question: How can any fairness tool satisfy such a diverse range of goals? While we cannot completely simplify the task of fairness testing, we can certainly reduce the problem. This article shows that many of those fairness metrics effectively measure the same thing. Based on experiments using seven real-world datasets, we find that (a) 26 classification metrics can be clustered into seven groups and (b) four dataset metrics can be clustered into three groups. Further, each reduced set may actually predict different things. Hence, it is no longer necessary (or even possible) to satisfy all fairness metrics. In summary, to simplify the fairness testing problem, we recommend the following steps: (1) determine what type of fairness is desirable (and we offer a handful of such types), then (2) lookup those types in our clusters, and then (3) just test for one item per cluster.","Social and professional topics,User characteristics,Software and its engineering,Software creation and management,Designing software,Software design tradeoffs,Software organization and properties,Extra-functional properties,Software reliability",足够公平：寻找公平的充分测度,测试机器学习软件的道德偏见已经成为当前一个紧迫的问题。作为回应，最近的研究提出了大量新的公平性指标，例如，IBM AIF360工具包中的数十个公平性指标。这就提出了一个问题：任何公平工具如何满足如此多样化的目标？虽然我们不能完全简化公平性测试的任务，但我们肯定可以减少这个问题。这篇文章表明，许多公平性指标有效地衡量了相同的东西。基于使用七个真实世界数据集的实验，我们发现（a）26个分类度量可以被聚类为七组，（b）四个数据集度量可以聚类为三组。此外，每个简化集实际上可以预测不同的事情。因此，不再需要（甚至不可能）满足所有公平性度量。总之，为了简化公平性测试问题，我们建议以下步骤：（1）确定需要什么类型的公平性（我们提供了一些这样的类型），然后（2）在我们的集群中查找这些类型，然后（3）每个集群只测试一个项目。,社会和专业主题，用户特征，软件及其工程，软件创建和管理，软件设计，软件设计权衡，软件组织和属性，额外功能属性，软件可靠性,,,
BX9245UM,2023,https://doi.org/10.1145/3607181,TOSEM 2023,Exploring the Impact of Code Clones on Deep Learning Software,"Deep learning (DL) is a really active topic in recent years. Code cloning is a common code implementation that could negatively impact software maintenance. For DL software, developers rely heavily on frameworks to implement DL features. Meanwhile, to guarantee efficiency, developers often reuse the steps and configuration settings for building DL models. These may bring code copy-pastes or reuses inducing code clones. However, there is little work exploring code clones’ impact on DL software. In this article, we conduct an empirical study and show that: (1) code clones are prevalent in DL projects, about 16.3% of code fragments encounter clones, which is almost twice larger than the traditional projects; (2) 75.6% of DL projects contain co-changed clones, meaning changes are propagated among cloned fragments, which can bring maintenance difficulties; (3) Percentage of the clones and Number of clone lines are associated with the emergence of co-changes; (4) the prevalence of Code clones varies in DL projects with different frameworks, but the difference is not significant; (5) Type 1 co-changed clones often spread over different folders, but Types 2 and 3 co-changed clones mainly occur within the same files or folders; (6) 57.1% of all co-changed clones are involved in bugs.","General and reference,Cross-computing tools and techniques,Empirical studies,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software evolution",探索代码克隆对深度学习软件的影响,深度学习（DL）是近年来一个非常活跃的话题。代码克隆是一种常见的代码实现，可能会对软件维护产生负面影响。对于DL软件，开发人员在很大程度上依赖于框架来实现DL功能。同时，为了保证效率，开发人员经常重用构建DL模型的步骤和配置设置。这可能会带来代码复制粘贴或重复使用，从而导致代码克隆。然而，研究代码克隆对DL软件的影响的工作很少。在本文中，我们进行了一项实证研究，结果表明：（1）DL项目中普遍存在代码克隆，约16.3%的代码片段遇到克隆，这几乎是传统项目的两倍；（2） 75.6%的DL项目包含共同改变的克隆，这意味着改变在克隆片段之间传播，这可能会带来维护困难；（3） 克隆的百分比和克隆系的数量与共变异的出现有关；（4） 在不同框架的DL项目中，代码克隆的流行率各不相同，但差异并不显著；（5） 类型1共变克隆通常分布在不同的文件夹中，但类型2和类型3共变克隆主要发生在相同的文件或文件夹中；（6） 57.1%的共变克隆都与bug有关。,概论和参考，交叉计算工具和技术，实证研究，软件及其工程，软件创建和管理，软件开发后问题，软件维护，软件进化,,,
IYDQ8LSD,2023,https://doi.org/10.1145/3579636,TOSEM 2023,Challenges of Working from Home in Software Development During Covid-19 Lockdowns,"The COVID-19 pandemic in 2020/2021/2022 and the resulting lockdowns forced many companies to switch to working from home, swiftly, on a large scale, and without preparation. This situation created unique challenges for software development, where individual software professionals had to shift instantly from working together at a physical venue to working remotely from home. Our research questions focus on the challenges of software professionals who work from home due to the COVID-19 pandemic, which we studied empirically at a German bank. We conducted a case study employing a mixed methods approach. We aimed to cover both the breadth of challenges via a quantitative survey, as well as a deeper understanding of these challenges via the follow-up qualitative analysis of 15 semi-structured interviews. In this article, we present the key impediments employees faced during the crisis, as well as their similarities and differences to the known challenges in distributed software development (DSD). We also analyze the employees’ job satisfaction and how the identified challenges impact job satisfaction. In our study, we focus on challenges in communication, collaboration, tooling, and management. The findings of the study provide insights into this emerging topic of high industry relevance. At the same time, the study contributes to the existing academic research on work from home and on the COVID-19 pandemic aftermath.","Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Programming teams,Software development process management,Software development methods,Agile software development,Software notations and tools",新冠肺炎封锁期间在家工作的软件开发挑战,2020/2021/2022年的新冠肺炎疫情及其导致的封锁迫使许多公司迅速、大规模、毫无准备地转向在家工作。这种情况给软件开发带来了独特的挑战，单个软件专业人员必须立即从在物理场所一起工作转变为在家远程工作。我们的研究问题集中在新冠肺炎大流行导致在家工作的软件专业人员面临的挑战上，我们在德国一家银行进行了实证研究。我们采用混合方法进行了案例研究。我们的目标是通过定量调查涵盖挑战的广度，并通过对15次半结构化访谈的后续定性分析来更深入地了解这些挑战。在本文中，我们介绍了员工在危机期间面临的主要障碍，以及它们与分布式软件开发（DSD）中已知挑战的异同。我们还分析了员工的工作满意度，以及确定的挑战如何影响工作满意度。在我们的研究中，我们关注沟通、协作、工具和管理方面的挑战。这项研究的发现为这一新兴的具有高度行业相关性的主题提供了见解。同时，该研究有助于现有的关于在家工作和新冠肺炎大流行后果的学术研究。,软件及其工程，软件创建和管理，软件开发协作，开源模型，编程团队，软件开发过程管理，软件开发方法，敏捷软件开发，软件符号和工具,,,
32SU88YS,2023,https://doi.org/10.1145/3563214,TOSEM 2023,Video Game Bad Smells: What They Are and How Developers Perceive Them,"Video games represent a substantial and increasing share of the software market. However, their development is particularly challenging as it requires multi-faceted knowledge, which is not consolidated in computer science education yet. This article aims at defining a catalog of bad smells related to video game development. To achieve this goal, we mined discussions on general-purpose and video game-specific forums. After querying such a forum, we adopted an open coding strategy on a statistically significant sample of 572 discussions, stratified over different forums. As a result, we obtained a catalog of 28 bad smells, organized into five categories, covering problems related to game design and logic, physics, animation, rendering, or multiplayer. Then, we assessed the perceived relevance of such bad smells by surveying 76 game development professionals. The survey respondents agreed with the identified bad smells but also provided us with further insights about the discussed smells. Upon reporting results, we discuss bad smell examples, their consequences, as well as possible mitigation/fixing strategies and trade-offs to be pursued by developers. The catalog can be used not only as a guideline for developers and educators but also can pave the way toward better automated tool support for video game developers.","Human-centered computing,Visualization,Visualization techniques,Social and professional topics,Professional topics,Software and its engineering,Software creation and management,Designing software,Software design engineering,Software development process management,Software post-development issues,Software evolution,Software notations and tools,Software configuration management and version control systems,Software organization and properties",电子游戏的臭味：它们是什么以及开发者如何看待它们,电子游戏在软件市场中所占的份额越来越大。然而，它们的发展尤其具有挑战性，因为它需要多方面的知识，而这些知识尚未在计算机科学教育中得到巩固。本文旨在定义一个与电子游戏开发相关的臭味目录。为了实现这一目标，我们挖掘了通用和电子游戏专用论坛上的讨论。在查询了这样一个论坛后，我们对572个具有统计意义的讨论样本采用了开放编码策略，这些讨论在不同的论坛上进行了分层。因此，我们获得了一个包含28种臭味的目录，分为五类，涵盖了与游戏设计和逻辑、物理、动画、渲染或多人游戏相关的问题。然后，我们通过调查76名游戏开发专业人员来评估这种臭味的感知相关性。调查对象同意所发现的臭味，但也为我们提供了关于所讨论气味的进一步见解。在报告结果后，我们讨论了臭味的例子及其后果，以及开发人员可能采取的缓解/修复策略和权衡。该目录不仅可以作为开发人员和教育工作者的指南，还可以为视频游戏开发人员提供更好的自动化工具支持铺平道路。,以人为中心的计算，可视化，可视化技术，社会和专业主题，专业主题，软件及其工程，软件创建和管理，设计软件，软件设计工程，软件开发过程管理，软件开发后问题，软件进化，软件符号和工具，软件配置管理和版本控制系统，软件组织和属性,,,
RP6QHYAG,2023,https://doi.org/10.1145/3571855,TOSEM 2023,Similarity-based Web Element Localization for Robust Test Automation,"Non-robust (fragile) test execution is a commonly reported challenge in GUI-based test automation, despite much research and several proposed solutions. A test script needs to be resilient to (minor) changes in the tested application but, at the same time, fail when detecting potential issues that require investigation. Test script fragility is a multi-faceted problem. However, one crucial challenge is how to reliably identify and locate the correct target web elements when the website evolves between releases or otherwise fail and report an issue. This article proposes and evaluates a novel approach called similarity-based web element localization (Similo), which leverages information from multiple web element locator parameters to identify a target element using a weighted similarity score. This experimental study compares Similo to a baseline approach for web element localization. To get an extensive empirical basis, we target 48 of the most popular websites on the Internet in our evaluation. Robustness is considered by counting the number of web elements found in a recent website version compared to how many of these existed in an older version. Results of the experiment show that Similo outperforms the baseline; it failed to locate the correct target web element in 91 out of 801 considered cases (i.e., 11%) compared to 214 failed cases (i.e., 27%) for the baseline approach. The time efficiency of Similo was also considered, where the average time to locate a web element was determined to be 4 milliseconds. However, since the cost of web interactions (e.g., a click) is typically on the order of hundreds of milliseconds, the additional computational demands of Similo can be considered negligible. This study presents evidence that quantifying the similarity between multiple attributes of web elements when trying to locate them, as in our proposed Similo approach, is beneficial. With acceptable efficiency, Similo gives significantly higher effectiveness (i.e., robustness) than the baseline web element localization approach.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",基于相似性的鲁棒测试自动化Web元素定位,尽管进行了大量研究并提出了几种解决方案，但在基于GUI的测试自动化中，非健壮（脆弱）的测试执行是一个常见的挑战。测试脚本需要对测试应用程序中的（微小）更改具有弹性，但同时在检测到需要调查的潜在问题时会失败。测试脚本的脆弱性是一个多方面的问题。然而，一个关键的挑战是，当网站在不同版本之间发展或以其他方式失败并报告问题时，如何可靠地识别和定位正确的目标网络元素。本文提出并评估了一种称为基于相似性的web元素定位（Similo）的新方法，该方法利用来自多个web元素定位器参数的信息，使用加权相似性得分来识别目标元素。这项实验研究将Similo与网页元素定位的基线方法进行了比较。为了获得广泛的实证基础，我们在评估中针对互联网上最受欢迎的48个网站。稳健性是通过计算在最近的网站版本中发现的网络元素的数量与在旧版本中存在的数量来考虑的。实验结果表明，Similo的性能优于基线；在801个考虑的案例中，有91个案例（即11%）未能定位到正确的目标web元素，而基线方法的失败案例为214个（即27%）。还考虑了Similo的时间效率，其中定位web元素的平均时间被确定为4毫秒。然而，由于网络交互（例如，点击）的成本通常在数百毫秒的数量级，因此Similo的额外计算需求可以忽略不计。这项研究提供了证据，证明在试图定位网络元素的多个属性时，量化它们之间的相似性是有益的，就像我们提出的Similo方法一样。在可接受的效率下，Similo提供了比基线web元素定位方法高得多的有效性（即鲁棒性）。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
4ZSP69IU,2023,https://doi.org/10.1145/3510416,TOSEM 2023,Coverage-directed Differential Testing of X.509 Certificate Validation in SSL/TLS Implementations,"Secure Sockets Layer (SSL) and Transport Security (TLS) are two secure protocols for creating secure connections over the Internet. X.509 certificate validation is important for security and needs to be performed before an SSL/TLS connection is established. Some advanced testing techniques, such as frankencert, have revealed, through randomly mutating Internet accessible certificates, that there exist unexpected, sometimes critical, validation differences among different SSL/TLS implementations. Despite these efforts, X.509 certificate validation still needs to be thoroughly tested as this work shows. ","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",SSL/TLS实现中X.509证书验证的覆盖定向差异测试,安全套接字层（SSL）和传输安全性（TLS）是用于在Internet上创建安全连接的两种安全协议。X.509证书验证对安全性很重要，需要在建立SSL/TLS连接之前执行。一些先进的测试技术，如frankencert，通过随机改变互联网可访问证书，揭示了不同的SSL/TLS实现之间存在意想不到的、有时是关键的验证差异。尽管做出了这些努力，X.509证书验证仍然需要进行彻底的测试，正如这项工作所显示的那样。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
ZIK5X3QY,2023,https://doi.org/10.1145/3530786,TOSEM 2023,Mutation Testing in Evolving Systems: Studying the Relevance of Mutants to Code Evolution,"Context:
When software evolves, opportunities for introducing faults appear. Therefore, it is important to test the evolved program behaviors during each evolution cycle. However, while software evolves, its complexity is also evolving, introducing challenges to the testing process. To deal with this issue, testing techniques should be adapted to target the effect of the program changes instead of the entire program functionality. To this end, commit-aware mutation testing, a powerful testing technique, has been proposed. Unfortunately, commit-aware mutation testing is challenging due to the complex program semantics involved. Hence, it is pertinent to understand the characteristics, predictability, and potential of the technique.","Software and its engineering,Software creation and management,Software post-development issues,Software evolution,Software verification and validation,Software defect analysis,Software testing and debugging",进化系统中的突变测试：研究突变与代码进化的相关性,上下文：,软件及其工程，软件创建和管理，软件开发后问题，软件演化，软件验证和确认，软件缺陷分析，软件测试和调试,,,
IY6H8NAT,2023,https://doi.org/10.1145/3549542,TOSEM 2023,Is My Transaction Done Yet? An Empirical Study of Transaction Processing Times in the Ethereum Blockchain Platform,"Ethereum is one of the most popular platforms for the development of blockchain-powered applications. These applications are known as ÐApps. When engineering ÐApps, developers need to translate requests captured in the front-end of their application into one or more smart contract transactions. Developers need to pay for these transactions and, the more they pay (i.e., the higher the gas price), the faster the transaction is likely to be processed. Developing cost-effective ÐApps is far from trivial, as developers need to optimize the balance between cost (transaction fees) and user experience (transaction processing times). Online services have been developed to provide transaction issuers (e.g., ÐApp developers) with an estimate of how long transactions will take to be processed given a certain gas price. These estimation services are crucial in the Ethereum domain and several popular wallets such as Metamask rely on them. However, despite their key role, their accuracy has not been empirically investigated so far. In this article, we quantify the transaction processing times in Ethereum, investigate the relationship between processing times and gas prices, and determine the accuracy of state-of-the-practice estimation services. Our results indicate that transactions are processed in a median of 57 seconds and that 90% of the transactions are processed within 8 minutes. We also show that higher gas prices result in faster transaction processing times with diminishing returns. In particular, we observe no practical difference in processing time between expensive and very expensive transactions. With regards to the accuracy of processing time estimation services, we observe that they are equivalent. However, when stratifying transactions by gas prices, we observe that Etherscan’s Gas Tracker is the most accurate estimation service for the very cheap and cheap transactions. EthGasStation’s Gas Price API, in turn, is the most accurate estimation service for regular, expensive, and very expensive transactions. In a post-hoc study, we design a simple linear regression model with only one feature that outperforms the Gas Tracker for very cheap and cheap transactions and that performs as accurately as the EthGasStation model for the remaining categories. Based on our findings, ÐApp developers can make more informed decisions concerning the choice of the gas price of their application-issued transactions.","Computer systems organization,Architectures,Distributed architectures,General and reference,Cross-computing tools and techniques,Empirical studies,Estimation",我的交易完成了吗？以太坊区块链平台交易处理时间的实证研究,以太坊是开发区块链应用程序最受欢迎的平台之一。这些应用程序被称为“应用程序”。在设计应用程序时，开发人员需要将应用程序前端捕获的请求转换为一个或多个智能合约事务。开发商需要为这些交易付费，他们支付的费用越多（即天然气价格越高），交易处理的速度就越快。开发具有成本效益的应用程序绝非易事，因为开发人员需要优化成本（交易费用）和用户体验（交易处理时间）之间的平衡。已经开发了在线服务，为交易发行人（例如，应用程序开发商）提供在一定天然气价格下处理交易所需的估计时间。这些估计服务在以太坊领域至关重要，Metamask等几个流行的钱包都依赖于它们。然而，尽管它们起着关键作用，但迄今为止，它们的准确性尚未得到实证研究。在本文中，我们量化了以太坊中的交易处理时间，研究了处理时间与天然气价格之间的关系，并确定了实践状态估计服务的准确性。我们的结果表明，交易的处理时间平均为57秒，90%的交易在8分钟内完成。我们还表明，天然气价格越高，交易处理时间越快，回报率越低。特别是，我们观察到昂贵和非常昂贵的交易在处理时间上没有实际差异。关于处理时间估计服务的准确性，我们观察到它们是相等的。然而，当根据天然气价格对交易进行分层时，我们观察到Etherscan的天然气跟踪器是非常便宜和便宜的交易的最准确的估计服务。EthGasStation的天然气价格API反过来是针对常规、昂贵和非常昂贵的交易的最准确的估计服务。在一项事后研究中，我们设计了一个简单的线性回归模型，该模型只有一个功能，在非常便宜和廉价的交易中，它的性能优于Gas Tracker，在其余类别中，它与EthGasStation模型一样准确。根据我们的研究结果，应用程序开发人员可以在选择应用程序发行交易的天然气价格方面做出更明智的决定。,计算机系统组织，体系结构，分布式体系结构，一般和参考，交叉计算工具和技术，经验研究，评估,,,
Z9A77I4C,2023,https://doi.org/10.1145/3511804,TOSEM 2023,Preference-wise Testing of Android Apps via Test Amplification,"Preferences, the setting options provided by Android, are an essential part of Android apps. Preferences allow users to change app features and behaviors dynamically, and therefore their impacts need to be considered when testing the apps. Unfortunately, few test cases explicitly specify the assignments of valid values to the preferences, or configurations, under which they should be executed, and few existing mobile testing tools take the impact of preferences into account or provide help to testers in identifying and setting up the configurations for running the tests. This article presents the Prefest approach to effective testing of Android apps with preferences. Given an Android app and a set of test cases for the app, Prefest amplifies the test cases with a small number of configurations to exercise more behaviors and detect more bugs that are related to preferences. In an experimental evaluation conducted on real-world Android apps, amplified test cases produced by Prefest from automatically generated test cases covered significantly more code of the apps and detected seven real bugs, and the tool’s test amplification time was at the same order of magnitude as the running time of the input test cases. Prefest’s effectiveness and efficiency in amplifying programmer-written test cases was comparable with that in amplifying automatically generated test cases.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",通过测试放大对Android应用程序进行偏好测试,首选项是安卓系统提供的设置选项，是安卓应用程序的重要组成部分。首选项允许用户动态更改应用程序的功能和行为，因此在测试应用程序时需要考虑其影响。不幸的是，很少有测试用例明确指定将有效值分配给首选项或配置，在这种情况下应该执行这些测试，而且很少有现有的移动测试工具考虑到首选项的影响，或者在识别和设置运行测试的配置时为测试人员提供帮助。本文介绍了使用首选项对安卓应用程序进行有效测试的预取方法。给定一个Android应用程序和该应用程序的一组测试用例，Prefest通过少量配置放大测试用例，以练习更多行为并检测更多与偏好相关的错误。在对现实世界的Android应用程序进行的一项实验评估中，Prefest从自动生成的测试用例中生成的放大测试用例覆盖了更多的应用程序代码，并检测到了七个真正的错误，该工具的测试放大时间与输入测试用例的运行时间在同一数量级。Prefest在放大程序员编写的测试用例方面的有效性和效率与放大自动生成的测试用例相当。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
58GAM4G8,2023,https://doi.org/10.1145/3542937,TOSEM 2023,Aide-mémoire: Improving a Project's Collective Memory via Pull Request-Issue Links,"Links between pull request and the issues they address document and accelerate the development of a software project but are often omitted. We present a new tool, Aide-mémoire, to suggest such links when a developer submits a pull request or closes an issue, smoothly integrating into existing workflows. In contrast to previous state-of-the-art approaches that repair related commit histories, Aide-mémoire is designed for continuous, real-time, and long-term use, employing Mondrian forest to adapt over a project’s lifetime and continuously improve traceability. Aide-mémoire is tailored for two specific instances of the general traceability problem—namely, commit to issue and pull request to issue links, with a focus on the latter—and exploits data inherent to these two problems to outperform tools for general purpose link recovery. Our approach is online, language-agnostic, and scalable. We evaluate over a corpus of 213 projects and six programming languages, achieving a mean average precision of 0.95. Adopting Aide-mémoire is both efficient and effective: A programmer need only evaluate a single suggested link 94% of the time, and 16% of all discovered links were originally missed by developers.","Software and its engineering,Software creation and management,Designing software,Requirements analysis,Software post-development issues,Maintaining software,Software evolution,Software version control",备忘录：通过拉请求问题链接提高项目的集体记忆,拉取请求和它们所解决的问题之间的链接记录并加速软件项目的开发，但通常被忽略。我们提供了一个新的工具Aide-mémoire，当开发人员提交拉取请求或关闭问题时，可以建议这样的链接，从而顺利地集成到现有的工作流中。与以前最先进的修复相关提交历史的方法不同，Aide-mémoire是为连续、实时和长期使用而设计的，采用蒙德里安森林在项目的整个生命周期内进行调整，并不断提高可追溯性。Aide-mémoire针对一般可追溯性问题的两个特定实例量身定制，即承诺发布和拉取请求发布链接，重点关注后者，并利用这两个问题固有的数据，以优于通用链接恢复工具。我们的方法是在线的，与语言无关，并且可扩展。我们对213个项目和6种编程语言的语料库进行了评估，平均精度达到0.95。采用Aide-mémoire既高效又有效：程序员94%的时间只需要评估一个建议的链接，16%的发现链接最初被开发人员遗漏。,软件及其工程，软件创建和管理，软件设计，需求分析，软件开发后问题，软件维护，软件进化，软件版本控制,,,
IVW6ZPRD,2023,https://doi.org/10.1145/3561383,TOSEM 2023,SLR: From Saltzer and Schroeder to 2021...47 Years of Research on the Development and Validation of Security API Recommendations,"Producing secure software is challenging. The poor usability of security Application Programming Interfaces (APIs) makes this even harder. Many recommendations have been proposed to support developers by improving the usability of cryptography libraries—rooted in wider best practice guidance in software engineering and API design. In this SLR, we systematize knowledge regarding these recommendations. We identify and analyze 65 papers, offering 883 recommendations. Through thematic analysis, we identify seven core ways to improve usability of APIs. Most of the recommendations focus on helping API developers to construct and structure their code and make it more usable and easier for programmers to understand. There is less focus, however, on documentation, writing requirements, code quality assessment, and the impact of organizational software development practices. By tracing and analyzing paper ancestry, we map how this knowledge becomes validated and translated over time. We find that very few API usability recommendations are empirically validated, and that recommendations specific to usable security APIs lag even further behind.",Security and privacy,SLR：从Saltzer和Schroeder到2021…安全API建议的开发和验证研究47年,生产安全软件具有挑战性。安全应用程序编程接口（API）的可用性较差，这使得这一点更加困难。已经提出了许多建议，通过提高加密库的可用性来支持开发人员，这些建议源于软件工程和API设计中更广泛的最佳实践指南。在SLR中，我们将这些建议的相关知识系统化。我们识别并分析了65篇论文，提供了883条建议。通过主题分析，我们确定了提高API可用性的七种核心方法。大多数建议都集中在帮助API开发人员构建和构造代码，使其更易于使用，更易于程序员理解。然而，较少关注文档、编写需求、代码质量评估以及组织软件开发实践的影响。通过追踪和分析纸张的祖先，我们绘制出这些知识如何随着时间的推移而得到验证和翻译。我们发现很少有API可用性建议得到了经验验证，而且特定于可用安全API的建议甚至进一步落后。,安全和隐私,,,
SSD3IH9B,2023,https://doi.org/10.1145/3561384,TOSEM 2023,How the Quality of Maintenance Tasks is Affected by Criteria for Selecting Engineers for Collaboration,"In industry, software projects might span over decades, with many engineers joining or leaving the company over time. In these circumstances, no single engineer has all of the knowledge when maintenance tasks such as Traceability Link Recovery (TLR), Bug Localization (BL), and Feature Location (FL) are performed. Thus, collaboration has the potential to boost the quality of maintenance tasks since the solution advanced by one engineer might be enhanced with contributions from other engineers. However, assembling a team of software engineers to collaborate may not be as intuitive as we might think. In the context of a worldwide industrial supplier of railway solutions, this work evaluates how the quality of TLR, BL, and FL is affected by the criteria for selecting engineers for collaboration. The criteria for collaboration are based on engineers’ profile information to select the set of search queries that are involved in the maintenance task. Collaboration is achieved by applying automatic query reformulation, and the location relies on an evolutionary algorithm. Our work uncovers how software engineers who might be seen as not being relevant in the collaboration can lead to significantly better results. A focus group confirmed the relevance of the findings.","General and reference,Cross-computing tools and techniques,Empirical studies,Software and its engineering,Software creation and management,Search-based software engineering,Software post-development issues,Maintaining software",选择合作工程师的标准如何影响维修任务的质量,在行业中，软件项目可能会跨越几十年，随着时间的推移，许多工程师会加入或离开公司。在这种情况下，在执行可追溯性链路恢复（TLR）、缺陷定位（BL）和特征定位（FL）等维护任务时，没有一个工程师具备所有知识。因此，协作有可能提高维护任务的质量，因为一名工程师提出的解决方案可能会得到其他工程师的贡献。然而，组建一个软件工程师团队进行协作可能并不像我们想象的那样直观。在全球铁路解决方案工业供应商的背景下，本工作评估了TLR、BL和FL的质量如何受到选择合作工程师的标准的影响。协作的标准基于工程师的配置文件信息，以选择维护任务中涉及的一组搜索查询。协作是通过应用自动查询重新表述来实现的，位置依赖于进化算法。我们的工作揭示了那些可能被视为与合作无关的软件工程师如何能够带来更好的结果。一个焦点小组确认了调查结果的相关性。,概论和参考，交叉计算工具和技术，实证研究，软件及其工程，软件创建和管理，基于搜索的软件工程，软件后开发问题，维护软件,,,
LEWAPVVK,2023,https://doi.org/10.1145/3583569,TOSEM 2023,EDITORIAL: Announcing Six TOSEM Issues Per Year,,,编辑：宣布每年发行六期TOSEM,,,,,
KMPDXE45,2023,https://doi.org/10.1145/3585005,TOSEM 2023,\emphArchRepair: Block-Level Architecture-Oriented Repairing for Deep Neural Networks,"Over the past few years, deep neural networks (DNNs) have achieved tremendous success and have been continuously applied in many application domains. However, during the practical deployment in industrial tasks, DNNs are found to be erroneous-prone due to various reasons such as overfitting and lacking of robustness to real-world corruptions during practical usage. To address these challenges, many recent attempts have been made to repair DNNs for version updates under practical operational contexts by updating weights (i.e., network parameters) through retraining, fine-tuning, or direct weight fixing at a neural level. Nevertheless, existing solutions often neglect the effects of neural network architecture and weight relationships across neurons and layers. In this work, as the first attempt, we initiate to repair DNNs by jointly optimizing the architecture and weights at a higher (i.e., block level).","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Search-based software engineering,Software post-development issues,Maintaining software",\emphArchRepair：面向块级结构的深度神经网络修复,在过去的几年里，深度神经网络取得了巨大的成功，并在许多应用领域得到了持续的应用。然而，在工业任务的实际部署过程中，由于各种原因，如过拟合和在实际使用过程中对现实世界的破坏缺乏鲁棒性，DNN容易出错。为了解决这些挑战，最近已经进行了许多尝试，通过在神经水平上进行再训练、微调或直接权重固定来更新权重（即网络参数），从而在实际操作环境下修复DNN以进行版本更新。然而，现有的解决方案往往忽略了神经网络架构以及神经元和层之间的权重关系的影响。在这项工作中，作为第一次尝试，我们通过在更高级别（即块级别）联合优化架构和权重来修复DNN。,计算方法，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，基于搜索的软件工程，软件开发后问题，维护软件,,,
V92N5D2K,2023,https://doi.org/10.1145/3580598,TOSEM 2023,NSFuzz: Towards Efficient and State-Aware Network Service Fuzzing,"As an essential component responsible for communication, network services are security critical, thus, it is vital to find their vulnerabilities. Fuzzing is currently one of the most popular software vulnerability discovery techniques, widely adopted due to its high efficiency and low false positives. However, existing coverage-guided fuzzers mainly aim at stateless local applications, leaving stateful network services underexplored. Recently, some fuzzers targeting network services have been proposed but have certain limitations, for example, insufficient or inaccurate state representation and low testing efficiency.","Networks,Network protocols,Protocol correctness,Protocol testing and verification,Security and privacy,Software and application security",NSFuzz：实现高效和状态感知的网络服务模糊化,作为负责通信的重要组成部分，网络服务的安全至关重要，因此，发现其漏洞至关重要。模糊处理是目前最流行的软件漏洞发现技术之一，因其效率高、误报率低而被广泛采用。然而，现有的覆盖引导模糊器主要针对无状态的本地应用程序，使得有状态的网络服务没有得到充分的开发。最近，已经提出了一些针对网络服务的模糊器，但存在一定的局限性，例如，状态表示不足或不准确以及测试效率低。,网络，网络协议，协议正确性，协议测试和验证，安全和隐私，软件和应用程序安全,,,
MZDF4TCG,2023,https://doi.org/10.1145/3607179,TOSEM 2023,A Systematic Review of Automated Query Reformulations in Source Code Search,"Fixing software bugs and adding new features are two of the major maintenance tasks. Software bugs and features are reported as change requests. Developers consult these requests and often choose a few keywords from them as an ad hoc query. Then they execute the query with a search engine to find the exact locations within software code that need to be changed. Unfortunately, even experienced developers often fail to choose appropriate queries, which leads to costly trials and errors during a code search. Over the years, many studies have attempted to reformulate the ad hoc queries from developers to support them. In this systematic literature review, we carefully select 70 primary studies on query reformulations from 2,970 candidate studies, perform an in-depth qualitative analysis (e.g., Grounded Theory), and then answer seven research questions with major findings. First, to date, eight major methodologies (e.g., term weighting, term co-occurrence analysis, thesaurus lookup) have been adopted to reformulate queries. Second, the existing studies suffer from several major limitations (e.g., lack of generalizability, the vocabulary mismatch problem, subjective bias) that might prevent their wide adoption. Finally, we discuss the best practices and future opportunities to advance the state of research in search query reformulations.","Software and its engineering,Software creation and management,Search-based software engineering,Software post-development issues,Maintaining software,Software reverse engineering,Software verification and validation,Process validation,Traceability,Software notations and tools,Software maintenance tools",源代码搜索中自动查询改革的系统综述,修复软件错误和添加新功能是两项主要的维护任务。软件错误和功能作为更改请求报告。开发人员查阅这些请求，并经常从中选择一些关键字作为临时查询。然后，他们用搜索引擎执行查询，以找到软件代码中需要更改的确切位置。不幸的是，即使是有经验的开发人员也经常无法选择合适的查询，这会导致在代码搜索过程中出现代价高昂的尝试和错误。多年来，许多研究试图重新制定开发人员的临时查询以支持它们。在这篇系统的文献综述中，我们从2970项候选研究中仔细选择了70项关于查询重新表述的初步研究，进行了深入的定性分析（例如，基础理论），然后回答了7个有重大发现的研究问题。首先，到目前为止，已经采用了八种主要方法（例如，术语加权、术语共现分析、词库查找）来重新表述查询。其次，现有的研究存在几个主要局限性（例如，缺乏可推广性、词汇不匹配问题、主观偏见），这可能会阻碍它们的广泛采用。最后，我们讨论了最佳实践和未来的机会，以推进搜索查询重新表述的研究现状。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件后开发问题，维护软件，软件逆向工程，软件验证和确认，过程验证，可跟踪性，软件符号和工具，软件维护工具,,,
3PCGDU3Z,2023,https://doi.org/10.1145/3579639,TOSEM 2023,Security Misconfigurations in Open Source Kubernetes Manifests: An Empirical Study,"Context: Kubernetes has emerged as the de-facto tool for automated container orchestration. Business and government organizations are increasingly adopting Kubernetes for automated software deployments. Kubernetes is being used to provision applications in a wide range of domains, such as time series forecasting, edge computing, and high-performance computing. Due to such a pervasive presence, Kubernetes-related security misconfigurations can cause large-scale security breaches. Thus, a systematic analysis of security misconfigurations in Kubernetes manifests, i.e., configuration files used for Kubernetes, can help practitioners secure their Kubernetes clusters.","Security and privacy,Software and application security,Software security engineering",开源Kubernetes Manifests中的安全错误配置：一项实证研究,上下文：Kubernetes已经成为自动化容器编排的事实工具。商业和政府组织越来越多地采用Kubernetes进行自动化软件部署。Kubernetes正被用于提供广泛领域的应用程序，如时间序列预测、边缘计算和高性能计算。由于这种普遍存在，与Kubernetes相关的安全错误配置可能会导致大规模的安全漏洞。因此，系统分析Kubernetes清单中的安全错误配置，即用于Kubernete的配置文件，可以帮助从业者保护他们的Kubernette集群。,安全和隐私，软件和应用程序安全，软件安全工程,,,
NIN2NUAR,2023,https://doi.org/10.1145/3511805,TOSEM 2023,"A Taxonomy of Information Attributes for Test Case Prioritisation: Applicability, Machine Learning","Most software companies have extensive test suites and re-run parts of them continuously to ensure that recent changes have no adverse effects. Since test suites are costly to execute, industry needs methods for test case prioritisation (TCP). Recently, TCP methods use machine learning (ML) to exploit the information known about the system under test and its test cases. However, the value added by ML-based TCP methods should be critically assessed with respect to the cost of collecting the information. This article analyses two decades of TCP research and presents a taxonomy of 91 information attributes that have been used. The attributes are classified with respect to their information sources and the characteristics of their extraction process. Based on this taxonomy, TCP methods validated with industrial data and those applying ML are analysed in terms of information availability, attribute combination and definition of data features suitable for ML. Relying on a high number of information attributes, assuming easy access to system under test code and simplified testing environments are identified as factors that might hamper industrial applicability of ML-based TCP. The TePIA taxonomy provides a reference framework to unify terminology and evaluate alternatives considering the cost-benefit of the information attributes.","Computing methodologies,Machine learning,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",测试用例优先级的信息属性分类：适用性，机器学习,大多数软件公司都有大量的测试套件，并不断地重新运行其中的部分，以确保最近的更改不会产生不利影响。由于测试套件的执行成本很高，行业需要测试用例优先级（TCP）的方法。最近，TCP方法使用机器学习（ML）来利用关于被测系统及其测试用例的已知信息。然而，基于ML的TCP方法的附加值应该根据收集信息的成本进行严格评估。本文分析了二十年来TCP的研究，并对已经使用的91个信息属性进行了分类。根据属性的信息来源和提取过程的特征对属性进行分类。基于这种分类法，从信息可用性、属性组合和适合ML的数据特征定义等方面分析了用工业数据验证的TCP方法和应用ML的TCP方法，假设容易访问测试中的系统代码和简化的测试环境被确定为可能阻碍基于ML的TCP的工业适用性的因素。TePIA分类法提供了一个参考框架，用于统一术语并评估备选方案，同时考虑信息属性的成本效益。,计算方法论，机器学习，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
EZMHSMAI,2023,https://doi.org/10.1145/3490754,TOSEM 2023,Consent Verification Monitoring,"Advances in personalization of digital services are driven by low-cost data collection and processing, in addition to the wide variety of third-party frameworks for authentication, storage, and marketing. New privacy regulations, such as the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act, increasingly require organizations to explicitly state their data practices in privacy policies. When data practices change, a new version of the policy is released. This can occur a few times a year, when data collection or processing requirements are rapidly changing. Consent evolution raises specific challenges to ensuring GDPR compliance. We propose a formal consent framework to support organizations, data users, and data subjects in their understanding of policy evolution under a consent regime that supports both the retroactive and non-retroactive granting and withdrawal of consent. The contributions include (i) a formal framework to reason about data collection and access under multiple consent granting and revocation scenarios, (ii) a scripting language that implements the consent framework for encoding and executing different scenarios, (iii) five consent evolution use cases that illustrate how organizations would evolve their policies using this framework, and (iv) a scalability evaluation of the reasoning framework. The framework models are used to verify when user consent prevents or detects unauthorized data collection and access. The framework can be integrated into a runtime architecture to monitor policy violations as data practices evolve in real time. The framework was evaluated using the five use cases and a simulation to measure the framework scalability. The simulation results show that the approach is computationally scalable for use in runtime consent monitoring under a standard model of data collection and access and practice and policy evolution.","Security and privacy,Formal methods and theory of security,Logic and verification,Human and societal aspects of security and privacy,Privacy protections,Software and application security,Software and its engineering,Software creation and management,Designing software,Requirements analysis,Software verification and validation,Formal software verification",同意书验证监控,数字服务个性化的进步是由低成本的数据收集和处理以及用于身份验证、存储和营销的各种第三方框架推动的。新的隐私法规，如《通用数据保护条例》（GDPR）和《加州消费者隐私法》，越来越多地要求组织在隐私政策中明确说明其数据做法。当数据实践发生变化时，将发布新版本的策略。当数据收集或处理需求快速变化时，这种情况每年可能发生几次。同意演变对确保GDPR合规性提出了具体挑战。我们提出了一个正式的同意框架，以支持组织、数据用户和数据主体理解同意制度下的政策演变，该同意制度支持追溯性和非追溯性的同意授予和撤回。贡献包括：（i）一个正式的框架，用于在多种同意授予和撤销场景下对数据收集和访问进行推理；（ii）一种脚本语言，用于实现编码和执行不同场景的同意框架，以及（iv）推理框架的可扩展性评估。框架模型用于验证用户同意何时阻止或检测到未经授权的数据收集和访问。该框架可以集成到运行时体系结构中，以随着数据实践的实时发展监控策略违规行为。使用五个用例和一个模拟来评估框架的可伸缩性。仿真结果表明，该方法在计算上是可扩展的，可用于数据收集和访问以及实践和政策演变的标准模型下的运行时同意监控。,安全和隐私，安全的正式方法和理论，逻辑和验证，安全和隐私的人和社会方面，隐私保护，软件和应用程序安全，软件及其工程，软件创建和管理，设计软件，需求分析，软件验证和验证，正式软件验证,,,
5LNG2DI5,2023,https://doi.org/10.1145/3522587,TOSEM 2023,There's no Such Thing as a Free Lunch: Lessons Learned from Exploring the Overhead Introduced by the Greenkeeper Dependency Bot in Npm,"Dependency management bots are increasingly being used to support the software development process, for example, to automatically update a dependency when a new version is available. Yet, human intervention is often required to either accept or reject any action or recommendation the bot creates. In this article, our objective is to study the extent to which dependency management bots create additional, and sometimes unnecessary, work for their users. To accomplish this, we analyze 93,196 issue reports opened by Greenkeeper, a popular dependency management bot used in open source software projects in the npm ecosystem. We find that Greenkeeper is responsible for half of all issues reported in client projects, inducing a significant amount of overhead that must be addressed by clients, since many of these issues were created as a result of Greenkeeper taking incorrect action on a dependency update (i.e., false alarms). Reverting a broken dependency update to an older version, which is a potential solution that requires the least overhead and is automatically attempted by Greenkeeper, turns out to not be an effective mechanism. Finally, we observe that 56% of the commits referenced by Greenkeeper issue reports only change the client’s dependency specification file to resolve the issue. Based on our findings, we argue that dependency management bots should (i) be configurable to allow clients to reduce the amount of generated activity by the bots, (ii) take into consideration more sources of information than only the pass/fail status of the client’s build pipeline to help eliminate false alarms, and (iii) provide more effective incentives to encourage clients to resolve dependency issues.","Software and its engineering,Software creation and management,Designing software,Software design engineering,Software design tradeoffs,Software notations and tools,Software maintenance tools",没有免费午餐：从探索Npm的Greenkeeper Dependency Bot引入的开销中吸取的教训,依赖关系管理机器人越来越多地被用于支持软件开发过程，例如，在新版本可用时自动更新依赖关系。然而，通常需要人工干预来接受或拒绝机器人创建的任何行动或建议。在本文中，我们的目标是研究依赖管理机器人为用户创建额外（有时是不必要的）工作的程度。为了实现这一点，我们分析了Greenkeeper打开的93196个问题报告，Greenkeeper是一个流行的依赖管理机器人，用于npm生态系统中的开源软件项目。我们发现，Greenkeeper负责客户项目中报告的所有问题的一半，这导致了客户必须解决的大量开销，因为其中许多问题是由于Greenkeeper对依赖项更新采取了错误的操作（即误报）而产生的。将中断的依赖关系更新恢复到旧版本，这是一个潜在的解决方案，需要最少的开销，并且由Greenkeeper自动尝试，但事实证明这不是一个有效的机制。最后，我们观察到Greenkeeper问题报告引用的56%的提交只更改了客户端的依赖性规范文件来解决问题。基于我们的发现，我们认为依赖管理机器人应该（i）可配置，以允许客户端减少机器人生成的活动量，（ii）考虑更多的信息来源，而不仅仅是客户端构建管道的通过/失败状态，以帮助消除假警报，以及（三）提供更有效的激励措施，鼓励客户解决依赖问题。,软件及其工程，软件创建和管理，设计软件，软件设计工程，软件设计权衡，软件符号和工具，软件维护工具,,,
D9AM5BZM,2023,https://doi.org/10.1145/3603111,TOSEM 2023,What Quality Aspects Influence the Adoption of Docker Images?,"Docker is a containerization technology that allows developers to ship software applications along with their dependencies in Docker images. Developers can extend existing images using them as base images when writing Dockerfiles. However, a lot of alternative functionally equivalent base images are available. Although many studies define and evaluate quality features that can be extracted from Docker artifacts, the criteria on which developers choose a base image over another remain unclear.","Software and its engineering,Software notations and tools",哪些质量因素影响Docker图像的采用？,Docker是一种容器化技术，允许开发人员在Docker映像中发布软件应用程序及其依赖项。开发人员可以在编写Dockerfiles时将现有映像用作基础映像来扩展它们。然而，许多可供选择的功能等效的基础图像是可用的。尽管许多研究定义和评估了可以从Docker工件中提取的质量特征，但开发人员选择基础图像而不是另一个的标准仍然不清楚。,软件及其工程，软件符号和工具,,,
EMXPLH64,2023,https://doi.org/10.1145/3583562,TOSEM 2023,Automated Identification of Toxic Code Reviews Using ToxiCR,"Toxic conversations during software development interactions may have serious repercussions on a Free and Open Source Software (FOSS) development project. For example, victims of toxic conversations may become afraid to express themselves, therefore get demotivated, and may eventually leave the project. Automated filtering of toxic conversations may help a FOSS community maintain healthy interactions among its members. However, off-the-shelf toxicity detectors perform poorly on a software engineering dataset, such as one curated from code review comments. To counter this challenge, we present ToxiCR, a supervised learning based toxicity identification tool for code review interactions. ToxiCR includes a choice to select one of the 10 supervised learning algorithms, an option to select text vectorization techniques, eight preprocessing steps, and a large-scale labeled dataset of 19,651 code review comments. Two out of those eight preprocessing steps are software engineering domain specific. With our rigorous evaluation of the models with various combinations of preprocessing steps and vectorization techniques, we have identified the best combination for our dataset that boosts 95.8% accuracy and an 88.9% F1-score in identifying toxic texts. ToxiCR significantly outperforms existing toxicity detectors on our dataset. We have released our dataset, pre-trained models, evaluation results, and source code publicly, which is available at  https://github.com/WSU-SEAL/ToxiCR.","Computing methodologies,Machine learning,Learning paradigms,Supervised learning,Software and its engineering,Software creation and management,Collaboration in software development,Software notations and tools,Development frameworks and environments,Integrated and visual development environments",使用ToxiCR自动识别有毒代码审查,软件开发互动过程中的有毒对话可能会对自由和开源软件（FOSS）开发项目产生严重影响。例如，有毒对话的受害者可能会害怕表达自己，因此失去动力，最终可能会离开项目。对有毒对话的自动过滤可能有助于自由和开放源码软件社区在其成员之间保持健康的互动。然而，现成的毒性检测器在软件工程数据集上表现不佳，例如根据代码审查评论策划的数据集。为了应对这一挑战，我们提出了ToxiCR，这是一种用于代码审查交互的基于监督学习的毒性识别工具。ToxiCR包括从10种监督学习算法中选择一种的选项，选择文本矢量化技术的选项，八个预处理步骤，以及19651条代码审查评论的大规模标记数据集。这八个预处理步骤中有两个是特定于软件工程领域的。通过预处理步骤和矢量化技术的各种组合对模型进行严格评估，我们为数据集确定了最佳组合，在识别有毒文本方面提高了95.8%的准确率和88.9%的F1分数。ToxiCR显著优于我们数据集中现有的毒性检测器。我们已经公开发布了我们的数据集、预训练模型、评估结果和源代码，可在https://github.com/WSU-SEAL/ToxiCR.,计算方法学，机器学习，学习范式，监督学习，软件及其工程，软件创建和管理，软件开发协作，软件符号和工具，开发框架和环境，集成和可视化开发环境,,,
ALU7CUYT,2023,https://doi.org/10.1145/3585008,TOSEM 2023,SEAL: Integrating Program Analysis and Repository Mining,"Software projects are complex technical and organizational systems involving large numbers of artifacts and developers. To understand and tame software complexity, a wide variety of program analysis techniques have been developed for bug detection, program comprehension, verification, and more. At the same time, repository mining techniques aim at obtaining insights into the inner socio-technical workings of software projects at a larger scale. While both program analysis and repository mining have been successful on their own, they are largely isolated, which leaves considerable potential for synergies untapped. We present SEAL, the first integrated approach that combines low-level program analysis with high-level repository information. SEAL maps repository information, mined from the development history of a project, onto a low-level intermediate program representation, making it available for state-of-the-art program analysis. SEAL’s integrated approach allows us to efficiently address software engineering problems that span multiple levels of abstraction, from low-level data flow to high-level organizational information. To demonstrate its merits and practicality, we use SEAL to determine which code changes modify central parts of a given software project, how authors interact (indirectly) with each other through code, and we demonstrate that putting static analysis’ results into a socio-technical context improves their expressiveness and interpretability.","Software and its engineering,Software creation and management,Collaboration in software development,Software verification and validation,Software notations and tools,Software configuration management and version control systems,Software organization and properties,Software functional properties,Formal methods,Automated static analysis",SEAL:集成程序分析和存储库挖掘,软件项目是复杂的技术和组织系统，涉及大量工件和开发人员。为了理解和驯服软件复杂性，已经开发了各种各样的程序分析技术，用于错误检测、程序理解、验证等。同时，存储库挖掘技术旨在更大规模地深入了解软件项目的内部社会技术运作。虽然程序分析和存储库挖掘本身都取得了成功，但它们在很大程度上是孤立的，这就留下了巨大的协同潜力。我们介绍了SEAL，这是第一个将低级程序分析与高级存储库信息相结合的集成方法。SEAL将从项目开发历史中挖掘的存储库信息映射到低级中间程序表示中，使其可用于最先进的程序分析。SEAL的集成方法使我们能够有效地解决跨越多个抽象级别的软件工程问题，从低级数据流到高级组织信息。为了证明其优点和实用性，我们使用SEAL来确定哪些代码更改修改了给定软件项目的核心部分，作者如何通过代码相互（间接）交互，我们还证明了将静态分析结果放入社会技术环境中可以提高其表达性和可解释性。,软件及其工程，软件创建和管理，软件开发协作，软件验证和确认，软件符号和工具，软件配置管理和版本控制系统，软件组织和属性，软件功能属性，形式化方法，自动静态分析,,,
VWIQRS6B,2023,https://doi.org/10.1145/3554732,TOSEM 2023,An In-depth Study of Java Deserialization Remote-Code Execution Exploits and Vulnerabilities,"Nowadays, an increasing number of applications use deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities are so critical that they are in OWASP’s list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How are they performed? How are weaknesses introduced and patched? And for how long are vulnerabilities present in the codebase? To yield a deeper understanding of this important kind of vulnerability, we perform two main analyses: one on attack gadgets, i.e., exploitable pieces of code, present in Java libraries, and one on vulnerabilities present in Java applications. For the first analysis, we conduct an exploratory large-scale study by running 256515  experiments in which we vary the versions of libraries for each of the 19 publicly available exploits. Such attacks rely on a combination of gadgets present in one or multiple Java libraries. A gadget is a method which is using objects or fields that can be attacker-controlled. Our goal is to precisely identify library versions containing gadgets and to understand how gadgets have been introduced and how they have been patched. We observe that the modification of one innocent-looking detail in a class – such as making it public – can already introduce a gadget. Furthermore, we noticed that among the studied libraries, 37.5% are not patched, leaving gadgets available for future attacks.","Security and privacy,Software and application security,Software security engineering",深入研究Java反序列化远程代码执行漏洞,如今，越来越多的应用程序使用反序列化。这种基于从序列化字节流重建对象实例的技术可能很危险，因为如果要反序列化的数据来自不受信任的源，它可能会使应用程序受到远程代码执行（RCE）等攻击。反序列化漏洞是如此关键，以至于它们被列入OWASP的web应用程序十大安全风险列表。这主要是由应用程序开发过程中的错误及其依赖关系中的缺陷引起的，即这些应用程序使用的库中的缺陷。以前没有任何工作深入研究反序列化攻击：它们是如何执行的？弱点是如何引入和修补的？漏洞在代码库中存在的时间有多长？为了更深入地了解这类重要的漏洞，我们进行了两个主要分析：一个是关于Java库中存在的攻击小工具，即可利用的代码片段，另一个是Java应用程序中存在的漏洞。在第一次分析中，我们通过运行256515个实验进行了一项探索性的大规模研究，在这些实验中，我们为19个公开可用的漏洞中的每一个改变了库的版本。这种攻击依赖于一个或多个Java库中存在的小工具的组合。小工具是一种使用攻击者控制的对象或字段的方法。我们的目标是准确识别包含小工具的库版本，并了解小工具是如何引入的以及如何修补的。我们观察到，对类中一个看起来无辜的细节进行修改——比如将其公开——就已经可以引入一个小工具了。此外，我们注意到，在所研究的库中，37.5%的库没有打补丁，这使得小工具可以用于未来的攻击。,安全和隐私，软件和应用程序安全，软件安全工程,,,
ZX2UJYCZ,2023,https://doi.org/10.1145/3546944,TOSEM 2023,A Characterization Study of Merge Conflicts in Java Projects,"In collaborative software development, programmers create software branches to add features and fix bugs tentatively, and then merge branches to integrate edits. When edits from different branches textually overlap (i.e., textual conflicts) or lead to compilation and runtime errors (i.e., build and test conflicts), it is challenging for developers to remove such conflicts. Prior work proposed tools to detect and solve conflicts. They investigate how conflicts relate to code smells and the software development process. However, many questions are still not fully investigated, such as what types of conflicts exist in real-world applications and how developers or tools handle them. For this article, we used automated textual merge, compilation, and testing to reveal three types of conflicts in 208 open-source repositories: textual conflicts, build conflicts (i.e., conflicts causing build errors), and test conflicts (i.e., conflicts triggering test failures). We manually inspected 538 conflicts and their resolutions to characterize merge conflicts from different angles. ","General and reference,Cross-computing tools and techniques,Empirical studies,Software and its engineering,Software creation and management,Collaboration in software development,Software notations and tools,Software maintenance tools",Java项目中合并冲突的特征化研究,在协作软件开发中，程序员创建软件分支来添加功能并暂时修复错误，然后合并分支来集成编辑。当来自不同分支的编辑在文本上重叠（即文本冲突）或导致编译和运行时错误（即构建和测试冲突）时，开发人员很难消除此类冲突。先前的工作提出了检测和解决冲突的工具。他们研究了冲突与代码气味和软件开发过程之间的关系。然而，许多问题仍未得到充分调查，例如现实世界中的应用程序中存在哪些类型的冲突，以及开发人员或工具如何处理这些冲突。在本文中，我们使用自动文本合并、编译和测试来揭示208个开源存储库中的三种类型的冲突：文本冲突、构建冲突（即导致构建错误的冲突）和测试冲突（即触发测试失败的冲突）。我们手动检查了538个冲突及其解决方案，以从不同角度描述合并冲突。,概述和参考，交叉计算工具和技术，经验研究，软件及其工程，软件创建和管理，软件开发中的协作，软件符号和工具，软件维护工具,,,
PVCI67V6,2023,https://doi.org/10.1145/3548682,TOSEM 2023,Making Sense of the Unknown: How Managers Make Cyber Security Decisions,"Managers rarely have deep knowledge of cyber security and yet are expected to make decisions with cyber security implications for software-based systems. We investigate the decision-making conversations of seven teams of senior managers from the same organisation as they complete the Decisions & Disruptions cyber security exercise. We use grounded theory to situate our analysis of their decision-making and help us explore how these complex socio-cognitive interactions occur. We have developed a goal-model (using iStar 2.0) of the teams’ dialogue that illustrates what cyber security goals teams identify and how they operationalise their decisions to reach these goals. We complement this with our model of cyber security reasoning that describes how these teams make their decisions, showing how each team members’ experience, intuition, and understanding affects the team’s overall shared reasoning and decision-making. ","General and reference,Cross-computing tools and techniques,Evaluation,Human-centered computing,Security and privacy,Human and societal aspects of security and privacy,Social aspects of security and privacy,Social and professional topics,Computing / technology policy,Computer crime,Professional topics,Management of computing and information systems",理解未知：管理者如何做出网络安全决策,管理者很少对网络安全有深入的了解，但他们会做出对基于软件的系统具有网络安全影响的决策。我们调查了来自同一组织的七个高级管理团队在完成决策与干扰网络安全演习时的决策对话。我们使用扎根理论来定位我们对他们决策的分析，并帮助我们探索这些复杂的社会认知互动是如何发生的。我们开发了一个团队对话的目标模型（使用iStar 2.0），该模型说明了团队确定的网络安全目标，以及他们如何实施决策以实现这些目标。我们用我们的网络安全推理模型来补充这一点，该模型描述了这些团队如何做出决策，显示了每个团队成员的经验、直觉和理解如何影响团队的整体共享推理和决策。,概述和参考，交叉计算工具和技术，评估，以人为中心的计算，安全和隐私的人和社会方面，安全和隐私的社会方面，社会和专业主题，计算/技术政策，计算机犯罪，专业主题，计算和信息系统的管理,,,
DGPNR89H,2023,https://doi.org/10.1145/3583565,TOSEM 2023,Assessing the Early Bird Heuristic (for Predicting Project Quality),"Before researchers rush to reason across all available data or try complex methods, perhaps it is prudent to first check for simpler alternatives. Specifically, if the historical data has the most information in some small region, then perhaps a model learned from that region would suffice for the rest of the project.","Software and its engineering,Software creation and management,Software post-development issues,Maintaining software",评估早期鸟类启发式（用于预测项目质量）,在研究人员急于对所有可用数据进行推理或尝试复杂的方法之前，也许明智的做法是首先检查更简单的替代方案。具体来说，如果历史数据在某个小区域中具有最多的信息，那么从该区域学习的模型可能足以用于项目的其余部分。,软件及其工程，软件创建和管理，软件开发后问题，软件维护,,,
LAKPAHCV,2023,https://doi.org/10.1145/3579640,TOSEM 2023,Katana: Dual Slicing Based Context for Learning Bug Fixes,"Contextual information plays a vital role for software developers when understanding and fixing a bug. Consequently, deep learning based program repair techniques leverage context for bug fixes. However, existing techniques treat context in an arbitrary manner, by extracting code in close proximity of the buggy statement within the enclosing file, class, or method, without any analysis to find actual relations with the bug. To reduce noise, they use a predefined maximum limit on the number of tokens to be used as context. We present a program slicing based approach, in which instead of arbitrarily including code as context, we analyze statements that have a control or data dependency on the buggy statement. We propose a novel concept called dual slicing, which leverages the context of both buggy and fixed versions of the code to capture relevant repair ingredients. We present our technique and tool called Katana, the first to apply slicing-based context for a program repair task. The results show that Katana effectively preserves sufficient information for a model to choose contextual information while reducing noise. We compare against four recent state-of-the-art context-aware program repair techniques. Our results show that Katana fixes between 1.5 and 3.7 times more bugs than existing techniques.","Theory of computation,Semantics and reasoning,Program reasoning,Program analysis",Katana：用于学习Bug修复的基于双重切片的上下文,上下文信息对于软件开发人员理解和修复错误起着至关重要的作用。因此，基于深度学习的程序修复技术利用上下文进行错误修复。然而，现有的技术以任意的方式处理上下文，通过在封闭文件、类或方法中提取与bug语句非常接近的代码，而不进行任何分析来找到与bug的实际关系。为了减少噪音，他们对要用作上下文的令牌数量使用预定义的最大限制。我们提出了一种基于程序切片的方法，在该方法中，我们分析对错误语句具有控制或数据依赖性的语句，而不是任意地将代码作为上下文。我们提出了一个称为双重切片的新概念，它利用代码的错误版本和固定版本的上下文来捕获相关的修复成分。我们介绍了我们的技术和工具Katana，它是第一个将基于切片的上下文应用于程序修复任务的工具。结果表明，Katana在减少噪声的同时，有效地为模型选择上下文信息保留了足够的信息。我们将其与最近四种最先进的上下文感知程序修复技术进行比较。我们的结果显示，Katana修复的bug是现有技术的1.5到3.7倍。,计算理论，语义与推理，程序推理，程序分析,,,
BVT936BZ,2023,https://doi.org/10.1145/3563210,TOSEM 2023,Arachne: Search-Based Repair of Deep Neural Networks,"The rapid and widespread adoption of Deep Neural Networks (DNNs) has called for ways to test their behaviour, and many testing approaches have successfully revealed misbehaviour of DNNs. However, it is relatively unclear what one can do to correct such behaviour after revelation, as retraining involves costly data collection and does not guarantee to fix the underlying issue. This article introduces Arachne, a novel program repair technique for DNNs, which directly repairs DNNs using their input-output pairs as a specification. Arachne localises neural weights on which it can generate effective patches and uses differential evolution to optimise the localised weights and correct the misbehaviour. An empirical study using different benchmarks shows that Arachne can fix specific misclassifications of a DNN without reducing general accuracy significantly. On average, patches generated by Arachne generalise to 61.3% of unseen misbehaviour, whereas those by a state-of-the-art DNN repair technique generalise only to 10.2% and sometimes to none while taking tens of times more than Arachne. We also show that Arachne can address fairness issues by debiasing a gender classification model. Finally, we successfully apply Arachne to a text sentiment model to show that it generalises beyond convolutional neural networks.","Computing methodologies,Machine learning,Software and its engineering,Software creation and management,Search-based software engineering",Arachne：基于搜索的深度神经网络修复,深度神经网络（DNN）的快速和广泛采用要求有方法来测试它们的行为，许多测试方法已经成功地揭示了DNN的不当行为。然而，在被揭露后，人们能做些什么来纠正这种行为还相对不清楚，因为再培训涉及到昂贵的数据收集，并且不能保证解决根本问题。本文介绍了一种新的DNN程序修复技术Arachne，它使用DNN的输入输出对作为规范来直接修复DNN。Arachne对可以生成有效补丁的神经权重进行定位，并使用差分进化来优化定位权重并纠正不当行为。一项使用不同基准的实证研究表明，Arachne可以修复DNN的特定错误分类，而不会显著降低总体准确性。平均而言，Arachne生成的补丁可概括为61.3%的看不见的不当行为，而最先进的DNN修复技术生成的补丁仅可概括为10.2%，有时甚至没有，耗时是Arachne的几十倍。我们还表明，Arachne可以通过消除性别分类模型的偏见来解决公平问题。最后，我们成功地将Arachne应用于文本情感模型，以表明它超越了卷积神经网络。,计算方法学，机器学习，软件及其工程，软件创建和管理，基于搜索的软件工程,,,
ZU7XE2WL,2023,https://doi.org/10.1145/3564821,TOSEM 2023,Automated Identification and Qualitative Characterization of Safety Concerns Reported in UAV Software Platforms,"Unmanned Aerial Vehicles (UAVs) are nowadays used in a variety of applications. Given the cyber-physical nature of UAVs, software defects in these systems can cause issues with safety-critical implications. An important aspect of the lifecycle of UAV software is to minimize the possibility of harming humans or damaging properties through a continuous process of hazard identification and safety risk management. Specifically, safety-related concerns typically emerge during the operation of UAV systems, reported by end-users and developers in the form of issue reports and pull requests. However, popular UAV systems daily receive tens or hundreds of reports of varying types and quality. To help developers timely identify and triage safety-critical UAV issues, we (i) experiment with automated approaches (previously used for issue classification) for detecting the safety-related matters appearing in the titles and descriptions of issues and pull requests reported in UAV platforms and (ii) propose a categorization of the main hazards and accidents discussed in such issues. Our results (i) show that shallow machine learning (ML)-based approaches can identify safety-related sentences with precision, recall, and F-measure values of about 80%; and (ii) provide a categorization and description of the relationships between safety issue hazards and accidents.","General and reference,Cross-computing tools and techniques,Empirical studies,Software and its engineering,Software creation and management,Software development techniques,Error handling and recovery,Software post-development issues,Maintaining software,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems,Software organization and properties,Extra-functional properties,Software safety,Software functional properties",无人机软件平台中报告的安全问题的自动识别和定性表征,无人机（UAV）如今被用于各种应用中。考虑到无人机的网络物理性质，这些系统中的软件缺陷可能会导致安全关键问题。无人机软件生命周期的一个重要方面是通过持续的危险识别和安全风险管理过程，最大限度地减少伤害人类或损坏财产的可能性。具体而言，安全相关问题通常出现在无人机系统的运行过程中，由最终用户和开发人员以问题报告和拉取请求的形式报告。然而，流行的无人机系统每天都会收到数十或数百份不同类型和质量的报告。为了帮助开发人员及时识别和分类安全关键的无人机问题，我们（i）试验自动方法（以前用于问题分类），以检测无人机平台中报告的问题和拉取请求的标题和描述中出现的安全相关问题，以及（ii）对此类问题中讨论的主要危险和事故提出分类。我们的结果（i）表明，基于浅层机器学习（ML）的方法可以识别出精度、召回率和F-测度值约为80%的安全相关句子；以及（ii）提供安全问题危害和事故之间关系的分类和描述。,一般和参考文献，交叉计算工具和技术，经验研究，软件及其工程，软件创建和管理，软件开发技术，错误处理和恢复，软件开发后问题，维护软件，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统，软件组织和属性，额外功能属性，软件安全，软件功能属性,,,
HQFL78SS,2023,https://doi.org/10.1145/3546948,TOSEM 2023,Coverage-Based Debloating for Java Bytecode,"Software bloat is code that is packaged in an application but is actually not necessary to run the application. The presence of software bloat is an issue for security, performance, and for maintenance. In this article, we introduce a novel technique for debloating, which we call coverage-based debloating. We implement the technique for one single language: Java bytecode. We leverage a combination of state-of-the-art Java bytecode coverage tools to precisely capture what parts of a project and its dependencies are used when running with a specific workload. Then, we automatically remove the parts that are not covered, in order to generate a debloated version of the project. We succeed to debloat 211 library versions from a dataset of 94 unique  open-source Java libraries. The debloated versions are syntactically correct and preserve their original behaviour according to the workload. Our results indicate that 68.3% of the libraries’ bytecode and 20.3% of their total dependencies can be removed through coverage-based debloating.","Software and its engineering,Software creation and management,Software verification and validation,Empirical software validation,Software notations and tools,Software libraries and repositories,Software maintenance tools",基于覆盖率的Java字节码去浮动,软件膨胀是打包在应用程序中的代码，但实际上并不是运行应用程序所必需的。软件膨胀的存在对安全性、性能和维护都是一个问题。在本文中，我们介绍了一种新的去浮动技术，我们称之为基于覆盖的去浮动。我们为一种语言实现了该技术：Java字节码。我们利用最先进的Java字节码覆盖工具的组合，精确地捕捉在特定工作负载下运行时使用项目的哪些部分及其依赖关系。然后，我们自动删除未涵盖的部分，以便生成项目的去浮动版本。我们成功地从94个独特的开源Java库的数据集中剥离了211个库版本。去浮动版本在语法上是正确的，并根据工作负载保留其原始行为。我们的结果表明，68.3%的库的字节码和20.3%的库的总依赖项可以通过基于覆盖的去浮动来去除。,软件及其工程，软件创建和管理，软件验证和确认，经验软件确认，软件符号和工具，软件库和存储库，软件维护工具,,,
9HJP2T7D,2023,https://doi.org/10.1145/3574158,TOSEM 2023,Demystifying Hidden Sensitive Operations in Android Apps,"Security of Android devices is now paramount, given their wide adoption among consumers. As researchers develop tools for statically or dynamically detecting suspicious apps, malware writers regularly update their attack mechanisms to hide malicious behavior implementation. This poses two problems to current research techniques: static analysis approaches, given their over-approximations, can report an overwhelming number of false alarms, while dynamic approaches will miss those behaviors that are hidden through evasion techniques. We propose in this work a static approach specifically targeted at highlighting hidden sensitive operations (HSOs), mainly sensitive data flows. The prototype version of HiSenDroid has been evaluated on a large-scale dataset of thousands of malware and goodware samples on which it successfully revealed anti-analysis code snippets aiming at evading detection by dynamic analysis. We further experimentally show that, with FlowDroid, some of the hidden sensitive behaviors would eventually lead to private data leaks. Those leaks would have been hard to spot either manually among the large number of false positives reported by the state-of-the-art static analyzers, or by dynamic tools. Overall, by putting the light on hidden sensitive operations, HiSenDroid helps security analysts in validating potentially sensitive data operations, which would be previously unnoticed.","Security and privacy,Software and application security,Domain-specific security and privacy architectures",破解安卓应用程序中隐藏的敏感操作,考虑到安卓设备在消费者中的广泛采用，安卓设备的安全性现在至关重要。随着研究人员开发静态或动态检测可疑应用程序的工具，恶意软件编写者会定期更新其攻击机制，以隐藏恶意行为的实现。这给当前的研究技术带来了两个问题：静态分析方法，鉴于其过度近似，可能会报告大量的假警报，而动态方法会错过那些通过规避技术隐藏的行为。在这项工作中，我们提出了一种静态方法，专门针对突出隐藏的敏感操作（HSO），主要是敏感数据流。HiSenDroid的原型版本已经在一个由数千个恶意软件和商品样本组成的大规模数据集上进行了评估，在该数据集上，它成功地揭示了旨在通过动态分析逃避检测的反分析代码片段。我们进一步通过实验表明，使用FlowDroid，一些隐藏的敏感行为最终会导致私人数据泄露。无论是在最先进的静态分析仪报告的大量误报中，还是通过动态工具，都很难手动发现这些泄漏。总的来说，HiSenDroid通过揭露隐藏的敏感操作，帮助安全分析师验证以前未被注意到的潜在敏感数据操作。,安全和隐私，软件和应用程序安全，特定于域的安全和隐私架构,,,
AFZGEG2B,2023,https://doi.org/10.1145/3533314,TOSEM 2023,Feedback-Directed Metamorphic Testing,"Over the past decade, metamorphic testing has gained rapidly increasing attention from both academia and industry, particularly thanks to its high efficacy on revealing real-life software faults in a wide variety of application domains. On the basis of a set of metamorphic relations among multiple software inputs and their expected outputs, metamorphic testing not only provides a test case generation strategy by constructing new (or follow-up) test cases from some original (or source) test cases, but also a test result verification mechanism through checking the relationship between the outputs of source and follow-up test cases. Many efforts have been made to further improve the cost-effectiveness of metamorphic testing from different perspectives. Some studies attempted to identify “good” metamorphic relations, while other studies were focused on applying effective test case generation strategies especially for source test cases. In this article, we propose improving the cost-effectiveness of metamorphic testing by leveraging the feedback information obtained in the test execution process. Consequently, we develop a new approach, namely feedback-directed metamorphic testing, which makes use of test execution information to dynamically adjust the selection of metamorphic relations and selection of source test cases. We conduct an empirical study to evaluate the proposed approach based on four laboratory programs, one GNU program, and one industry program. The empirical results show that feedback-directed metamorphic testing can use fewer test cases and take less time than the traditional metamorphic testing for detecting the same number of faults. It is clearly demonstrated that the use of feedback information about test execution does help enhance the cost-effectiveness of metamorphic testing. Our work provides a new perspective to improve the efficacy and applicability of metamorphic testing as well as many other software testing techniques.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",反馈导向的变形测试,在过去的十年里，变形测试迅速受到学术界和工业界的关注，特别是由于它在揭示各种应用领域中真实软件故障方面的高效性。基于多个软件输入及其预期输出之间的一组变形关系，变形测试不仅通过从一些原始（或源）测试用例构建新的（或后续）测试用例来提供测试用例生成策略，还通过检查源测试用例和后续测试用例的输出之间的关系来验证测试结果。已经从不同的角度做出了许多努力来进一步提高变质测试的成本效益。一些研究试图确定“良好”的变质关系，而其他研究则专注于应用有效的测试用例生成策略，尤其是对源测试用例。在本文中，我们建议通过利用测试执行过程中获得的反馈信息来提高变形测试的成本效益。因此，我们开发了一种新的方法，即反馈导向的变形测试，它利用测试执行信息来动态调整变形关系的选择和源测试用例的选择。我们基于四个实验室项目、一个GNU项目和一个行业项目进行了实证研究，以评估所提出的方法。实证结果表明，在检测相同数量的故障时，反馈导向变质测试可以使用更少的测试用例，花费更少的时间。这清楚地表明，使用关于测试执行的反馈信息确实有助于提高变形测试的成本效益。我们的工作为提高变形测试以及许多其他软件测试技术的有效性和适用性提供了一个新的视角。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
AYAURGGP,2023,https://doi.org/10.1145/3607183,TOSEM 2023,Revisiting the Identification of the Co-evolution of Production and Test Code,"Many software processes advocate that the test code should co-evolve with the production code. Prior work usually studies such co-evolution based on production-test co-evolution samples mined from software repositories. A production-test co-evolution sample refers to a pair of a test code change and a production code change where the test code change triggers or is triggered by the production code change. The quality of the mined samples is critical to the reliability of research conclusions. Existing studies mined production-test co-evolution samples based on the following assumption: if a test class and its associated production class change together in one commit, or a test class changes immediately after the changes of the associated production class within a short time interval, this change pair should be a production-test co-evolution sample. However, the validity of this assumption has never been investigated.","Software and its engineering,Software creation and management,Software post-development issues,Software evolution,Software version control,Software verification and validation,Empirical software validation",对生产代码与测试代码协同进化识别的再认识,许多软件流程主张测试代码应该与生产代码共同发展。先前的工作通常基于从软件存储库中挖掘的生产测试协同进化样本来研究这种协同进化。生产测试协同进化样本是指一对测试代码更改和生产代码更改，其中测试代码更改触发或由生产代码更改触发。挖掘样本的质量对研究结论的可靠性至关重要。现有研究基于以下假设挖掘了生产测试协同进化样本：如果一个测试类及其相关的生产类在一次提交中一起发生变化，或者在短时间间隔内，在相关生产类发生变化后，测试类立即发生变化，则该变化对应为生产测试协同演化样本。然而，这一假设的有效性从未被调查过。,软件及其工程，软件创建和管理，软件后开发问题，软件演化，软件版本控制，软件验证和确认，经验软件确认,,,
6YJC7YMQ,2023,https://doi.org/10.1145/3597202,TOSEM 2023,Incorporating Signal Awareness in Source Code Modeling: An Application to Vulnerability Detection,"AI models of code have made significant progress over the past few years. However, many models are actually not learning task-relevant source code features. Instead, they often fit non-relevant but correlated data, leading to a lack of robustness and generalizability, and limiting the subsequent practical use of such models. In this work, we focus on improving the model quality through signal awareness, i.e., learning the relevant signals in the input for making predictions. We do so by leveraging the heterogeneity of code samples in terms of their signal-to-noise content. We perform an end-to-end exploration of model signal awareness, comprising: (i) uncovering the reliance of AI models of code on task-irrelevant signals, via prediction-preserving input minimization; (ii) improving models’ signal awareness by incorporating the notion of code complexity during model training, via curriculum learning; (iii) improving models’ signal awareness by generating simplified signal-preserving programs and augmenting them to the training dataset; and (iv) presenting a novel interpretation of the model learning behavior from the perspective of the dataset, using its code complexity distribution. We propose a new metric to measure model signal awareness, Signal-aware Recall, which captures how much of the model’s performance is attributable to task-relevant signal learning. Using a software vulnerability detection use-case, our model probing approach uncovers a significant lack of signal awareness in the models, across three different neural network architectures and three datasets. Signal-aware Recall is observed to be in the sub-50s for models with traditional Recall in the high 90s, suggesting that the models are presumably picking up a lot of noise or dataset nuances while learning their logic. With our code-complexity-aware model learning enhancement techniques, we are able to assist the models toward more task-relevant learning, recording up-to 4.8× improvement in model signal awareness. Finally, we employ our model learning introspection approach to uncover the aspects of source code where the model is facing difficulty, and we analyze how our learning enhancement techniques alleviate it.","Computing methodologies,Machine learning,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",将信号感知纳入源代码建模：在漏洞检测中的应用,人工智能代码模型在过去几年中取得了重大进展。然而，许多模型实际上并没有学习与任务相关的源代码特性。相反，它们通常适合不相关但相关的数据，导致缺乏稳健性和可推广性，并限制了此类模型的后续实际使用。在这项工作中，我们专注于通过信号感知来提高模型质量，即学习输入中的相关信号以进行预测。我们通过利用代码样本在信噪比方面的异质性来做到这一点。我们对模型信号感知进行了端到端的探索，包括：（i）通过预测保持输入最小化，揭示代码的AI模型对任务无关信号的依赖；（ii）通过课程学习，在模型训练过程中引入代码复杂性的概念，提高模型的信号意识；（iii）通过生成简化的信号保持程序并将其扩充到训练数据集来提高模型的信号感知；以及（iv）使用其代码复杂性分布，从数据集的角度对模型学习行为进行了新颖的解释。我们提出了一种新的衡量模型信号感知的指标，即信号感知回忆，它捕捉了模型的性能在多大程度上归因于任务相关的信号学习。通过使用软件漏洞检测用例，我们的模型探测方法发现，在三种不同的神经网络架构和三个数据集中，模型中明显缺乏信号感知。对于90年代中期的传统召回模型，观察到信号感知召回处于50年代以下，这表明这些模型在学习逻辑时可能会拾取大量噪声或数据集的细微差别。通过我们的代码复杂性感知模型学习增强技术，我们能够帮助模型进行更多与任务相关的学习，在模型信号感知方面提高了4.8倍。最后，我们使用我们的模型学习内省方法来揭示源代码中模型面临困难的方面，并分析我们的学习增强技术是如何缓解困难的。,计算方法论，机器学习，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
QQS25VA6,2023,https://doi.org/10.1145/3512768,TOSEM 2023,APIRO: A Framework for Automated Security Tools API Recommendation,"Security Orchestration, Automation, and Response (SOAR) platforms integrate and orchestrate a wide variety of security tools to accelerate the operational activities of Security Operation Center (SOC). Integration of security tools in a SOAR platform is mostly done manually using APIs, plugins, and scripts. SOC teams need to navigate through API calls of different security tools to find a suitable API to define or update an incident response action. Analyzing various types of API documentation with diverse API format and presentation structure involves significant challenges such as data availability, data heterogeneity, and semantic variation for automatic identification of security tool APIs specific to a particular task. Given these challenges can have negative impact on SOC team’s ability to handle security incident effectively and efficiently, we consider it important to devise suitable automated support solutions to address these challenges. We propose a novel learning-based framework for automated security tool API Recommendation for security Orchestration, automation, and response, APIRO. To mitigate data availability constraint, APIRO enriches security tool API description by applying a wide variety of data augmentation techniques. To learn data heterogeneity of the security tools and semantic variation in API descriptions, APIRO consists of an API-specific word embedding model and a Convolutional Neural Network (CNN) model that are used for prediction of top three relevant APIs for a task. We experimentally demonstrate the effectiveness of APIRO in recommending APIs for different tasks using three security tools and 36 augmentation techniques. Our experimental results demonstrate the feasibility of APIRO for achieving 91.9% Top-1 Accuracy. Compared to the state-of-the-art baseline, APIRO is 26.93%, 23.03%, and 20.87% improved in terms of Top-1, Top-2, and Top-3 Accuracy and outperforms the baseline by 23.7% in terms of Mean Reciprocal Rank (MRR).","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software post-development issues,Documentation,Software evolution",APIRO:自动化安全工具框架API建议,安全编排、自动化和响应（SOAR）平台集成和编排了各种各样的安全工具，以加速安全操作中心（SOC）的操作活动。SOAR平台中安全工具的集成主要是使用API、插件和脚本手动完成的。SOC团队需要浏览不同安全工具的API调用，以找到合适的API来定义或更新事件响应操作。分析具有不同API格式和表示结构的各种类型的API文档涉及重大挑战，如数据可用性、数据异构性和语义变化，以自动识别特定任务的安全工具API。鉴于这些挑战可能会对SOC团队有效处理安全事件的能力产生负面影响，我们认为设计合适的自动化支持解决方案来应对这些挑战很重要。我们为自动化安全工具API提出了一个新的基于学习的框架。安全编排、自动化和响应建议，APIRO。为了缓解数据可用性限制，APIRO通过应用多种数据增强技术丰富了安全工具API的描述。为了了解API描述中安全工具的数据异质性和语义变化，APIRO由特定于API的单词嵌入模型和卷积神经网络（CNN）模型组成，用于预测任务的前三个相关API。我们使用三种安全工具和36种增强技术，通过实验证明了APIRO在为不同任务推荐API方面的有效性。我们的实验结果证明了APIRO实现91.9%Top-1准确度的可行性。与最先进的基线相比，APIRO在Top-1、Top-2和Top-3准确度方面分别提高了26.93%、23.03%和20.87%，在平均倒数排名（MRR）方面优于基线23.7%。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件开发后问题，文档，软件发展,,,
VIZWXXS3,2023,https://doi.org/10.1145/3522586,TOSEM 2023,Defining a Knowledge Graph Development Process Through a Systematic Review,"Knowledge graphs are widely used in industry and studied within the academic community. However, the models applied in the development of knowledge graphs vary. Analysing and providing a synthesis of the commonly used approaches to knowledge graph development would provide researchers and practitioners a better understanding of the overall process and methods involved. Hence, this article aims at defining the overall process of knowledge graph development and its key constituent steps. For this purpose, a systematic review and a conceptual analysis of the literature was conducted. The resulting process was compared to case studies to evaluate its applicability. The proposed process suggests a unified approach and provides guidance for both researchers and practitioners when constructing and managing knowledge graphs.","Computing methodologies,Artificial intelligence,Knowledge representation and reasoning,Ontology engineering,Semantic networks,Information systems,Data management systems,Information integration,World Wide Web,Web data description languages,Semantic web description languages,Software and its engineering,Software creation and management,Software development process management",通过系统回顾定义知识图开发过程,知识图在工业中被广泛使用，并在学术界进行研究。然而，应用于知识图开发的模型各不相同。分析和综合常用的知识图开发方法，将使研究人员和从业者更好地了解所涉及的整个过程和方法。因此，本文旨在定义知识图开发的总体过程及其关键组成步骤。为此，对文献进行了系统的回顾和概念分析。将所产生的过程与案例研究进行比较，以评估其适用性。所提出的过程提出了一种统一的方法，并为研究人员和从业者在构建和管理知识图时提供了指导。,计算方法论，人工智能，知识表示和推理，本体工程，语义网络，信息系统，数据管理系统，信息集成，万维网，万维网数据描述语言，语义网描述语言，软件及其工程，软件创建和管理，软件开发过程管理,,,
4A6EL6CY,2023,https://doi.org/10.1145/3579642,TOSEM 2023,A Survey on Automated Driving System Testing: Landscapes and Trends,"Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field.","Computer systems organization,Embedded and cyber-physical systems,Embedded systems,Computing methodologies,Artificial intelligence,Security and privacy,Systems security,Software and its engineering,Software creation and management,Software verification and validation",自动驾驶系统测试综述：前景与趋势,近年来，在学术界和工业界的共同努力下，自动驾驶系统取得了巨大的成就。典型的ADS由多个模块组成，包括传感、感知、规划和控制，这些模块汇集了不同领域的最新进展。尽管取得了这些成就，但ADS的安全保障具有重要意义，因为ADS的不安全行为会带来灾难性后果。测试已被公认为一种重要的系统验证方法，旨在揭露不安全的系统行为；然而，在ADS的背景下，由于系统的高度复杂性和多学科性，设计有效的测试技术是极具挑战性的。有很多文献关注ADS的测试，也出现了一些调查来总结技术进步。大多数调查都集中在软件模拟器中执行的系统级测试上，因此忽略了不同模块的不同特征。在本文中，我们对现有的ADS测试文献进行了全面的调查，其中考虑了模块级和系统级测试。具体而言，我们做出了以下贡献：（1）我们调查了ADS的模块级测试技术，并强调了受不同模块特性影响的技术差异；（2） 我们还调查了系统级测试技术，重点是实证研究，总结了系统开发或部署中出现的问题，不同模块之间的协作导致的问题，以及模拟器中ADS测试与现实世界之间的差距；（3）我们发现了ADS测试中的挑战和机遇，为该领域的未来研究铺平了道路。,计算机系统组织，嵌入式和网络物理系统，嵌入式系统，计算方法，人工智能，安全和隐私，系统安全，软件及其工程，软件创建和管理，软件验证和确认,,,
E4D433XP,2023,https://doi.org/10.1145/3576039,TOSEM 2023,The Best of Both Worlds: Combining Learned Embeddings with Engineered Features for Accurate Prediction of Correct Patches,"A large body of the literature on automated program repair develops approaches where patches are automatically generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state-of-the-art explores research directions that require dynamic information or rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations of patch correctness identification, and assess the possibility of accurate classification of correct patch by combining learned embeddings with engineered features. Experimental results demonstrate the potential of learned embeddings to empower Leopard (a patch correctness predicting framework implemented in this work) with learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based learned embeddings associated with XGBoost achieves an AUC value of about 0.803 in the prediction of patch correctness on a new dataset of 2,147 labeled patches that we collected for the experiments. Our investigations show that deep learned embeddings can lead to complementary/better performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. By combining deep learned embeddings and engineered features, Panther (the upgraded version of Leopard implemented in this work) outperforms Leopard with higher scores in terms of AUC, +Recall and -Recall, and can accurately identify more (in)correct patches that cannot be predicted by the classifiers only with learned embeddings or engineered features. Finally, we use an explainable ML technique, SHAP, to empirically interpret how the learned embeddings and engineered features are contributed to the patch correctness prediction.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",两全其美：将学习嵌入与工程特征相结合，准确预测正确的补丁,大量关于自动程序修复的文献开发了自动生成补丁以根据预言机（例如，测试套件）进行验证的方法。因为这样的预言机可能是不完美的，所以生成的补丁虽然经过预言机的验证，但实际上可能是不正确的。虽然最先进的技术探索了需要动态信息或依赖手动制定的启发式方法的研究方向，但我们研究了学习代码表示的好处，以便学习可能编码补丁正确性属性的深层特征。我们的经验工作调查了代码更改的不同表示学习方法，以推导出适用于补丁正确性识别的相似性计算的嵌入，并通过将学习的嵌入与工程特征相结合来评估正确补丁的准确分类的可能性。实验结果证明了学习嵌入的潜力，使Leopard（本工作中实现的补丁正确性预测框架）能够使用学习算法推理补丁正确性：与XGBoost相关的基于BERT变换器的学习嵌入的机器学习预测器在预测补丁正确性时的AUC值约为0.803我们为实验收集的2147个标记补丁的新数据集。我们的研究表明，与依赖动态信息的最先进的PATCH-SIM相比，深度学习嵌入可以带来互补/更好的性能。通过将深度学习嵌入和工程特征相结合，Panther（本工作中实现的Leopard的升级版）在AUC、+Recall和-Recall方面以更高的分数优于Leopard，并且可以准确地识别出更多（in）正确的补丁，而这些补丁只能通过学习嵌入或工程特征由分类器预测。最后，我们使用一种可解释的ML技术SHAP来实证解释学习到的嵌入和工程特征是如何对补丁正确性预测做出贡献的。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
I78DA3FL,2023,https://doi.org/10.1145/3583564,TOSEM 2023,Finding Deviated Behaviors of the Compressed DNN Models for Image Classifications,"Model compression can significantly reduce the sizes of deep neural network (DNN) models and thus facilitate the dissemination of sophisticated, sizable DNN models, especially for deployment on mobile or embedded devices. However, the prediction results of compressed models may deviate from those of their original models. To help developers thoroughly understand the impact of model compression, it is essential to test these models to find those deviated behaviors before dissemination. However, this is a non-trivial task, because the architectures and gradients of compressed models are usually not available.","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",发现图像分类的压缩DNN模型的偏差行为,模型压缩可以显著减小深度神经网络（DNN）模型的大小，从而促进复杂、大规模的DNN模型的传播，尤其是用于在移动或嵌入式设备上部署。然而，压缩模型的预测结果可能会偏离其原始模型的预测。为了帮助开发人员彻底理解模型压缩的影响，在传播之前测试这些模型以发现那些偏离的行为是至关重要的。然而，这是一项不平凡的任务，因为压缩模型的架构和梯度通常不可用。,计算方法，机器学习，机器学习方法，神经网络，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
BIPGTZHJ,2023,https://doi.org/10.1145/3576037,TOSEM 2023,I Depended on You and You Broke Me: An Empirical Study of Manifesting Breaking Changes in Client Packages,"Complex software systems have a network of dependencies. Developers often configure package managers (e.g., npm) to automatically update dependencies with each publication of new releases containing bug fixes and new features. When a dependency release introduces backward-incompatible changes, commonly known as breaking changes, dependent packages may not build anymore. This may indirectly impact downstream packages, but the impact of breaking changes and how dependent packages recover from these breaking changes remain unclear. To close this gap, we investigated the manifestation of breaking changes in the npm ecosystem, focusing on cases where packages’ builds are impacted by breaking changes from their dependencies. We measured the extent to which breaking changes affect dependent packages. Our analyses show that around 12% of the dependent packages and 14% of their releases were impacted by a breaking change during updates of non-major releases of their dependencies. We observed that, from all of the manifesting breaking changes, 44% were introduced in both minor and patch releases, which in principle should be backward compatible. Clients recovered themselves from these breaking changes in half of the cases, most frequently by upgrading or downgrading the provider’s version without changing the versioning configuration in the package manager. We expect that these results help developers understand the potential impact of such changes and recover from them.","Software and its engineering,Software creation and management,Software post-development issues,Software evolution",我依赖你，你让我崩溃：客户包中破裂变化的实证研究,复杂的软件系统具有依赖关系网络。开发人员经常配置包管理器（例如，npm），以便在每次发布包含错误修复和新功能的新版本时自动更新依赖关系。当依赖发布引入向后不兼容的更改（通常称为中断更改）时，依赖包可能不再构建。这可能会间接影响下游软件包，但中断更改的影响以及依赖软件包如何从这些中断更改中恢复仍不清楚。为了缩小这一差距，我们研究了npm生态系统中破坏性更改的表现，重点关注包的构建受到依赖性破坏性更改影响的情况。我们测量了中断更改对依赖包的影响程度。我们的分析表明，大约12%的依赖包和14%的依赖版本在更新非主要依赖版本时受到了突破性变化的影响。我们观察到，从所有表现出的突破性变化中，44%是在次要版本和补丁版本中引入的，原则上应该是向后兼容的。在一半的情况下，客户端从这些破坏性的更改中恢复过来，最常见的是在不更改包管理器中的版本控制配置的情况下升级或降级提供程序的版本。我们希望这些结果能帮助开发人员了解此类更改的潜在影响，并从中恢复过来。,软件及其工程，软件创建和管理，软件后开发问题，软件演化,,,
FT4PG5AM,2023,https://doi.org/10.1145/3571849,TOSEM 2023,A Theory of Scrum Team Effectiveness,"Scrum teams are at the heart of the Scrum framework. Nevertheless, an integrated and systemic theory that can explain what makes some Scrum teams more effective than others is still missing. To address this gap, we performed a 7-year-long mixed-methods investigation composed of two main phases. First, we induced a theoretical model from 13 exploratory field studies. Our model proposes that the effectiveness of Scrum teams depends on five high-level factors (responsiveness, stakeholder concern, continuous improvement, team autonomy, and management support) and 13 lower-level factors. In the second phase of our study, we validated our model with a covariance-based structural equation modeling analysis using data from about 5,000 developers and 2,000 Scrum teams that we gathered with a custom-built survey. Results suggest a very good fit of the empirical data in our theoretical model (CFI = 0.959, RMSEA = 0.038, SRMR = 0.035). Accordingly, this research allowed us to (1) propose and validate a generalizable theory for effective Scrum teams and (2) formulate clear recommendations for how organizations can better support Scrum teams.","Human-centered computing,Social and professional topics,Professional topics,Computing and business,Computing profession,Management of computing and information systems,Project and people management,Software management,Software and its engineering,Software creation and management,Software development process management,Software development techniques,Software notations and tools,Software configuration management and version control systems",Scrum团队有效性理论,Scrum团队是Scrum框架的核心。尽管如此，仍然缺少一个综合的、系统的理论来解释是什么让一些Scrum团队比其他团队更有效。为了解决这一差距，我们进行了为期7年的混合方法调查，包括两个主要阶段。首先，我们从13项探索性实地研究中归纳出一个理论模型。我们的模型提出，Scrum团队的有效性取决于五个高层因素（响应能力、利益相关者关注、持续改进、团队自主性和管理支持）和13个低层因素。在我们研究的第二阶段，我们使用来自大约5000名开发人员和2000个Scrum团队的数据，通过基于协方差的结构方程建模分析验证了我们的模型，这些数据是我们通过定制的调查收集的。结果表明，我们的理论模型中的经验数据非常吻合（CFI=0.959，RMSEA=0.038，SRMR=0.035）。因此，这项研究使我们能够（1）为有效的Scrum团队提出并验证一个可推广的理论，以及（2）为组织如何更好地支持Scrum团队制定明确的建议。,以人为中心的计算，社会和专业主题，专业主题，计算和商业，计算职业，计算和信息系统管理，项目和人员管理，软件管理，软件及其工程，软件创建和管理，软件开发过程管理，软件开发技术，软件符号和工具，软件配置管理和版本控制系统,,,
9ISJHZ5Z,2023,https://doi.org/10.1145/3517194,TOSEM 2023,Parametric Timed Pattern Matching,"Given a log and a specification, timed pattern matching aims at exhibiting for which start and end dates a specification holds on that log. For example, “a given action is always followed by another action before a given deadline”. This problem has strong connections with monitoring real-time systems. We address here timed pattern matching in the presence of an uncertain specification, i.e., that may contain timing parameters (e.g., the deadline can be uncertain or unknown). We want to know for which start and end dates, and for what values of the timing parameters, a property holds. For instance, we look for the minimum or maximum deadline (together with the corresponding start and end dates) for which the property holds. We propose two frameworks for parametric timed pattern matching. The first one is based on parametric timed model checking. In contrast to most parametric timed problems, the solution is effectively computable. The second one is a dedicated method; not only we largely improve the efficiency compared to the first method, but we further propose optimizations with skipping. Our experiment results suggest that our algorithms, especially the second one, are efficient and practically relevant.","Computer systems organization,Real-time systems,Real-time system specification,Theory of computation,Formal languages and automata theory,Automata extensions,Logic,Logic and verification,Verification by model checking,Models of computation,Timed and hybrid models",参数化定时模式匹配,给定一个日志和一个规范，定时模式匹配旨在展示规范在该日志上的起始日期和结束日期。例如，“在给定的截止日期之前，一个给定的动作后面总是跟着另一个动作”。这个问题与实时监控系统有着密切的联系。我们在这里讨论了在存在不确定规范的情况下的定时模式匹配，即，可能包含定时参数（例如，截止日期可能不确定或未知）。我们想知道一个属性适用于哪些开始日期和结束日期，以及计时参数的值。例如，我们寻找属性持有的最短或最长截止日期（以及相应的开始和结束日期）。我们提出了两个用于参数定时模式匹配的框架。第一种是基于参数定时模型检查。与大多数参数定时问题相比，该解决方案是可有效计算的。第二种是专门的方法；与第一种方法相比，我们不仅大大提高了效率，而且还进一步提出了跳过优化。我们的实验结果表明，我们的算法，特别是第二种算法，是有效的，并且具有实际意义。,计算机系统组织，实时系统，实时系统规范，计算理论，形式语言和自动机理论，自动机扩展，逻辑，逻辑和验证，通过模型检查验证，计算模型，时间模型和混合模型,,,
A3ZBUG69,2023,https://doi.org/10.1145/3546949,TOSEM 2023,Suboptimal Comments in Java Projects: From Independent Comment Changes to Commenting Practices,"High-quality source code comments are valuable for software development and maintenance, however, code often contains low-quality comments or lacks them altogether. We name such source code comments as suboptimal comments. Such suboptimal comments create challenges in code comprehension and maintenance. Despite substantial research on low-quality source code comments, empirical knowledge about commenting practices that produce suboptimal comments and reasons that lead to suboptimal comments are lacking. We help bridge this knowledge gap by investigating (1) independent comment changes (ICCs)—comment changes committed independently of code changes—which likely address suboptimal comments, (2) commenting guidelines, and (3) comment-checking tools and comment-generating tools, which are often employed to help commenting practice—especially to prevent suboptimal comments. ","Software and its engineering,Software creation and management,Software post-development issues,Documentation,Maintaining software",Java项目中的次优注释：从独立注释更改到注释实践,高质量的源代码注释对于软件开发和维护是有价值的，然而，代码通常包含低质量的注释或完全没有这些注释。我们将这种源代码注释命名为次优注释。这样的次优注释给代码理解和维护带来了挑战。尽管对低质量源代码评论进行了大量研究，但缺乏关于产生次优评论的评论实践以及导致次优评论原因的经验知识。我们通过研究（1）独立注释更改（ICCs）——独立于代码更改进行的注释更改——可能会解决次优注释，（2）注释指南，以及（3）注释检查工具和注释生成工具，这些工具通常用于帮助注释实践，尤其是防止次优注释。,软件及其工程，软件创建和管理，软件开发后问题，文档，软件维护,,,
BD36EZBV,2023,https://doi.org/10.1145/3569933,TOSEM 2023,sem2vec: Semantics-aware Assembly Tracelet Embedding,"Binary code similarity is the foundation of many security and software engineering applications. Recent works leverage deep neural networks (DNN) to learn a numeric vector representation (namely, embeddings) of assembly functions, enabling similarity analysis in the numeric space. However, existing DNN-based techniques capture syntactic-, control flow-, or data flow-level information of assembly code, which is too coarse-grained to represent program functionality. These methods can suffer from low robustness to challenging settings such as compiler optimizations and obfuscations.","Security and privacy,Software and application security,Software security engineering",sem2vec:语义感知程序集Tracelet嵌入,二进制代码相似性是许多安全和软件工程应用程序的基础。最近的工作利用深度神经网络（DNN）来学习汇编函数的数字向量表示（即嵌入），从而能够在数字空间中进行相似性分析。然而，现有的基于DNN的技术捕获汇编代码的语法、控制流或数据流级别的信息，这些信息太粗，无法表示程序功能。这些方法对编译器优化和模糊处理等具有挑战性的设置的鲁棒性可能较低。,安全和隐私，软件和应用程序安全，软件安全工程,,,
44ETVJCB,2023,https://doi.org/10.1145/3593800,TOSEM 2023,XCoS: Explainable Code Search Based on Query Scoping and Knowledge Graph,"When searching code, developers may express additional constraints (e.g., functional constraints and nonfunctional constraints) on the implementations of desired functionalities in the queries. Existing code search tools treat the queries as a whole and ignore the different implications of different parts of the queries. Moreover, these tools usually return a ranked list of candidate code snippets without any explanations. Therefore, the developers often find it hard to choose the desired results and build confidence on them. In this article, we conduct a developer survey to better understand and address these issues and induct some insights from the survey results. Based on the insights, we propose XCoS, an explainable code search approach based on query scoping and knowledge graph. XCoS extracts a background knowledge graph from general knowledge bases like Wikidata and Wikipedia. Given a code search query, XCoS identifies different parts (i.e., functionalities, functional constraints, nonfunctional constraints) from it and use the expressions of functionalities and functional constraints to search the codebase. It then links both the query and the candidate code snippets to the concepts in the background knowledge graph and generates explanations based on the association paths between these two parts of concepts together with relevant descriptions. XCoS uses an interactive user interface that allows the user to better understand the associations between candidate code snippets and the query from different aspects and choose the desired results. Our evaluation shows that the quality of the extracted background knowledge and the concept linkings in codebase is generally high. Furthermore, the generated explanations are considered complete, concise, and readable, and the approach can help developers find the desired code snippets more accurately and confidently.","Software and its engineering,Software creation and management,Software development techniques",XCoS:基于查询范围和知识图的可解释代码搜索,在搜索代码时，开发人员可能会对查询中所需功能的实现表达额外的约束（例如，功能约束和非功能约束）。现有的代码搜索工具将查询视为一个整体，并忽略查询不同部分的不同含义。此外，这些工具通常返回一个候选代码片段的排序列表，而没有任何解释。因此，开发人员经常发现很难选择想要的结果并对其建立信心。在本文中，我们进行了一项开发人员调查，以更好地理解和解决这些问题，并从调查结果中得出一些见解。基于这些见解，我们提出了XCoS，这是一种基于查询范围和知识图的可解释代码搜索方法。XCoS从Wikidata和Wikipedia等通用知识库中提取背景知识图。给定代码搜索查询，XCoS从中识别不同的部分（即功能、功能约束、非功能约束），并使用功能和功能约束的表达式来搜索代码库。然后，它将查询和候选代码片段链接到背景知识图中的概念，并根据这两部分概念之间的关联路径以及相关描述生成解释。XCoS使用交互式用户界面，使用户可以从不同方面更好地理解候选代码片段和查询之间的关联，并选择所需的结果。我们的评估表明，提取的背景知识和代码库中的概念链接的质量通常很高。此外，生成的解释被认为是完整、简洁和可读的，这种方法可以帮助开发人员更准确、更自信地找到所需的代码片段。,软件及其工程，软件创建和管理，软件开发技术,,,
UNKNFY62,2023,https://doi.org/10.1145/3597203,TOSEM 2023,Pre-implementation Method Name Prediction for Object-oriented Programming,"Method naming is a challenging development task in object-oriented programming. In recent years, several research efforts have been undertaken to provide automated tool support for assisting developers in this task. In general, literature approaches assume the availability of method implementation to infer its name. Methods, however, are usually named before their implementations. In this work, we fill the gap in the literature about method name prediction by developing an approach that predicts the names of all methods to be implemented within a class. Our work considers the class name as the input: The overall intuition is that classes with semantically similar names tend to provide similar functionalities, and hence similar method names. We first conduct a large-scale empirical analysis on 258K+ classes from real-world projects to validate our hypotheses. Then, we propose a hybrid big code-driven approach, Mario, to predict method names based on the class name: We combine a deep learning model with heuristics summarized from code analysis. Extensive experiments on 22K+ classes yielded promising results: compared to the state-of-the-art code2seq model (which leverages method implementation data), our approach achieves comparable results in terms of F-score at token-level prediction; our approach, additionally, outperforms code2seq in prediction at the name level. We further show that our approach significantly outperforms several other baselines.","Software and its engineering,Software creation and management,Designing software,Software implementation planning,Software design techniques,Software development techniques,Object oriented development",面向对象程序设计的预实现方法名称预测,方法命名是面向对象编程中一项具有挑战性的开发任务。近年来，已经进行了几项研究工作，以提供自动化工具支持，帮助开发人员完成这项任务。一般来说，文献方法假设方法实现的可用性来推断其名称。然而，方法通常在实现之前进行命名。在这项工作中，我们通过开发一种方法来预测要在类中实现的所有方法的名称，填补了文献中关于方法名称预测的空白。我们的工作将类名视为输入：总体直觉是，具有语义相似名称的类往往提供相似的功能，因此提供相似的方法名称。我们首先对来自现实世界项目的258K+个班级进行了大规模的实证分析，以验证我们的假设。然后，我们提出了一种基于类名预测方法名称的混合大代码驱动方法Mario：我们将深度学习模型与代码分析总结的启发式方法相结合。在22K+类上进行的广泛实验产生了有希望的结果：与最先进的code2seq模型（利用方法实现数据）相比，我们的方法在令牌级预测的F分数方面取得了可比的结果；此外，我们的方法在名称级别的预测方面优于code2seq。我们进一步表明，我们的方法显著优于其他几个基线。,软件及其工程，软件创建和管理，软件设计，软件实施规划，软件设计技术，软件开发技术，面向对象开发,,,
TJLMKBJF,2023,https://doi.org/10.1145/3565800,TOSEM 2023,Anchor: Fast and Precise Value-flow Analysis for Containers via Memory Orientation,"Containers are ubiquitous data structures that support a variety of manipulations on the elements, inducing the indirect value flows in the program. Tracking value flows through containers is stunningly difficult, because it depends on container memory layouts, which are expensive to be discovered.","Software and its engineering,Software organization and properties,Extra-functional properties,Software safety,Theory of computation,Semantics and reasoning,Program reasoning,Program analysis",锚：基于记忆定向的容器快速精确价值流分析,容器是无处不在的数据结构，支持对元素的各种操作，从而在程序中引发间接的值流。跟踪容器中的值流非常困难，因为它依赖于容器内存布局，而容器内存布局的发现成本很高。,软件及其工程，软件组织和性质，额外功能性质，软件安全，计算理论，语义和推理，程序推理，程序分析,,,
RPSQKYT8,2023,https://doi.org/10.1145/3591868,TOSEM 2023,A Hypothesis Testing-based Framework for Software Cross-modal Retrieval in Heterogeneous Semantic Spaces,"Software cross-modal retrieval is a popular yet challenging direction, such as bug localization and code search. Previous studies generally map natural language texts and codes into a homogeneous semantic space for similarity measurement. However, it is not easy to accurately capture their similar semantics in a homogeneous semantic space due to the semantic gap. Therefore, we propose to map the multi-modal data into heterogeneous semantic spaces to capture their unique semantics. Specifically, we propose a novel software cross-modal retrieval framework named Deep Hypothesis Testing (DeepHT). In DeepHT, to capture the unique semantics of the code’s control flow structure, all control flow paths (CFPs) in the control flow graph are mapped to a CFP sample set in the sample space. Meanwhile, the text is mapped to a CFP correlation distribution in the distribution space to model its correlation with different CFPs. The matching score is calculated according to how well the sample set obeys the distribution using hypothesis testing. The experimental results on two text-to-code retrieval tasks (i.e., bug localization and code search) and two code-to-text retrieval tasks (i.e., vulnerability knowledge retrieval and historical patch retrieval) show that DeepHT outperforms the baseline methods.","Software and its engineering,Software creation and management,Search-based software engineering",一种基于假设测试的异构语义空间软件跨模态检索框架,软件跨模式检索是一个流行但具有挑战性的方向，如错误定位和代码搜索。以往的研究通常将自然语言文本和代码映射到一个同质的语义空间中进行相似性测量。然而，由于语义差距的存在，在同构的语义空间中准确捕捉它们的相似语义并不容易。因此，我们建议将多模态数据映射到异构语义空间中，以捕获它们的独特语义。具体来说，我们提出了一个新的软件跨模态检索框架，名为深度假设测试（DeepHT）。在DeepHT中，为了捕获代码的控制流结构的唯一语义，控制流图中的所有控制流路径（CFP）都映射到样本空间中的CFP样本集。同时，文本被映射到分布空间中的CFP相关性分布，以对其与不同CFP的相关性进行建模。使用假设检验，根据样本集服从分布的程度来计算匹配分数。在两个文本到代码检索任务（即漏洞定位和代码搜索）和两个代码到文本检索任务（如漏洞知识检索和历史补丁检索）上的实验结果表明，DeepHT优于基线方法。,软件及其工程，软件创建和管理，基于搜索的软件工程,,,
8X6DUWV7,2023,https://doi.org/10.1145/3604609,TOSEM 2023,DeepPatch: Maintaining Deep Learning Model Programs to Retain Standard Accuracy with Substantial Robustness Improvement,"Maintaining a deep learning (DL) model by making the model substantially more robust through retraining with plenty of adversarial examples of non-trivial perturbation strength often reduces the model’s standard accuracy. Many existing model repair or maintenance techniques sacrifice standard accuracy to produce a large gain in robustness or vice versa. This article proposes DeepPatch, a novel technique to maintain filter-intensive DL models. To the best of our knowledge, DeepPatch is the first work to address the challenge of standard accuracy retention while substantially improving the robustness of DL models with plenty of adversarial examples of non-trivial and diverse perturbation strengths. Rather than following the conventional wisdom to generalize all the components of a DL model over the union set of clean and adversarial samples, DeepPatch formulates a novel division of labor method to adaptively activate a subset of its inserted processing units to process individual samples. Its produced model can generate the original or replacement feature maps in each forward pass of the patched model, making the patched model carry an intrinsic property of behaving like the model under maintenance on demand. The overall experimental results show that DeepPatch successfully retains the standard accuracy of all pretrained models while improving the robustness accuracy substantially. However, the models produced by the peer techniques suffer from either large standard accuracy loss or small robustness improvement compared with the models under maintenance, rendering them unsuitable in general to replace the latter.","Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software verification and validation,Software defect analysis,Software testing and debugging",DeepPatch：维护深度学习模型程序以保持标准精度并大幅提高鲁棒性,通过使用大量非平凡扰动强度的对抗性示例进行再训练，使深度学习（DL）模型变得更加健壮，从而维护该模型，这通常会降低模型的标准精度。许多现有的模型修复或维护技术牺牲了标准精度来产生鲁棒性的大增益，反之亦然。本文提出了DeepPatch，这是一种维护滤波器密集型DL模型的新技术。据我们所知，DeepPatch是第一个解决标准精度保持挑战的工作，同时通过大量非平凡和不同扰动强度的对抗性例子大大提高了DL模型的鲁棒性。DeepPatch没有遵循传统的智慧，将DL模型的所有组件推广到干净样本和对抗性样本的并集上，而是制定了一种新的分工方法，以自适应地激活其插入的处理单元的子集来处理单个样本。其生成的模型可以在修补模型的每个前向过程中生成原始或替换特征图，使修补模型具有与按需维护的模型相似的内在特性。总体实验结果表明，DeepPatch成功地保持了所有预训练模型的标准精度，同时显著提高了鲁棒性精度。然而，与维护中的模型相比，对等技术产生的模型要么标准精度损失大，要么鲁棒性改进小，这使得它们通常不适合取代后者。,软件及其工程，软件创建和管理，软件开发后问题，维护软件，软件验证和确认，软件缺陷分析，软件测试和调试,,,
9HRBPJWC,2023,https://doi.org/10.1145/3579643,TOSEM 2023,Rise of the Planet of Serverless Computing: A Systematic Review,"Serverless computing is an emerging cloud computing paradigm, being adopted to develop a wide range of software applications. It allows developers to focus on the application logic in the granularity of function, thereby freeing developers from tedious and error-prone infrastructure management. Meanwhile, its unique characteristic poses new challenges to the development and deployment of serverless-based applications. To tackle these challenges, enormous research efforts have been devoted. This article provides a comprehensive literature review to characterize the current research state of serverless computing. Specifically, this article covers 164 articles on 17 research directions of serverless computing, including performance optimization, programming framework, application migration, multi-cloud development, testing and debugging, and so on. It also derives research trends, focus, and commonly-used platforms for serverless computing, as well as promising research opportunities.","Computer systems organization,Architectures,Distributed architectures,Cloud computing,General and reference,Document types,Surveys and overviews",无服务器计算星球的崛起：系统综述,无服务器计算是一种新兴的云计算模式，被用于开发广泛的软件应用程序。它允许开发人员在功能的粒度上关注应用程序逻辑，从而将开发人员从繁琐且容易出错的基础设施管理中解放出来。同时，其独特的特性对基于无服务器的应用程序的开发和部署提出了新的挑战。为了应对这些挑战，我们投入了大量的研究工作。本文提供了一篇全面的文献综述，以描述无服务器计算的研究现状。具体而言，本文涵盖了164篇关于无服务器计算17个研究方向的文章，包括性能优化、编程框架、应用程序迁移、多云开发、测试和调试等，并得出了无服务器计算的研究趋势、重点和常用平台，以及有希望的研究机会。,计算机系统组织，体系结构，分布式体系结构，云计算，一般和参考，文档类型，调查和概述,,,
KBRDSAWU,2023,https://doi.org/10.1145/3522585,TOSEM 2023,ActivFORMS: A Formally Founded Model-based Approach to Engineer Self-adaptive Systems,"Self-adaptation equips a computing system with a feedback loop that enables it to deal with change caused by uncertainties during operation, such as changing availability of resources and fluctuating workloads. To ensure that the system complies with the adaptation goals, recent research suggests the use of formal techniques at runtime. Yet, existing approaches have three limitations that affect their practical applicability: (i) they ignore correctness of the behavior of the feedback loop, (ii) they rely on exhaustive verification at runtime to select adaptation options to realize the adaptation goals, which is time- and resource-demanding, and (iii) they provide limited or no support for changing adaptation goals at runtime. To tackle these shortcomings, we present ActivFORMS (Active FORmal Models for Self-adaptation). ActivFORMS contributes an end-to-end approach for engineering self-adaptive systems, spanning four main stages of the life cycle of a feedback loop: design, deployment, runtime adaptation, and evolution. We also present ActivFORMS-ta, a tool-supported instance of ActivFORMS that leverages timed automata models and statistical model checking at runtime. We validate the research results using an IoT application for building security monitoring that is deployed in Leuven. The experimental results demonstrate that ActivFORMS supports correctness of the behavior of the feedback loop, achieves the adaptation goals in an efficient way, and supports changing adaptation goals at runtime.","Software and its engineering,Software creation and management,Designing software,Software post-development issues,Maintaining software,Software verification and validation,Formal software verification,Software organization and properties,Software system structures",ActivFORMS：一种正式建立的基于模型的自适应系统工程方法,自适应为计算系统配备了一个反馈回路，使其能够处理操作过程中由不确定性引起的变化，例如资源可用性的变化和工作负载的波动。为了确保系统符合适应目标，最近的研究建议在运行时使用正式技术。然而，现有的方法有三个限制，影响了它们的实际适用性：（i）它们忽略了反馈回路行为的正确性，（ii）它们依赖于运行时的详尽验证来选择适应选项来实现适应目标，这需要时间和资源，以及（iii）它们对运行时改变适应目标提供有限或不支持。为了解决这些缺点，我们提出了ActivFORMS（用于自适应的主动FORmal模型）。ActivFORMS为工程自适应系统提供了一种端到端的方法，跨越了反馈回路生命周期的四个主要阶段：设计、部署、运行时自适应和进化。我们还介绍了ActivFORMS ta，这是一个支持ActivFORM的工具实例，它利用了时间自动机模型和运行时的统计模型检查。我们使用部署在鲁汶的用于建筑安全监控的物联网应用程序验证了研究结果。实验结果表明，ActivFORMS支持反馈回路行为的正确性，以有效的方式实现自适应目标，并支持在运行时更改自适应目标。,软件及其工程，软件创建和管理，软件设计，软件后开发问题，软件维护，软件验证和确认，正式软件验证，软件组织和属性，软件系统结构,,,
7F77EHIM,2023,https://doi.org/10.1145/3571852,TOSEM 2023,Open Source License Inconsistencies on GitHub,"Almost all software, open or closed, builds on open source software and therefore needs to comply with the license obligations of the open source code. Not knowing which licenses to comply with poses a legal danger to anyone using open source software. This article investigates the extent of inconsistencies between licenses declared by an open source project at the top level of the repository and the licenses found in the code. We analyzed a sample of 1,000 open source GitHub repositories. We find that about half of the repositories did not fully declare all licenses found in the code. Of these, approximately 10% represented a permissive vs. copyleft license mismatch. Furthermore, existing tools cannot fully identify licences. We conclude that users of open source code should not just look at the declared licenses of the open source code they intend to use, but rather examine the software to understand its actual licenses.","Social and professional topics,Computing / technology policy,Intellectual property,Licensing,Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Software development process management,Risk management",GitHub上的开源许可证不一致,几乎所有的软件，无论是开放的还是封闭的，都建立在开源软件之上，因此需要遵守开源代码的许可义务。不知道要遵守哪些许可证会对任何使用开源软件的人构成法律危险。本文调查了由存储库顶层的开源项目声明的许可证与代码中发现的许可证之间的不一致程度。我们分析了1000个开源GitHub存储库的样本。我们发现，大约一半的存储库没有完全声明代码中发现的所有许可证。其中，大约10%表示许可证与copyleft许可证不匹配。此外，现有工具无法完全识别许可证。我们得出的结论是，开源代码的用户不应该只查看他们打算使用的开源代码的声明许可证，而是应该检查软件以了解其实际许可证。,社会和专业主题，计算/技术政策，知识产权，许可，软件及其工程，软件创作和管理，软件开发中的合作，开放源码模型，软件开发过程管理，风险管理,,,
GXFQBYTS,2023,https://doi.org/10.1145/3533313,TOSEM 2023,Automated Identification of Uniqueness in JUnit Tests,"In the context of testing, descriptive test names are desirable because they document the purpose of tests and facilitate comprehension tasks during maintenance. Unfortunately, prior work has shown that tests often do not have descriptive names. To address this limitation, techniques have been developed to automatically generate descriptive names. However, they often generated names that are invalid or do not meet developer approval. To help address these limitations, we present a novel approach to extract the attributes of a given test that make it unique among its siblings. Because such attributes often serve as the basis for descriptive names, identifying them is an important first step towards improving test name generation approaches. To evaluate the approach, we created a prototype implementation for JUnit tests and compared its output with human judgment. The results of the evaluation demonstrate that the attributes identified by the approach are consistent with human judgment and are likely to be useful for future name generation techniques.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Theory of computation,Semantics and reasoning,Program reasoning,Program analysis",JUnit测试中唯一性的自动识别,在测试环境中，描述性测试名称是可取的，因为它们记录了测试的目的，并有助于维护期间的理解任务。不幸的是，先前的研究表明，测试通常没有描述性的名称。为了解决这一限制，已经开发了自动生成描述性名称的技术。然而，它们通常生成无效或不符合开发人员批准的名称。为了帮助解决这些限制，我们提出了一种新的方法来提取给定测试的属性，使其在兄弟测试中独一无二。因为这些属性通常是描述性名称的基础，识别它们是改进测试名称生成方法的重要第一步。为了评估这种方法，我们为JUnit测试创建了一个原型实现，并将其输出与人工判断进行了比较。评估结果表明，该方法确定的属性与人类判断一致，可能对未来的名称生成技术有用。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，计算理论，语义和推理，程序推理，程序分析,,,
NPNYM2FT,2023,https://doi.org/10.1145/3565799,TOSEM 2023,Retrieving API Knowledge from Tutorials and Stack Overflow Based on Natural Language Queries,"When encountering unfamiliar APIs, developers tend to seek help from API tutorials and Stack Overflow (SO). API tutorials help developers understand the API knowledge in a general context, while SO often explains the API knowledge in a specific programming task. Thus, tutorials and SO posts together can provide more API knowledge. However, it is non-trivial to retrieve API knowledge from both API tutorials and SO posts based on natural language queries. Two major problems are irrelevant API knowledge in two different resources and the lexical gap between the queries and documents. In this article, we regard a fragment in tutorials and a Question and Answering (Q&A) pair in SO as a knowledge item (KI). We generate ⟨ API, FRA⟩ pairs (FRA stands for fragment) from tutorial fragments and APIs and build ⟨ API, QA⟩ pairs based on heuristic rules of SO posts. We fuse ⟨ API, FRA⟩ pairs and ⟨ API, QA⟩ pairs to generate API knowledge (AK for short) datasets, where each data item is an ⟨ API, KI⟩ pair. We propose a novel approach, called PLAN, to automatically retrieve API knowledge from both API tutorials and SO posts based on natural language queries. PLAN contains three main stages: (1) API knowledge modeling, (2) query mapping, and (3) API knowledge retrieving. It first utilizes a deep-transfer-metric-learning-based relevance identification (DTML) model to effectively find relevant ⟨ API, KI⟩ pairs containing two different knowledge items (⟨ API, QA⟩ pairs and ⟨ API, FRA⟩ pairs) simultaneously. Then, PLAN generates several potential APIs as a way to reduce the lexical gap between the query and ⟨ API, KI⟩ pairs. According to potential APIs, we can select relevant ⟨ API, KI⟩ pairs to generate potential results. Finally, PLAN returns a list of ranked ⟨ API, KI⟩ pairs that are related to the query. We evaluate the effectiveness of PLAN with 270 queries on Java and Android AK datasets containing 10,072 ⟨ API, KI⟩ pairs. Our experimental results show that PLAN is effective and outperforms the state-of-the-art approaches. Our user study further confirms the effectiveness of PLAN in locating useful API knowledge.","Software and its engineering,Software creation and management,Search-based software engineering",基于自然语言查询从教程和堆栈溢出中检索API知识,当遇到不熟悉的API时，开发人员倾向于从API教程和堆栈溢出（SO）中寻求帮助。API教程帮助开发人员在一般上下文中理解API知识，而SO通常在特定编程任务中解释API知识。因此，教程和SO帖子一起可以提供更多API知识。然而，从API教程和基于自然语言查询的SO文章中检索API知识是不平凡的。两个主要问题是两个不同资源中的无关API知识以及查询和文档之间的词汇差距。在本文中，我们将教程中的一个片段和SO中的问答对视为知识项（KI）。我们从教程片段和API中生成API、FRA对（FRA代表片段），并基于SO帖子的启发式规则构建API、QA对。我们融合了API、FRA对和API、QA对，生成API知识（简称AK）数据集，其中每个数据项都是一个API、KI对。我们提出了一种新的方法，称为PLAN，基于自然语言查询，从API教程和SO文章中自动检索API知识。PLAN包含三个主要阶段：（1）API知识建模、（2）查询映射和（3）API知识检索。它首先利用基于深度传递度量学习的相关性识别（DTML）模型来有效地同时找到包含两个不同知识项的相关的API、KI对（API、QA对和API、FRA对）。然后，PLAN生成几个潜在的API，以减少查询和API、KI对之间的词汇差距。根据潜在的API，我们可以选择相关的API、KI对来生成潜在的结果。最后，PLAN返回与查询相关的排序的API、KI对的列表。我们在包含10072⟨API、KI⟩对的Java和Android AK数据集上用270个查询来评估PLAN的有效性。我们的实验结果表明，PLAN是有效的，并且优于最先进的方法。我们的用户研究进一步证实了PLAN在查找API有用知识方面的有效性。,软件及其工程，软件创建与管理，基于搜索的软件工程,,,
B8S3T4XE,2023,https://doi.org/10.1145/3582571,TOSEM 2023,Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems,"Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores such that learning from easy to difficult data can be performed. To evaluate LATTICE, we use five publicly available datasets collected from five real-world CPS testbeds. We compare LATTICE with ATTAIN and two other state-of-the-art anomaly detectors. Evaluation results show that LATTICE outperforms the three baselines and ATTAIN by 0.906%-2.367% in terms of the F1 score. LATTICE also, on average, reduces the training time of ATTAIN by 4.2% on the five datasets and is on par with the baselines in terms of detection delay time.","Computer systems organization,Embedded and cyber-physical systems,Embedded systems,Sensors and actuators,Computing methodologies,Machine learning,Learning paradigms,Supervised learning,Supervised learning by classification,Learning settings,Online learning settings,Machine learning approaches,Neural networks,Security and privacy,Intrusion/anomaly detection and malware mitigation,Intrusion detection systems,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software",基于数字孪生的网络物理系统课程学习异常检测,异常检测对于确保网络物理系统（CPS）的安全至关重要。然而，由于攻击和CPS本身的复杂性不断增加，CPS中的异常检测变得越来越具有挑战性。在我们之前的工作中，我们提出了一种基于数字孪生的异常检测方法，称为ATTAIN，它利用了CPS的历史和实时数据。然而，这些数据在难度方面差异很大。因此，与人类的学习过程类似，深度学习模型（例如ATTAIN）可以受益于从易到难的课程。为此，在本文中，我们提出了一种新的方法，称为基于数字孪生的Anomaly deTecTion wIth Curriculum lEarning（LATTICE），该方法通过引入课程学习来扩展ATTAIN，以优化其学习范式。LATTICE在将每个样本输入训练调度器之前，为其赋予一个难度分数。训练调度器基于这些难度分数对训练数据的批次进行采样，使得可以执行从易到难的数据的学习。为了评估LATTICE，我们使用了从五个真实世界的CPS测试台收集的五个公开可用的数据集。我们将LATTICE与ATTAIN以及另外两个最先进的异常检测器进行了比较。评估结果表明，LATTICE在F1得分方面优于三个基线和ATTAIN 0.906%-2.367%。LATTICE还平均将ATTAIN在五个数据集上的训练时间减少了4.2%，并且在检测延迟时间方面与基线持平。,计算机系统组织，嵌入式和网络物理系统，嵌入式系统，传感器和执行器，计算方法，机器学习，学习范例，监督学习，分类监督学习，学习设置，在线学习设置，机器学习方法，神经网络，安全和隐私，入侵/异常检测和恶意软件缓解，入侵检测系统，软件及其工程，软件创建和管理，软件后开发问题，维护软件,,,
QZ3NUWJQ,2023,https://doi.org/10.1145/3518994,TOSEM 2023,LiDetector: License Incompatibility Detection for Open Source Software,"Open-source software (OSS) licenses dictate the conditions, which should be followed to reuse, distribute, and modify software. Apart from widely-used licenses such as the MIT License, developers are also allowed to customize their own licenses (called custom license), whose descriptions are more flexible. The presence of such various licenses imposes challenges to understand licenses and their compatibility. To avoid financial and legal risks, it is essential to ensure license compatibility when integrating third-party packages or reusing code accompanied with licenses. In this work, we propose LiDetector, an effective tool that extracts and interprets OSS licenses (including both official licenses and custom licenses), and detects license incompatibility among these licenses. Specifically, LiDetector introduces a learning-based method to automatically identify meaningful license terms from an arbitrary license, and employs Probabilistic Context-Free Grammar (PCFG) to infer rights and obligations for incompatibility detection. Experiments demonstrate that LiDetector outperforms existing methods with 93.28% precision for term identification, and 91.09% accuracy for right and obligation inference, and can effectively detect incompatibility with 10.06% FP rate and 2.56% FN rate. Furthermore, with LiDetector, our large-scale empirical study on 1,846 projects reveals that 72.91% of the projects are suffering from license incompatibility, including popular ones such as the MIT License and the Apache License. We highlighted lessons learned from perspectives of different stakeholders and made all related data and the replication package publicly available to facilitate follow-up research.","Software and its engineering,Software creation and management,Collaboration in software development,Open source model,Software development techniques,Reusability,Software notations and tools,Software libraries and repositories",LiDetector：用于开源软件的许可证不兼容检测,开放源码软件（OSS）许可证规定了重复使用、分发和修改软件应遵循的条件。除了广泛使用的许可证（如MIT许可证）外，开发人员还可以自定义自己的许可证，这些许可证的描述更加灵活。这种各种许可证的存在给理解许可证及其兼容性带来了挑战。为了避免财务和法律风险，在集成第三方软件包或重复使用许可证附带的代码时，确保许可证兼容性至关重要。在这项工作中，我们提出了LiDetector，这是一个有效的工具，可以提取和解释OSS许可证（包括官方许可证和自定义许可证），并检测这些许可证之间的许可证不兼容。具体而言，LiDetector引入了一种基于学习的方法来从任意许可证中自动识别有意义的许可条款，并使用概率上下文无关语法（PCFG）来推断不兼容检测的权利和义务。实验表明，LiDetector在术语识别方面的准确率为93.28%，在权利和义务推断方面的准确度为91.09%，优于现有方法，并且可以有效地检测不兼容性，FP率为10.06%，FN率为2.56%。此外，通过LiDetector，我们对1846个项目的大规模实证研究表明，72.91%的项目存在许可证不兼容的问题，包括MIT许可证和Apache许可证等流行项目。我们强调了从不同利益攸关方的角度吸取的经验教训，并公开了所有相关数据和复制包，以促进后续研究。,软件及其工程，软件创建和管理，软件开发协作，开放源码模型，软件开发技术，可重用性，软件符号和工具，软件库和存储库,,,
H27Y3YD5,2023,https://doi.org/10.1145/3517192,TOSEM 2023,Semantics Foundation for Cyber-physical Systems Using Higher-order UTP,"Model-based design has become the predominant approach to the design of hybrid and cyber-physical systems (CPSs). It advocates the use of mathematically founded models to capture heterogeneous digital and analog behaviours from domain-specific formalisms, allowing all engineering tasks of verification, code synthesis, and validation to be performed within a single semantic body. Guaranteeing the consistency among the different views and heterogeneous models of a system at different levels of abstraction, however, poses significant challenges. To address these issues, Hoare and He’s Unifying Theories of Programming (UTP) proposes a calculus to capture domain-specific programming and modelling paradigms into a unified semantic framework. Our goal is to extend UTP to form a semantic foundation for CPS design. Higher-order UTP (HUTP) is a conservative extension to Hoare and He’s theory that supports the specification of discrete, real-time, and continuous dynamics, concurrency and communication, and higher-order quantification. Within HUTP, we define a calculus of normal hybrid designs to model, analyse, compose, refine, and verify heterogeneous hybrid system models. In addition, we define respective formal semantics for Hybrid Communicating Sequential Processes and Simulink using HUTP.","Computer systems organization,Embedded and cyber-physical systems,Real-time systems,Computing methodologies,Modeling and simulation,Model development and analysis,Software and its engineering,Software organization and properties,Theory of computation,Models of computation,Timed and hybrid models,Semantics and reasoning,Program semantics,Denotational semantics",使用高阶UTP的网络物理系统的语义基础,基于模型的设计已经成为混合和网络物理系统（CPSs）设计的主要方法。它提倡使用数学建模来从特定领域的形式主义中捕捉异构的数字和模拟行为，允许在单个语义体中执行验证、代码合成和验证的所有工程任务。然而，在不同抽象级别上保证系统的不同视图和异构模型之间的一致性，带来了重大挑战。为了解决这些问题，霍尔和何的统一编程理论（UTP）提出了一种微积分，将特定领域的编程和建模范式捕获到一个统一的语义框架中。我们的目标是扩展UTP以形成CPS设计的语义基础。高阶UTP（HUTP）是霍尔和何理论的保守扩展，支持离散、实时和连续动力学、并发和通信以及高阶量化的规范。在HUTP中，我们定义了一个普通混合设计的演算，用于建模、分析、组合、精化和验证异构混合系统模型。此外，我们使用HUTP定义了混合通信顺序过程和Simulink各自的形式语义。,计算机系统组织，嵌入式和网络物理系统，实时系统，计算方法，建模和仿真，模型开发和分析，软件及其工程，软件组织和性质，计算理论，计算模型，时间和混合模型，语义和推理，程序语义学，外延语义学,,,
SWTJI3IT,2023,https://doi.org/10.1145/3582574,TOSEM 2023,Toward Interpretable Graph Tensor Convolution Neural Network for Code Semantics Embedding,"Intelligent deep learning-based models have made significant progress for automated source code semantics embedding, and current research works mainly leverage natural language-based methods and graph-based methods. However, natural language-based methods do not capture the rich semantic structural information of source code, and graph-based methods do not utilize rich distant information of source code due to the high cost of message-passing steps.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation,Software notations and tools,General programming languages,Language features,Software organization and properties,Extra-functional properties,Software safety,Software system structures,Software system models,Feature interaction",面向代码语义嵌入的可解释图张量卷积神经网络,基于智能深度学习的模型在自动源代码语义嵌入方面取得了重大进展，目前的研究工作主要利用基于自然语言的方法和基于图的方法。然而，基于自然语言的方法没有捕捉到源代码丰富的语义结构信息，并且由于消息传递步骤的高成本，基于图的方法没有利用源代码的丰富的远程信息。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和验证，软件符号和工具，通用编程语言，语言特征，软件组织和属性，额外功能属性，软件安全，软件系统结构，软件系统模型，功能交互,,,
XRM8YZRC,2023,https://doi.org/10.1145/3534117,TOSEM 2023,On the Significance of Category Prediction for Code-Comment Synchronization,"Software comments sometimes are not promptly updated in sync when the associated code is changed. The inconsistency between code and comments may mislead the developers and result in future bugs. Thus, studies concerning code-comment synchronization have become highly important, which aims to automatically synchronize comments with code changes. Existing code-comment synchronization approaches mainly contain two types, i.e., (1) deep learning-based (e.g., CUP), and (2) heuristic-based (e.g., HebCUP). The former constructs a neural machine translation-structured semantic model, which has a more generalized capability on synchronizing comments with software evolution and growth. However, the latter designs a series of rules for performing token-level replacements on old comments, which can generate the completely correct comments for the samples fully covered by their fine-designed heuristic rules. In this article, we propose a composite approach named CBS (i.e., Classifying Before Synchronizing) to further improve the code-comment synchronization performance, which combines the advantages of CUP and HebCUP with the assistance of inferred categories of Code-Comment Inconsistent (CCI) samples. Specifically, we firstly define two categories (i.e., heuristic-prone and non-heuristic-prone) for CCI samples and propose five features to assist category prediction. The samples whose comments can be correctly synchronized by HebCUP are heuristic-prone, while others are non-heuristic-prone. Then, CBS employs our proposed Multi-Subsets Ensemble Learning (MSEL) classification algorithm to alleviate the class imbalance problem and construct the category prediction model. Next, CBS uses the trained MSEL to predict the category of the new sample. If the predicted category is heuristic-prone, CBS employs HebCUP to conduct the code-comment synchronization for the sample, otherwise, CBS allocates CUP to handle it. Our extensive experiments demonstrate that CBS statistically significantly outperforms CUP and HebCUP, and obtains an average improvement of 23.47%, 22.84%, 3.04%, 3.04%, 1.64%, and 19.39% in terms of Accuracy, Recall@5, Average Edit Distance (AED), Relative Edit Distance (RED), BLEU-4, and Effective Synchronized Sample (ESS) ratio, respectively, which highlights that category prediction for CCI samples can boost the code-comment synchronization performance.","Software and its engineering,Software notations and tools,Software maintenance tools",论范畴预测对代码注释同步的意义,当相关代码发生更改时，软件注释有时不会及时同步更新。代码和注释之间的不一致可能会误导开发人员，并导致未来的错误。因此，关于代码注释同步的研究变得非常重要，其目的是使注释与代码更改自动同步。现有的代码注释同步方法主要包括两种类型，即（1）基于深度学习的（例如CUP）和（2）基于启发式的（例如HebCUP）。前者构建了一个神经机器翻译结构化语义模型，该模型在将注释与软件进化和增长同步方面具有更广泛的能力。然而，后者设计了一系列规则来对旧评论进行标记级替换，可以为其精心设计的启发式规则完全覆盖的样本生成完全正确的评论。在本文中，我们提出了一种名为CBS的复合方法（即同步前分类），以进一步提高代码注释同步性能，该方法结合了CUP和HebCUP的优势，并借助于代码注释不一致（CCI）样本的推断类别。具体来说，我们首先为CCI样本定义了两个类别（即启发式倾向和非启发式倾向），并提出了五个特征来辅助类别预测。HebCUP可以正确同步其评论的样本具有启发式倾向，而其他样本则不具有启发式倾向。然后，CBS采用我们提出的多子集集成学习（MSEL）分类算法来缓解类别不平衡问题，并构建类别预测模型。接下来，CBS使用训练后的MSEL来预测新样本的类别。如果预测的类别是启发式的，CBS使用HebCUP对样本进行代码注释同步，否则，CBS分配CUP来处理。我们的大量实验表明，CBS在统计上显著优于CUP和HebCUP，在准确性方面平均提高了23.47%、22.84%、3.04%、3.04%和1.64%，Recall@5，平均编辑距离（AED）、相对编辑距离（RED）、BLEU-4和有效同步样本（ESS）比率，这突出表明CCI样本的类别预测可以提高代码注释同步性能。,软件及其工程，软件符号和工具，软件维护工具,,,
8UF473LP,2023,https://doi.org/10.1145/3561382,TOSEM 2023,Seeing the Whole Elephant: Systematically Understanding and Uncovering Evaluation Biases in Automated Program Repair,"Evaluation is the foundation of automated program repair (APR), as it provides empirical evidence on strengths and weaknesses of APR techniques. However, the reliability of such evaluation is often threatened by various introduced biases. Consequently, bias exploration, which uncovers biases in the APR evaluation, has become a pivotal activity and performed since the early years when pioneer APR techniques were proposed. Unfortunately, there is still no methodology to support a systematic comprehension and discovery of evaluation biases in APR, which impedes the mitigation of such biases and threatens the evaluation of APR techniques.","Software and its engineering,Software creation and management,Software development techniques,Automatic programming,Software verification and validation,Software defect analysis,Software testing and debugging",通观全局：系统认识和揭示程序自动修复中的评价偏差,评估是自动程序修复（APR）的基础，因为它为APR技术的优势和劣势提供了经验证据。然而，这种评估的可靠性经常受到各种引入的偏见的威胁。因此，揭示APR评估中偏差的偏差探索已成为一项关键活动，自早期提出先驱APR技术以来一直在进行。不幸的是，仍然没有方法支持系统地理解和发现APR中的评估偏差，这阻碍了这种偏差的缓解，并威胁到APR技术的评估。,软件及其工程，软件创建和管理，软件开发技术，自动编程，软件验证和确认，软件缺陷分析，软件测试和调试,,,
8AHEKEYF,2023,https://doi.org/10.1145/3604608,TOSEM 2023,Towards Practical Binary Code Similarity Detection: Vulnerability Verification via Patch Semantic Analysis,"Vulnerability is a major threat to software security. It has been proven that binary code similarity detection approaches are efficient to search for recurring vulnerabilities introduced by code sharing in binary software. However, these approaches suffer from high false-positive rates (FPRs) since they usually take the patched functions as vulnerable, and they usually do not work well when binaries are compiled with different compilation settings. ","Security and privacy,Software and application security,Software security engineering",面向实用的二进制代码相似性检测：基于补丁语义分析的漏洞验证,漏洞是软件安全的主要威胁。已经证明，二进制代码相似性检测方法对于搜索二进制软件中代码共享引入的重复漏洞是有效的。然而，这些方法的假阳性率（FPR）很高，因为它们通常将修补的函数视为易受攻击的函数，并且当使用不同的编译设置编译二进制文件时，它们通常不能很好地工作。,安全和隐私，软件和应用程序安全，软件安全工程,,,
8BQBF54C,2023,https://doi.org/10.1145/3517036,TOSEM 2023,Scanner++: Enhanced Vulnerability Detection of Web Applications with Attack Intent Synchronization,"Scanners are commonly applied for detecting vulnerabilities in web applications. Various scanners with different strategies are widely in use, but their performance is challenged by the increasing diversity of target applications that have more complex attack surfaces (i.e., website paths) and covert vulnerabilities that can only be exploited by more sophisticated attack vectors (i.e., payloads). In this paper, we propose Scanner++, a framework that improves web vulnerability detection of existing scanners through combining their capabilities with attack intent synchronization. We design Scanner++ as a proxy-based architecture while using a package-based intent synchronization approach. Scanner++ first uses a purification mechanism to aggregate and refine attack intents, consisting of attack surfaces and attack vectors extracted from the base scanners’ request packets. Then, Scanner++ uses a runtime intent synchronization mechanism to select relevant attack intents according to the scanners’ detection spots to guide their scanning process. Consequently, base scanners can expand their attack surfaces, generate more diverse attack vectors and achieve better vulnerability detection performance.","Security and privacy,Intrusion/anomaly detection and malware mitigation,Software and application security,Web application security,Systems security,Operating systems security,Vulnerability management,Vulnerability scanners,Social and professional topics,Computing / technology policy,Computer crime,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",Scanner++：具有攻击意图同步的增强型Web应用程序漏洞检测,扫描仪通常用于检测web应用程序中的漏洞。具有不同策略的各种扫描仪正在广泛使用，但其性能受到目标应用程序日益多样化的挑战，这些应用程序具有更复杂的攻击面（即网站路径）和只能被更复杂的攻击者（即有效载荷）利用的隐蔽漏洞。在本文中，我们提出了Scanner++，这是一个框架，通过将现有扫描仪的功能与攻击意图同步相结合，改进了现有扫描仪的web漏洞检测。我们将Scanner++设计为基于代理的架构，同时使用基于包的意向同步方法。Scanner++首先使用一种净化机制来聚合和细化攻击意图，包括从基本扫描程序的请求包中提取的攻击表面和攻击向量。然后，Scanner++使用运行时意图同步机制，根据扫描仪的检测点选择相关的攻击意图，以指导其扫描过程。因此，基础扫描仪可以扩展其攻击面，生成更多样的攻击向量，并实现更好的漏洞检测性能。,安全和隐私，入侵/异常检测和恶意软件缓解，软件和应用程序安全，Web应用程序安全，系统安全，操作系统安全，漏洞管理，漏洞扫描器，社会和专业主题，计算/技术政策，计算机犯罪，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
DX6BSBM2,2023,https://doi.org/10.1145/3579638,TOSEM 2023,Extraction of Phrase-based Concepts in Vulnerability Descriptions through Unsupervised Labeling,"Software vulnerabilities, once disclosed, can be documented in vulnerability databases, which have great potential to advance vulnerability analysis and security research. People describe the key characteristics of software vulnerabilities in natural language mixed with domain-specific names and concepts. This textual nature poses a significant challenge for the automatic analysis of vulnerability knowledge embedded in text. Automatic extraction of key vulnerability aspects is highly desirable but demands significant effort to manually label data for model training. ","Computer systems organization,Dependable and fault-tolerant systems and networks,Redundancy,Embedded and cyber-physical systems,Embedded systems",通过无监督标记提取漏洞描述中基于短语的概念,软件漏洞一旦公开，就可以记录在漏洞数据库中，这对推进漏洞分析和安全研究具有巨大潜力。人们用混合了特定域名和概念的自然语言描述软件漏洞的关键特征。这种文本性质对嵌入文本中的漏洞知识的自动分析提出了重大挑战。关键漏洞方面的自动提取是非常可取的，但需要付出大量努力手动标记数据以进行模型训练。,计算机系统组织，可靠和容错系统和网络，冗余，嵌入式和网络物理系统，嵌入式系统,,,
5PZE7LEQ,2023,https://doi.org/10.1145/3587154,TOSEM 2023,Structured Theorem for Quantum Programs and its Applications,"This article proves a structured program theorem for flowchart quantum programs. The theorem states that any flowchart quantum program is equivalent to a single quantum program that repeatedly executes a quantum measurement and a subprogram, so long as the measurement outcome is true. Moreover, their expected runtime, variance, and general moments are the same. This theorem simplifies the quantum program’s verification significantly.","Theory of computation,Logic,Hoare logic,Models of computation,Quantum computation theory,Semantics and reasoning,Program reasoning,Program verification",量子程序的结构定理及其应用,本文证明了流程量子程序的一个结构化程序定理。该定理指出，只要测量结果为真，任何流程图量子程序都等效于重复执行量子测量和子程序的单个量子程序。此外，它们的预期运行时间、方差和一般矩是相同的。这个定理大大简化了量子程序的验证。,计算理论，逻辑，霍尔逻辑，计算模型，量子计算理论，语义学和推理，程序推理，程序验证,,,
D9XJS9S2,2023,https://doi.org/10.1145/3571854,TOSEM 2023,Continuous Integration and Delivery Practices for Cyber-Physical Systems: An Interview-Based Study,"Continuous Integration and Delivery (CI/CD) practices have shown several benefits for software development and operations, such as faster release cycles and early discovery of defects. For Cyber-Physical System (CPS) development, CI/CD can help achieving required goals, such as high dependability, yet it may be challenging to apply. This article empirically investigates challenges, barriers, and their mitigation occurring when applying CI/CD practices to develop CPSs in 10 organizations working in eight different domains. The study has been conducted through semi-structured interviews, by applying an open card sorting procedure together with a member-checking survey within the same organizations, and by validating the results through a further survey involving 55 professional developers. The study reveals several peculiarities in the application of CI/CD to CPSs. These include the need for (i) combining continuous and periodic builds while balancing the use of Hardware-in-the-Loop and simulators, (ii) coping with difficulties in software deployment (iii) accounting for simulators and Hardware-in-the-Loop differing in their behavior, and (vi) combining hardware/software expertise in the development team. Our findings open the road toward recommenders aimed at supporting the setting and evolution of CI/CD pipelines, as well as university curricula requiring interdisciplinarity, such as knowledge about hardware, software, and their interplay.","Social and professional topics,Software and its engineering,Software creation and management,Software development process management,Software development methods,Agile software development,Software post-development issues,Maintaining software,Software verification and validation,Software defect analysis,Software notations and tools,Software configuration management and version control systems,Software organization and properties,Software system structures",网络物理系统的持续集成与交付实践：基于访谈的研究,持续集成和交付（CI/CD）实践已经显示出软件开发和运营的几个好处，例如更快的发布周期和早期发现缺陷。对于网络物理系统（CPS）的开发，CI/CD可以帮助实现所需的目标，例如高可靠性，但应用起来可能很有挑战性。本文实证研究了在八个不同领域的10个组织中应用CI/CD实践来开发CPSs时出现的挑战、障碍及其缓解措施。这项研究是通过半结构化访谈进行的，采用开放式卡片分类程序和同一组织内的成员检查调查，并通过涉及55名专业开发人员的进一步调查验证结果。该研究揭示了CI/CD在消费品安全应用中的几个特点。其中包括需要（i）在平衡硬件在环和模拟器使用的同时，结合连续和定期构建，（ii）应对软件部署中的困难，（iii）考虑模拟器和硬件在环的不同行为，以及（vi）结合开发团队中的硬件/软件专业知识。我们的研究结果为推荐人开辟了道路，旨在支持CI/CD管道的设置和发展，以及需要跨学科的大学课程，如硬件、软件及其相互作用的知识。,社会和专业主题，软件及其工程，软件创建和管理，软件开发过程管理，软件开发方法，敏捷软件开发，软件后开发问题，维护软件，软件验证和确认，软件缺陷分析，软件符号和工具，软件配置管理和版本控制系统，软件组织和属性，软件系统结构,,,
QEA8RN2Q,2023,https://doi.org/10.1145/3532183,TOSEM 2023,"Microservice Security Metrics for Secure Communication, Identity Management, and Observability","Microservice architectures are increasingly being used to develop application systems. Despite many guidelines and best practices being published, architecting microservice systems for security is challenging. Reasons are the size and complexity of microservice systems, their polyglot nature, and the demand for the continuous evolution of these systems. In this context, to manually validate that security architecture tactics are employed as intended throughout the system is a time-consuming and error-prone task. In this article, we present an approach to avoid such manual validation before each continuous evolution step in a microservice system, which we demonstrate using three widely used categories of security tactics: secure communication, identity management, and observability. Our approach is based on a review of existing security guidelines, the gray literature, and the scientific literature, from which we derived Architectural Design Decisions (ADDs) with the found security tactics as decision options. In our approach, we propose novel detectors to detect these decision options automatically and formally defined metrics to measure the conformance of a system to the different options of the ADDs. We apply the approach to a case study data set of 10 open source microservice systems, plus another 20 variants of these systems, for which we manually inspected the source code for security tactics. We demonstrate and assess the validity and appropriateness of our metrics by performing an assessment of their conformance to the ADDs in our systems’ dataset through statistical methods.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software organization and properties,Software system structures,Distributed systems organizing principles,Software architectures",用于安全通信、身份管理和可观测性的微服务安全度量,微服务体系结构正越来越多地用于开发应用系统。尽管已经发布了许多指导方针和最佳实践，但为安全性构建微服务系统仍然具有挑战性。原因是微服务系统的规模和复杂性，它们的多语言性质，以及对这些系统不断发展的需求。在这种情况下，手动验证安全体系结构策略是否在整个系统中按预期使用是一项耗时且容易出错的任务。在本文中，我们提出了一种在微服务系统的每个连续进化步骤之前避免这种手动验证的方法，我们使用三类广泛使用的安全策略进行了演示：安全通信、身份管理和可观察性。我们的方法基于对现有安全指南、灰色文献和科学文献的审查，从中我们得出了建筑设计决策（ADDs），并将发现的安全策略作为决策选项。在我们的方法中，我们提出了新的检测器来自动检测这些决策选项，并正式定义度量，以衡量系统对ADDs的不同选项的一致性。我们将该方法应用于10个开源微服务系统的案例研究数据集，以及这些系统的另外20个变体，我们手动检查了其源代码的安全策略。我们通过统计方法评估指标与系统数据集中ADDs的一致性，来证明和评估指标的有效性和适当性。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件组织和属性，软件系统结构，分布式系统组织原则，软件体系结构,,,
A6GVUQYY,2023,https://doi.org/10.1145/3546066,TOSEM 2023,deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search,"With the rapid increase of public code repositories, developers maintain a great desire to retrieve precise code snippets by using natural language. Despite existing deep learning-based approaches that provide end-to-end solutions (i.e., accept natural language as queries and show related code fragments), the performance of code search in the large-scale repositories is still low in accuracy because of the code representation (e.g., AST) and modeling (e.g., directly fusing features in the attention stage). ","Software and its engineering,Software creation and management,Software development techniques",deGraphCS:用于神经代码搜索的嵌入基于变量的流图,随着公共代码库的快速增长，开发人员一直渴望通过使用自然语言来检索精确的代码片段。尽管现有的基于深度学习的方法提供端到端的解决方案（即，接受自然语言作为查询并显示相关的代码片段），但由于代码表示（例如，AST）和建模（例如，在注意力阶段直接融合特征），大规模存储库中的代码搜索性能的准确性仍然很低。,软件及其工程，软件创建和管理，软件开发技术,,,
ALT6P4LV,2023,https://doi.org/10.1145/3597205,TOSEM 2023,Open Problems in Fuzzing RESTful APIs: A Comparison of Tools,"RESTful APIs are a type of web service that are widely used in industry. In the past few years, a lot of effort in the research community has been spent in designing novel techniques to automatically fuzz those APIs to find faults in them. Many real faults were automatically found in a large variety of RESTful APIs. However, usually the analyzed fuzzers treat the APIs as black-box, and no analysis of what is actually covered in these systems is done. Therefore, although these fuzzers are clearly useful for practitioners, we do not know their current limitations and actual effectiveness. Solving this is a necessary step to be able to design better, more efficient, and effective techniques. To address this issue, in this article we compare seven state-of-the-art fuzzers on 18 open source—1 industrial and 1 artificial—RESTful APIs. We then analyze the source code for which parts of these APIs the fuzzers fail to generate tests. This analysis points to clear limitations of these current fuzzers, listing concrete follow-up challenges for the research community.","Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation",RESTful API模糊化中的开放问题：工具比较,RESTful API是一种在工业中广泛使用的web服务。在过去的几年里，研究界花了很多精力来设计新的技术，自动模糊这些API以查找其中的故障。许多真正的错误是在各种RESTful API中自动发现的。然而，通常被分析的模糊器将API视为黑盒，并且没有对这些系统中实际涵盖的内容进行分析。因此，尽管这些模糊器显然对从业者有用，但我们不知道它们目前的局限性和实际有效性。解决这一问题是设计更好、更高效和有效技术的必要步骤。为了解决这个问题，在本文中，我们比较了18个开源API（1个工业API和1个人工RESTful API）上的7个最先进的模糊器。然后，我们分析源代码，模糊器无法为这些API的哪些部分生成测试。这一分析指出了当前模糊论的明显局限性，列出了研究界面临的具体后续挑战。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认,,,
UXDTNSXQ,2023,https://doi.org/10.1145/3585009,TOSEM 2023,White-Box Fuzzing RPC-Based APIs with EvoMaster: An Industrial Case Study,"Remote Procedure Call (RPC) is a communication protocol to support client-server interactions among services over a network. RPC is widely applied in industry for building large-scale distributed systems, such as Microservices. Modern RPC frameworks include, for example, Thrift, gRPC, SOFARPC, and Dubbo. Testing such systems using RPC communications is very challenging, due to the complexity of distributed systems and various RPC frameworks the system could employ. To the best of our knowledge, there does not exist any tool or solution that could enable automated testing of modern RPC-based services. To fill this gap, in this article we propose the first approach in the literature, together with an open source tool, for fuzzing modern RPC-based APIs. The approach is in the context of white-box testing with search-based techniques. To tackle schema extraction of various RPC frameworks, we formulate a RPC schema specification along with a parser that allows the extraction from source code of any JVM RPC-based APIs. Then, with the extracted schema we employ a search to produce tests by maximizing white-box heuristics and newly defined heuristics specific to the RPC domain. We built our approach as an extension to an open source fuzzer (i.e., EvoMaster), and the approach has been integrated into a real industrial pipeline that could be applied to a real industrial development process for fuzzing RPC-based APIs. To assess our novel approach, we conducted an empirical study with two artificial and four industrial web services selected by our industrial partner. In addition, to further demonstrate its effectiveness and application in industrial settings, we report results of employing our tool for fuzzing another 50 industrial APIs autonomously conducted by our industrial partner in their testing processes. Results show that our novel approach is capable of enabling automated test case generation for industrial RPC-based APIs (i.e., 2 artificial and 54 industrial). We also compared with a simple gray-box technique and existing manually written tests. Our white-box solution achieves significant improvements on code coverage. Regarding fault detection, by conducting a careful review with our industrial partner of the tests generated by our novel approach in the selected four industrial APIs, a total of 41 real faults were identified, which have now been fixed. Another 8,377 detected faults are currently under investigation.","Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation",EvoMaster实现基于RPC的白盒模糊API：一个工业案例研究,远程过程调用（RPC）是一种通信协议，用于支持网络上服务之间的客户端-服务器交互。RPC在工业上被广泛应用于构建大规模分布式系统，如微服务。现代RPC框架包括例如Thrift、gRPC、SOFARPC和Dubbo。由于分布式系统和系统可能采用的各种RPC框架的复杂性，使用RPC通信测试这样的系统是非常具有挑战性的。据我们所知，目前还没有任何工具或解决方案可以实现对现代基于RPC的服务的自动化测试。为了填补这一空白，在本文中，我们提出了文献中的第一种方法，以及一个开源工具，用于模糊化现代基于RPC的API。该方法是在白盒测试的背景下使用基于搜索的技术。为了解决各种RPC框架的模式提取问题，我们制定了一个RPC模式规范和一个解析器，该解析器允许从任何基于JVM RPC的API的源代码中提取。然后，使用提取的模式，我们通过最大化白盒启发式和RPC域特有的新定义的启发式来使用搜索来生成测试。我们将我们的方法构建为开源模糊器（即EvoMaster）的扩展，该方法已集成到一个真实的工业管道中，该管道可应用于基于RPC的API的模糊化的真实工业开发过程。为了评估我们的新方法，我们对我们的工业合作伙伴选择的两个人工和四个工业web服务进行了实证研究。此外，为了进一步证明其在工业环境中的有效性和应用，我们报告了使用我们的工具对我们的工业合作伙伴在其测试过程中自主进行的另外50种工业API进行模糊化的结果。结果表明，我们的新方法能够为基于RPC的工业API（即2个人工API和54个工业API）实现自动化测试用例生成。我们还与简单的灰盒技术和现有的手动编写的测试进行了比较。我们的白盒解决方案显著提高了代码覆盖率。关于故障检测，通过与我们的工业合作伙伴对我们的新方法在选定的四个工业API中生成的测试进行仔细审查，共发现了41个实际故障，这些故障现已修复。另外8377个检测到的故障目前正在调查中。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认,,,
PYMXDJ6R,2023,https://doi.org/10.1145/3593801,TOSEM 2023,JavaScript SBST Heuristics to Enable Effective Fuzzing of NodeJS Web APIs,"JavaScript is one of the most popular programming languages. However, its dynamic nature poses several challenges to automated testing techniques. In this paper, we propose an approach and open-source tool support to enable white-box testing of JavaScript applications using Search-Based Software Testing (SBST) techniques. We provide an automated approach to collect search-based heuristics like the common Branch Distance and to enable Testability Transformations. To empirically evaluate our results, we integrated our technique into the EvoMaster test generation tool, and carried out analyses on the automated system testing of RESTful and GraphQL APIs. Experiments on eight Web APIs running on NodeJS show that our technique leads to significantly better results than existing black-box and grey-box testing tools, in terms of code coverage and fault detection.","Software and its engineering,Software creation and management,Search-based software engineering,Software verification and validation",JavaScript SBST启发式实现NodeJS Web API的有效模糊,JavaScript是最流行的编程语言之一。然而，它的动态特性对自动化测试技术提出了一些挑战。在本文中，我们提出了一种方法和开源工具支持，以使用基于搜索的软件测试（SBST）技术实现JavaScript应用程序的白盒测试。我们提供了一种自动方法来收集基于搜索的启发式方法，如公共分支距离，并启用可测试性转换。为了实证评估我们的结果，我们将我们的技术集成到EvoMaster测试生成工具中，并对RESTful和GraphQLAPI的自动化系统测试进行了分析。在NodeJS上运行的八个Web API上的实验表明，在代码覆盖率和故障检测方面，我们的技术比现有的黑盒和灰盒测试工具取得了更好的结果。,软件及其工程，软件创建和管理，基于搜索的软件工程，软件验证和确认,,,
YXLHQ2WV,2023,https://doi.org/10.1145/3569936,TOSEM 2023,DAISY: Dynamic-Analysis-Induced Source Discovery for Sensitive Data,"Mobile apps are widely used and often process users’ sensitive data. Many taint analysis tools have been applied to analyze sensitive information flows and report data leaks in apps. These tools require a list of sources (where sensitive data is accessed) as input, and researchers have constructed such lists within the Android platform by identifying Android API methods that allow access to sensitive data. However, app developers may also define methods or use third-party library’s methods for accessing data. It is difficult to collect such source methods, because they are unique to the apps, and there are a large number of third-party libraries available on the market that evolve over time. To address this problem, we propose DAISY, a Dynamic-Analysis-Induced Source discoverY approach for identifying methods that return sensitive information from apps and third-party libraries. Trained on an automatically labeled dataset of methods and their calling context, DAISY identifies sensitive methods in unseen apps. We evaluated DAISY on real-world apps, and the results show that DAISY can achieve an overall precision of 77.9% when reporting the most confident results. Most of the identified sources and leaks cannot be detected by existing technologies.","Security and privacy,Human and societal aspects of security and privacy,Software and its engineering",DAISY：敏感数据的动态分析源发现,移动应用程序被广泛使用，经常处理用户的敏感数据。许多污点分析工具已被应用于分析敏感信息流和报告应用程序中的数据泄露。这些工具需要一个源列表（访问敏感数据的地方）作为输入，研究人员通过识别允许访问敏感数据地Android API方法，在Android平台内构建了这样的列表。然而，应用程序开发人员也可以定义方法或使用第三方库的方法来访问数据。很难收集这样的源方法，因为它们是应用程序独有的，而且市场上有大量的第三方库，它们会随着时间的推移而演变。为了解决这个问题，我们提出了DAISY，这是一种动态分析诱导源发现方法，用于识别从应用程序和第三方库返回敏感信息的方法。DAISY在一个自动标记的方法及其调用上下文数据集上进行了训练，可以在看不见的应用程序中识别敏感方法。我们在真实世界的应用程序上评估了DAISY，结果表明，当报告最有信心的结果时，DAISY可以实现77.9%的总体精度。大多数已确定的来源和泄漏无法通过现有技术检测到。,安全和隐私，安全和隐私的人类和社会方面，软件及其工程,,,
T6ECD9HR,2023,https://doi.org/10.1145/3576042,TOSEM 2023,Duplicate Bug Report Detection: How Far Are We?,"Many Duplicate Bug Report Detection (DBRD) techniques have been proposed in the research literature. The industry uses some other techniques. Unfortunately, there is insufficient comparison among them, and it is unclear how far we have been. This work fills this gap by comparing the aforementioned techniques. To compare them, we first need a benchmark that can estimate how a tool would perform if applied in a realistic setting today. Thus, we first investigated potential biases that affect the fair comparison of the accuracy of DBRD techniques. Our experiments suggest that data age and issue tracking system (ITS) choice cause a significant difference. Based on these findings, we prepared a new benchmark. We then used it to evaluate DBRD techniques to estimate better how far we have been. Surprisingly, a simpler technique outperforms recently proposed sophisticated techniques on most projects in our benchmark. In addition, we compared the DBRD techniques proposed in research with those used in Mozilla and VSCode. Surprisingly, we observe that a simple technique already adopted in practice can achieve comparable results as a recently proposed research tool. Our study gives reflections on the current state of DBRD, and we share our insights to benefit future DBRD research.","Social and professional topics,Professional topics,Management of computing and information systems,Software management,Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software configuration management and version control systems",重复错误报告检测：我们有多远？,在研究文献中已经提出了许多重复错误报告检测（DBRD）技术。该行业使用了一些其他技术。不幸的是，它们之间没有足够的比较，也不清楚我们已经走了多远。这项工作通过比较上述技术填补了这一空白。为了比较它们，我们首先需要一个基准，它可以估计如果在当今现实的环境中应用一个工具会如何表现。因此，我们首先研究了影响DBRD技术准确性公平比较的潜在偏差。我们的实验表明，数据年龄和问题跟踪系统（ITS）的选择造成了显著的差异。基于这些发现，我们制定了一个新的基准。然后，我们使用它来评估DBRD技术，以更好地估计我们已经走了多远。令人惊讶的是，在我们的基准测试中，一种更简单的技术在大多数项目中都优于最近提出的复杂技术。此外，我们还将研究中提出的DBRD技术与Mozilla和VSCode中使用的技术进行了比较。令人惊讶的是，我们观察到，作为最近提出的研究工具，一种已经在实践中采用的简单技术可以获得类似的结果。我们的研究对DBRD的现状进行了反思，我们分享了我们的见解，以造福未来的DBRD研究。,社会和专业主题，专业主题，计算和信息系统管理，软件管理，软件及其工程，软件创建和管理，软件开发后问题，维护软件，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件配置管理和版本控制系统,,,
9LM4P5R8,2023,https://doi.org/10.1145/3580597,TOSEM 2023,Fuzzing Configurations of Program Options,"While many real-world programs are shipped with configurations to enable/disable functionalities, fuzzers have mostly been applied to test single configurations of these programs. In this work, we first conduct an empirical study to understand how program configurations affect fuzzing performance. We find that limiting a campaign to a single configuration can result in failing to cover a significant amount of code. We also observe that different program configurations contribute differing amounts of code coverage, challenging the idea that each one can be efficiently fuzzed individually. Motivated by these two observations, we propose ConfigFuzz , which can fuzz configurations along with normal inputs. ConfigFuzz transforms the target program to encode its program options within part of the fuzzable input, so existing fuzzers’ mutation operators can be reused to fuzz program configurations. We instantiate ConfigFuzz on six configurable, common fuzzing targets, and integrate their executions in FuzzBench. In our evaluation, ConfigFuzz outperforms two baseline fuzzers in four targets, while the results are mixed in the other targets due to program size and configuration space. We also analyze the options fuzzed by ConfigFuzz and how they affect the performance.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",程序选项的引信配置,虽然许多真实世界的程序都附带了启用/禁用功能的配置，但模糊器大多用于测试这些程序的单个配置。在这项工作中，我们首先进行了一项实证研究，以了解程序配置如何影响模糊性能。我们发现，将活动限制在单个配置可能会导致无法覆盖大量代码。我们还观察到，不同的程序配置贡献了不同数量的代码覆盖率，这对每个程序配置都可以有效地单独模糊的想法提出了挑战。基于这两个观察结果，我们提出了ConfigFuzz，它可以模糊配置和正常输入。ConfigFuzz转换目标程序，将其程序选项编码在可模糊输入的一部分中，因此可以重用现有模糊器的变异运算符来模糊程序配置。我们在六个可配置的通用模糊目标上实例化ConfigFuzz，并在FuzzBench中集成它们的执行。在我们的评估中，ConfigFuzz在四个目标上优于两个基线模糊器，而由于程序大小和配置空间的原因，其他目标的结果参差不齐。我们还分析了ConfigFuzz模糊的选项以及它们如何影响性能。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
VV3T5YIN,2023,https://doi.org/10.1145/3580601,TOSEM 2023,Fuzzing Configurations of Program Options - RCR Report,"This artifact contains the source code and instructions to reproduce the evaluation results of the article “Fuzzing Configurations of Program Options.” The source code includes the configuration grammars for six target programs, the scripts to generate configuration stubs, and the scripts to post-process fuzzing results. The README of the artifact includes the steps to prepare the experimental environment on a clean Ubuntu machine and step-by-step commands to reproduce the evaluation experiments. A VirtualBox image with ConfigFuzz properly set up is also included.","Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",程序选项的引信配置-RCR报告,该工件包含源代码和指令，用于重现文章“程序选项的模糊配置”的评估结果。源代码包括六个目标程序的配置语法、生成配置存根的脚本以及后期处理模糊结果的脚本。工件的自述包括在干净的Ubuntu机器上准备实验环境的步骤，以及重现评估实验的分步命令。还包括经过正确设置ConfigFuzz的VirtualBox映像。,安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
6P63ZWVW,2023,https://doi.org/10.1145/3603109,TOSEM 2023,An Accurate Identifier Renaming Prediction and Suggestion Approach,"Identifiers play an important role in helping developers analyze and comprehend source code. However, many identifiers exist that are inconsistent with the corresponding code conventions or semantic functions, leading to flawed identifiers. Hence, identifiers need to be renamed regularly. Even though researchers have proposed several approaches to identify identifiers that need renaming and further suggest correct identifiers for them, these approaches only focus on a single or a limited number of granularities of identifiers without universally considering all the granularities and suggest a series of sub-tokens for composing identifiers without completely generating new identifiers. In this article, we propose a novel identifier renaming prediction and suggestion approach. Specifically, given a set of training source code, we first extract all the identifiers in multiple granularities. Then, we design and extract five groups of features from identifiers to capture inherent properties of identifiers themselves and the relationships between identifiers and code conventions, as well as other related code entities, enclosing files, and change history. By parsing the change history of identifiers, we can figure out whether specific identifiers have been renamed or not. These identifier features and their renaming history are used to train a Random Forest classifier, which can be further used to predict whether a given new identifier needs to be renamed or not. Subsequently, for the identifiers that need renaming, we extract all the related code entities and their renaming change history. Based on the intuition that identifiers are co-evolved as their relevant code entities with similar patterns and renaming sequences, we could suggest and recommend a series of new identifiers for those identifiers. We conduct extensive experiments to validate our approach in both the Java projects and the Android projects. Experimental results demonstrate that our approach could identify identifiers that need renaming with an average F-measure of more than 89%, which outperforms the state-of-the-art approach by 8.30% in the Java projects and 21.38% in the Android projects. In addition, our approach achieves a Hit@10 of 48.58% and 40.97% in the Java and Android projects in suggesting correct identifiers and outperforms the state-of-the-art approach by 29.62% and 15.75%, respectively.","Software and its engineering,Software creation and management,Software post-development issues,Software notations and tools,Software libraries and repositories",一种准确的标识符重命名预测与建议方法,标识符在帮助开发人员分析和理解源代码方面发挥着重要作用。然而，存在许多与相应的代码约定或语义函数不一致的标识符，导致标识符存在缺陷。因此，需要定期重命名标识符。尽管研究人员已经提出了几种方法来识别需要重命名的标识符，并进一步为其建议正确的标识符，这些方法只关注标识符的单个或有限数量的粒度，而没有普遍考虑所有粒度，并且提出了一系列用于组成标识符的子令牌，而没有完全生成新的标识符。在本文中，我们提出了一种新的标识符重命名预测和建议方法。具体来说，给定一组训练源代码，我们首先提取多个粒度中的所有标识符。然后，我们从标识符中设计并提取五组特征，以捕获标识符本身的固有属性、标识符与代码约定之间的关系，以及其他相关的代码实体、封闭文件和更改历史。通过解析标识符的更改历史，我们可以了解特定标识符是否已被重命名。这些标识符特征及其重命名历史用于训练随机森林分类器，该分类器可以进一步用于预测给定的新标识符是否需要重命名。随后，对于需要重命名的标识符，我们提取所有相关的代码实体及其重命名更改历史。基于标识符是作为具有相似模式和重命名序列的相关代码实体共同进化的直觉，我们可以为这些标识符建议和推荐一系列新的标识符。我们在Java项目和Android项目中进行了大量的实验来验证我们的方法。实验结果表明，我们的方法可以识别需要重命名的标识符，平均F度量超过89%，在Java项目中比最先进的方法高8.30%，在Android项目中高21.38%。此外，我们的方法实现了Hit@10在Java和Android项目中，在建议正确的标识符方面分别占48.58%和40.97%，并分别比最先进的方法高29.62%和15.75%。,软件及其工程，软件创建和管理，软件开发后问题，软件符号和工具，软件库和存储库,,,
RKE7RCB8,2023,https://doi.org/10.1145/3576043,TOSEM 2023,Influential Global and Local Contexts Guided Trace Representation for Fault Localization,"Trace data is critical for fault localization (FL) to analyze suspicious statements potentially responsible for a failure. However, existing trace representation meets its bottleneck mainly in two aspects: (1) the trace information of a statement is restricted to a local context (i.e., a test case) without the consideration of a global context (i.e., all test cases of a test suite); (2) it just uses the ‘occurrence’ for representation without strong FL semantics. ","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",用于故障定位的有影响力的全局和局部上下文引导的轨迹表示,跟踪数据对于故障定位（FL）分析可能导致故障的可疑语句至关重要。然而，现有的跟踪表示主要在两个方面遇到了瓶颈：（1）语句的跟踪信息仅限于局部上下文（即测试用例），而不考虑全局上下文（即一个测试套件的所有测试用例）；（2） 它只是使用“occurrence”来表示，而没有强FL语义。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
UKQDRWIP,2023,https://doi.org/10.1145/3591869,TOSEM 2023,TestSGD: Interpretable Testing of Neural Networks against Subtle Group Discrimination,"Discrimination has been shown in many machine learning applications, which calls for sufficient fairness testing before their deployment in ethic-relevant domains. One widely concerning type of discrimination, testing against group discrimination, mostly hidden, is much less studied, compared with identifying individual discrimination. In this work, we propose TestSGD, an interpretable testing approach that systematically identifies and measures hidden (which we call “subtle”) group discrimination of a neural network characterized by conditions over combinations of the sensitive attributes. Specifically, given a neural network, TestSGD first automatically generates an interpretable rule set that categorizes the input space into two groups. Alongside, TestSGD also provides an estimated group discrimination score based on sampling the input space to measure the degree of the identified subtle group discrimination, which is guaranteed to be accurate up to an error bound. We evaluate TestSGD on multiple neural network models trained on popular datasets including both structured data and text data. The experiment results show that TestSGD is effective and efficient in identifying and measuring such subtle group discrimination that has never been revealed before. Furthermore, we show that the testing results of TestSGD can be used to mitigate such discrimination through retraining with negligible accuracy drop.","Software and its engineering,Software organization and properties,Extra-functional properties",TestSGD：针对微妙群体歧视的神经网络可解释测试,歧视已经在许多机器学习应用程序中表现出来，这些应用程序在部署到道德相关领域之前需要进行充分的公平性测试。一种广泛关注的歧视类型，即针对群体歧视的测试，与识别个人歧视相比，研究得要少得多。在这项工作中，我们提出了TestSGD，这是一种可解释的测试方法，它系统地识别和测量以敏感属性组合的条件为特征的神经网络的隐藏（我们称之为“微妙”）组判别。具体来说，给定一个神经网络，TestSGD首先自动生成一个可解释的规则集，将输入空间分为两组。除此之外，TestSGD还基于对输入空间的采样来提供估计的群体歧视分数，以测量所识别的微妙群体歧视的程度，这保证了在误差范围内是准确的。我们在流行数据集（包括结构化数据和文本数据）上训练的多个神经网络模型上评估TestSGD。实验结果表明，TestSGD在识别和测量这种前所未有的微妙群体歧视方面是有效的。此外，我们还表明，TestSGD的测试结果可以通过重新训练来减轻这种歧视，而精度下降可以忽略不计。,软件及其工程，软件组织和属性，额外功能属性,,,
HAB6J8JW,2023,https://doi.org/10.1145/3563212,TOSEM 2023,Precise Quantitative Analysis of Binarized Neural Networks: A BDD-based Approach,"As a new programming paradigm, neural-network-based machine learning has expanded its application to many real-world problems. Due to the black-box nature of neural networks, verifying and explaining their behavior are becoming increasingly important, especially when they are deployed in safety-critical applications. Existing verification work mostly focuses on qualitative verification, which asks whether there exists an input (in a specified region) for a neural network such that a property (e.g., local robustness) is violated. However, in many practical applications, such an (adversarial) input almost surely exists, which makes a qualitative answer less meaningful. In this work, we study a more interesting yet more challenging problem, i.e., quantitative verification of neural networks, which asks how often a property is satisfied or violated. We target binarized neural networks (BNNs), the 1-bit quantization of general neural networks. BNNs have attracted increasing attention in deep learning recently, as they can drastically reduce memory storage and execution time with bit-wise operations, which is crucial in recourse-constrained scenarios, e.g., embedded devices for Internet of Things. Toward quantitative verification of BNNs, we propose a novel algorithmic approach for encoding BNNs as Binary Decision Diagrams (BDDs), a widely studied model in formal verification and knowledge representation. By exploiting the internal structure of the BNNs, our encoding translates the input-output relation of blocks in BNNs to cardinality constraints, which are then encoded by BDDs. Based on the new BDD encoding, we develop a quantitative verification framework for BNNs where precise and comprehensive analysis of BNNs can be performed. To improve the scalability of BDD encoding, we also investigate parallelization strategies at various levels. We demonstrate applications of our framework by providing quantitative robustness verification and interpretability for BNNs. An extensive experimental evaluation confirms the effectiveness and efficiency of our approach.","Computing methodologies,Machine learning,Machine learning approaches,Neural networks,Security and privacy,Software and application security,Software security engineering,Software and its engineering,Software organization and properties,Software functional properties,Formal methods,Software verification",二值化神经网络的精确定量分析：一种基于BDD的方法,作为一种新的编程范式，基于神经网络的机器学习已经将其应用扩展到许多现实问题中。由于神经网络的黑匣子性质，验证和解释其行为变得越来越重要，尤其是当它们被部署在安全关键应用中时。现有的验证工作主要集中在定性验证上，它询问神经网络是否存在输入（在指定区域），从而违反了特性（例如，局部鲁棒性）。然而，在许多实际应用中，这种（对抗性）输入几乎肯定存在，这使得定性答案的意义不大。在这项工作中，我们研究了一个更有趣但更具挑战性的问题，即神经网络的定量验证，该问题询问满足或违反属性的频率。我们的目标是二进制神经网络（BNN），即一般神经网络的1位量化。BNN最近在深度学习中引起了越来越多的关注，因为它们可以通过逐位操作大幅减少内存存储和执行时间，这在资源受限的场景中至关重要，例如用于物联网的嵌入式设备。为了定量验证BNN，我们提出了一种新的算法方法，将BNN编码为二进制决策图（BDD），这是一种在形式验证和知识表示中广泛研究的模型。通过利用BNN的内部结构，我们的编码将BNN中块的输入输出关系转换为基数约束，然后由BDD进行编码。基于新的BDD编码，我们开发了一个BNN的定量验证框架，可以对BNN进行精确和全面的分析。为了提高BDD编码的可扩展性，我们还研究了不同级别的并行化策略。我们通过提供BNN的定量稳健性验证和可解释性来展示我们的框架的应用。广泛的实验评估证实了我们方法的有效性和效率。,计算方法，机器学习，机器学习方法，神经网络，安全和隐私，软件和应用程序安全，软件安全工程，软件及其工程，软件组织和属性，软件功能属性，形式方法，软件验证,,,
79Z7UIKV,2023,https://doi.org/10.1145/3571847,TOSEM 2023,Securing the Ethereum from Smart Ponzi Schemes: Identification Using Static Features,"Malware detection approaches have been extensively studied for traditional software systems. However, the development of blockchain technology has promoted the birth of a new type of software system–decentralized applications. Composed of smart contracts, a type of application that implements the Ponzi scheme logic (called smart Ponzi schemes) has caused irreversible loss and hindered the development of blockchain technology. These smart contracts generally had a short life but involved a large amount of money. Whereas identification of these Ponzi schemes before causing financial loss has been significantly important, existing methods suffer from three main deficiencies, i.e., the insufficient dataset, the reliance on the transaction records, and the low accuracy. In this study, we first build a larger dataset. Then, a large number of features from multiple views, including bytecode, semantic, and developers, are extracted. These features are independent of the transaction records. Furthermore, we leveraged machine learning methods to build our identification model, i.e., Multi-view Cascade Ensemble model (MulCas). The experiment results show that MulCas can achieve higher performance and robustness in the scope of our dataset. Most importantly, the proposed method can identify smart Ponzi scheme at the creation time.","Security and privacy,Intrusion/anomaly detection and malware mitigation,Malware and its mitigation,Software and application security",从智能庞氏骗局中保护以太坊：使用静态特征的识别,针对传统软件系统，恶意软件检测方法已被广泛研究。然而，区块链技术的发展推动了一种新型软件系统——去中心化应用的诞生。由智能合约组成的一种实现庞氏骗局逻辑的应用程序（称为智能庞氏骗局）造成了不可逆转的损失，阻碍了区块链技术的发展。这些智能合约通常寿命很短，但涉及大量资金。尽管在造成财务损失之前识别这些庞氏骗局非常重要，但现有方法存在三个主要缺陷，即数据集不足、依赖交易记录和准确性低。在这项研究中，我们首先构建了一个更大的数据集。然后，从多个视图中提取大量特征，包括字节码、语义和开发人员。这些功能独立于交易记录。此外，我们利用机器学习方法建立了我们的识别模型，即多视图级联集成模型（MulCas）。实验结果表明，在我们的数据集范围内，MulCas可以实现更高的性能和鲁棒性。最重要的是，所提出的方法可以在创建时识别智能庞氏骗局。,安全和隐私，入侵/异常检测和恶意软件缓解，恶意软件及其缓解，软件和应用程序安全,,,
BKJKTU8E,2023,https://doi.org/10.1145/3582569,TOSEM 2023,Client-Specific Upgrade Compatibility Checking via Knowledge-Guided Discovery,"Modern software systems are complex, and they heavily rely on external libraries developed by different teams and organizations. Such systems suffer from higher instability due to incompatibility issues caused by library upgrades. In this article, we address the problem by investigating the impact of a library upgrade on the behaviors of its clients. We developed CompCheck, an automated upgrade compatibility checking framework that generates incompatibility-revealing tests based on previous examples. CompCheck first establishes an offline knowledge base of incompatibility issues by mining from open source projects and their upgrades. It then discovers incompatibilities for a specific client project, by searching for similar library usages in the knowledge base and generating tests to reveal the problems. We evaluated CompCheck on 202 call sites of 37 open source projects and the results show that CompCheck successfully revealed incompatibility issues on 76 call sites, 72.7% and 94.9% more than two existing techniques, confirming CompCheck’s applicability and effectiveness.","Software and its engineering,Software creation and management,Software post-development issues,Maintaining software,Software evolution,Software notations and tools,Software libraries and repositories",通过知识引导的发现进行特定于客户端的升级兼容性检查,现代软件系统是复杂的，它们严重依赖于不同团队和组织开发的外部库。由于库升级引起的不兼容性问题，这样的系统遭受更高的不稳定性。在本文中，我们通过调查库升级对其客户端行为的影响来解决这个问题。我们开发了CompCheck，这是一个自动升级兼容性检查框架，它基于以前的示例生成不兼容揭示测试。CompCheck首先通过从开源项目及其升级中挖掘，建立了一个不兼容问题的离线知识库。然后，它通过在知识库中搜索类似的库用法并生成测试来揭示问题，从而发现特定客户端项目的不兼容性。我们对37个开源项目的202个调用站点进行了CompCheck评估，结果显示，CompCheck在76个调用站点上成功地发现了不兼容问题，比两种现有技术分别高出72.7%和94.9%，证实了CompCheck的适用性和有效性。,软件及其工程，软件创建和管理，软件开发后问题，维护软件，软件演化，软件符号和工具，软件库和存储库,,,
CAJPIICK,2023,https://doi.org/10.1145/3544792,TOSEM 2023,Efficient and Effective Feature Space Exploration for Testing Deep Learning Systems,"Assessing the quality of Deep Learning (DL) systems is crucial, as they are increasingly adopted in safety-critical domains. Researchers have proposed several input generation techniques for DL systems. While such techniques can expose failures, they do not explain which features of the test inputs influenced the system’s (mis-) behaviour. DeepHyperion was the first test generator to overcome this limitation by exploring the DL systems’ feature space at large. In this article, we propose DeepHyperion-CS, a test generator for DL systems that enhances DeepHyperion by promoting the inputs that contributed more to feature space exploration during the previous search iterations. We performed an empirical study involving two different test subjects (i.e., a digit classifier and a lane-keeping system for self-driving cars). Our results proved that the contribution-based guidance implemented within DeepHyperion-CS outperforms state-of-the-art tools and significantly improves the efficiency and the effectiveness of DeepHyperion. DeepHyperion-CS exposed significantly more misbehaviours for five out of six feature combinations and was up to 65% more efficient than DeepHyperion in finding misbehaviour-inducing inputs and exploring the feature space. DeepHyperion-CS was useful for expanding the datasets used to train the DL systems, populating up to 200% more feature map cells than the original training set.","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging",用于测试深度学习系统的高效特征空间探索,评估深度学习（DL）系统的质量至关重要，因为它们越来越多地被用于安全关键领域。研究人员提出了几种DL系统的输入生成技术。虽然这些技术可以暴露故障，但它们并不能解释测试输入的哪些特征影响了系统的（错误）行为。DeepHyperion是第一个通过探索DL系统的整体特征空间来克服这一限制的测试生成器。在本文中，我们提出了DeepHyperion CS，这是一种DL系统的测试生成器，它通过促进在之前的搜索迭代中对特征空间探索做出更多贡献的输入来增强DeepHyperon。我们进行了一项实证研究，涉及两个不同的测试对象（即自动驾驶汽车的数字分类器和车道保持系统）。我们的结果证明，在DeepHyperion CS中实现的基于贡献的制导优于最先进的工具，并显著提高了DeepHyperon的效率和有效性。DeepHyperion CS在六个特征组合中有五个暴露出明显更多的不当行为，在寻找引起不当行为的输入和探索特征空间方面的效率比DeepHyperon高出65%。DeepHyperion CS有助于扩展用于训练DL系统的数据集，填充的特征图单元比原始训练集多200%。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试,,,
V8CK5GWQ,2023,https://doi.org/10.1145/3544790,TOSEM 2023,Toward More Efficient Statistical Debugging with Abstraction Refinement,"Debugging is known to be a notoriously painstaking and time-consuming task. As one major family of automated debugging, statistical debugging approaches have been well investigated over the past decade, which collect failing and passing executions and apply statistical techniques to identify discriminative elements as potential bug causes. Most of the existing approaches instrument the entire program to produce execution profiles for debugging, thus incurring hefty instrumentation and analysis cost. However, as in fact a major part of the program code is error-free, full-scale program instrumentation is wasteful and unnecessary. ","Software and its engineering,Software creation and management,Software verification and validation,Software defect analysis,Software testing and debugging,Software notations and tools,Software maintenance tools,Software organization and properties,Software functional properties,Formal methods,Dynamic analysis",使用抽象精化实现更高效的统计调试,众所周知，调试是一项出了名的费时费力的任务。作为自动化调试的一个主要家族，统计调试方法在过去十年中得到了很好的研究，它收集失败和通过的执行，并应用统计技术来识别潜在的错误原因。大多数现有方法对整个程序进行检测，以生成用于调试的执行配置文件，从而产生高昂的检测和分析成本。然而，由于事实上程序代码的主要部分是无错误的，因此全尺寸程序插入是浪费和不必要的。,软件及其工程，软件创建和管理，软件验证和确认，软件缺陷分析，软件测试和调试，软件符号和工具，软件维护工具，软件组织和属性，软件功能属性，形式化方法，动态分析,,,
